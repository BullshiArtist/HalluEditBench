{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(763, 23)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "domain_topic_name = 'places_country'\n",
    "df_hallu = pd.read_csv(f\"../data/questions/hallucination/meta_llama_3.1_8b_instruct/{domain_topic_name}.csv\")\n",
    "df_hallu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 'Constitution Act, 1867', 'B': 'Royal Proclamation', 'C': 'Charter of Rights and Freedoms', 'D': 'British North America Act'}\n",
      "{'A': 'Bernardo Arévalo de León', 'B': 'Alejandro Giammattei', 'C': 'Jimmy Morales', 'D': 'Otto Pérez Molina'}\n"
     ]
    }
   ],
   "source": [
    "answer = df_hallu['object'].tolist()\n",
    "for i, e in enumerate(df_hallu['multiple_choice_question'].tolist()[:2]):\n",
    "    idx_option = e.find(\"'options': \")\n",
    "    idx_truth = e.find(\", 'ground_truth': \")\n",
    "    # question, ground_truth = e[:idx].replace(\"{\", '').replace(\"}\", ''), e[idx+len(\", 'ground_truth': \"):].replace(\"}\", '').replace(\"'\", '') + '. ' + answer[i]\n",
    "    str_options = e[idx_option+len(\"'options': \"):idx_truth]\n",
    "    print(str_options)\n",
    "    # shuffle the options\n",
    "    str_options = str_options.split(',')\n",
    "\n",
    "# answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-05 19:46:39,193 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from process_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: {'instance of': 'literary work'}\n",
      "Error querying for Q7725634: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?subjectLabel ?relation ?objectLabel WHERE {\\n      ?subject wdt:P31 wd:Q7725634.\\n      ?subject ?relation ?object.\\n      ?subject wikibase:identifiers ?subject_identifierCount.\\n      ?object wikibase:identifiers ?object_identifierCount.\\n      FILTER (?subject_identifierCount >= 8 && ?object_identifierCount >= 5) .  \\n      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n    }\\n    LIMIT 50000\\n    \\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:123)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n'\n",
      "\n",
      "Topic: {'instance of': 'film festival'}\n",
      "Topic Q220505 has 487 relations.\n",
      "\n",
      "Topic: {'instance of': 'human', 'country of citizenship': \"People's Republic of China\", 'occupation': 'actor'}\n",
      "Error querying for Q5: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?subjectLabel ?relation ?objectLabel WHERE {\\n      ?subject wdt:P31 wd:Q5.\\n      ?subject ?relation ?object.\\n      ?subject wikibase:identifiers ?subject_identifierCount.\\n      ?object wikibase:identifiers ?object_identifierCount.\\n      FILTER (?subject_identifierCount >= 8 && ?object_identifierCount >= 5) .  \\n      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n    }\\n    LIMIT 50000\\n    \\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:123)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n'\n",
      "\n",
      "Topic: {'instance of': 'tourist attraction'}\n",
      "Topic Q570116 has 7224 relations.\n",
      "\n",
      "Topic: {'instance of': 'revolution'}\n",
      "Topic Q10931 has 184 relations.\n",
      "\n",
      "Topic: {'instance of': 'medical treatment'}\n",
      "Topic Q179661 has 154 relations.\n",
      "\n",
      "Topic: {'instance of': 'painting'}\n",
      "Error querying for Q3305213: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?subjectLabel ?relation ?objectLabel WHERE {\\n      ?subject wdt:P31 wd:Q3305213.\\n      ?subject ?relation ?object.\\n      ?subject wikibase:identifiers ?subject_identifierCount.\\n      ?object wikibase:identifiers ?object_identifierCount.\\n      FILTER (?subject_identifierCount >= 8 && ?object_identifierCount >= 5) .  \\n      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n    }\\n    LIMIT 50000\\n    \\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:123)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n'\n",
      "\n",
      "Topic: {'instance of': 'disease'}\n",
      "Topic Q12136 has 4819 relations.\n",
      "\n",
      "Topic: {'instance of': 'song'}\n",
      "Topic Q7366 has 5153 relations.\n",
      "\n",
      "Topic: {'instance of': 'country'}\n",
      "Topic Q6256 has 30259 relations.\n",
      "\n",
      "Topic: {'instance of': 'recurring sporting event'}\n",
      "Topic Q18608583 has 521 relations.\n",
      "\n",
      "Topic: {'instance of': 'human', 'country of citizenship': \"People's Republic of China\", 'occupation': 'writer'}\n",
      "Error querying for Q5: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?subjectLabel ?relation ?objectLabel WHERE {\\n      ?subject wdt:P31 wd:Q5.\\n      ?subject ?relation ?object.\\n      ?subject wikibase:identifiers ?subject_identifierCount.\\n      ?object wikibase:identifiers ?object_identifierCount.\\n      FILTER (?subject_identifierCount >= 8 && ?object_identifierCount >= 5) .  \\n      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n    }\\n    LIMIT 50000\\n    \\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:123)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n'\n",
      "\n",
      "Topic: {'instance of': 'human', 'country of citizenship': \"People's Republic of China\", 'occupation': 'politician'}\n",
      "Error querying for Q5: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?subjectLabel ?relation ?objectLabel WHERE {\\n      ?subject wdt:P31 wd:Q5.\\n      ?subject ?relation ?object.\\n      ?subject wikibase:identifiers ?subject_identifierCount.\\n      ?object wikibase:identifiers ?object_identifierCount.\\n      FILTER (?subject_identifierCount >= 8 && ?object_identifierCount >= 5) .  \\n      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n    }\\n    LIMIT 50000\\n    \\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:123)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n'\n",
      "\n",
      "Topic: {'instance of': 'city'}\n",
      "Topic Q515 has 44191 relations.\n",
      "\n",
      "Topic: {'instance of': 'sculpture'}\n",
      "Topic Q860861 has 1086 relations.\n",
      "\n",
      "Topic: {'instance of': 'medication'}\n",
      "Topic Q12140 has 927 relations.\n"
     ]
    }
   ],
   "source": [
    "topic_folder = \"../data/topic\"\n",
    "for filename in os.listdir(topic_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(topic_folder, filename), 'r', encoding='utf-8') as topics_file:\n",
    "            topics = topics_file.readlines()\n",
    "        topic = json.loads(topics[0])\n",
    "        relation_object_pairs = convert_topic_to_symbol(topic)\n",
    "        print(f\"\\nTopic: {topic}\")\n",
    "        get_topic_size(relation_object_pairs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: health_medication.json, first_pair (used in question generation): ('instance of', 'medication')\n",
      "filename: places_landmark.json, first_pair (used in question generation): ('instance of', 'tourist attraction')\n",
      "filename: event_history.json, first_pair (used in question generation): ('instance of', 'revolution')\n",
      "filename: human_politician.json, first_pair (used in question generation): ('instance of', 'human')\n",
      "filename: entertainment_music_genre.json, first_pair (used in question generation): ('instance of', 'music genre')\n",
      "filename: entertainment_song.json, first_pair (used in question generation): ('instance of', 'song')\n",
      "filename: art_sculpture.json, first_pair (used in question generation): ('instance of', 'sculpture')\n",
      "filename: event_film.json, first_pair (used in question generation): ('instance of', 'film festival')\n",
      "filename: human_actor.json, first_pair (used in question generation): ('instance of', 'human')\n",
      "filename: art_painting.json, first_pair (used in question generation): ('instance of', 'painting')\n",
      "filename: art_literary.json, first_pair (used in question generation): ('instance of', 'literary work')\n",
      "filename: places_country.json, first_pair (used in question generation): ('instance of', 'country')\n",
      "filename: human_writer.json, first_pair (used in question generation): ('instance of', 'human')\n",
      "filename: health_disease.json, first_pair (used in question generation): ('instance of', 'disease')\n",
      "filename: health_treatment.json, first_pair (used in question generation): ('instance of', 'medical treatment')\n",
      "filename: entertainment_film_genre.json, first_pair (used in question generation): ('instance of', 'film genre')\n",
      "filename: entertainment_anime.json, first_pair (used in question generation): ('instance of', 'anime')\n",
      "filename: places_city.json, first_pair (used in question generation): ('instance of', 'city')\n",
      "filename: event_sport.json, first_pair (used in question generation): ('instance of', 'recurring sporting event')\n",
      "filename: university.json, first_pair (used in question generation): ('instance of', 'university')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "ls = []\n",
    "topic_folder = \"../data/topic\"\n",
    "for filename in os.listdir(topic_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # print(filename)\n",
    "        with open(os.path.join(topic_folder, filename), 'r', encoding='utf-8') as topics_file:\n",
    "            topics = topics_file.readlines()\n",
    "        # print(topics)\n",
    "        ls.append(topics)\n",
    "        # get_topic_size(topics)\n",
    "\n",
    "        topic = json.loads(topics[0])\n",
    "        first_pair = next(iter(topic.items()))\n",
    "        print(f\"filename: {filename}, first_pair (used in question generation): {first_pair}\")\n",
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic {'instance of': 'tourist attraction'} size: 5000\n",
      "Topic {'instance of': 'music genre'} size: 3011\n",
      "Topic {'instance of': 'song'} size: 5000\n",
      "Topic {'instance of': 'sculpture'} size: 1090\n",
      "Topic {'instance of': 'film festival'} size: 487\n",
      "Topic {'instance of': 'literary work'} size: 5000\n",
      "Topic {'instance of': 'country'} size: 5000\n",
      "Topic {'instance of': 'disease'} size: 4820\n"
     ]
    },
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'SPARQL-QUERY: queryStr=SELECT ?subjectLabel ?relation ?objectLabel WHERE {\\n?subject wdt:P31 wd:Q3305213 .\\n                ?subject  ?relation  ?object.\\n                ?subject wikibase:identifiers ?subject_identifierCount.\\n                ?object wikibase:identifiers ?object_identifierCount.\\n                 \\n                FILTER (?subject_identifierCount >= 8 && ?object_identifierCount >= 5) .  \\n                SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n            }\\n            LIMIT 5000\\n            \\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:123)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[0m, in \u001b[0;36mSPARQLWrapper._query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnFormat\n",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    560\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    493\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 500: Internal Server Error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEndPointInternalError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topics \u001b[38;5;129;01min\u001b[39;00m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance of\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtourist attraction\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      2\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance of\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusic genre\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance of\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msong\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance of\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpainting\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance of\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwall painting\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance of\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevolution\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance of\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwar\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m]]:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mget_topic_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/baixiang/workspace/edit/hallucination/code/process_data.py:152\u001b[0m, in \u001b[0;36mget_topic_size\u001b[0;34m(topics)\u001b[0m\n\u001b[1;32m    150\u001b[0m sparql\u001b[38;5;241m.\u001b[39msetQuery(query)\n\u001b[1;32m    151\u001b[0m sparql\u001b[38;5;241m.\u001b[39msetReturnFormat(JSON)\n\u001b[0;32m--> 152\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msparql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert()\n\u001b[1;32m    153\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbindings\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[0m, in \u001b[0;36mSPARQLWrapper.query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryResult\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    943\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    Execute the query.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;124;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/SPARQLWrapper/Wrapper.py:938\u001b[0m, in \u001b[0;36mSPARQLWrapper._query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URITooLong(e\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mEndPointInternalError\u001b[0m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'SPARQL-QUERY: queryStr=SELECT ?subjectLabel ?relation ?objectLabel WHERE {\\n?subject wdt:P31 wd:Q3305213 .\\n                ?subject  ?relation  ?object.\\n                ?subject wikibase:identifiers ?subject_identifierCount.\\n                ?object wikibase:identifiers ?object_identifierCount.\\n                 \\n                FILTER (?subject_identifierCount >= 8 && ?object_identifierCount >= 5) .  \\n                SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n            }\\n            LIMIT 5000\\n            \\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:123)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n'"
     ]
    }
   ],
   "source": [
    "for topics in [['{\"instance of\": \"tourist attraction\"}\\n'],\n",
    "    ['{\"instance of\": \"music genre\"}'],\n",
    "    ['{\"instance of\": \"song\"}'],\n",
    "    ['{\"instance of\": \"sculpture\"}\\n'],\n",
    "    ['{\"instance of\": \"film festival\"}'],\n",
    "    ['{\"instance of\": \"literary work\"}'],\n",
    "    ['{\"instance of\": \"country\"}\\n'],\n",
    "    ['{\"instance of\": \"disease\"}'],\n",
    "    ['{\"instance of\": \"painting\"}\\n', '{\"instance of\": \"wall painting\"}'],\n",
    "    ['{\"instance of\": \"revolution\"}\\n', '{\"instance of\": \"war\"}']]:\n",
    "    get_topic_size(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual check for the data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(763, 7) (759, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "domain_topic_name = \"places_country\"\n",
    "model_id_format = \"meta_llama_3.1_8b_instruct\"\n",
    "folder_hallu = f\"../data/questions/hallucination_all/{model_id_format}\"\n",
    "df = pd.read_csv(f\"{folder_hallu}/{domain_topic_name}.csv\")\n",
    "df_new = df[~df['subject'].isin(['Republic of China'])]\n",
    "df_new.to_csv(f\"{folder_hallu}/{domain_topic_name}.csv\", index=False)\n",
    "print(df.shape, df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(763, 7) (759, 7)\n"
     ]
    }
   ],
   "source": [
    "# data/questions/hallucination_all/meta_llama_3.1_8b_instruct/business_brand.csv\n",
    "# business_brand,Lump of Sugar,named after,sugar,What was Lump of Sugar named after?,A horse,0.0  # removed because multi-hop answers are N/A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_eval = 'cuda:7'\n",
    "# model_id_eval = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# # model_id_eval = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# tok_eval = transformers.AutoTokenizer.from_pretrained(model_id_eval)\n",
    "# terminators = [tok_eval.eos_token_id, tok_eval.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "# model_eval = transformers.AutoModelForCausalLM.from_pretrained(model_id_eval, torch_dtype='auto').to(device_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all the files\n",
    "import os\n",
    "directory = '../data/triplet'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        old_path = os.path.join(directory, filename)\n",
    "        new_path = os.path.join(directory, filename.replace('new_', ''))\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed: {old_path} to {new_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multiple_choice': {'prompt': ['When is the foundational text of Canada? A. Encyclopædia Britannica 11th edition  B. Constitution Act, 1867  C. Royal Proclamation  D. hispanism',\n",
       "   'Who is the head of state of Guatemala? A. Prime Minister of Kazakhstan  B. flag of Kazakhstan  C. Bernardo Arévalo de León  D. Alejandro Giammattei',\n",
       "   'Who is the head of government of Guatemala? A. Qurayn Abu al Bawl  B. right  C. Gustavo López Morales  D. Bernardo Arévalo de León',\n",
       "   'What was Japan named after? A. Ryūkyū Kingdom  B. sunrise  C. geography of Indonesia  D. Dolohmwar',\n",
       "   'What is the located in/on physical feature of Japan? A. Mount Fuji  B. Petteri Orpo  C. Japanese archipelago  D. Hubert Minnis'],\n",
       "  'ground_truth': ['B', 'C', 'D', 'B', 'C']}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"../data/questions/hallucination/{model_id_format}/{domain_topic_name}.csv\")\n",
    "n = 5\n",
    "df['multiple_choice_full'] = df['question'] + ' ' + df['multiple_choices']\n",
    "{'multiple_choice': {'prompt': df['multiple_choice_full'].tolist()[:n], 'ground_truth': df['multiple_choice_labels'].tolist()[:n]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check fact triplets (not helpful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id_eval = 'meta-llama/Meta-Llama-3.1-70B-Instruct'\n",
    "tok_eval = AutoTokenizer.from_pretrained(model_id_eval)\n",
    "model_eval = AutoModelForCausalLM.from_pretrained(model_id_eval, torch_dtype='auto', device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(model, tok, messages, max_new_tokens=1):\n",
    "    terminators = [tok.eos_token_id, tok.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    msg_tokenized = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(model.device)\n",
    "    output_ids = model.generate(msg_tokenized, max_new_tokens=max_new_tokens, eos_token_id=terminators, do_sample=False, pad_token_id=tok.eos_token_id)\n",
    "    return tok.decode(output_ids[0][msg_tokenized.shape[-1]:], skip_special_tokens=True).replace('\\n', ' ').strip().rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: subject: 2018 FIFA World Cup, relation: has part(s), object: FIFA World Cup  response: YES\n"
     ]
    }
   ],
   "source": [
    "system_msg_check = \"\"\"Check whether the given triplet satisfies the following criteria. If all criteria are satisfied, answer “YES”, otherwise answer “NO”:\n",
    "1. the triplet is factually correct.\n",
    "2. the triplet is understandable to humans.\n",
    "Only answer “YES” or “NO”.\n",
    "\"\"\"\n",
    "\n",
    "triplet = \"\"\"subject: 2018 FIFA World Cup, relation: has part(s), object: FIFA World Cup\"\"\"\n",
    "messages_qa = [{\"role\": \"system\", \"content\": system_msg_check}, {\"role\": \"user\", \"content\": triplet}]\n",
    "current_output = get_response(model_eval, tok_eval, messages_qa, max_new_tokens=64)\n",
    "print(f\"Input: {triplet}  response: {current_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: subject: The Rebirth of Buddha, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: The Rebirth of Buddha, relation: country of origin, object: Japan  response: NO\n",
      "Input: subject: The Rebirth of Buddha, relation: production company, object: Group TAC  response: YES\n",
      "Input: subject: The Rebirth of Buddha, relation: creator, object: IRH Press  response: NO\n",
      "Input: subject: The Rebirth of Buddha, relation: director, object: Takaaki Ishiyama  response: YES\n",
      "Input: subject: The Rebirth of Buddha, relation: screenwriter, object: Hiroshi  response: NO\n",
      "Input: subject: The Rebirth of Buddha, relation: executive producer, object: Ryūhō Ōkawa  response: YES\n",
      "Input: subject: The Rebirth of Buddha, relation: art director, object: Masaru Sato  response: YES\n",
      "Input: subject: The Rebirth of Buddha, relation: distributed by, object: Toei Company  response: YES\n",
      "Input: subject: The Rebirth of Buddha, relation: character designer, object: Masami Suda  response: YES\n",
      "Input: subject: Ehrgeiz, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Ehrgeiz, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Ehrgeiz, relation: genre, object: mecha  response: NO\n",
      "Input: subject: YōYō no Neko Tsumami, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: YōYō no Neko Tsumami, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Kiitaro's Yokai Picture Diary, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Kiitaro's Yokai Picture Diary, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Tabimachi Late Show, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Tabimachi Late Show, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Tabimachi Late Show, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Tabimachi Late Show, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Holo no Graffiti, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Holo no Graffiti, relation: distributed by, object: Google  response: NO\n",
      "Input: subject: Holo no Graffiti, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Holo no Graffiti, relation: publisher, object: Hololive Production  response: YES\n",
      "Input: subject: Holo no Graffiti, relation: original broadcaster, object: Bilibili  response: YES\n",
      "Input: subject: Holo no Graffiti, relation: genre, object: parody  response: YES\n",
      "Input: subject: Holo no Graffiti, relation: main subject, object: VTuber  response: YES\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: country of origin, object: Japan  response: NO\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: language of work or name, object: Japanese  response: NO\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: art director, object: Tatsuya Kushida  response: YES\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: distributed by, object: Toei Company  response: YES\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: executive producer, object: Ryūhō Ōkawa  response: YES\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: production company, object: SONY PCL  response: NO\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: director, object: Tetsuo Imazawa  response: NO\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: creator, object: IRH Press  response: YES\n",
      "Input: subject: THE LIFE OF GREAT HERMES, relation: genre, object: historical film  response: NO\n",
      "Input: subject: This Boy Suffers from Crystallization, relation: language of work or name, object: Japanese  response: NO\n",
      "Input: subject: This Boy Suffers from Crystallization, relation: country of origin, object: Japan  response: NO\n",
      "Input: subject: This Boy Suffers from Crystallization, relation: distribution format, object: video on demand  response: NO\n",
      "Input: subject: This Boy Suffers from Crystallization, relation: director, object: Sōbi Yamamoto  response: NO\n",
      "Input: subject: This Boy Suffers from Crystallization, relation: distributed by, object: Crunchyroll  response: NO\n",
      "Input: subject: Hey, Your Cat Ears Are Showing!, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 3, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 3, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 3, relation: part of the series, object: Digimon Adventure tri.  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 3, relation: director, object: Keitarō Motonaga  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 3, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 3, relation: follows, object: Digimon Adventure Tri. 2  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 3, relation: followed by, object: Digimon Adventure Tri. 4  response: YES\n",
      "Input: subject: Five Numbers!, relation: original language of film or TV show, object: Japanese  response: NO\n",
      "Input: subject: Horror News, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Horror News, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Q.E.D., relation: country of origin, object: Japan  response: NO\n",
      "Input: subject: Q.E.D., relation: intended public, object: shōnen  response: NO\n",
      "Input: subject: Q.E.D., relation: language of work or name, object: Japanese  response: NO\n",
      "Input: subject: Q.E.D., relation: genre, object: detective fiction  response: YES\n",
      "Input: subject: Q.E.D., relation: author, object: Motohiro Katou  response: NO\n",
      "Input: subject: Keifuku-san, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Keifuku-san, relation: original broadcaster, object: Tokyo Metropolitan Television  response: NO\n",
      "Input: subject: Keifuku-san, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: W Wish, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: W Wish, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: W Wish, relation: distribution format, object: DVD  response: YES\n",
      "Input: subject: W Wish, relation: game mode, object: single-player video game  response: NO\n",
      "Input: subject: Idol Land PriPara, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Idol Land PriPara, relation: part of the series, object: Pretty Rhythm  response: YES\n",
      "Input: subject: Idol Land PriPara, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Kiratto Pri Chan, relation: production company, object: Tatsunoko Production  response: YES\n",
      "Input: subject: Kiratto Pri Chan, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Kiratto Pri Chan, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Kiratto Pri Chan, relation: followed by, object: Pretty All Friends Selection  response: YES\n",
      "Input: subject: Kiratto Pri Chan, relation: developer, object: Syn Sophia  response: YES\n",
      "Input: subject: Kiratto Pri Chan, relation: publisher, object: T-ARTS  response: YES\n",
      "Input: subject: Kiratto Pri Chan, relation: director, object: Hiroshi Ikehata  response: YES\n",
      "Input: subject: Don't Lose!! Evil Corps!, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Don't Lose!! Evil Corps!, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Don't Lose!! Evil Corps!, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Aki Sora, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Aki Sora, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Aki Sora, relation: author, object: Masahiro Itosugi  response: YES\n",
      "Input: subject: Kowarekake no Orgel, relation: director, object: Keiichiro Kawaguchi  response: YES\n",
      "Input: subject: Kowarekake no Orgel, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Rescue Me!, relation: intended public, object: shōnen  response: YES\n",
      "Input: subject: Rescue Me!, relation: author, object: Makita Keiharu  response: NO\n",
      "Input: subject: Macross Delta the Movie: Passionate Walküre, relation: production company, object: Satelight  response: YES\n",
      "Input: subject: Macross Delta the Movie: Passionate Walküre, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Macross Delta the Movie: Passionate Walküre, relation: form of creative work, object: feature film  response: YES\n",
      "Input: subject: Macross Delta the Movie: Passionate Walküre, relation: director, object: Shōji Kawamori  response: NO\n",
      "Input: subject: Macross Delta the Movie: Passionate Walküre, relation: genre, object: mecha  response: YES\n",
      "Input: subject: Macross Delta the Movie: Passionate Walküre, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Macross Delta the Movie: Passionate Walküre, relation: based on, object: Macross Delta  response: YES\n",
      "Input: subject: Zukkoke Knight - Don De La Mancha, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Zukkoke Knight - Don De La Mancha, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Hitorijime My Hero, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Hitorijime My Hero, relation: genre, object: yaoi  response: YES\n",
      "Input: subject: Double Hard, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Double Hard, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Bible Black, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Bible Black, relation: publisher, object: Active  response: NO\n",
      "Input: subject: Bible Black, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Bible Black, relation: game mode, object: single-player video game  response: NO\n",
      "Input: subject: Bible Black, relation: distribution format, object: compact disc  response: YES\n",
      "Input: subject: Crimson Wolf, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Crimson Wolf, relation: director, object: Shōichi Masuo  response: YES\n",
      "Input: subject: Crimson Wolf, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Tenshi no Drop, relation: intended public, object: shōnen  response: YES\n",
      "Input: subject: Tenshi no Drop, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Tenshi no Drop, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Tenshi no Drop, relation: author, object: Chizuna Nakajima  response: NO\n",
      "Input: subject: Gakuen Handsome, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Gakuen Handsome, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Gakuen Handsome, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Gakuen Handsome, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Gakuen Handsome, relation: genre, object: dating sim  response: YES\n",
      "Input: subject: IDOLM@STER SideM, relation: character designer, object: Haruko Iizuka  response: YES\n",
      "Input: subject: IDOLM@STER SideM, relation: input device, object: touchscreen  response: NO\n",
      "Input: subject: IDOLM@STER SideM, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: IDOLM@STER SideM, relation: business model, object: free-to-play  response: YES\n",
      "Input: subject: Love Rice, relation: distribution format, object: video on demand  response: NO\n",
      "Input: subject: Love Rice, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Love Rice, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Schick x Evangelion, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Schick x Evangelion, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Schick x Evangelion, relation: media franchise, object: Neon Genesis Evangelion  response: NO\n",
      "Input: subject: Schick x Evangelion, relation: voice actor, object: Fumihiko Tachiki  response: YES\n",
      "Input: subject: Running Boy: Star Soldier no Himitsu, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Running Boy: Star Soldier no Himitsu, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Kinnikuman Nisei, relation: intended public, object: shōnen  response: YES\n",
      "Input: subject: Kinnikuman Nisei, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Kinnikuman Nisei, relation: main subject, object: sport  response: YES\n",
      "Input: subject: Kinnikuman Nisei, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Kinnikuman Nisei, relation: author, object: Yudetamago  response: YES\n",
      "Input: subject: Voice of Fox, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Voice of Fox, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Voice of Fox, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Voice of Fox, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: This Boy Is a Professional Wizard, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: This Boy Is a Professional Wizard, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: This Boy Is a Professional Wizard, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: This Boy Is a Professional Wizard, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Kurokan, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Kurokan, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Kurokan, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Kurokan, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Euphoria, relation: language of work or name, object: Japanese  response: NO\n",
      "Input: subject: Euphoria, relation: input device, object: computer keyboard  response: NO\n",
      "Input: subject: Euphoria, relation: game mode, object: single-player video game  response: NO\n",
      "Input: subject: Euphoria, relation: distribution format, object: DVD  response: YES\n",
      "Input: subject: Euphoria, relation: publisher, object: CLOCKUP  response: YES\n",
      "Input: subject: Euphoria, relation: country of origin, object: Japan  response: NO\n",
      "Input: subject: Ranma ½ One Grew Over the Kuno's Nest, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Ranma ½ One Grew Over the Kuno's Nest, relation: based on, object: Ranma ½  response: YES\n",
      "Input: subject: Ranma ½ One Grew Over the Kuno's Nest, relation: director, object: Junji Nishimura  response: YES\n",
      "Input: subject: Ranma ½ One Grew Over the Kuno's Nest, relation: main subject, object: high school student  response: YES\n",
      "Input: subject: Ranma ½ One Grew Over the Kuno's Nest, relation: distributed by, object: Dynit  response: YES\n",
      "Input: subject: Ranma ½ One Grew Over the Kuno's Nest, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Fukusuke, relation: director, object: Ryūichi Yokoyama  response: NO\n",
      "Input: subject: Papa Datte, Shitai, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Papa Datte, Shitai, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Papa Datte, Shitai, relation: genre, object: yaoi  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 6, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 6, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 6, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 6, relation: follows, object: Digimon Adventure Tri. 5  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 6, relation: part of the series, object: Digimon Adventure tri.  response: YES\n",
      "Input: subject: Doamaiger D, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Doamaiger D, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Doamaiger D, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Armitage III: Poly Matrix, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Armitage III: Poly Matrix, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Armitage III: Poly Matrix, relation: director, object: Takuya Satō  response: YES\n",
      "Input: subject: Armitage III: Poly Matrix, relation: genre, object: cyberpunk  response: YES\n",
      "Input: subject: Chō Supercar Gattiger, relation: follows, object: The Great Grape Ape Show  response: NO\n",
      "Input: subject: Chō Supercar Gattiger, relation: original broadcaster, object: TV Tokyo  response: YES\n",
      "Input: subject: Chō Supercar Gattiger, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Chō Supercar Gattiger, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Lupin the 3rd vs Detective Conan, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Lupin the 3rd vs Detective Conan, relation: production company, object: TMS Entertainment  response: YES\n",
      "Input: subject: Lupin the 3rd vs Detective Conan, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Lupin the 3rd vs Detective Conan, relation: form of creative work, object: feature film  response: YES\n",
      "Input: subject: Lupin the 3rd vs Detective Conan, relation: intended public, object: shōnen  response: YES\n",
      "Input: subject: Lupin the 3rd vs Detective Conan, relation: voice actor, object: Kanichi Kurita  response: YES\n",
      "Input: subject: Lupin the 3rd vs Detective Conan, relation: director, object: Hajime Kamegaki  response: NO\n",
      "Input: subject: Lupin the 3rd vs Detective Conan, relation: screenwriter, object: Atsushi Maekawa  response: NO\n",
      "Input: subject: Peeping Life, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Peeping Life, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Peeping Life, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Giovanni's Island, relation: distributed by, object: Warner Bros.  response: NO\n",
      "Input: subject: Giovanni's Island, relation: production company, object: Production I.G  response: YES\n",
      "Input: subject: Giovanni's Island, relation: producer, object: Mitsuhisa Ishikawa  response: YES\n",
      "Input: subject: Giovanni's Island, relation: screenwriter, object: Yoshiki Sakurai  response: YES\n",
      "Input: subject: Giovanni's Island, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Giovanni's Island, relation: director, object: Mizuho Nishikubo  response: YES\n",
      "Input: subject: Pan de Peace!, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Pan de Peace!, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Pan de Peace!, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Pan de Peace!, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: The Laws of the Sun, relation: followed by, object: The Golden Laws  response: NO\n",
      "Input: subject: The Laws of the Sun, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: The Laws of the Sun, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: The Laws of the Sun, relation: author, object: Ryūhō Ōkawa  response: YES\n",
      "Input: subject: Burglars of Baghdad Castle, relation: based on, object: The Thief of Bagdad  response: YES\n",
      "Input: subject: Burglars of Baghdad Castle, relation: color, object: black-and-white  response: NO\n",
      "Input: subject: Burglars of Baghdad Castle, relation: country of origin, object: Japan  response: NO\n",
      "Input: subject: Burglars of Baghdad Castle, relation: fabrication method, object: cutout animation  response: NO\n",
      "Input: subject: Hello Kitty: Stump Village, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Hello Kitty: Stump Village, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: JK-Meshi!, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: JK-Meshi!, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: JK-Meshi!, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Tesagure! Bukatsu-mono, relation: composer, object: Jun Ōtomo  response: YES\n",
      "Input: subject: Tesagure! Bukatsu-mono, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Tesagure! Bukatsu-mono, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Tesagure! Bukatsu-mono, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Demon Beast Invasion, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Demon Beast Invasion, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Demon Beast Invasion, relation: author, object: Toshio Maeda  response: YES\n",
      "Input: subject: Demon Beast Invasion, relation: genre, object: henta  response: YES\n",
      "Input: subject: Cat Soup Theater, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Cat Soup Theater, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Aikatsu on Parade!, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Aikatsu on Parade!, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Aikatsu on Parade!, relation: publisher, object: Bandai  response: YES\n",
      "Input: subject: Aikatsu on Parade!, relation: production company, object: Bandai Namco Pictures  response: YES\n",
      "Input: subject: Power DoLLS: Detachment of Limited Line Service, relation: genre, object: turn-based tactics  response: YES\n",
      "Input: subject: Rinshi!! Ekoda-chan, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Lupin VIII, relation: production company, object: TMS Entertainment  response: YES\n",
      "Input: subject: Lupin VIII, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Lupin VIII, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: Lupin VIII, relation: director, object: Rintaro  response: YES\n",
      "Input: subject: Chain Chronicle, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Chain Chronicle, relation: publisher, object: Sega  response: YES\n",
      "Input: subject: Chain Chronicle, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Chain Chronicle, relation: developer, object: Sega Interactive  response: YES\n",
      "Input: subject: Chain Chronicle, relation: input device, object: touchscreen  response: YES\n",
      "Input: subject: Chain Chronicle, relation: performer, object: Aya Uchida  response: YES\n",
      "Input: subject: Chain Chronicle, relation: genre, object: J-pop  response: NO\n",
      "Input: subject: Harlock Saga, relation: composer, object: Kaoru Wada  response: YES\n",
      "Input: subject: Harlock Saga, relation: distributed by, object: Netflix  response: YES\n",
      "Input: subject: Harlock Saga, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Harlock Saga, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Harlock Saga, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Harlock Saga, relation: director, object: Yoshio Takeuchi  response: YES\n",
      "Input: subject: To Be Hero, relation: language of work or name, object: Japanese  response: YES\n",
      "Input: subject: To Be Hero, relation: original language of film or TV show, object: Classical Chinese  response: NO\n",
      "Input: subject: To Be Hero, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: To Be Hero, relation: followed by, object: To Be Heroine  response: NO\n",
      "Input: subject: To Be Hero, relation: main subject, object: alien invasion  response: YES\n",
      "Input: subject: To Be Hero, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: To Be Hero, relation: genre, object: comedy  response: YES\n",
      "Input: subject: To Be Hero, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: The Black Cat, relation: color, object: black-and-white  response: NO\n",
      "Input: subject: The Black Cat, relation: country of origin, object: Japan  response: NO\n",
      "Input: subject: The Black Cat, relation: original language of film or TV show, object: Japanese  response: NO\n",
      "Input: subject: The Black Cat, relation: genre, object: animated film  response: NO\n",
      "Input: subject: Jankenman, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Jankenman, relation: genre, object: action game  response: YES\n",
      "Input: subject: Jankenman, relation: platform, object: Game Boy  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 5, relation: followed by, object: Digimon Adventure Tri. 6  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 5, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 5, relation: part of the series, object: Digimon Adventure tri.  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 5, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Digimon Adventure Tri. 5, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Doraemon: Nobita and the Green Giant Legend, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Doraemon: Nobita and the Green Giant Legend, relation: main subject, object: house cat  response: NO\n",
      "Input: subject: Doraemon: Nobita and the Green Giant Legend, relation: country of origin, object: Japan  response: YES\n",
      "Input: subject: Doraemon: Nobita and the Green Giant Legend, relation: distributed by, object: Toho  response: YES\n",
      "Input: subject: Doraemon: Nobita and the Green Giant Legend, relation: film editor, object: Toshihiko Kojima  response: YES\n",
      "Input: subject: Doraemon: Nobita and the Green Giant Legend, relation: screenwriter, object: Hiroshi Ōnogi  response: YES\n",
      "Input: subject: Doraemon: Nobita and the Green Giant Legend, relation: director, object: Ayumu Watanabe  response: YES\n",
      "Input: subject: Double Decker! Doug & Kirill, relation: original language of film or TV show, object: Japanese  response: YES\n",
      "Input: subject: Double Decker! Doug & Kirill, relation: distribution format, object: video on demand  response: YES\n",
      "Input: subject: Double Decker! Doug & Kirill, relation: character designer, object: Masakazu Katsura  response: YES\n",
      "Input: subject: Double Decker! Doug & Kirill, relation: distributed by, object: Crunchyroll  response: YES\n",
      "Input: subject: Double Decker! Doug & Kirill, relation: country of origin, object: Japan  response: YES\n"
     ]
    }
   ],
   "source": [
    "domain_topic_tmp = 'entertainment_anime'\n",
    "df = pd.read_csv(f\"../data/questions/unfiltered/{domain_topic_tmp}_questions.csv\")\n",
    "for i in df.index:\n",
    "    subject, relation, object = df.loc[i, 'subject'], df.loc[i, 'relation'], df.loc[i, 'object']\n",
    "    triplet = f\"\"\"subject: {subject}, relation: {relation}, object: {object}\"\"\"\n",
    "    messages_qa = [{\"role\": \"system\", \"content\": system_msg_check}, {\"role\": \"user\", \"content\": triplet}]\n",
    "    current_output = get_response(model_eval, tok_eval, messages_qa, max_new_tokens=64)\n",
    "    print(f\"Input: {triplet}  response: {current_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: question: What does Berlin Marathon sponsor?, answer: BMW  response: YES\n"
     ]
    }
   ],
   "source": [
    "system_msg_check = \"\"\"Check whether the question-answer pair is logically correct. Only answer “YES” or “NO”.\"\"\"\n",
    "triplet = f\"\"\"question: What does Berlin Marathon sponsor?, answer: BMW\"\"\"\n",
    "messages_qa = [{\"role\": \"system\", \"content\": system_msg_check}, {\"role\": \"user\", \"content\": triplet}]\n",
    "current_output = get_response(model_eval, tok_eval, messages_qa, max_new_tokens=64)\n",
    "print(f\"Input: {triplet}  response: {current_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_mc_q, ls_mc_a = [], []\n",
    "for i, e in enumerate(df['multiple_choice_question'].tolist()[:n]):\n",
    "    idx = e.find(\", 'ground_truth': \")\n",
    "    question, ground_truth = e[:idx].replace(\"{\", '').replace(\"}\", ''), e[idx+len(\", 'ground_truth': \"):].replace(\"}\", '').replace(\"'\", '') + '. ' + targets[i]\n",
    "    ls_mc_q.append(question)\n",
    "    ls_mc_a.append(ground_truth)\n",
    "multiple_choice_questions = {'multiple_choice': {'prompt': ls_mc_q, 'ground_truth': ls_mc_a},}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from data_prep.py\n",
    "def generate_question(subject, relation, object, topic, multi_hops=1, query_subject=False):\n",
    "    object_type1 = None\n",
    "    object_type2 = None\n",
    "    object_type = None\n",
    "    discard_flag = False\n",
    "    convert_dict1 = {\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"LOC\": \"GPE\"\n",
    "    }\n",
    "\n",
    "    ####### method 1\n",
    "    sentence = Sentence(object)\n",
    "    # Predict entities\n",
    "    sequence_tagger.predict(sentence)\n",
    "    # Access entity annotations\n",
    "    entities = sentence.get_spans('ner')\n",
    "    # Print the recognized entities\n",
    "    if entities:\n",
    "        object_type1 = entities[0].tag\n",
    "        if object_type1 == \"PER\" or object_type1 == \"LOC\":\n",
    "            object_type1 = convert_dict1[object_type1]\n",
    "        else:\n",
    "            object_type1 = None\n",
    "\n",
    "    ####### method 2\n",
    "    object_doc = spacy_en_core_web(object)\n",
    "    if object_doc.ents:\n",
    "        object_type2 = object_doc.ents[0].label_\n",
    "\n",
    "    if object_type1:        \n",
    "        if object_type1 == object_type2:\n",
    "            object_type = object_type1\n",
    "        else:\n",
    "            discard_flag = True\n",
    "    else:\n",
    "        if object_type2 != \"GPE\" and object_type2 != \"PERSON\":\n",
    "            object_type = object_type2\n",
    "        else:\n",
    "            discard_flag = True\n",
    "            \n",
    "    if discard_flag:\n",
    "        return None\n",
    "\n",
    "    if multi_hops == 1:\n",
    "        subject_doc = spacy_en_core_web(relation)\n",
    "    else:\n",
    "        subject_doc_list = []\n",
    "        for i in range(multi_hops):\n",
    "            tmp_subject_doc = spacy_en_core_web(relation['edge_labels'][i])\n",
    "            subject_doc_list.append(tmp_subject_doc)\n",
    "\n",
    "    if multi_hops > 1:\n",
    "        if not query_subject:\n",
    "            for i in range(multi_hops-1):\n",
    "                if subject_doc_list[i][0].tag_ in [\"VBN\", \"VBD\", \"VB\", \"VBZ\", \"JJ\"]:\n",
    "                    return None\n",
    "        else:\n",
    "            for i in range(1, multi_hops):\n",
    "                if subject_doc_list[i][0].tag_ in [\"VBN\", \"VBD\", \"VB\", \"VBZ\", \"JJ\"]:\n",
    "                    return None\n",
    "        if not query_subject:\n",
    "            subject_doc = subject_doc_list[-1]\n",
    "        else:\n",
    "            subject_doc = subject_doc_list[0]\n",
    "\n",
    "    if multi_hops == 1:\n",
    "        if subject_doc[-1].tag_ == \"IN\" and subject_doc[0].tag_ not in [\"VBN\", \"VBD\", \"VB\", \"VBZ\"]:\n",
    "            return None\n",
    "    else:\n",
    "        for i in range(multi_hops):\n",
    "            if subject_doc_list[i][-1].tag_ == \"IN\" and subject_doc_list[i][0].tag_ not in [\"VBN\", \"VBD\", \"VB\", \"VBZ\"]:\n",
    "                return None\n",
    "        \n",
    "    question_answer_pair = {}\n",
    "    # question_answer_pair[\"type\"] = \"wh\"\n",
    "    question_answer_pair[\"subject\"] = subject\n",
    "    question_answer_pair[\"relation\"] = relation\n",
    "    question_answer_pair[\"object\"] = object\n",
    "\n",
    "    relation_set = set()\n",
    "    for token in subject_doc:\n",
    "        relation_set.add(token.tag_)\n",
    "\n",
    "    object_to_interrogative = {\n",
    "        \"PERSON\": \"Who\",\n",
    "        \"DATE\": \"When\",\n",
    "    }\n",
    "\n",
    "    default_interrogative = \"What\"  # Default value      \n",
    "    interrogative = object_to_interrogative.get(object_type, default_interrogative)\n",
    "    if query_subject:\n",
    "        tmp = subject\n",
    "        subject = object\n",
    "        object = tmp\n",
    "\n",
    "    if subject_doc[0].tag_ == \"VBN\" and subject_doc[-1].tag_ == \"IN\" and all(token.tag_ not in [\"NN\", \"NNP\", \"NNPS\", \"NNS\"] for token in subject_doc[0:]):\n",
    "        if not query_subject:\n",
    "            if multi_hops == 1:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" was \" + subject + \" \" + relation + \"?\"\n",
    "                question_answer_pair[\"label\"] = object\n",
    "            else:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" was \" + subject \n",
    "                for i in range(multi_hops-1):\n",
    "                    question_answer_pair[\"question\"] += \"'s \" +  relation['edge_labels'][i] \n",
    "                question_answer_pair[\"question\"] += \" \"  + relation['edge_labels'][-1] + \"?\"\n",
    "                question_answer_pair[\"label\"] = object\n",
    "        else:\n",
    "            if multi_hops == 1:\n",
    "                if object_type != \"PERSON\":\n",
    "                    first_pair = next(iter(topic.items()))\n",
    "                    if first_pair[1] != \"revolution\":\n",
    "                        interrogative = \"Which \" + first_pair[1]\n",
    "                    else:\n",
    "                        interrogative = \"Which revolution or war\"\n",
    "                \n",
    "                question_answer_pair[\"question\"] = interrogative + \" was \" + relation + \" \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "            else:\n",
    "                first_pair = next(iter(topic.items()))\n",
    "                question_answer_pair[\"question\"] = interrogative \n",
    "                for i in range(multi_hops-1, 0, -1):\n",
    "                    if first_pair[1] == \"human\" and i == multi_hops-1:\n",
    "                        question_answer_pair[\"question\"] += \"se \" +  relation['edge_labels'][i]\n",
    "                    elif i == multi_hops-1:\n",
    "                        first_pair = next(iter(topic.items()))\n",
    "                        if first_pair[1] != \"revolution\":\n",
    "                            interrogative = \"Which \" + first_pair[1]\n",
    "                        else:\n",
    "                            interrogative = \"Which revolution or war\"\n",
    "                    else:\n",
    "                        question_answer_pair[\"question\"] += \" \" +  relation['edge_labels'][i]\n",
    "                question_answer_pair[\"question\"] += \" was \" + relation['edge_labels'][0] + \" \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "\n",
    "    elif subject_doc[0].tag_ == \"JJ\" and subject_doc[-1].tag_ == \"IN\" and all(token.tag_ not in [\"NN\", \"NNP\", \"NNPS\", \"NNS\"] for token in subject_doc[0:]):\n",
    "        if not query_subject:\n",
    "            if multi_hops == 1:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" is \" + subject + \" \"+ relation + \"?\"\n",
    "                question_answer_pair[\"label\"] = object\n",
    "            else:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" is \" + subject \n",
    "                for i in range(multi_hops-1):\n",
    "                    question_answer_pair[\"question\"] += \"'s \" +  relation['edge_labels'][i] \n",
    "                question_answer_pair[\"question\"] += \" \"  + relation['edge_labels'][-1] + \"?\"\n",
    "                question_answer_pair[\"label\"] = object\n",
    "        else:\n",
    "            if multi_hops == 1:\n",
    "                if object_type != \"PERSON\":\n",
    "                    first_pair = next(iter(topic.items()))\n",
    "                    if first_pair[1] != \"revolution\":\n",
    "                        interrogative = \"Which \" + first_pair[1]\n",
    "                    else:\n",
    "                        interrogative = \"Which revolution or war\"\n",
    "                question_answer_pair[\"question\"] = interrogative + \" is \" + \" \" + relation + \" \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "            else:\n",
    "                question_answer_pair[\"question\"] = interrogative \n",
    "                first_pair = next(iter(topic.items()))\n",
    "                for i in range(multi_hops-1, 0, -1):\n",
    "                    if first_pair[1] == \"human\" and i == multi_hops-1:\n",
    "                        question_answer_pair[\"question\"] += \"se \" +  relation['edge_labels'][i]\n",
    "                    elif i == multi_hops-1:\n",
    "                        first_pair = next(iter(topic.items()))\n",
    "                        if first_pair[1] != \"revolution\":\n",
    "                            interrogative = \"Which \" + first_pair[1]\n",
    "                        else:\n",
    "                            interrogative = \"Which revolution or war\"\n",
    "                    else:\n",
    "                        question_answer_pair[\"question\"] += \" \" +  relation['edge_labels'][i]\n",
    "                question_answer_pair[\"question\"] += \" is \"  + relation['edge_labels'][0] + \" \" + object + \"?\"\n",
    "            \n",
    "    elif subject_doc[0].tag_ == \"VBD\" and subject_doc[-1].tag_ not in [\"NN\", \"NNP\", \"NNPS\", \"NNS\"]:\n",
    "        if not query_subject:\n",
    "            if multi_hops == 1:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" did \" + subject + \" \" \n",
    "            else:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" did \" + subject\n",
    "                for i in range(multi_hops-1):\n",
    "                    question_answer_pair[\"question\"] += \"'s \" +  relation['edge_labels'][i] \n",
    "                question_answer_pair[\"question\"] += \" \" \n",
    "            for token in subject_doc:\n",
    "                if token.tag_ == \"VBD\":\n",
    "                    question_answer_pair[\"question\"] += token.lemma_ + \" \"\n",
    "                else:\n",
    "                    question_answer_pair[\"question\"] += token.text + \" \"\n",
    "            question_answer_pair[\"question\"] = question_answer_pair[\"question\"][:-1] + \"?\"\n",
    "            question_answer_pair[\"label\"] = object\n",
    "        else:\n",
    "            if multi_hops == 1:\n",
    "                if object_type != \"PERSON\":\n",
    "                    first_pair = next(iter(topic.items()))\n",
    "                    if first_pair[1] != \"revolution\":\n",
    "                        interrogative = \"Which \" + first_pair[1]\n",
    "                    else:\n",
    "                        interrogative = \"Which revolution or war\"\n",
    "                question_answer_pair[\"question\"] = interrogative + \" \" + relation + \" \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "            else:\n",
    "                first_pair = next(iter(topic.items()))\n",
    "                question_answer_pair[\"question\"] = interrogative \n",
    "                for i in range(multi_hops-1, 0, -1):\n",
    "                    if first_pair[1] == \"human\" and i == multi_hops-1:\n",
    "                        question_answer_pair[\"question\"] += \"se \" +  relation['edge_labels'][i]\n",
    "                    elif i == multi_hops-1:\n",
    "                        first_pair = next(iter(topic.items()))\n",
    "                        if first_pair[1] != \"revolution\":\n",
    "                            interrogative = \"Which \" + first_pair[1]\n",
    "                        else:\n",
    "                            interrogative = \"Which revolution or war\"\n",
    "                    else:\n",
    "                        question_answer_pair[\"question\"] += \" \" +  relation['edge_labels'][i]\n",
    "                question_answer_pair[\"question\"] += \" \"  + relation['edge_labels'][0] + \" \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "\n",
    "    elif (subject_doc[0].tag_ == \"VB\" or subject_doc[0].tag_ == \"VBZ\") and subject_doc[-1].tag_ not in [\"NN\", \"NNP\", \"NNPS\", \"NNS\"]:\n",
    "        if not query_subject:\n",
    "            if multi_hops == 1:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" does \" + subject + \" \"\n",
    "            else:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" does \" + subject \n",
    "                for i in range(multi_hops-1):\n",
    "                    question_answer_pair[\"question\"] += \"'s \" +  relation['edge_labels'][i] \n",
    "                question_answer_pair[\"question\"] += \" \" \n",
    "            for token in subject_doc:\n",
    "                if token.tag_ == \"VBZ\":\n",
    "                    question_answer_pair[\"question\"] += token.lemma_ + \" \"\n",
    "                else:\n",
    "                    question_answer_pair[\"question\"] += token.text + \" \"\n",
    "            question_answer_pair[\"question\"] = question_answer_pair[\"question\"][:-1] + \"?\"\n",
    "            question_answer_pair[\"label\"] = object\n",
    "        else:\n",
    "            if multi_hops == 1:\n",
    "                if object_type != \"PERSON\":\n",
    "                    first_pair = next(iter(topic.items()))\n",
    "                    if first_pair[1] != \"revolution\":\n",
    "                        interrogative = \"Which \" + first_pair[1]\n",
    "                    else:\n",
    "                        interrogative = \"Which revolution or war\"\n",
    "                question_answer_pair[\"question\"] = interrogative + \" \" + relation + \" \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "            else:\n",
    "                first_pair = next(iter(topic.items()))\n",
    "                question_answer_pair[\"question\"] = interrogative \n",
    "                for i in range(multi_hops-1, 0, -1):\n",
    "                    if first_pair[1] == \"human\" and i == multi_hops-1:\n",
    "                        question_answer_pair[\"question\"] += \"se \" +  relation['edge_labels'][i]\n",
    "                    elif i == multi_hops-1:\n",
    "                        first_pair = next(iter(topic.items()))\n",
    "                        if first_pair[1] != \"revolution\":\n",
    "                            interrogative = \"Which \" + first_pair[1]\n",
    "                        else:\n",
    "                            interrogative = \"Which revolution or war\"\n",
    "                    else:\n",
    "                        question_answer_pair[\"question\"] += \" \" +  relation['edge_labels'][i]\n",
    "                question_answer_pair[\"question\"] += \" \"  + relation['edge_labels'][0] + \" \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "\n",
    "    elif (subject_doc[-1].tag_ == \"NN\" or subject_doc[-1].tag_ == \"NNP\") and subject_doc[0].tag_ not in [\"VB\", \"VBZ\", \"VBD\"]: \n",
    "        if not query_subject:\n",
    "            if multi_hops == 1:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" is the \" + relation + \" of \" + subject + \"?\"\n",
    "                question_answer_pair[\"label\"] = object\n",
    "            else:\n",
    "                question_answer_pair[\"question\"] = interrogative + \" is the \" + relation['edge_labels'][1] + \" of \" + subject \n",
    "                for i in range(multi_hops-1):\n",
    "                    question_answer_pair[\"question\"] += \"'s \" + relation['edge_labels'][i] \n",
    "                question_answer_pair[\"question\"] += \"?\"\n",
    "                question_answer_pair[\"label\"] = object\n",
    "        else:\n",
    "            first_pair = next(iter(topic.items()))\n",
    "            if multi_hops == 1:\n",
    "                if first_pair[1] == \"human\":\n",
    "                    question_answer_pair[\"question\"] = interrogative + \"se \" + relation + \" is \" + object + \"?\"\n",
    "                else:\n",
    "                    first_pair = next(iter(topic.items()))\n",
    "                    if first_pair[1] != \"revolution\":\n",
    "                        interrogative = \"Which \" + first_pair[1]\n",
    "                    else:\n",
    "                        interrogative = \"Which revolution or war\"\n",
    "                    question_answer_pair[\"question\"] = interrogative + \"'s \" + relation + \" is \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "            else:\n",
    "                question_answer_pair[\"question\"] = interrogative \n",
    "                for i in range(multi_hops-1, -1, -1):\n",
    "                    if first_pair[1] == \"human\" and i == multi_hops-1:\n",
    "                        question_answer_pair[\"question\"] += \"se \" +  relation['edge_labels'][i]\n",
    "                    elif i == multi_hops-1:\n",
    "                        first_pair = next(iter(topic.items()))\n",
    "                        if first_pair[1] != \"revolution\":\n",
    "                            interrogative = \"Which \" + first_pair[1]\n",
    "                        else:\n",
    "                            interrogative = \"Which revolution or war\"\n",
    "                    else:\n",
    "                        question_answer_pair[\"question\"] += \"'s \" +  relation['edge_labels'][i]\n",
    "                question_answer_pair[\"question\"] += \" is \" + object + \"?\"\n",
    "                question_answer_pair[\"label\"] = subject\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return question_answer_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response for wh questions from data/questions_old/old_3_types\n",
    "print(model_id_format)\n",
    "df_all_topics = pd.DataFrame()\n",
    "\n",
    "for domain_topic_tmp in topic_ls:\n",
    "    # if domain_topic_tmp != 'places_landmark':\n",
    "    #     continue\n",
    "    print(f\"file name: {domain_topic_tmp}\")\n",
    "\n",
    "    ls_output = []\n",
    "    for i in tqdm(df_wh.index):\n",
    "        question, label = df_wh.loc[i, 'question'], df_wh.loc[i, 'label']\n",
    "        # user_msg_qa = Wh_content + \"\\nQuestion:\" + question  # places_landmark_old.csv\n",
    "        # user_msg_qa = f'Question: {question} Answer:'\n",
    "        user_msg_qa = f'{question}'\n",
    "        if model_id_format == 'gemma_2_9b_it':  # System role not supported for gemma\n",
    "            messages_qa = [{\"role\": \"user\", \"content\": system_msg_qa+' '+user_msg_qa}]\n",
    "        else:\n",
    "            messages_qa = [{\"role\": \"system\", \"content\": system_msg_qa}, {\"role\": \"user\", \"content\": user_msg_qa}]\n",
    "        output_qa = get_response(model_qa, tok_qa, messages_qa, max_new_tokens=16)\n",
    "        # if '\\n' in output_qa:\n",
    "            # print(f\"Question: {question} Label: {label} | Prediction: {repr(output_qa)}\")\n",
    "        # output_qa = output_qa.replace('\\n', ' ').strip().rstrip('.')  # remove trailing period for llama output\n",
    "        ls_output.append(output_qa)\n",
    "    \n",
    "    df_wh['topic'] = domain_topic_tmp\n",
    "    df_wh[f\"output_{model_id_format}\"] = ls_output\n",
    "    df_all_topics = pd.concat([df_all_topics, df_wh], axis=0)\n",
    "    print(\"df_all_topics.shape:\", df_all_topics.shape)\n",
    "        \n",
    "df_all_topics = df_all_topics[['topic', 'type', 'subject', 'relation', 'object', 'question', 'label', f'output_{model_id_format}']]\n",
    "df_all_topics.to_csv(f\"../data/questions/wh_only/all_topics_{model_id_format}.csv\", index=False)\n",
    "# del model_qa\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_json(f\"../data/questions/all_3_types/{domain_topic_name}_questions.json\", lines=True)\n",
    "# Only a small part of yes_no and MC questions have same subject, object, and relation as the hallucinated wh questions\n",
    "for i in df_wh_hallu.index[:]:\n",
    "    subject, relation, object = df_wh_hallu.loc[i, 'subject'], df_wh_hallu.loc[i, 'relation'], df_wh_hallu.loc[i, 'object']\n",
    "    df_other_type = df[(df.subject==subject) & (df.relation==relation) & (df.object==object) & (df.type!='wh')]\n",
    "    # print(len(df_other_type))\n",
    "    # Add yes_no and MC questions to the df_wh_hallu as new columns named 'question_yes_no' and 'question_MC'\n",
    "    for j in df_other_type.index:\n",
    "        other_type = df_other_type.loc[j, 'type']\n",
    "        df_wh_hallu.loc[i, f'question_{other_type}'] = df_other_type.loc[j, 'question']\n",
    "print(df_wh_hallu[df_wh_hallu.question_yes_no.notna() & df_wh_hallu.question_MC.notna()].shape)\n",
    "# df_wh_hallu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_content = \"The following question's topic is about \" + domain_topic_name + \". Choose the only correct option for the multiple choice problem. (Answer 'A', 'B', 'C' or 'D')(Don't explain)\"\n",
    "# yes_no_content = \"The following question's topic is about \" + domain_topic_name + \". Only need to answer 'Yes' or 'No', and don't explain\"\n",
    "# Wh_content = \"The following question's topic is about \" + domain_topic_name + \". Directly give me the answer in 'phrase' or 'word' format. Don't give me a sentence or explain\"\n",
    "\n",
    "# for filename in os.listdir(\"../data/questions/all_3_types/\"):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         df = pd.read_json(f\"../data/questions/all_3_types/{filename}\", lines=True)\n",
    "#         df_wh = df[df.type=='wh'].copy()\n",
    "#         topic_ls.append(filename.replace('_questions.json', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract triplets from knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/graph/places_country.gpickle', 'rb') as f:\n",
    "    directed_graph = pickle.load(f)\n",
    "for node in directed_graph.nodes():\n",
    "    for _, target, edge_data in directed_graph.edges(node, data=True):\n",
    "        print(f\"out_edges, node: {node}, relation: {edge_data['label']}, target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_edges, node: Thailand, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Thailand, relation: contains the administrative territorial entity, target: Si Sa Ket\n",
      "out_edges, node: Thailand, relation: official symbol, target: Asian elephant\n",
      "out_edges, node: Thailand, relation: member of, target: UNESCO\n",
      "out_edges, node: Thailand, relation: head of government, target: Prayut Chan-o-cha\n",
      "out_edges, node: Thailand, relation: shares border with, target: Laos\n",
      "out_edges, node: Thailand, relation: part of, target: Southeast Asia\n",
      "out_edges, node: Thailand, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Thailand, relation: official language, target: Thai\n",
      "out_edges, node: Thailand, relation: instance of, target: constitutional monarchy\n",
      "out_edges, node: Thailand, relation: religion or worldview, target: Christianity\n",
      "out_edges, node: Thailand, relation: continent, target: Asia\n",
      "out_edges, node: Thailand, relation: head of state, target: Vajiralongkorn\n",
      "out_edges, node: Thailand, relation: lowest point, target: Thailand\n",
      "out_edges, node: Thailand, relation: language used, target: Mon\n",
      "out_edges, node: North Korea, relation: diplomatic relation, target: Afghanistan\n",
      "out_edges, node: North Korea, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: North Korea, relation: language used, target: Korean\n",
      "out_edges, node: North Korea, relation: replaces, target: Korea\n",
      "out_edges, node: North Korea, relation: contains the administrative territorial entity, target: Pyongyang\n",
      "out_edges, node: North Korea, relation: located in or next to body of water, target: Yellow Sea\n",
      "out_edges, node: North Korea, relation: continent, target: Asia\n",
      "out_edges, node: North Korea, relation: country, target: North Korea\n",
      "out_edges, node: North Korea, relation: head of state, target: Kim Jong-un\n",
      "out_edges, node: North Korea, relation: instance of, target: country\n",
      "out_edges, node: North Korea, relation: part of, target: East Asia\n",
      "out_edges, node: North Korea, relation: shares border with, target: South Korea\n",
      "out_edges, node: Democratic Republic of the Congo, relation: shares border with, target: Cameroon\n",
      "out_edges, node: Democratic Republic of the Congo, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Democratic Republic of the Congo, relation: member of, target: United Nations\n",
      "out_edges, node: Democratic Republic of the Congo, relation: replaces, target: Zaire\n",
      "out_edges, node: Democratic Republic of the Congo, relation: continent, target: Africa\n",
      "out_edges, node: Democratic Republic of the Congo, relation: language used, target: Luba-Kasai\n",
      "out_edges, node: Democratic Republic of the Congo, relation: country, target: Democratic Republic of the Congo\n",
      "out_edges, node: Democratic Republic of the Congo, relation: contains the administrative territorial entity, target: Katanga Province\n",
      "out_edges, node: Democratic Republic of the Congo, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Democratic Republic of the Congo, relation: official symbol, target: Okapia johnstoni\n",
      "out_edges, node: Democratic Republic of the Congo, relation: instance of, target: country\n",
      "out_edges, node: Democratic Republic of the Congo, relation: basic form of government, target: republic\n",
      "out_edges, node: Democratic Republic of the Congo, relation: named after, target: Congo\n",
      "out_edges, node: Cameroon, relation: instance of, target: republic\n",
      "out_edges, node: Cameroon, relation: diplomatic relation, target: France\n",
      "out_edges, node: Cameroon, relation: shares border with, target: Central African Republic\n",
      "out_edges, node: Cameroon, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Cameroon, relation: located in/on physical feature, target: Central Africa\n",
      "out_edges, node: Cameroon, relation: head of state, target: Paul Biya\n",
      "out_edges, node: Cameroon, relation: language used, target: Fang\n",
      "out_edges, node: Cameroon, relation: official language, target: English\n",
      "out_edges, node: Cameroon, relation: capital, target: Yaoundé\n",
      "out_edges, node: Cameroon, relation: located in or next to body of water, target: Lake Chad\n",
      "out_edges, node: Cameroon, relation: country, target: Cameroon\n",
      "out_edges, node: Cameroon, relation: continent, target: Africa\n",
      "out_edges, node: People's Republic of China, relation: diplomatic relation, target: Burkina Faso\n",
      "out_edges, node: People's Republic of China, relation: member of, target: African Development Bank\n",
      "out_edges, node: People's Republic of China, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: People's Republic of China, relation: language used, target: Uyghur\n",
      "out_edges, node: People's Republic of China, relation: shares border with, target: Bhutan\n",
      "out_edges, node: People's Republic of China, relation: contains the administrative territorial entity, target: Hubei\n",
      "out_edges, node: People's Republic of China, relation: instance of, target: country\n",
      "out_edges, node: People's Republic of China, relation: territory claimed by, target: Taiwan\n",
      "out_edges, node: People's Republic of China, relation: part of, target: East Asia\n",
      "out_edges, node: People's Republic of China, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: People's Republic of China, relation: currency, target: renminbi\n",
      "out_edges, node: People's Republic of China, relation: capital, target: Beijing\n",
      "out_edges, node: People's Republic of China, relation: continent, target: Asia\n",
      "out_edges, node: People's Republic of China, relation: ethnic group, target: Han Chinese people\n",
      "out_edges, node: People's Republic of China, relation: official language, target: Chinese\n",
      "out_edges, node: People's Republic of China, relation: founded by, target: Chinese Communist Party\n",
      "out_edges, node: People's Republic of China, relation: executive body, target: State Council of the People's Republic of China\n",
      "out_edges, node: People's Republic of China, relation: country, target: People's Republic of China\n",
      "out_edges, node: People's Republic of China, relation: head of state, target: Xi Jinping\n",
      "out_edges, node: People's Republic of China, relation: legislative body, target: National People's Congress\n",
      "out_edges, node: People's Republic of China, relation: highest point, target: Mount Everest\n",
      "out_edges, node: Burkina Faso, relation: member of, target: International Finance Corporation\n",
      "out_edges, node: Burkina Faso, relation: country, target: Burkina Faso\n",
      "out_edges, node: Burkina Faso, relation: replaces, target: French West Africa\n",
      "out_edges, node: Burkina Faso, relation: language used, target: French\n",
      "out_edges, node: Burkina Faso, relation: shares border with, target: Ghana\n",
      "out_edges, node: Burkina Faso, relation: located in/on physical feature, target: West Africa\n",
      "out_edges, node: Burkina Faso, relation: diplomatic relation, target: Kosovo\n",
      "out_edges, node: Burkina Faso, relation: capital, target: Ouagadougou\n",
      "out_edges, node: Burkina Faso, relation: twinned administrative body, target: Allier\n",
      "out_edges, node: Burkina Faso, relation: continent, target: Africa\n",
      "out_edges, node: Burkina Faso, relation: basic form of government, target: republic\n",
      "out_edges, node: Burkina Faso, relation: instance of, target: country\n",
      "out_edges, node: East Timor, relation: religion or worldview, target: Islam\n",
      "out_edges, node: East Timor, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: East Timor, relation: diplomatic relation, target: Brazil\n",
      "out_edges, node: East Timor, relation: capital, target: Dili\n",
      "out_edges, node: East Timor, relation: language used, target: Tetum\n",
      "out_edges, node: East Timor, relation: shares border with, target: ASEAN\n",
      "out_edges, node: East Timor, relation: head of state, target: José Ramos-Horta\n",
      "out_edges, node: East Timor, relation: basic form of government, target: republic\n",
      "out_edges, node: East Timor, relation: continent, target: Asia\n",
      "out_edges, node: East Timor, relation: part of, target: Southeast Asia\n",
      "out_edges, node: East Timor, relation: instance of, target: country\n",
      "out_edges, node: East Timor, relation: country, target: East Timor\n",
      "out_edges, node: East Timor, relation: currency, target: United States dollar\n",
      "in_edges, node: Islam, relation: religion or worldview, target: Islam\n",
      "in_edges, node: Islam, relation: official religion, target: Islam\n",
      "out_edges, node: Benin, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Benin, relation: official language, target: French\n",
      "out_edges, node: Benin, relation: continent, target: Africa\n",
      "out_edges, node: Benin, relation: member of, target: African Development Bank\n",
      "out_edges, node: Benin, relation: instance of, target: country\n",
      "out_edges, node: Benin, relation: different from, target: Benin City\n",
      "out_edges, node: Benin, relation: part of, target: West Africa\n",
      "out_edges, node: Benin, relation: shares border with, target: Niger\n",
      "out_edges, node: Benin, relation: replaces, target: French West Africa\n",
      "out_edges, node: Benin, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Benin, relation: language used, target: Yoruba\n",
      "out_edges, node: Benin, relation: basic form of government, target: representative democracy\n",
      "out_edges, node: Benin, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Benin, relation: country, target: Benin\n",
      "out_edges, node: Benin, relation: capital, target: Porto-Novo\n",
      "out_edges, node: United States of America, relation: significant event, target: Watergate scandal\n",
      "out_edges, node: United States of America, relation: diplomatic relation, target: Tuvalu\n",
      "out_edges, node: United States of America, relation: contains the statistical territorial entity, target: United States Minor Outlying Islands\n",
      "out_edges, node: United States of America, relation: shares border with, target: Mexico\n",
      "out_edges, node: United States of America, relation: contains the administrative territorial entity, target: Rhode Island\n",
      "out_edges, node: United States of America, relation: office held by head of government, target: President of the United States\n",
      "out_edges, node: United States of America, relation: main regulatory text, target: United States Constitution\n",
      "out_edges, node: United States of America, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: United States of America, relation: instance of, target: country\n",
      "out_edges, node: United States of America, relation: capital, target: Washington, D.C.\n",
      "out_edges, node: United States of America, relation: public holiday, target: Martin Luther King Jr. Day\n",
      "out_edges, node: United States of America, relation: different from, target: Americas\n",
      "out_edges, node: United States of America, relation: located in or next to body of water, target: Pacific Ocean\n",
      "out_edges, node: United States of America, relation: foundational text, target: United States Declaration of Independence\n",
      "out_edges, node: United States of America, relation: official language, target: English\n",
      "out_edges, node: United States of America, relation: country, target: United States of America\n",
      "out_edges, node: United States of America, relation: owner of, target: White House\n",
      "out_edges, node: United States of America, relation: executive body, target: federal government of the United States\n",
      "out_edges, node: United States of America, relation: official symbol, target: Bald Eagle\n",
      "out_edges, node: United States of America, relation: head of government, target: Joe Biden\n",
      "out_edges, node: United States of America, relation: basic form of government, target: presidential system\n",
      "out_edges, node: United States of America, relation: central bank, target: Federal Reserve System\n",
      "out_edges, node: United States of America, relation: continent, target: North America\n",
      "out_edges, node: United States of America, relation: highest point, target: Denali\n",
      "out_edges, node: United States of America, relation: ethnic group, target: Native Americans in the United States\n",
      "out_edges, node: United States of America, relation: replaces, target: Confederate States of America\n",
      "out_edges, node: United States of America, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: United States of America, relation: legislative body, target: United States Congress\n",
      "out_edges, node: United States of America, relation: participant in, target: Vietnam War\n",
      "out_edges, node: United States of America, relation: highest judicial authority, target: Supreme Court of the United States\n",
      "out_edges, node: United States of America, relation: currency, target: United States dollar\n",
      "out_edges, node: Slovakia, relation: currency, target: euro\n",
      "out_edges, node: Slovakia, relation: head of state, target: Zuzana Čaputová\n",
      "out_edges, node: Slovakia, relation: member of, target: International Development Association\n",
      "out_edges, node: Slovakia, relation: capital, target: Bratislava\n",
      "out_edges, node: Slovakia, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Slovakia, relation: shares border with, target: Poland\n",
      "out_edges, node: Slovakia, relation: part of, target: European Union\n",
      "out_edges, node: Slovakia, relation: country, target: Slovakia\n",
      "out_edges, node: Slovakia, relation: official language, target: Slovak\n",
      "out_edges, node: Slovakia, relation: contains the administrative territorial entity, target: Prešov Region\n",
      "out_edges, node: Slovakia, relation: instance of, target: country\n",
      "out_edges, node: Slovakia, relation: language used, target: Polish\n",
      "out_edges, node: Slovakia, relation: located in or next to body of water, target: Danube\n",
      "out_edges, node: Slovakia, relation: continent, target: Europe\n",
      "out_edges, node: Slovakia, relation: located in/on physical feature, target: Central Europe\n",
      "out_edges, node: Slovakia, relation: public holiday, target: Christmas\n",
      "in_edges, node: Slovakia, relation: different from, target: Slovakia\n",
      "in_edges, node: Slovakia, relation: designated as terrorist by, target: Slovakia\n",
      "in_edges, node: euro, relation: currency, target: euro\n",
      "out_edges, node: France, relation: contains the administrative territorial entity, target: Saint Barthélemy\n",
      "out_edges, node: France, relation: part of, target: European Union\n",
      "out_edges, node: France, relation: diplomatic relation, target: Trinidad and Tobago\n",
      "out_edges, node: France, relation: public holiday, target: Assumption of Mary\n",
      "out_edges, node: France, relation: anthem, target: La Marseillaise\n",
      "out_edges, node: France, relation: head of state, target: Emmanuel Macron\n",
      "out_edges, node: France, relation: member of, target: Asian Development Bank\n",
      "out_edges, node: France, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: France, relation: shares border with, target: Belgium\n",
      "out_edges, node: France, relation: language used, target: Breton\n",
      "out_edges, node: France, relation: highest judicial authority, target: Constitutional Council of France\n",
      "out_edges, node: France, relation: has works in the collection, target: National Gallery of Victoria\n",
      "out_edges, node: France, relation: head of government, target: Élisabeth Borne\n",
      "out_edges, node: France, relation: official language, target: French\n",
      "out_edges, node: France, relation: capital, target: Paris\n",
      "out_edges, node: France, relation: country, target: France\n",
      "out_edges, node: France, relation: currency, target: euro\n",
      "out_edges, node: France, relation: production statistics, target: cider\n",
      "out_edges, node: France, relation: highest point, target: Mont Blanc\n",
      "out_edges, node: France, relation: central bank, target: Bank of France\n",
      "out_edges, node: France, relation: named after, target: Franks\n",
      "out_edges, node: France, relation: instance of, target: republic\n",
      "out_edges, node: France, relation: has part(s), target: Loire Valley\n",
      "out_edges, node: France, relation: continent, target: Europe\n",
      "out_edges, node: France, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: France, relation: has facility, target: crematorium\n",
      "in_edges, node: Saint Barthélemy, relation: contains the administrative territorial entity, target: Saint Barthélemy\n",
      "out_edges, node: Uruguay, relation: diplomatic relation, target: Lebanon\n",
      "out_edges, node: Uruguay, relation: member of, target: Andean Community\n",
      "out_edges, node: Uruguay, relation: shares border with, target: Brazil\n",
      "out_edges, node: Uruguay, relation: part of, target: Latin America\n",
      "out_edges, node: Uruguay, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Uruguay, relation: language used, target: Spanish\n",
      "out_edges, node: Uruguay, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Uruguay, relation: public holiday, target: All Souls' Day\n",
      "out_edges, node: Uruguay, relation: instance of, target: country\n",
      "out_edges, node: Uruguay, relation: country, target: Uruguay\n",
      "out_edges, node: Uruguay, relation: capital, target: Montevideo\n",
      "out_edges, node: Uruguay, relation: contains the administrative territorial entity, target: Florida Department\n",
      "out_edges, node: Uruguay, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Lebanon, relation: member of, target: International Development Association\n",
      "out_edges, node: Lebanon, relation: located in/on physical feature, target: Levant\n",
      "out_edges, node: Lebanon, relation: diplomatic relation, target: Armenia\n",
      "out_edges, node: Lebanon, relation: official language, target: Arabic\n",
      "out_edges, node: Lebanon, relation: shares border with, target: Israel\n",
      "out_edges, node: Lebanon, relation: part of, target: West Asia\n",
      "out_edges, node: Lebanon, relation: continent, target: Asia\n",
      "out_edges, node: Lebanon, relation: capital, target: Beirut\n",
      "out_edges, node: Lebanon, relation: lowest point, target: Mediterranean Sea\n",
      "out_edges, node: Lebanon, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Lebanon, relation: country, target: Lebanon\n",
      "out_edges, node: Lebanon, relation: language used, target: French\n",
      "out_edges, node: Lebanon, relation: instance of, target: country\n",
      "out_edges, node: Norway, relation: contains the administrative territorial entity, target: Møre og Romsdal\n",
      "out_edges, node: Norway, relation: diplomatic relation, target: Greece\n",
      "out_edges, node: Norway, relation: shares border with, target: Finland\n",
      "out_edges, node: Norway, relation: official language, target: Nynorsk\n",
      "out_edges, node: Norway, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Norway, relation: language used, target: Southern Sami\n",
      "out_edges, node: Norway, relation: member of, target: International Energy Agency\n",
      "out_edges, node: Norway, relation: public holiday, target: Good Friday\n",
      "out_edges, node: Norway, relation: head of state, target: Harald V of Norway\n",
      "out_edges, node: Norway, relation: located in or next to body of water, target: Barents Sea\n",
      "out_edges, node: Norway, relation: country, target: Norway\n",
      "out_edges, node: Norway, relation: part of, target: European Economic Area\n",
      "out_edges, node: Norway, relation: named after, target: road\n",
      "out_edges, node: Norway, relation: instance of, target: country\n",
      "out_edges, node: Norway, relation: lowest point, target: Norwegian Sea\n",
      "out_edges, node: Norway, relation: central bank, target: Norges Bank\n",
      "out_edges, node: Norway, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Norway, relation: located in/on physical feature, target: Scandinavia\n",
      "out_edges, node: Norway, relation: head of government, target: Jonas Gahr Støre\n",
      "in_edges, node: Møre og Romsdal, relation: contains the administrative territorial entity, target: Møre og Romsdal\n",
      "out_edges, node: Brazil, relation: diplomatic relation, target: Ecuador\n",
      "out_edges, node: Brazil, relation: public holiday, target: Christmas\n",
      "out_edges, node: Brazil, relation: member of, target: Latin American Economic System\n",
      "out_edges, node: Brazil, relation: contains the administrative territorial entity, target: Amazonas\n",
      "out_edges, node: Brazil, relation: country, target: Brazil\n",
      "out_edges, node: Brazil, relation: central bank, target: Central Bank of Brazil\n",
      "out_edges, node: Brazil, relation: located in or next to body of water, target: Paraná River\n",
      "out_edges, node: Brazil, relation: head of state, target: Luiz Inácio Lula da Silva\n",
      "out_edges, node: Brazil, relation: shares border with, target: Paraguay\n",
      "out_edges, node: Brazil, relation: capital, target: Brasília\n",
      "out_edges, node: Brazil, relation: described by source, target: The World Factbook\n",
      "out_edges, node: Brazil, relation: language used, target: Portuguese\n",
      "out_edges, node: Brazil, relation: basic form of government, target: representative democracy\n",
      "out_edges, node: Brazil, relation: named after, target: Caesalpinia echinata\n",
      "out_edges, node: Brazil, relation: discoverer or inventor, target: Pedro Álvares Cabral\n",
      "out_edges, node: Brazil, relation: part of, target: South America\n",
      "out_edges, node: Brazil, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Brazil, relation: instance of, target: country\n",
      "out_edges, node: Ecuador, relation: diplomatic relation, target: Mexico\n",
      "out_edges, node: Ecuador, relation: currency, target: United States dollar\n",
      "out_edges, node: Ecuador, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Ecuador, relation: shares border with, target: Colombia\n",
      "out_edges, node: Ecuador, relation: language used, target: Spanish\n",
      "out_edges, node: Ecuador, relation: instance of, target: country\n",
      "out_edges, node: Ecuador, relation: basic form of government, target: republic\n",
      "out_edges, node: Ecuador, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Ecuador, relation: capital, target: Quito\n",
      "out_edges, node: Ecuador, relation: part of, target: Americas\n",
      "out_edges, node: Ecuador, relation: highest point, target: Chimborazo\n",
      "out_edges, node: Ecuador, relation: central bank, target: Central Bank of Ecuador\n",
      "out_edges, node: Ecuador, relation: country, target: Ecuador\n",
      "out_edges, node: Ecuador, relation: named after, target: equator\n",
      "out_edges, node: Ecuador, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Republic of Ireland, relation: contains the administrative territorial entity, target: County Leitrim\n",
      "out_edges, node: Republic of Ireland, relation: diplomatic relation, target: Isle of Man\n",
      "out_edges, node: Republic of Ireland, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Republic of Ireland, relation: named after, target: Ireland\n",
      "out_edges, node: Republic of Ireland, relation: official language, target: Irish\n",
      "out_edges, node: Republic of Ireland, relation: instance of, target: country\n",
      "out_edges, node: Republic of Ireland, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Republic of Ireland, relation: central bank, target: Central Bank of Ireland\n",
      "out_edges, node: Republic of Ireland, relation: part of, target: European Union\n",
      "out_edges, node: Republic of Ireland, relation: capital, target: Dublin\n",
      "out_edges, node: Republic of Ireland, relation: currency, target: euro\n",
      "out_edges, node: Republic of Ireland, relation: continent, target: Europe\n",
      "out_edges, node: Republic of Ireland, relation: language used, target: English\n",
      "out_edges, node: Republic of Ireland, relation: head of government, target: Leo Varadkar\n",
      "out_edges, node: Republic of Ireland, relation: country, target: Republic of Ireland\n",
      "out_edges, node: Republic of Ireland, relation: head of state, target: Michael D. Higgins\n",
      "out_edges, node: Republic of Ireland, relation: different from, target: Northern Ireland\n",
      "out_edges, node: Republic of Ireland, relation: basic form of government, target: republic\n",
      "in_edges, node: Republic of Ireland, relation: shares border with, target: Republic of Ireland\n",
      "in_edges, node: County Leitrim, relation: contains the administrative territorial entity, target: County Leitrim\n",
      "out_edges, node: Slovenia, relation: diplomatic relation, target: Canada\n",
      "out_edges, node: Slovenia, relation: capital, target: Ljubljana\n",
      "out_edges, node: Slovenia, relation: part of, target: European Union\n",
      "out_edges, node: Slovenia, relation: central bank, target: Bank of Slovenia\n",
      "out_edges, node: Slovenia, relation: located in or next to body of water, target: Adriatic Sea\n",
      "out_edges, node: Slovenia, relation: different from, target: Slovakia\n",
      "out_edges, node: Slovenia, relation: instance of, target: country\n",
      "out_edges, node: Slovenia, relation: member of, target: UNESCO\n",
      "out_edges, node: Slovenia, relation: language used, target: Italian\n",
      "out_edges, node: Slovenia, relation: currency, target: euro\n",
      "out_edges, node: Slovenia, relation: shares border with, target: Austria\n",
      "out_edges, node: Slovenia, relation: contains the administrative territorial entity, target: Ajdovščina Municipality\n",
      "out_edges, node: Slovenia, relation: official language, target: Slovene\n",
      "out_edges, node: Slovenia, relation: located in/on physical feature, target: Balkans\n",
      "out_edges, node: Slovenia, relation: replaces, target: Socialist Federal Republic of Yugoslavia\n",
      "out_edges, node: Slovenia, relation: country, target: Slovenia\n",
      "out_edges, node: Slovenia, relation: highest point, target: Triglav\n",
      "out_edges, node: Slovenia, relation: continent, target: Europe\n",
      "out_edges, node: Canada, relation: diplomatic relation, target: Qatar\n",
      "out_edges, node: Canada, relation: contains the administrative territorial entity, target: Manitoba\n",
      "out_edges, node: Canada, relation: member of, target: UNESCO\n",
      "out_edges, node: Canada, relation: patron saint, target: Saint Anne\n",
      "out_edges, node: Canada, relation: legislative body, target: Parliament of Canada\n",
      "out_edges, node: Canada, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Canada, relation: located in or next to body of water, target: Pacific Ocean\n",
      "out_edges, node: Canada, relation: continent, target: North America\n",
      "out_edges, node: Canada, relation: country, target: Canada\n",
      "out_edges, node: Canada, relation: head of government, target: Justin Trudeau\n",
      "out_edges, node: Canada, relation: shares border with, target: Greenland\n",
      "out_edges, node: Canada, relation: central bank, target: Bank of Canada\n",
      "out_edges, node: Canada, relation: official language, target: French\n",
      "out_edges, node: Canada, relation: lowest point, target: Arctic Ocean\n",
      "out_edges, node: Canada, relation: instance of, target: country\n",
      "out_edges, node: Canada, relation: basic form of government, target: federation\n",
      "out_edges, node: Canada, relation: ethnic group, target: Canadians\n",
      "out_edges, node: Canada, relation: capital, target: Ottawa\n",
      "out_edges, node: Canada, relation: language used, target: English\n",
      "out_edges, node: Canada, relation: official symbol, target: North American beaver\n",
      "out_edges, node: Canada, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Austria, relation: contains the administrative territorial entity, target: Carinthia\n",
      "out_edges, node: Austria, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Austria, relation: replaces, target: Austria-Hungary\n",
      "out_edges, node: Austria, relation: named after, target: east\n",
      "out_edges, node: Austria, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Austria, relation: central bank, target: Oesterreichische Nationalbank\n",
      "out_edges, node: Austria, relation: continent, target: Europe\n",
      "out_edges, node: Austria, relation: country, target: Austria\n",
      "out_edges, node: Austria, relation: instance of, target: country\n",
      "out_edges, node: Austria, relation: public holiday, target: Assumption of Mary\n",
      "out_edges, node: Austria, relation: has quality, target: democracy\n",
      "out_edges, node: Austria, relation: part of, target: European Union\n",
      "out_edges, node: Austria, relation: ethnic group, target: Germans\n",
      "out_edges, node: Austria, relation: language used, target: German\n",
      "out_edges, node: Austria, relation: shares border with, target: Switzerland\n",
      "out_edges, node: Austria, relation: located in or next to body of water, target: Danube\n",
      "out_edges, node: Austria, relation: currency, target: euro\n",
      "out_edges, node: Austria, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Austria, relation: head of state, target: Alexander Van der Bellen\n",
      "in_edges, node: Carinthia, relation: contains the administrative territorial entity, target: Carinthia\n",
      "out_edges, node: Israel, relation: public holiday, target: Yom Kippur\n",
      "out_edges, node: Israel, relation: diplomatic relation, target: Colombia\n",
      "out_edges, node: Israel, relation: continent, target: Asia\n",
      "out_edges, node: Israel, relation: member of, target: International Development Association\n",
      "out_edges, node: Israel, relation: official religion, target: Islam\n",
      "out_edges, node: Israel, relation: legislative body, target: Knesset\n",
      "out_edges, node: Israel, relation: language used, target: Adyghe\n",
      "out_edges, node: Israel, relation: production statistics, target: olive\n",
      "out_edges, node: Israel, relation: located in or next to body of water, target: Dead Sea\n",
      "out_edges, node: Israel, relation: founded by, target: David Ben-Gurion\n",
      "out_edges, node: Israel, relation: instance of, target: country\n",
      "out_edges, node: Israel, relation: head of government, target: Benjamin Netanyahu\n",
      "out_edges, node: Israel, relation: shares border with, target: Jordan\n",
      "out_edges, node: Israel, relation: official language, target: Hebrew\n",
      "out_edges, node: Israel, relation: part of, target: Middle East\n",
      "out_edges, node: Israel, relation: official symbol, target: Anemone coronaria\n",
      "out_edges, node: Israel, relation: partially coincident with, target: Palestine\n",
      "out_edges, node: Israel, relation: country, target: Israel\n",
      "out_edges, node: Israel, relation: head of state, target: Isaac Herzog\n",
      "out_edges, node: Israel, relation: capital, target: Jerusalem\n",
      "in_edges, node: Yom Kippur, relation: public holiday, target: Yom Kippur\n",
      "out_edges, node: Guyana, relation: instance of, target: country\n",
      "out_edges, node: Guyana, relation: shares border with, target: Suriname\n",
      "out_edges, node: Guyana, relation: member of, target: Organization of American States\n",
      "out_edges, node: Guyana, relation: country, target: Guyana\n",
      "out_edges, node: Guyana, relation: part of, target: South America\n",
      "out_edges, node: Guyana, relation: diplomatic relation, target: Venezuela\n",
      "out_edges, node: Guyana, relation: capital, target: Georgetown\n",
      "out_edges, node: Guyana, relation: official language, target: English\n",
      "out_edges, node: Guyana, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Guyana, relation: different from, target: French Guiana\n",
      "out_edges, node: Guyana, relation: language used, target: Carib\n",
      "in_edges, node: country, relation: instance of, target: country\n",
      "out_edges, node: Indonesia, relation: diplomatic relation, target: Colombia\n",
      "out_edges, node: Indonesia, relation: language used, target: English\n",
      "out_edges, node: Indonesia, relation: religion or worldview, target: Catholicism\n",
      "out_edges, node: Indonesia, relation: shares border with, target: East Timor\n",
      "out_edges, node: Indonesia, relation: instance of, target: presidential system\n",
      "out_edges, node: Indonesia, relation: contains the administrative territorial entity, target: Gorontalo\n",
      "out_edges, node: Indonesia, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Indonesia, relation: continent, target: Asia\n",
      "out_edges, node: Indonesia, relation: basic form of government, target: republic\n",
      "out_edges, node: Indonesia, relation: head of state, target: Joko Widodo\n",
      "out_edges, node: Indonesia, relation: ethnic group, target: Chinese people\n",
      "out_edges, node: Indonesia, relation: part of, target: Southeast Asia\n",
      "out_edges, node: Indonesia, relation: country, target: Indonesia\n",
      "out_edges, node: Indonesia, relation: replaces, target: Dutch East Indies\n",
      "out_edges, node: Indonesia, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Indonesia, relation: located in or next to body of water, target: South China Sea\n",
      "out_edges, node: Indonesia, relation: capital, target: Jakarta\n",
      "out_edges, node: Colombia, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Colombia, relation: shares border with, target: Ecuador\n",
      "out_edges, node: Colombia, relation: diplomatic relation, target: Norway\n",
      "out_edges, node: Colombia, relation: language used, target: Spanish\n",
      "out_edges, node: Colombia, relation: head of state, target: Gustavo Petro\n",
      "out_edges, node: Colombia, relation: capital, target: Bogotá\n",
      "out_edges, node: Colombia, relation: contains the administrative territorial entity, target: Cundinamarca Department\n",
      "out_edges, node: Colombia, relation: country, target: Colombia\n",
      "out_edges, node: Colombia, relation: instance of, target: country\n",
      "out_edges, node: Colombia, relation: head of government, target: Iván Duque\n",
      "out_edges, node: Colombia, relation: continent, target: South America\n",
      "out_edges, node: Colombia, relation: named after, target: Christopher Columbus\n",
      "out_edges, node: Colombia, relation: basic form of government, target: republic\n",
      "out_edges, node: Colombia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Colombia, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Qatar, relation: has part(s), target: Doha\n",
      "out_edges, node: Qatar, relation: country, target: Qatar\n",
      "out_edges, node: Qatar, relation: diplomatic relation, target: Syria\n",
      "out_edges, node: Qatar, relation: shares border with, target: Saudi Arabia\n",
      "out_edges, node: Qatar, relation: member of, target: World Health Organization\n",
      "out_edges, node: Qatar, relation: official religion, target: Islam\n",
      "out_edges, node: Qatar, relation: head of state, target: Tamim bin Hamad Al Thani\n",
      "out_edges, node: Qatar, relation: lowest point, target: Persian Gulf\n",
      "out_edges, node: Qatar, relation: located in/on physical feature, target: Middle East\n",
      "out_edges, node: Qatar, relation: official language, target: Arabic\n",
      "out_edges, node: Qatar, relation: instance of, target: country\n",
      "out_edges, node: Qatar, relation: continent, target: Asia\n",
      "out_edges, node: Qatar, relation: part of, target: West Asia\n",
      "out_edges, node: Qatar, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Italy, relation: member of, target: African Development Bank\n",
      "out_edges, node: Italy, relation: diplomatic relation, target: Armenia\n",
      "out_edges, node: Italy, relation: contains the administrative territorial entity, target: Lazio\n",
      "out_edges, node: Italy, relation: shares border with, target: Austria\n",
      "out_edges, node: Italy, relation: shape, target: boot\n",
      "out_edges, node: Italy, relation: part of, target: European Economic Area\n",
      "out_edges, node: Italy, relation: patron saint, target: Francis of Assisi\n",
      "out_edges, node: Italy, relation: language used, target: German\n",
      "out_edges, node: Italy, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Italy, relation: highest point, target: Mont Blanc\n",
      "out_edges, node: Italy, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Italy, relation: capital, target: Rome\n",
      "out_edges, node: Italy, relation: currency, target: euro\n",
      "out_edges, node: Italy, relation: located in or next to body of water, target: Tyrrhenian Sea\n",
      "out_edges, node: Italy, relation: continent, target: Europe\n",
      "out_edges, node: Italy, relation: instance of, target: country\n",
      "out_edges, node: Italy, relation: official language, target: Italian\n",
      "out_edges, node: Italy, relation: head of state, target: Sergio Mattarella\n",
      "out_edges, node: Italy, relation: located in/on physical feature, target: Mediterranean Basin\n",
      "out_edges, node: Italy, relation: country, target: Italy\n",
      "out_edges, node: Italy, relation: head of government, target: Giorgia Meloni\n",
      "out_edges, node: Italy, relation: central bank, target: Bank of Italy\n",
      "in_edges, node: African Development Bank, relation: member of, target: African Development Bank\n",
      "out_edges, node: Nigeria, relation: diplomatic relation, target: Malaysia\n",
      "out_edges, node: Nigeria, relation: ethnic group, target: Hausa people\n",
      "out_edges, node: Nigeria, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Nigeria, relation: country, target: Nigeria\n",
      "out_edges, node: Nigeria, relation: official language, target: English\n",
      "out_edges, node: Nigeria, relation: language used, target: Kanuri\n",
      "out_edges, node: Nigeria, relation: contains the administrative territorial entity, target: Rivers State\n",
      "out_edges, node: Nigeria, relation: shares border with, target: Cameroon\n",
      "out_edges, node: Nigeria, relation: named by, target: Flora Shaw\n",
      "out_edges, node: Nigeria, relation: continent, target: Africa\n",
      "out_edges, node: Nigeria, relation: capital, target: Abuja\n",
      "out_edges, node: Nigeria, relation: head of government, target: Muhammadu Buhari\n",
      "out_edges, node: Nigeria, relation: part of, target: West Africa\n",
      "out_edges, node: Nigeria, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Nigeria, relation: named after, target: Niger River\n",
      "out_edges, node: Nigeria, relation: instance of, target: country\n",
      "out_edges, node: Malaysia, relation: contains the administrative territorial entity, target: Terengganu\n",
      "out_edges, node: Malaysia, relation: diplomatic relation, target: Ethiopia\n",
      "out_edges, node: Malaysia, relation: capital, target: Kuala Lumpur\n",
      "out_edges, node: Malaysia, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Malaysia, relation: instance of, target: country\n",
      "out_edges, node: Malaysia, relation: continent, target: Asia\n",
      "out_edges, node: Malaysia, relation: ethnic group, target: Arabs\n",
      "out_edges, node: Malaysia, relation: language used, target: English\n",
      "out_edges, node: Malaysia, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Malaysia, relation: head of government, target: Anwar Ibrahim\n",
      "out_edges, node: Malaysia, relation: official religion, target: Islam\n",
      "out_edges, node: Malaysia, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Malaysia, relation: part of, target: Southeast Asia\n",
      "out_edges, node: Malaysia, relation: country, target: Malaysia\n",
      "out_edges, node: Malaysia, relation: official language, target: Malay\n",
      "out_edges, node: Malaysia, relation: different from, target: Melanesia\n",
      "in_edges, node: Malaysia, relation: shares border with, target: Malaysia\n",
      "out_edges, node: Croatia, relation: diplomatic relation, target: Greece\n",
      "out_edges, node: Croatia, relation: located in or next to body of water, target: Adriatic Sea\n",
      "out_edges, node: Croatia, relation: shares border with, target: Bosnia and Herzegovina\n",
      "out_edges, node: Croatia, relation: language used, target: Serbian\n",
      "out_edges, node: Croatia, relation: member of, target: Council of Europe\n",
      "out_edges, node: Croatia, relation: replaces, target: Socialist Federal Republic of Yugoslavia\n",
      "out_edges, node: Croatia, relation: official language, target: Croatian\n",
      "out_edges, node: Croatia, relation: currency, target: euro\n",
      "out_edges, node: Croatia, relation: head of government, target: Andrej Plenković\n",
      "out_edges, node: Croatia, relation: country, target: Croatia\n",
      "out_edges, node: Croatia, relation: head of state, target: Zoran Milanović\n",
      "out_edges, node: Croatia, relation: located in/on physical feature, target: Balkans\n",
      "out_edges, node: Croatia, relation: contains the administrative territorial entity, target: Zagreb\n",
      "out_edges, node: Croatia, relation: basic form of government, target: republic\n",
      "out_edges, node: Croatia, relation: continent, target: Europe\n",
      "out_edges, node: Croatia, relation: instance of, target: country\n",
      "out_edges, node: Greece, relation: member of, target: European Space Agency\n",
      "out_edges, node: Greece, relation: diplomatic relation, target: New Zealand\n",
      "out_edges, node: Greece, relation: head of government, target: Kyriakos Mitsotakis\n",
      "out_edges, node: Greece, relation: language used, target: Greek\n",
      "out_edges, node: Greece, relation: part of, target: European Economic Area\n",
      "out_edges, node: Greece, relation: named after, target: Greeks\n",
      "out_edges, node: Greece, relation: located in/on physical feature, target: Balkans\n",
      "out_edges, node: Greece, relation: capital, target: Athens\n",
      "out_edges, node: Greece, relation: public holiday, target: New Year\n",
      "out_edges, node: Greece, relation: continent, target: Europe\n",
      "out_edges, node: Greece, relation: central bank, target: Bank of Greece\n",
      "out_edges, node: Greece, relation: contains the administrative territorial entity, target: Epirus Region\n",
      "out_edges, node: Greece, relation: instance of, target: country\n",
      "out_edges, node: Greece, relation: shares border with, target: Turkey\n",
      "out_edges, node: Greece, relation: currency, target: euro\n",
      "out_edges, node: Greece, relation: country, target: Greece\n",
      "out_edges, node: Greece, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Greece, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Greece, relation: highest point, target: Mount Olympus\n",
      "out_edges, node: Grenada, relation: diplomatic relation, target: Libya\n",
      "out_edges, node: Grenada, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Grenada, relation: different from, target: Granada\n",
      "out_edges, node: Grenada, relation: located in/on physical feature, target: Lesser Antilles\n",
      "out_edges, node: Grenada, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Grenada, relation: instance of, target: country\n",
      "out_edges, node: Grenada, relation: language used, target: English\n",
      "out_edges, node: Grenada, relation: capital, target: St. George's\n",
      "out_edges, node: Grenada, relation: shares border with, target: Venezuela\n",
      "out_edges, node: Grenada, relation: part of, target: Windward Islands\n",
      "out_edges, node: Grenada, relation: continent, target: North America\n",
      "out_edges, node: Grenada, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Grenada, relation: country, target: Grenada\n",
      "out_edges, node: Libya, relation: member of, target: United Nations\n",
      "out_edges, node: Libya, relation: shares border with, target: Chad\n",
      "out_edges, node: Libya, relation: diplomatic relation, target: Montenegro\n",
      "out_edges, node: Libya, relation: capital, target: Tripoli\n",
      "out_edges, node: Libya, relation: continent, target: Africa\n",
      "out_edges, node: Libya, relation: located in/on physical feature, target: North Africa\n",
      "out_edges, node: Libya, relation: official language, target: Arabic\n",
      "out_edges, node: Libya, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Libya, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Libya, relation: basic form of government, target: republic\n",
      "out_edges, node: Libya, relation: contains the administrative territorial entity, target: Benghazi\n",
      "out_edges, node: Libya, relation: country, target: Libya\n",
      "out_edges, node: Libya, relation: instance of, target: country\n",
      "out_edges, node: Ethiopia, relation: member of, target: United Nations\n",
      "out_edges, node: Ethiopia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Ethiopia, relation: contains the administrative territorial entity, target: Dire Dawa\n",
      "out_edges, node: Ethiopia, relation: diplomatic relation, target: Australia\n",
      "out_edges, node: Ethiopia, relation: language used, target: Tigrinya\n",
      "out_edges, node: Ethiopia, relation: shares border with, target: Arab League\n",
      "out_edges, node: Ethiopia, relation: capital, target: Addis Ababa\n",
      "out_edges, node: Ethiopia, relation: patron saint, target: Saint George\n",
      "out_edges, node: Ethiopia, relation: head of government, target: Abiy Ahmed Ali\n",
      "out_edges, node: Ethiopia, relation: ethnic group, target: Somalis\n",
      "out_edges, node: Ethiopia, relation: part of, target: East Africa\n",
      "out_edges, node: Ethiopia, relation: country, target: Ethiopia\n",
      "out_edges, node: Ethiopia, relation: instance of, target: country\n",
      "out_edges, node: Ethiopia, relation: continent, target: Africa\n",
      "in_edges, node: United Nations, relation: member of, target: United Nations\n",
      "in_edges, node: United Nations, relation: part of, target: United Nations\n",
      "out_edges, node: Portugal, relation: diplomatic relation, target: Cape Verde\n",
      "out_edges, node: Portugal, relation: part of, target: European Union\n",
      "out_edges, node: Portugal, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Portugal, relation: contains the administrative territorial entity, target: Madeira\n",
      "out_edges, node: Portugal, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Portugal, relation: capital, target: Lisbon\n",
      "out_edges, node: Portugal, relation: head of government, target: António Costa\n",
      "out_edges, node: Portugal, relation: country, target: Portugal\n",
      "out_edges, node: Portugal, relation: central bank, target: Banco de Portugal\n",
      "out_edges, node: Portugal, relation: head of state, target: Marcelo Rebelo de Sousa\n",
      "out_edges, node: Portugal, relation: currency, target: euro\n",
      "out_edges, node: Portugal, relation: official language, target: Portuguese\n",
      "out_edges, node: Portugal, relation: located in/on physical feature, target: Iberian Peninsula\n",
      "out_edges, node: Portugal, relation: instance of, target: country\n",
      "out_edges, node: Portugal, relation: basic form of government, target: republic\n",
      "out_edges, node: Portugal, relation: language used, target: Galician\n",
      "out_edges, node: Portugal, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Portugal, relation: continent, target: Europe\n",
      "out_edges, node: Portugal, relation: patron saint, target: Saint George\n",
      "out_edges, node: Cape Verde, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Cape Verde, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Cape Verde, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Cape Verde, relation: instance of, target: country\n",
      "out_edges, node: Cape Verde, relation: continent, target: Africa\n",
      "out_edges, node: Cape Verde, relation: part of, target: West Africa\n",
      "out_edges, node: Cape Verde, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Cape Verde, relation: country, target: Cape Verde\n",
      "out_edges, node: Cape Verde, relation: official language, target: Portuguese\n",
      "out_edges, node: Cape Verde, relation: capital, target: Praia\n",
      "out_edges, node: Belgium, relation: shares border with, target: Netherlands\n",
      "out_edges, node: Belgium, relation: diplomatic relation, target: Russia\n",
      "out_edges, node: Belgium, relation: patron saint, target: Joseph\n",
      "out_edges, node: Belgium, relation: member of, target: African Development Bank\n",
      "out_edges, node: Belgium, relation: public holiday, target: Assumption of Mary\n",
      "out_edges, node: Belgium, relation: has part(s), target: Brussels-Capital Region\n",
      "out_edges, node: Belgium, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Belgium, relation: part of, target: United Nations\n",
      "out_edges, node: Belgium, relation: instance of, target: country\n",
      "out_edges, node: Belgium, relation: official language, target: German\n",
      "out_edges, node: Belgium, relation: capital, target: City of Brussels\n",
      "out_edges, node: Belgium, relation: currency, target: euro\n",
      "out_edges, node: Belgium, relation: official symbol, target: Papaver rhoeas\n",
      "out_edges, node: Belgium, relation: central bank, target: National Bank of Belgium\n",
      "out_edges, node: Belgium, relation: language used, target: Flemish\n",
      "out_edges, node: Belgium, relation: continent, target: Europe\n",
      "out_edges, node: Belgium, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Belgium, relation: located in or next to body of water, target: North Sea\n",
      "out_edges, node: Belgium, relation: located in/on physical feature, target: Western Europe\n",
      "out_edges, node: Belgium, relation: head of state, target: Philippe I of Belgium\n",
      "out_edges, node: Belgium, relation: country, target: Belgium\n",
      "out_edges, node: Netherlands, relation: continent, target: Europe\n",
      "out_edges, node: Netherlands, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Netherlands, relation: member of, target: International Civil Aviation Organization\n",
      "out_edges, node: Netherlands, relation: contains the administrative territorial entity, target: Fryslân\n",
      "out_edges, node: Netherlands, relation: language used, target: Dutch\n",
      "out_edges, node: Netherlands, relation: central bank, target: De Nederlandsche Bank\n",
      "out_edges, node: Netherlands, relation: capital, target: Amsterdam\n",
      "out_edges, node: Netherlands, relation: located in or next to body of water, target: Caribbean Sea\n",
      "out_edges, node: Netherlands, relation: owner of, target: Het Loo Palace\n",
      "out_edges, node: Netherlands, relation: head of state, target: Willem-Alexander of the Netherlands\n",
      "out_edges, node: Netherlands, relation: part of, target: Kingdom of the Netherlands\n",
      "out_edges, node: Netherlands, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Netherlands, relation: head of government, target: Mark Rutte\n",
      "out_edges, node: Netherlands, relation: instance of, target: country\n",
      "out_edges, node: Netherlands, relation: currency, target: euro\n",
      "in_edges, node: Netherlands, relation: shares border with, target: Netherlands\n",
      "out_edges, node: Japan, relation: diplomatic relation, target: Lithuania\n",
      "out_edges, node: Japan, relation: significant event, target: Korean War\n",
      "out_edges, node: Japan, relation: located in or next to body of water, target: Sea of Okhotsk\n",
      "out_edges, node: Japan, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Japan, relation: highest judicial authority, target: Supreme Court of Japan\n",
      "out_edges, node: Japan, relation: contains the administrative territorial entity, target: Nara Prefecture\n",
      "out_edges, node: Japan, relation: central bank, target: Bank of Japan\n",
      "out_edges, node: Japan, relation: archives at, target: National Archives of Japan\n",
      "out_edges, node: Japan, relation: language used, target: Japanese\n",
      "out_edges, node: Japan, relation: religion or worldview, target: Christianity\n",
      "out_edges, node: Japan, relation: instance of, target: country\n",
      "out_edges, node: Japan, relation: shares border with, target: South Korea\n",
      "out_edges, node: Japan, relation: capital, target: Tokyo\n",
      "out_edges, node: Japan, relation: ethnic group, target: Chinese people\n",
      "out_edges, node: Japan, relation: official symbol, target: Chrysanthemum morifolium\n",
      "out_edges, node: Japan, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Japan, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Japan, relation: part of, target: East Asia\n",
      "out_edges, node: Japan, relation: highest point, target: Mount Fuji\n",
      "out_edges, node: Japan, relation: currency, target: yen\n",
      "out_edges, node: Japan, relation: head of state, target: Naruhito\n",
      "out_edges, node: Japan, relation: country, target: Japan\n",
      "out_edges, node: Japan, relation: head of government, target: Fumio Kishida\n",
      "out_edges, node: Japan, relation: continent, target: Asia\n",
      "out_edges, node: Lithuania, relation: diplomatic relation, target: Latvia\n",
      "out_edges, node: Lithuania, relation: member of, target: Council of Europe\n",
      "out_edges, node: Lithuania, relation: contains the administrative territorial entity, target: Vilnius County\n",
      "out_edges, node: Lithuania, relation: shares border with, target: Russia\n",
      "out_edges, node: Lithuania, relation: official language, target: Lithuanian\n",
      "out_edges, node: Lithuania, relation: patron saint, target: Saint Casimir\n",
      "out_edges, node: Lithuania, relation: currency, target: euro\n",
      "out_edges, node: Lithuania, relation: capital, target: Vilnius\n",
      "out_edges, node: Lithuania, relation: country, target: Lithuania\n",
      "out_edges, node: Lithuania, relation: located in/on physical feature, target: Baltic states\n",
      "out_edges, node: Lithuania, relation: part of, target: Northern Europe\n",
      "out_edges, node: Lithuania, relation: language used, target: Karaim\n",
      "out_edges, node: Lithuania, relation: instance of, target: country\n",
      "out_edges, node: Lithuania, relation: head of state, target: Gitanas Nausėda\n",
      "out_edges, node: Lithuania, relation: continent, target: Europe\n",
      "out_edges, node: Lithuania, relation: located in or next to body of water, target: Baltic Sea\n",
      "in_edges, node: Lithuania, relation: designated as terrorist by, target: Lithuania\n",
      "out_edges, node: United Kingdom, relation: diplomatic relation, target: Malta\n",
      "out_edges, node: United Kingdom, relation: member of, target: European Union\n",
      "out_edges, node: United Kingdom, relation: country, target: United Kingdom\n",
      "out_edges, node: United Kingdom, relation: different from, target: Great Britain\n",
      "out_edges, node: United Kingdom, relation: public holiday, target: Christmas\n",
      "out_edges, node: United Kingdom, relation: located in/on physical feature, target: Ireland\n",
      "out_edges, node: United Kingdom, relation: shares border with, target: Republic of Ireland\n",
      "out_edges, node: United Kingdom, relation: language used, target: Cornish\n",
      "out_edges, node: United Kingdom, relation: contains the administrative territorial entity, target: Scotland\n",
      "out_edges, node: United Kingdom, relation: instance of, target: country\n",
      "out_edges, node: United Kingdom, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: United Kingdom, relation: head of government, target: Rishi Sunak\n",
      "out_edges, node: United Kingdom, relation: capital, target: London\n",
      "out_edges, node: United Kingdom, relation: official language, target: English\n",
      "out_edges, node: United Kingdom, relation: currency, target: pound sterling\n",
      "out_edges, node: United Kingdom, relation: highest point, target: Ben Nevis\n",
      "out_edges, node: United Kingdom, relation: part of, target: Commonwealth of Nations\n",
      "out_edges, node: United Kingdom, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: United Kingdom, relation: central bank, target: Bank of England\n",
      "out_edges, node: United Kingdom, relation: legislative body, target: Parliament of the United Kingdom\n",
      "out_edges, node: United Kingdom, relation: has works in the collection, target: Australian National Maritime Museum\n",
      "out_edges, node: United Kingdom, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: United Kingdom, relation: continent, target: Europe\n",
      "in_edges, node: United Kingdom, relation: separated from, target: United Kingdom\n",
      "in_edges, node: United Kingdom, relation: located in the administrative territorial entity, target: United Kingdom\n",
      "out_edges, node: Malta, relation: diplomatic relation, target: Russia\n",
      "out_edges, node: Malta, relation: continent, target: Europe\n",
      "out_edges, node: Malta, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Malta, relation: part of, target: European Economic Area\n",
      "out_edges, node: Malta, relation: official language, target: English\n",
      "out_edges, node: Malta, relation: currency, target: euro\n",
      "out_edges, node: Malta, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Malta, relation: named after, target: honey\n",
      "out_edges, node: Malta, relation: shares border with, target: Italy\n",
      "out_edges, node: Malta, relation: language used, target: Maltese\n",
      "out_edges, node: Malta, relation: country, target: Malta\n",
      "out_edges, node: Malta, relation: instance of, target: country\n",
      "out_edges, node: Malta, relation: capital, target: Valletta\n",
      "out_edges, node: Malta, relation: religion or worldview, target: Christianity\n",
      "in_edges, node: International Finance Corporation, relation: member of, target: International Finance Corporation\n",
      "in_edges, node: Terengganu, relation: contains the administrative territorial entity, target: Terengganu\n",
      "out_edges, node: Federated States of Micronesia, relation: shares border with, target: United States of America\n",
      "out_edges, node: Federated States of Micronesia, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Federated States of Micronesia, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Federated States of Micronesia, relation: capital, target: Palikir\n",
      "out_edges, node: Federated States of Micronesia, relation: member of, target: United Nations\n",
      "out_edges, node: Federated States of Micronesia, relation: language used, target: English\n",
      "out_edges, node: Federated States of Micronesia, relation: different from, target: Micronesia\n",
      "out_edges, node: Federated States of Micronesia, relation: country, target: Federated States of Micronesia\n",
      "out_edges, node: Federated States of Micronesia, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Federated States of Micronesia, relation: currency, target: United States dollar\n",
      "out_edges, node: Federated States of Micronesia, relation: instance of, target: country\n",
      "out_edges, node: Uganda, relation: shares border with, target: South Sudan\n",
      "out_edges, node: Uganda, relation: member of, target: World Health Organization\n",
      "out_edges, node: Uganda, relation: diplomatic relation, target: Australia\n",
      "out_edges, node: Uganda, relation: continent, target: Africa\n",
      "out_edges, node: Uganda, relation: language used, target: Acholi\n",
      "out_edges, node: Uganda, relation: instance of, target: country\n",
      "out_edges, node: Uganda, relation: part of, target: East Africa\n",
      "out_edges, node: Uganda, relation: head of state, target: Yoweri Museveni\n",
      "out_edges, node: Uganda, relation: official language, target: English\n",
      "out_edges, node: Uganda, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Uganda, relation: capital, target: Kampala\n",
      "out_edges, node: Uganda, relation: country, target: Uganda\n",
      "out_edges, node: South Sudan, relation: official language, target: English\n",
      "out_edges, node: South Sudan, relation: diplomatic relation, target: Brazil\n",
      "out_edges, node: South Sudan, relation: member of, target: World Health Organization\n",
      "out_edges, node: South Sudan, relation: shares border with, target: Arab League\n",
      "out_edges, node: South Sudan, relation: head of government, target: Salva Kiir Mayardit\n",
      "out_edges, node: South Sudan, relation: separated from, target: Sudan\n",
      "out_edges, node: South Sudan, relation: instance of, target: country\n",
      "out_edges, node: South Sudan, relation: language used, target: Dinka\n",
      "out_edges, node: South Sudan, relation: country, target: South Sudan\n",
      "out_edges, node: South Sudan, relation: part of, target: East Africa\n",
      "out_edges, node: South Sudan, relation: continent, target: Africa\n",
      "out_edges, node: South Sudan, relation: capital, target: Juba\n",
      "out_edges, node: Guinea-Bissau, relation: official language, target: Portuguese\n",
      "out_edges, node: Guinea-Bissau, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Guinea-Bissau, relation: member of, target: Interpol\n",
      "out_edges, node: Guinea-Bissau, relation: located in/on physical feature, target: West Africa\n",
      "out_edges, node: Guinea-Bissau, relation: different from, target: Guinea\n",
      "out_edges, node: Guinea-Bissau, relation: language used, target: Mandinka\n",
      "out_edges, node: Guinea-Bissau, relation: capital, target: Bissau\n",
      "out_edges, node: Guinea-Bissau, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Guinea-Bissau, relation: country, target: Guinea-Bissau\n",
      "out_edges, node: Guinea-Bissau, relation: continent, target: Africa\n",
      "out_edges, node: Guinea-Bissau, relation: instance of, target: country\n",
      "out_edges, node: Guinea-Bissau, relation: shares border with, target: Senegal\n",
      "in_edges, node: Portuguese, relation: official language, target: Portuguese\n",
      "in_edges, node: Portuguese, relation: language used, target: Portuguese\n",
      "in_edges, node: Portuguese, relation: ethnic group, target: Portuguese\n",
      "out_edges, node: Pakistan, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Pakistan, relation: diplomatic relation, target: Brazil\n",
      "out_edges, node: Pakistan, relation: language used, target: Brahui\n",
      "out_edges, node: Pakistan, relation: ethnic group, target: Pashtuns\n",
      "out_edges, node: Pakistan, relation: capital, target: Islamabad\n",
      "out_edges, node: Pakistan, relation: named after, target: Sindh\n",
      "out_edges, node: Pakistan, relation: contains the administrative territorial entity, target: Khyber Pakhtunkhwa\n",
      "out_edges, node: Pakistan, relation: official language, target: Urdu\n",
      "out_edges, node: Pakistan, relation: religion or worldview, target: Islam\n",
      "out_edges, node: Pakistan, relation: country, target: Pakistan\n",
      "out_edges, node: Pakistan, relation: highest point, target: K2\n",
      "out_edges, node: Pakistan, relation: lowest point, target: Arabian Sea\n",
      "out_edges, node: Pakistan, relation: located in/on physical feature, target: South Asia\n",
      "out_edges, node: Pakistan, relation: instance of, target: country\n",
      "out_edges, node: Pakistan, relation: continent, target: Asia\n",
      "out_edges, node: Pakistan, relation: shares border with, target: India\n",
      "in_edges, node: International Bank for Reconstruction and Development, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Bhutan, relation: head of state, target: Jigme Khesar Namgyel Wangchuck\n",
      "out_edges, node: Bhutan, relation: diplomatic relation, target: Denmark\n",
      "out_edges, node: Bhutan, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Bhutan, relation: religion or worldview, target: Mahāyāna\n",
      "out_edges, node: Bhutan, relation: continent, target: Asia\n",
      "out_edges, node: Bhutan, relation: capital, target: Thimphu\n",
      "out_edges, node: Bhutan, relation: language used, target: English\n",
      "out_edges, node: Bhutan, relation: different from, target: Bataan\n",
      "out_edges, node: Bhutan, relation: located in/on physical feature, target: South Asia\n",
      "out_edges, node: Bhutan, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Bhutan, relation: instance of, target: country\n",
      "out_edges, node: Bhutan, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Bhutan, relation: country, target: Bhutan\n",
      "in_edges, node: Bhutan, relation: shares border with, target: Bhutan\n",
      "in_edges, node: Jigme Khesar Namgyel Wangchuck, relation: head of state, target: Jigme Khesar Namgyel Wangchuck\n",
      "out_edges, node: Taiwan, relation: diplomatic relation, target: Holy See\n",
      "out_edges, node: Taiwan, relation: basic form of government, target: democracy\n",
      "out_edges, node: Taiwan, relation: instance of, target: country\n",
      "out_edges, node: Taiwan, relation: religion or worldview, target: Buddhism\n",
      "out_edges, node: Taiwan, relation: shares border with, target: People's Republic of China\n",
      "out_edges, node: Taiwan, relation: head of state, target: Tsai Ing-wen\n",
      "out_edges, node: Taiwan, relation: significant event, target: World War II\n",
      "out_edges, node: Taiwan, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Taiwan, relation: contains the administrative territorial entity, target: Taichung\n",
      "out_edges, node: Taiwan, relation: located in or next to body of water, target: East China Sea\n",
      "out_edges, node: Taiwan, relation: continent, target: Asia\n",
      "out_edges, node: Taiwan, relation: part of, target: East Asia\n",
      "out_edges, node: Taiwan, relation: country, target: Taiwan\n",
      "out_edges, node: Taiwan, relation: language used, target: Hakka\n",
      "in_edges, node: Taiwan, relation: territory claimed by, target: Taiwan\n",
      "in_edges, node: Holy See, relation: diplomatic relation, target: Holy See\n",
      "out_edges, node: Poland, relation: patron saint, target: Stanislaus of Szczepanów\n",
      "out_edges, node: Poland, relation: ethnic group, target: Germans\n",
      "out_edges, node: Poland, relation: diplomatic relation, target: Cyprus\n",
      "out_edges, node: Poland, relation: language used, target: Esperanto\n",
      "out_edges, node: Poland, relation: member of, target: UNESCO\n",
      "out_edges, node: Poland, relation: shares border with, target: Czech Republic\n",
      "out_edges, node: Poland, relation: public holiday, target: International Workers' Day\n",
      "out_edges, node: Poland, relation: head of government, target: Mateusz Morawiecki\n",
      "out_edges, node: Poland, relation: contains the administrative territorial entity, target: Łódź Voivodeship\n",
      "out_edges, node: Poland, relation: central bank, target: Narodowy Bank Polski\n",
      "out_edges, node: Poland, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Poland, relation: continent, target: Europe\n",
      "out_edges, node: Poland, relation: part of, target: Central Europe\n",
      "out_edges, node: Poland, relation: country, target: Poland\n",
      "out_edges, node: Poland, relation: basic form of government, target: parliamentary system\n",
      "out_edges, node: Poland, relation: located in or next to body of water, target: Baltic Sea\n",
      "out_edges, node: Poland, relation: instance of, target: country\n",
      "out_edges, node: Poland, relation: official language, target: Polish\n",
      "out_edges, node: Poland, relation: head of state, target: Andrzej Duda\n",
      "out_edges, node: Poland, relation: capital, target: Warsaw\n",
      "in_edges, node: Stanislaus of Szczepanów, relation: patron saint, target: Stanislaus of Szczepanów\n",
      "out_edges, node: Republic of the Congo, relation: language used, target: French\n",
      "out_edges, node: Republic of the Congo, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Republic of the Congo, relation: member of, target: African Development Bank\n",
      "out_edges, node: Republic of the Congo, relation: named after, target: Congo\n",
      "out_edges, node: Republic of the Congo, relation: instance of, target: country\n",
      "out_edges, node: Republic of the Congo, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Republic of the Congo, relation: country, target: Republic of the Congo\n",
      "out_edges, node: Republic of the Congo, relation: capital, target: Brazzaville\n",
      "out_edges, node: Republic of the Congo, relation: head of state, target: Denis Sassou-Nguesso\n",
      "out_edges, node: Republic of the Congo, relation: located in/on physical feature, target: Central Africa\n",
      "out_edges, node: Republic of the Congo, relation: shares border with, target: Central African Republic\n",
      "out_edges, node: Republic of the Congo, relation: continent, target: Africa\n",
      "out_edges, node: Republic of the Congo, relation: twinned administrative body, target: Seto\n",
      "in_edges, node: French, relation: language used, target: French\n",
      "in_edges, node: French, relation: official language, target: French\n",
      "in_edges, node: French, relation: ethnic group, target: French\n",
      "out_edges, node: Kosovo, relation: diplomatic relation, target: Malaysia\n",
      "out_edges, node: Kosovo, relation: country, target: Kosovo\n",
      "out_edges, node: Kosovo, relation: official language, target: Albanian\n",
      "out_edges, node: Kosovo, relation: shares border with, target: North Macedonia\n",
      "out_edges, node: Kosovo, relation: currency, target: euro\n",
      "out_edges, node: Kosovo, relation: capital, target: Prishtina\n",
      "out_edges, node: Kosovo, relation: location, target: Balkans\n",
      "out_edges, node: Kosovo, relation: instance of, target: country\n",
      "out_edges, node: Kosovo, relation: continent, target: Europe\n",
      "in_edges, node: English, relation: official language, target: English\n",
      "in_edges, node: English, relation: language used, target: English\n",
      "out_edges, node: Paraguay, relation: contains the administrative territorial entity, target: Presidente Hayes\n",
      "out_edges, node: Paraguay, relation: part of, target: South America\n",
      "out_edges, node: Paraguay, relation: diplomatic relation, target: Israel\n",
      "out_edges, node: Paraguay, relation: member of, target: Interpol\n",
      "out_edges, node: Paraguay, relation: language used, target: Spanish\n",
      "out_edges, node: Paraguay, relation: shares border with, target: Bolivia\n",
      "out_edges, node: Paraguay, relation: instance of, target: country\n",
      "out_edges, node: Paraguay, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Paraguay, relation: country, target: Paraguay\n",
      "out_edges, node: Paraguay, relation: capital, target: Asunción\n",
      "in_edges, node: Presidente Hayes, relation: contains the administrative territorial entity, target: Presidente Hayes\n",
      "out_edges, node: Burundi, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Burundi, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Burundi, relation: shares border with, target: Rwanda\n",
      "out_edges, node: Burundi, relation: named after, target: Kirundi\n",
      "out_edges, node: Burundi, relation: language used, target: French\n",
      "out_edges, node: Burundi, relation: part of, target: East Africa\n",
      "out_edges, node: Burundi, relation: headquarters location, target: Bujumbura\n",
      "out_edges, node: Burundi, relation: instance of, target: country\n",
      "out_edges, node: Burundi, relation: continent, target: Africa\n",
      "out_edges, node: Burundi, relation: lowest point, target: Lake Tanganyika\n",
      "out_edges, node: Burundi, relation: country, target: Burundi\n",
      "out_edges, node: Burundi, relation: official language, target: English\n",
      "out_edges, node: Russia, relation: diplomatic relation, target: Nigeria\n",
      "out_edges, node: Russia, relation: designated as terrorist by, target: North Atlantic Treaty Organization\n",
      "out_edges, node: Russia, relation: contains the administrative territorial entity, target: Krasnoyarsk Krai\n",
      "out_edges, node: Russia, relation: language used, target: Skolt Sami\n",
      "out_edges, node: Russia, relation: member of, target: International Development Association\n",
      "out_edges, node: Russia, relation: located in or next to body of water, target: Arctic Ocean\n",
      "out_edges, node: Russia, relation: shares border with, target: Belarus\n",
      "out_edges, node: Russia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Russia, relation: head of state, target: Vladimir Putin\n",
      "out_edges, node: Russia, relation: located in/on physical feature, target: Eurasia\n",
      "out_edges, node: Russia, relation: replaces, target: Russian Soviet Federative Socialist Republic\n",
      "out_edges, node: Russia, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Russia, relation: ethnic group, target: Ukrainians\n",
      "out_edges, node: Russia, relation: continent, target: Europe\n",
      "out_edges, node: Russia, relation: named after, target: Kievan Rus'\n",
      "out_edges, node: Russia, relation: office held by head of state, target: President of Russia\n",
      "out_edges, node: Russia, relation: patron saint, target: Andrew the Apostle\n",
      "out_edges, node: Russia, relation: history of topic, target: history of Russia\n",
      "out_edges, node: Russia, relation: official language, target: Russian\n",
      "out_edges, node: Russia, relation: separated from, target: Soviet Union\n",
      "out_edges, node: Russia, relation: highest point, target: Mount Elbrus\n",
      "out_edges, node: Russia, relation: executive body, target: Government of Russia\n",
      "out_edges, node: Russia, relation: instance of, target: country\n",
      "out_edges, node: Russia, relation: central bank, target: Central Bank of Russia\n",
      "out_edges, node: Russia, relation: country, target: Russia\n",
      "out_edges, node: Russia, relation: official observer status in organisation, target: CERN\n",
      "out_edges, node: Russia, relation: head of government, target: Mikhail Mishustin\n",
      "out_edges, node: Russia, relation: different from, target: Russian Empire\n",
      "out_edges, node: Russia, relation: participant in, target: Russian invasion of Ukraine\n",
      "out_edges, node: São Tomé and Príncipe, relation: continent, target: Africa\n",
      "out_edges, node: São Tomé and Príncipe, relation: diplomatic relation, target: Taiwan\n",
      "out_edges, node: São Tomé and Príncipe, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: São Tomé and Príncipe, relation: instance of, target: republic\n",
      "out_edges, node: São Tomé and Príncipe, relation: located in/on physical feature, target: Central Africa\n",
      "out_edges, node: São Tomé and Príncipe, relation: country, target: São Tomé and Príncipe\n",
      "out_edges, node: São Tomé and Príncipe, relation: capital, target: São Tomé\n",
      "out_edges, node: São Tomé and Príncipe, relation: shares border with, target: Gabon\n",
      "out_edges, node: São Tomé and Príncipe, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: São Tomé and Príncipe, relation: language used, target: Portuguese\n",
      "in_edges, node: Africa, relation: continent, target: Africa\n",
      "in_edges, node: Africa, relation: diplomatic relation, target: Africa\n",
      "out_edges, node: Australia, relation: diplomatic relation, target: Afghanistan\n",
      "out_edges, node: Australia, relation: official symbol, target: opal\n",
      "out_edges, node: Australia, relation: ethnic group, target: Indians\n",
      "out_edges, node: Australia, relation: language used, target: Urdu\n",
      "out_edges, node: Australia, relation: instance of, target: country\n",
      "out_edges, node: Australia, relation: member of, target: Asia-Pacific Economic Cooperation\n",
      "out_edges, node: Australia, relation: contains the administrative territorial entity, target: Queensland\n",
      "out_edges, node: Australia, relation: participant in, target: World War I\n",
      "out_edges, node: Australia, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Australia, relation: part of, target: Commonwealth of Nations\n",
      "out_edges, node: Australia, relation: official language, target: English\n",
      "out_edges, node: Australia, relation: shares border with, target: Indonesia\n",
      "out_edges, node: Australia, relation: history of topic, target: history of Australia\n",
      "out_edges, node: Australia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Australia, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Australia, relation: located in or next to body of water, target: Tasman Sea\n",
      "out_edges, node: Australia, relation: head of government, target: Anthony Albanese\n",
      "out_edges, node: Australia, relation: basic form of government, target: federation\n",
      "out_edges, node: Australia, relation: central bank, target: Reserve Bank of Australia\n",
      "out_edges, node: Australia, relation: country, target: Australia\n",
      "out_edges, node: Australia, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Australia, relation: executive body, target: Australian Government\n",
      "out_edges, node: Australia, relation: capital, target: Canberra\n",
      "out_edges, node: Afghanistan, relation: member of, target: United Nations\n",
      "out_edges, node: Afghanistan, relation: official language, target: Arabic\n",
      "out_edges, node: Afghanistan, relation: diplomatic relation, target: Iran\n",
      "out_edges, node: Afghanistan, relation: ethnic group, target: Hazaras\n",
      "out_edges, node: Afghanistan, relation: official religion, target: Islam\n",
      "out_edges, node: Afghanistan, relation: shares border with, target: Tajikistan\n",
      "out_edges, node: Afghanistan, relation: lowest point, target: Amu Darya\n",
      "out_edges, node: Afghanistan, relation: country, target: Afghanistan\n",
      "out_edges, node: Afghanistan, relation: language used, target: Dari\n",
      "out_edges, node: Afghanistan, relation: continent, target: Asia\n",
      "out_edges, node: Afghanistan, relation: capital, target: Kabul\n",
      "out_edges, node: Afghanistan, relation: contains the administrative territorial entity, target: Helmand\n",
      "out_edges, node: Afghanistan, relation: basic form of government, target: theocracy\n",
      "out_edges, node: Afghanistan, relation: instance of, target: country\n",
      "out_edges, node: Afghanistan, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Nauru, relation: diplomatic relation, target: Taiwan\n",
      "out_edges, node: Nauru, relation: member of, target: World Health Organization\n",
      "out_edges, node: Nauru, relation: located in/on physical feature, target: Micronesia\n",
      "out_edges, node: Nauru, relation: instance of, target: republic\n",
      "out_edges, node: Nauru, relation: country, target: Nauru\n",
      "out_edges, node: Nauru, relation: official language, target: English\n",
      "out_edges, node: Nauru, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Nauru, relation: continent, target: Insular Oceania\n",
      "in_edges, node: Nauru, relation: shares border with, target: Nauru\n",
      "out_edges, node: Sweden, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Sweden, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Sweden, relation: shares border with, target: Norway\n",
      "out_edges, node: Sweden, relation: member of, target: European Space Agency\n",
      "out_edges, node: Sweden, relation: contains the administrative territorial entity, target: Kalmar County\n",
      "out_edges, node: Sweden, relation: language used, target: Northern Sami\n",
      "out_edges, node: Sweden, relation: legislative body, target: Parliament of Sweden\n",
      "out_edges, node: Sweden, relation: head of state, target: Carl XVI Gustaf of Sweden\n",
      "out_edges, node: Sweden, relation: capital, target: Stockholm\n",
      "out_edges, node: Sweden, relation: located in or next to body of water, target: Øresund\n",
      "out_edges, node: Sweden, relation: instance of, target: country\n",
      "out_edges, node: Sweden, relation: lowest point, target: Kristianstad\n",
      "out_edges, node: Sweden, relation: central bank, target: Sveriges Riksbank\n",
      "out_edges, node: Sweden, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Sweden, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Sweden, relation: has part(s), target: Gotland\n",
      "out_edges, node: Sweden, relation: part of, target: European Union\n",
      "out_edges, node: Sweden, relation: owner of, target: Vattenfall\n",
      "out_edges, node: Sweden, relation: country, target: Sweden\n",
      "out_edges, node: Sweden, relation: head of government, target: Ulf Kristersson\n",
      "out_edges, node: Sweden, relation: continent, target: Europe\n",
      "out_edges, node: Sweden, relation: located in/on physical feature, target: Scandinavia\n",
      "in_edges, node: New Year's Day, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Finland, relation: language used, target: Pashto\n",
      "out_edges, node: Finland, relation: part of, target: European Union\n",
      "out_edges, node: Finland, relation: diplomatic relation, target: France\n",
      "out_edges, node: Finland, relation: member of, target: European Space Agency\n",
      "out_edges, node: Finland, relation: contains the administrative territorial entity, target: Uusimaa\n",
      "out_edges, node: Finland, relation: country, target: Finland\n",
      "out_edges, node: Finland, relation: official symbol, target: granite\n",
      "out_edges, node: Finland, relation: patron saint, target: Henry\n",
      "out_edges, node: Finland, relation: shares border with, target: Sweden\n",
      "out_edges, node: Finland, relation: lowest point, target: Baltic Sea\n",
      "out_edges, node: Finland, relation: legislative body, target: Parliament of Finland\n",
      "out_edges, node: Finland, relation: official language, target: Finnish\n",
      "out_edges, node: Finland, relation: capital, target: Helsinki\n",
      "out_edges, node: Finland, relation: participant in, target: Winter War\n",
      "out_edges, node: Finland, relation: instance of, target: country\n",
      "out_edges, node: Finland, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Finland, relation: religion or worldview, target: Evangelical Lutheran Church of Finland\n",
      "out_edges, node: Finland, relation: currency, target: euro\n",
      "out_edges, node: Finland, relation: central bank, target: Bank of Finland\n",
      "out_edges, node: Finland, relation: head of state, target: Sauli Niinistö\n",
      "out_edges, node: Finland, relation: continent, target: Europe\n",
      "in_edges, node: Pashto, relation: language used, target: Pashto\n",
      "in_edges, node: Pashto, relation: official language, target: Pashto\n",
      "out_edges, node: Guinea, relation: shares border with, target: Senegal\n",
      "out_edges, node: Guinea, relation: instance of, target: country\n",
      "out_edges, node: Guinea, relation: located in/on physical feature, target: West Africa\n",
      "out_edges, node: Guinea, relation: replaces, target: French West Africa\n",
      "out_edges, node: Guinea, relation: official language, target: French\n",
      "out_edges, node: Guinea, relation: diplomatic relation, target: Sweden\n",
      "out_edges, node: Guinea, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Guinea, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Guinea, relation: basic form of government, target: republic\n",
      "out_edges, node: Guinea, relation: capital, target: Conakry\n",
      "out_edges, node: Guinea, relation: country, target: Guinea\n",
      "out_edges, node: Guinea, relation: continent, target: Africa\n",
      "in_edges, node: Guinea, relation: different from, target: Guinea\n",
      "out_edges, node: Morocco, relation: diplomatic relation, target: Netherlands\n",
      "out_edges, node: Morocco, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Morocco, relation: public holiday, target: Eid al-Fitr\n",
      "out_edges, node: Morocco, relation: official religion, target: Islam\n",
      "out_edges, node: Morocco, relation: official language, target: Arabic\n",
      "out_edges, node: Morocco, relation: instance of, target: country\n",
      "out_edges, node: Morocco, relation: shares border with, target: Mauritania\n",
      "out_edges, node: Morocco, relation: continent, target: Africa\n",
      "out_edges, node: Morocco, relation: named after, target: Marrakesh\n",
      "out_edges, node: Morocco, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Morocco, relation: language used, target: English\n",
      "out_edges, node: Morocco, relation: part of, target: North Africa\n",
      "out_edges, node: Morocco, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Morocco, relation: capital, target: Rabat\n",
      "out_edges, node: Morocco, relation: country, target: Morocco\n",
      "out_edges, node: Morocco, relation: located in or next to body of water, target: Strait of Gibraltar\n",
      "out_edges, node: Morocco, relation: head of state, target: Mohammed VI\n",
      "out_edges, node: Peru, relation: part of, target: South America\n",
      "out_edges, node: Peru, relation: diplomatic relation, target: Israel\n",
      "out_edges, node: Peru, relation: member of, target: Asia-Pacific Economic Cooperation\n",
      "out_edges, node: Peru, relation: official language, target: Quechua\n",
      "out_edges, node: Peru, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Peru, relation: basic form of government, target: republic\n",
      "out_edges, node: Peru, relation: instance of, target: country\n",
      "out_edges, node: Peru, relation: shares border with, target: Chile\n",
      "out_edges, node: Peru, relation: contains the administrative territorial entity, target: Ancash department\n",
      "out_edges, node: Peru, relation: capital, target: Lima\n",
      "out_edges, node: Peru, relation: language used, target: Spanish\n",
      "out_edges, node: Peru, relation: country, target: Peru\n",
      "out_edges, node: Armenia, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Armenia, relation: member of, target: Asian Development Bank\n",
      "out_edges, node: Armenia, relation: continent, target: Asia\n",
      "out_edges, node: Armenia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Armenia, relation: part of, target: Eastern Europe\n",
      "out_edges, node: Armenia, relation: official religion, target: Christianity\n",
      "out_edges, node: Armenia, relation: capital, target: Yerevan\n",
      "out_edges, node: Armenia, relation: head of government, target: Nikol Pashinyan\n",
      "out_edges, node: Armenia, relation: country, target: Armenia\n",
      "out_edges, node: Armenia, relation: official language, target: Armenian\n",
      "out_edges, node: Armenia, relation: basic form of government, target: parliamentary system\n",
      "out_edges, node: Armenia, relation: ethnic group, target: Armenians\n",
      "out_edges, node: Armenia, relation: instance of, target: country\n",
      "out_edges, node: Dominica, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Dominica, relation: diplomatic relation, target: Taiwan\n",
      "out_edges, node: Dominica, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Dominica, relation: official language, target: English\n",
      "out_edges, node: Dominica, relation: instance of, target: country\n",
      "out_edges, node: Dominica, relation: continent, target: North America\n",
      "out_edges, node: Dominica, relation: located in/on physical feature, target: Caribbean\n",
      "out_edges, node: Dominica, relation: named after, target: Sunday\n",
      "out_edges, node: Dominica, relation: part of, target: Lesser Antilles\n",
      "out_edges, node: Dominica, relation: shares border with, target: Venezuela\n",
      "out_edges, node: Dominica, relation: capital, target: Roseau\n",
      "out_edges, node: Dominica, relation: different from, target: Dominican Republic\n",
      "out_edges, node: Dominica, relation: country, target: Dominica\n",
      "out_edges, node: Dominica, relation: lowest point, target: Caribbean Sea\n",
      "in_edges, node: Organisation for the Prohibition of Chemical Weapons, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "in_edges, node: Brahui, relation: language used, target: Brahui\n",
      "out_edges, node: Venezuela, relation: diplomatic relation, target: Spain\n",
      "out_edges, node: Venezuela, relation: shares border with, target: Brazil\n",
      "out_edges, node: Venezuela, relation: official language, target: Spanish\n",
      "out_edges, node: Venezuela, relation: member of, target: United Nations\n",
      "out_edges, node: Venezuela, relation: contains the administrative territorial entity, target: Sucre\n",
      "out_edges, node: Venezuela, relation: part of, target: South America\n",
      "out_edges, node: Venezuela, relation: instance of, target: country\n",
      "out_edges, node: Venezuela, relation: language used, target: Carib\n",
      "out_edges, node: Venezuela, relation: capital, target: Caracas\n",
      "out_edges, node: Venezuela, relation: central bank, target: Central Bank of Venezuela\n",
      "out_edges, node: Venezuela, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Venezuela, relation: head of state, target: Nicolás Maduro\n",
      "out_edges, node: Venezuela, relation: named after, target: Simón Bolívar\n",
      "out_edges, node: Venezuela, relation: country, target: Venezuela\n",
      "out_edges, node: Venezuela, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Venezuela, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "in_edges, node: Spain, relation: diplomatic relation, target: Spain\n",
      "in_edges, node: Spain, relation: shares border with, target: Spain\n",
      "in_edges, node: Spain, relation: country, target: Spain\n",
      "in_edges, node: Spain, relation: located in the administrative territorial entity, target: Spain\n",
      "out_edges, node: Rwanda, relation: diplomatic relation, target: Bangladesh\n",
      "out_edges, node: Rwanda, relation: member of, target: International Finance Corporation\n",
      "out_edges, node: Rwanda, relation: shares border with, target: Burundi\n",
      "out_edges, node: Rwanda, relation: official language, target: Kinyarwanda\n",
      "out_edges, node: Rwanda, relation: part of, target: African Union\n",
      "out_edges, node: Rwanda, relation: head of state, target: Paul Kagame\n",
      "out_edges, node: Rwanda, relation: capital, target: Kigali\n",
      "out_edges, node: Rwanda, relation: instance of, target: country\n",
      "out_edges, node: Rwanda, relation: language used, target: English\n",
      "out_edges, node: Rwanda, relation: country, target: Rwanda\n",
      "out_edges, node: Rwanda, relation: continent, target: Africa\n",
      "out_edges, node: Rwanda, relation: located in/on physical feature, target: Central Africa\n",
      "out_edges, node: Bangladesh, relation: diplomatic relation, target: Belarus\n",
      "out_edges, node: Bangladesh, relation: member of, target: Organisation of Islamic Cooperation\n",
      "out_edges, node: Bangladesh, relation: shares border with, target: Myanmar\n",
      "out_edges, node: Bangladesh, relation: religion or worldview, target: Buddhism\n",
      "out_edges, node: Bangladesh, relation: instance of, target: country\n",
      "out_edges, node: Bangladesh, relation: continent, target: Asia\n",
      "out_edges, node: Bangladesh, relation: language used, target: Santali\n",
      "out_edges, node: Bangladesh, relation: lowest point, target: Bay of Bengal\n",
      "out_edges, node: Bangladesh, relation: official religion, target: Islam\n",
      "out_edges, node: Bangladesh, relation: head of government, target: Sheikh Hasina\n",
      "out_edges, node: Bangladesh, relation: official symbol, target: Nymphaea nouchali\n",
      "out_edges, node: Bangladesh, relation: country, target: Bangladesh\n",
      "out_edges, node: Bangladesh, relation: capital, target: Dhaka\n",
      "out_edges, node: Bangladesh, relation: part of, target: South Asia\n",
      "out_edges, node: Bangladesh, relation: different from, target: Bengal\n",
      "out_edges, node: Ivory Coast, relation: basic form of government, target: republic\n",
      "out_edges, node: Ivory Coast, relation: part of, target: West Africa\n",
      "out_edges, node: Ivory Coast, relation: instance of, target: country\n",
      "out_edges, node: Ivory Coast, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Ivory Coast, relation: member of, target: African Development Bank\n",
      "out_edges, node: Ivory Coast, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Ivory Coast, relation: capital, target: Yamoussoukro\n",
      "out_edges, node: Ivory Coast, relation: country, target: Ivory Coast\n",
      "out_edges, node: Ivory Coast, relation: shares border with, target: Burkina Faso\n",
      "out_edges, node: Ivory Coast, relation: continent, target: Africa\n",
      "out_edges, node: Ivory Coast, relation: named after, target: coast\n",
      "out_edges, node: Ivory Coast, relation: lowest point, target: Gulf of Guinea\n",
      "out_edges, node: Ivory Coast, relation: replaces, target: French West Africa\n",
      "out_edges, node: Ivory Coast, relation: production statistics, target: cocoa bean\n",
      "out_edges, node: Ivory Coast, relation: head of state, target: Alassane Ouattara\n",
      "out_edges, node: Ivory Coast, relation: language used, target: Bambara\n",
      "in_edges, node: republic, relation: basic form of government, target: republic\n",
      "in_edges, node: republic, relation: instance of, target: republic\n",
      "out_edges, node: Philippines, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Philippines, relation: religion or worldview, target: Islam\n",
      "out_edges, node: Philippines, relation: diplomatic relation, target: European Union\n",
      "out_edges, node: Philippines, relation: language used, target: Tagalog\n",
      "out_edges, node: Philippines, relation: public holiday, target: Maundy Thursday\n",
      "out_edges, node: Philippines, relation: country, target: Philippines\n",
      "out_edges, node: Philippines, relation: contains the administrative territorial entity, target: Metro Manila\n",
      "out_edges, node: Philippines, relation: continent, target: Asia\n",
      "out_edges, node: Philippines, relation: capital, target: Manila\n",
      "out_edges, node: Philippines, relation: official language, target: English\n",
      "out_edges, node: Philippines, relation: part of, target: Southeast Asia\n",
      "out_edges, node: Philippines, relation: shares border with, target: People's Republic of China\n",
      "out_edges, node: Philippines, relation: head of state, target: Bongbong Marcos\n",
      "out_edges, node: Philippines, relation: named after, target: Philip II of Spain\n",
      "out_edges, node: Philippines, relation: instance of, target: country\n",
      "out_edges, node: Philippines, relation: basic form of government, target: republic\n",
      "out_edges, node: Philippines, relation: located in or next to body of water, target: South China Sea\n",
      "in_edges, node: International Telecommunication Union, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Montenegro, relation: shares border with, target: Serbia\n",
      "out_edges, node: Montenegro, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Montenegro, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Montenegro, relation: language used, target: Bosnian\n",
      "out_edges, node: Montenegro, relation: instance of, target: country\n",
      "out_edges, node: Montenegro, relation: located in/on physical feature, target: Balkans\n",
      "out_edges, node: Montenegro, relation: country, target: Montenegro\n",
      "out_edges, node: Montenegro, relation: continent, target: Europe\n",
      "out_edges, node: Montenegro, relation: basic form of government, target: republic\n",
      "out_edges, node: Montenegro, relation: replaces, target: Serbia and Montenegro\n",
      "out_edges, node: Montenegro, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Montenegro, relation: capital, target: Podgorica\n",
      "out_edges, node: Montenegro, relation: head of state, target: Milo Đukanović\n",
      "out_edges, node: Montenegro, relation: lowest point, target: Adriatic Sea\n",
      "out_edges, node: Montenegro, relation: currency, target: euro\n",
      "in_edges, node: Serbia, relation: shares border with, target: Serbia\n",
      "in_edges, node: Serbia, relation: diplomatic relation, target: Serbia\n",
      "out_edges, node: Angola, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Angola, relation: member of, target: International Finance Corporation\n",
      "out_edges, node: Angola, relation: shares border with, target: Republic of the Congo\n",
      "out_edges, node: Angola, relation: country, target: Angola\n",
      "out_edges, node: Angola, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Angola, relation: continent, target: Africa\n",
      "out_edges, node: Angola, relation: instance of, target: country\n",
      "out_edges, node: Angola, relation: language used, target: Kwanyama\n",
      "out_edges, node: Angola, relation: capital, target: Luanda\n",
      "out_edges, node: Angola, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Angola, relation: contains the administrative territorial entity, target: Luanda Province\n",
      "out_edges, node: Denmark, relation: part of, target: Northern Europe\n",
      "out_edges, node: Denmark, relation: official language, target: Danish\n",
      "out_edges, node: Denmark, relation: diplomatic relation, target: South Korea\n",
      "out_edges, node: Denmark, relation: located in or next to body of water, target: Baltic Sea\n",
      "out_edges, node: Denmark, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Denmark, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Denmark, relation: shares border with, target: Germany\n",
      "out_edges, node: Denmark, relation: contains the administrative territorial entity, target: Region Zealand\n",
      "out_edges, node: Denmark, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Denmark, relation: central bank, target: Danmarks Nationalbank\n",
      "out_edges, node: Denmark, relation: continent, target: Europe\n",
      "out_edges, node: Denmark, relation: head of government, target: Mette Frederiksen\n",
      "out_edges, node: Denmark, relation: instance of, target: state\n",
      "out_edges, node: Denmark, relation: capital, target: Copenhagen\n",
      "out_edges, node: Denmark, relation: head of state, target: Margrethe II of Denmark\n",
      "out_edges, node: Denmark, relation: language used, target: German\n",
      "out_edges, node: Denmark, relation: located in/on physical feature, target: Scandinavia\n",
      "in_edges, node: Northern Europe, relation: part of, target: Northern Europe\n",
      "out_edges, node: Mexico, relation: contains the administrative territorial entity, target: Puebla\n",
      "out_edges, node: Mexico, relation: diplomatic relation, target: Serbia\n",
      "out_edges, node: Mexico, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Mexico, relation: part of, target: North America\n",
      "out_edges, node: Mexico, relation: language used, target: Zapotec\n",
      "out_edges, node: Mexico, relation: participant in, target: World War II\n",
      "out_edges, node: Mexico, relation: instance of, target: empire\n",
      "out_edges, node: Mexico, relation: located in or next to body of water, target: Pacific Ocean\n",
      "out_edges, node: Mexico, relation: official language, target: Nahuatl\n",
      "out_edges, node: Mexico, relation: capital, target: Mexico City\n",
      "out_edges, node: Mexico, relation: head of government, target: Andrés Manuel López Obrador\n",
      "out_edges, node: Mexico, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Mexico, relation: central bank, target: Bank of Mexico\n",
      "out_edges, node: Mexico, relation: country, target: Mexico\n",
      "out_edges, node: Mexico, relation: highest point, target: Pico de Orizaba\n",
      "in_edges, node: Mexico, relation: shares border with, target: Mexico\n",
      "in_edges, node: Puebla, relation: contains the administrative territorial entity, target: Puebla\n",
      "out_edges, node: Algeria, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Algeria, relation: diplomatic relation, target: Bangladesh\n",
      "out_edges, node: Algeria, relation: member of, target: Organization of the Petroleum Exporting Countries\n",
      "out_edges, node: Algeria, relation: official language, target: Arabic\n",
      "out_edges, node: Algeria, relation: contains the administrative territorial entity, target: Algiers Province\n",
      "out_edges, node: Algeria, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Algeria, relation: continent, target: Africa\n",
      "out_edges, node: Algeria, relation: country, target: Algeria\n",
      "out_edges, node: Algeria, relation: shares border with, target: Mauritania\n",
      "out_edges, node: Algeria, relation: language used, target: Kabyle\n",
      "out_edges, node: Algeria, relation: part of, target: North Africa\n",
      "out_edges, node: Algeria, relation: instance of, target: country\n",
      "out_edges, node: Algeria, relation: significant event, target: Algerian War\n",
      "out_edges, node: Algeria, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Algeria, relation: official religion, target: Islam\n",
      "out_edges, node: Algeria, relation: capital, target: Algiers\n",
      "out_edges, node: Singapore, relation: language used, target: Mandarin Chinese\n",
      "out_edges, node: Singapore, relation: diplomatic relation, target: South Africa\n",
      "out_edges, node: Singapore, relation: head of government, target: Lee Hsien Loong\n",
      "out_edges, node: Singapore, relation: capital of, target: Singapore\n",
      "out_edges, node: Singapore, relation: member of, target: UNESCO\n",
      "out_edges, node: Singapore, relation: religion or worldview, target: Christianity\n",
      "out_edges, node: Singapore, relation: present in work, target: Civilization V\n",
      "out_edges, node: Singapore, relation: twinned administrative body, target: Gibraltar\n",
      "out_edges, node: Singapore, relation: official language, target: English\n",
      "out_edges, node: Singapore, relation: shares border with, target: Malaysia\n",
      "out_edges, node: Singapore, relation: instance of, target: city\n",
      "out_edges, node: Singapore, relation: part of, target: Southeast Asia\n",
      "out_edges, node: Singapore, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Singapore, relation: continent, target: Asia\n",
      "in_edges, node: Mandarin Chinese, relation: language used, target: Mandarin Chinese\n",
      "in_edges, node: Germany, relation: diplomatic relation, target: Germany\n",
      "in_edges, node: Germany, relation: shares border with, target: Germany\n",
      "out_edges, node: Yemen, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Yemen, relation: lowest point, target: Arabian Sea\n",
      "out_edges, node: Yemen, relation: diplomatic relation, target: United Kingdom\n",
      "out_edges, node: Yemen, relation: capital, target: Sanaa\n",
      "out_edges, node: Yemen, relation: instance of, target: country\n",
      "out_edges, node: Yemen, relation: official religion, target: Islam\n",
      "out_edges, node: Yemen, relation: basic form of government, target: republic\n",
      "out_edges, node: Yemen, relation: continent, target: Asia\n",
      "out_edges, node: Yemen, relation: part of, target: West Asia\n",
      "out_edges, node: Yemen, relation: official language, target: Arabic\n",
      "out_edges, node: Yemen, relation: public holiday, target: International Workers' Day\n",
      "out_edges, node: Yemen, relation: located in or next to body of water, target: Red Sea\n",
      "out_edges, node: Yemen, relation: country, target: Yemen\n",
      "out_edges, node: Yemen, relation: replaces, target: South Yemen\n",
      "out_edges, node: Yemen, relation: located in/on physical feature, target: Arabian Peninsula\n",
      "in_edges, node: Yemen, relation: shares border with, target: Yemen\n",
      "out_edges, node: Brunei, relation: diplomatic relation, target: India\n",
      "out_edges, node: Brunei, relation: official language, target: Malay\n",
      "out_edges, node: Brunei, relation: member of, target: United Nations\n",
      "out_edges, node: Brunei, relation: language used, target: Cantonese\n",
      "out_edges, node: Brunei, relation: located in/on physical feature, target: Borneo\n",
      "out_edges, node: Brunei, relation: shares border with, target: Malaysia\n",
      "out_edges, node: Brunei, relation: religion or worldview, target: Christianity\n",
      "out_edges, node: Brunei, relation: capital, target: Bandar Seri Begawan\n",
      "out_edges, node: Brunei, relation: country, target: Brunei\n",
      "out_edges, node: Brunei, relation: instance of, target: country\n",
      "out_edges, node: Brunei, relation: head of state, target: Haji Hassanal Bolkiah\n",
      "out_edges, node: Brunei, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Brunei, relation: part of, target: Southeast Asia\n",
      "out_edges, node: Brunei, relation: lowest point, target: South China Sea\n",
      "out_edges, node: Brunei, relation: continent, target: Asia\n",
      "out_edges, node: India, relation: diplomatic relation, target: Iran\n",
      "out_edges, node: India, relation: central bank, target: Reserve Bank of India\n",
      "out_edges, node: India, relation: language used, target: Maldivian\n",
      "out_edges, node: India, relation: religion or worldview, target: Hinduism\n",
      "out_edges, node: India, relation: contains the administrative territorial entity, target: Ladakh\n",
      "out_edges, node: India, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: India, relation: country, target: India\n",
      "out_edges, node: India, relation: official language, target: Hindi\n",
      "out_edges, node: India, relation: part of, target: South Asia\n",
      "out_edges, node: India, relation: instance of, target: country\n",
      "out_edges, node: India, relation: capital, target: New Delhi\n",
      "out_edges, node: India, relation: named after, target: Indus River\n",
      "out_edges, node: India, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: India, relation: public holiday, target: Independence Day\n",
      "out_edges, node: India, relation: located in or next to body of water, target: Indian Ocean\n",
      "out_edges, node: India, relation: shares border with, target: Pakistan\n",
      "out_edges, node: India, relation: head of government, target: Narendra Modi\n",
      "out_edges, node: India, relation: continent, target: Asia\n",
      "out_edges, node: India, relation: highest point, target: Kanchenjunga\n",
      "out_edges, node: Belarus, relation: contains the administrative territorial entity, target: Brest Region\n",
      "out_edges, node: Belarus, relation: member of, target: Interpol\n",
      "out_edges, node: Belarus, relation: capital, target: Minsk\n",
      "out_edges, node: Belarus, relation: diplomatic relation, target: Chile\n",
      "out_edges, node: Belarus, relation: shares border with, target: Poland\n",
      "out_edges, node: Belarus, relation: separated from, target: Soviet Union\n",
      "out_edges, node: Belarus, relation: official observer status in organisation, target: Shanghai Cooperation Organisation\n",
      "out_edges, node: Belarus, relation: instance of, target: country\n",
      "out_edges, node: Belarus, relation: public holiday, target: Christmas\n",
      "out_edges, node: Belarus, relation: continent, target: Europe\n",
      "out_edges, node: Belarus, relation: part of, target: Eastern Europe\n",
      "out_edges, node: Belarus, relation: replaces, target: Byelorussian Soviet Socialist Republic\n",
      "out_edges, node: Belarus, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Belarus, relation: official language, target: Belarusian\n",
      "out_edges, node: Belarus, relation: lowest point, target: Neman\n",
      "out_edges, node: Belarus, relation: language used, target: Russian\n",
      "out_edges, node: Belarus, relation: head of state, target: Alexander Lukashenko\n",
      "out_edges, node: Belarus, relation: participant in, target: Russian invasion of Ukraine\n",
      "out_edges, node: Belarus, relation: country, target: Belarus\n",
      "in_edges, node: Brest Region, relation: contains the administrative territorial entity, target: Brest Region\n",
      "in_edges, node: Korean War, relation: significant event, target: Korean War\n",
      "in_edges, node: Korean War, relation: participant in, target: Korean War\n",
      "out_edges, node: Latvia, relation: contains the administrative territorial entity, target: Rēzekne\n",
      "out_edges, node: Latvia, relation: located in/on physical feature, target: Baltic states\n",
      "out_edges, node: Latvia, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Latvia, relation: diplomatic relation, target: Poland\n",
      "out_edges, node: Latvia, relation: instance of, target: country\n",
      "out_edges, node: Latvia, relation: basic form of government, target: republic\n",
      "out_edges, node: Latvia, relation: shares border with, target: Lithuania\n",
      "out_edges, node: Latvia, relation: language used, target: Latvian\n",
      "out_edges, node: Latvia, relation: lowest point, target: Baltic Sea\n",
      "out_edges, node: Latvia, relation: located in time zone, target: daylight saving time\n",
      "out_edges, node: Latvia, relation: continent, target: Europe\n",
      "out_edges, node: Latvia, relation: part of, target: Northern Europe\n",
      "out_edges, node: Latvia, relation: country, target: Latvia\n",
      "out_edges, node: Latvia, relation: currency, target: euro\n",
      "in_edges, node: Vietnamese, relation: language used, target: Vietnamese\n",
      "in_edges, node: Vietnamese, relation: official language, target: Vietnamese\n",
      "out_edges, node: Gabon, relation: member of, target: United Nations\n",
      "out_edges, node: Gabon, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Gabon, relation: shares border with, target: São Tomé and Príncipe\n",
      "out_edges, node: Gabon, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Gabon, relation: basic form of government, target: republic\n",
      "out_edges, node: Gabon, relation: instance of, target: country\n",
      "out_edges, node: Gabon, relation: language used, target: Fang\n",
      "out_edges, node: Gabon, relation: country, target: Gabon\n",
      "out_edges, node: Gabon, relation: head of state, target: Ali Bongo Ondimba\n",
      "out_edges, node: Gabon, relation: located in/on physical feature, target: Central Africa\n",
      "out_edges, node: Gabon, relation: continent, target: Africa\n",
      "out_edges, node: Gabon, relation: capital, target: Libreville\n",
      "in_edges, node: Fukushima Daiichi nuclear disaster, relation: significant event, target: Fukushima Daiichi nuclear disaster\n",
      "out_edges, node: Iraq, relation: shares border with, target: Saudi Arabia\n",
      "out_edges, node: Iraq, relation: instance of, target: country\n",
      "out_edges, node: Iraq, relation: diplomatic relation, target: Cuba\n",
      "out_edges, node: Iraq, relation: country, target: Iraq\n",
      "out_edges, node: Iraq, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Iraq, relation: official religion, target: Islam\n",
      "out_edges, node: Iraq, relation: official language, target: Kurdish\n",
      "out_edges, node: Iraq, relation: part of, target: West Asia\n",
      "out_edges, node: Iraq, relation: capital, target: Baghdad\n",
      "out_edges, node: Iraq, relation: ethnic group, target: Arabs\n",
      "out_edges, node: Iraq, relation: continent, target: Asia\n",
      "out_edges, node: Zimbabwe, relation: diplomatic relation, target: Israel\n",
      "out_edges, node: Zimbabwe, relation: official language, target: English\n",
      "out_edges, node: Zimbabwe, relation: shares border with, target: Botswana\n",
      "out_edges, node: Zimbabwe, relation: member of, target: Interpol\n",
      "out_edges, node: Zimbabwe, relation: continent, target: Africa\n",
      "out_edges, node: Zimbabwe, relation: instance of, target: republic\n",
      "out_edges, node: Zimbabwe, relation: named after, target: Great Zimbabwe\n",
      "out_edges, node: Zimbabwe, relation: part of, target: Southern Africa\n",
      "out_edges, node: Zimbabwe, relation: capital, target: Harare\n",
      "out_edges, node: Zimbabwe, relation: country, target: Zimbabwe\n",
      "out_edges, node: Zimbabwe, relation: language used, target: Lozi\n",
      "out_edges, node: Zimbabwe, relation: currency, target: United States dollar\n",
      "out_edges, node: Zimbabwe, relation: contains the administrative territorial entity, target: Bulawayo\n",
      "in_edges, node: European Space Agency, relation: member of, target: European Space Agency\n",
      "in_edges, node: European Space Agency, relation: official observer status in organisation, target: European Space Agency\n",
      "out_edges, node: The Bahamas, relation: member of, target: World Health Organization\n",
      "out_edges, node: The Bahamas, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: The Bahamas, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: The Bahamas, relation: language used, target: English\n",
      "out_edges, node: The Bahamas, relation: instance of, target: administrative territorial entity\n",
      "out_edges, node: The Bahamas, relation: shares border with, target: Turks and Caicos Islands\n",
      "out_edges, node: The Bahamas, relation: part of, target: Caribbean\n",
      "out_edges, node: The Bahamas, relation: twinned administrative body, target: Miami-Dade County\n",
      "out_edges, node: The Bahamas, relation: official religion, target: Christianity\n",
      "out_edges, node: The Bahamas, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: The Bahamas, relation: capital, target: Nassau\n",
      "out_edges, node: The Bahamas, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: The Bahamas, relation: country, target: The Bahamas\n",
      "out_edges, node: The Bahamas, relation: continent, target: North America\n",
      "out_edges, node: The Bahamas, relation: head of state, target: Charles III of the United Kingdom\n",
      "in_edges, node: World Health Organization, relation: member of, target: World Health Organization\n",
      "out_edges, node: Turkey, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Turkey, relation: instance of, target: country\n",
      "out_edges, node: Turkey, relation: diplomatic relation, target: Switzerland\n",
      "out_edges, node: Turkey, relation: shares border with, target: Syria\n",
      "out_edges, node: Turkey, relation: continent, target: Asia\n",
      "out_edges, node: Turkey, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Turkey, relation: contains the administrative territorial entity, target: Samsun Province\n",
      "out_edges, node: Turkey, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Turkey, relation: part of, target: West Asia\n",
      "out_edges, node: Turkey, relation: ethnic group, target: Turks\n",
      "out_edges, node: Turkey, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Turkey, relation: lowest point, target: Black Sea\n",
      "out_edges, node: Turkey, relation: highest point, target: Mount Ararat\n",
      "out_edges, node: Turkey, relation: language used, target: Syriac\n",
      "out_edges, node: Turkey, relation: official language, target: Turkish\n",
      "out_edges, node: Turkey, relation: official religion, target: Islam\n",
      "out_edges, node: Turkey, relation: head of government, target: Recep Tayyip Erdoğan\n",
      "out_edges, node: Turkey, relation: replaces, target: Ottoman Empire\n",
      "out_edges, node: Turkey, relation: capital, target: Ankara\n",
      "out_edges, node: Turkey, relation: owner of, target: Topkapı Palace\n",
      "out_edges, node: Turkey, relation: country, target: Turkey\n",
      "out_edges, node: Turkey, relation: central bank, target: Central Bank of the Republic of Turkey\n",
      "out_edges, node: Tajikistan, relation: member of, target: International Finance Corporation\n",
      "out_edges, node: Tajikistan, relation: diplomatic relation, target: Afghanistan\n",
      "out_edges, node: Tajikistan, relation: official language, target: Russian\n",
      "out_edges, node: Tajikistan, relation: capital, target: Dushanbe\n",
      "out_edges, node: Tajikistan, relation: instance of, target: country\n",
      "out_edges, node: Tajikistan, relation: shares border with, target: People's Republic of China\n",
      "out_edges, node: Tajikistan, relation: language used, target: Kyrgyz\n",
      "out_edges, node: Tajikistan, relation: located in/on physical feature, target: Central Asia\n",
      "out_edges, node: Tajikistan, relation: head of state, target: Emomali Rahmon\n",
      "out_edges, node: Tajikistan, relation: continent, target: Asia\n",
      "out_edges, node: Tajikistan, relation: country, target: Tajikistan\n",
      "out_edges, node: Mali, relation: diplomatic relation, target: Taiwan\n",
      "out_edges, node: Mali, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Mali, relation: official language, target: French\n",
      "out_edges, node: Mali, relation: language used, target: Bambara\n",
      "out_edges, node: Mali, relation: continent, target: Africa\n",
      "out_edges, node: Mali, relation: shares border with, target: Guinea\n",
      "out_edges, node: Mali, relation: contains the administrative territorial entity, target: Bamako\n",
      "out_edges, node: Mali, relation: instance of, target: country\n",
      "out_edges, node: Mali, relation: country, target: Mali\n",
      "out_edges, node: Mali, relation: part of, target: West Africa\n",
      "out_edges, node: Mali, relation: replaces, target: French West Africa\n",
      "out_edges, node: Jordan, relation: official language, target: Arabic\n",
      "out_edges, node: Jordan, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Jordan, relation: diplomatic relation, target: Holy See\n",
      "out_edges, node: Jordan, relation: part of, target: West Asia\n",
      "out_edges, node: Jordan, relation: lowest point, target: Dead Sea\n",
      "out_edges, node: Jordan, relation: shares border with, target: Israel\n",
      "out_edges, node: Jordan, relation: head of state, target: Abdullah II of Jordan\n",
      "out_edges, node: Jordan, relation: instance of, target: country\n",
      "out_edges, node: Jordan, relation: capital, target: Amman\n",
      "out_edges, node: Jordan, relation: country, target: Jordan\n",
      "out_edges, node: Jordan, relation: named after, target: Jordan River\n",
      "out_edges, node: Jordan, relation: continent, target: Asia\n",
      "in_edges, node: Arabic, relation: official language, target: Arabic\n",
      "in_edges, node: Arabic, relation: language used, target: Arabic\n",
      "in_edges, node: Danish, relation: official language, target: Danish\n",
      "in_edges, node: Danish, relation: language used, target: Danish\n",
      "out_edges, node: Galicia, relation: contains the administrative territorial entity, target: Pontevedra Province\n",
      "out_edges, node: Galicia, relation: continent, target: Europe\n",
      "out_edges, node: Galicia, relation: twinned administrative body, target: Wakayama Prefecture\n",
      "out_edges, node: Galicia, relation: instance of, target: nation\n",
      "out_edges, node: Galicia, relation: shares border with, target: Asturias\n",
      "out_edges, node: Galicia, relation: located in the administrative territorial entity, target: Spain\n",
      "out_edges, node: Galicia, relation: different from, target: Galicia\n",
      "out_edges, node: Galicia, relation: official language, target: Spanish\n",
      "out_edges, node: Galicia, relation: language used, target: Galician\n",
      "out_edges, node: Galicia, relation: capital, target: Santiago de Compostela\n",
      "out_edges, node: Galicia, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "in_edges, node: Pontevedra Province, relation: contains the administrative territorial entity, target: Pontevedra Province\n",
      "in_edges, node: Watergate scandal, relation: significant event, target: Watergate scandal\n",
      "out_edges, node: Cambodia, relation: facet of, target: Cambodia\n",
      "out_edges, node: Cambodia, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Cambodia, relation: located in/on physical feature, target: Southeast Asia\n",
      "out_edges, node: Cambodia, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Cambodia, relation: shares border with, target: Vietnam\n",
      "out_edges, node: Cambodia, relation: instance of, target: country\n",
      "out_edges, node: Cambodia, relation: head of government, target: Hun Sen\n",
      "out_edges, node: Cambodia, relation: religion or worldview, target: Christianity\n",
      "out_edges, node: Cambodia, relation: language used, target: Lao\n",
      "out_edges, node: Cambodia, relation: contains the administrative territorial entity, target: Phnom Penh\n",
      "out_edges, node: Cambodia, relation: head of state, target: Norodom Sihamoni\n",
      "out_edges, node: Cambodia, relation: official language, target: Khmer\n",
      "out_edges, node: Cambodia, relation: ethnic group, target: Chinese people\n",
      "out_edges, node: Cambodia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Cambodia, relation: continent, target: Asia\n",
      "out_edges, node: Romania, relation: member of, target: Organization for Security and Co-operation in Europe\n",
      "out_edges, node: Romania, relation: diplomatic relation, target: France\n",
      "out_edges, node: Romania, relation: country, target: Romania\n",
      "out_edges, node: Romania, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Romania, relation: shares border with, target: Serbia\n",
      "out_edges, node: Romania, relation: capital, target: Bucharest\n",
      "out_edges, node: Romania, relation: contains the administrative territorial entity, target: Bihor County\n",
      "out_edges, node: Romania, relation: continent, target: Europe\n",
      "out_edges, node: Romania, relation: official language, target: Romanian\n",
      "out_edges, node: Romania, relation: part of, target: Eastern Europe\n",
      "out_edges, node: Romania, relation: language used, target: Ukrainian\n",
      "out_edges, node: Romania, relation: head of state, target: Klaus Iohannis\n",
      "out_edges, node: Romania, relation: lowest point, target: Black Sea\n",
      "out_edges, node: Romania, relation: named after, target: Ancient Rome\n",
      "out_edges, node: Romania, relation: instance of, target: country\n",
      "in_edges, node: Organization for Security and Co-operation in Europe, relation: member of, target: Organization for Security and Co-operation in Europe\n",
      "out_edges, node: Zambia, relation: basic form of government, target: representative democracy\n",
      "out_edges, node: Zambia, relation: member of, target: African Union\n",
      "out_edges, node: Zambia, relation: continent, target: Africa\n",
      "out_edges, node: Zambia, relation: shares border with, target: Mozambique\n",
      "out_edges, node: Zambia, relation: language used, target: Shona\n",
      "out_edges, node: Zambia, relation: diplomatic relation, target: India\n",
      "out_edges, node: Zambia, relation: located in/on physical feature, target: Southern Africa\n",
      "out_edges, node: Zambia, relation: instance of, target: country\n",
      "out_edges, node: Zambia, relation: country, target: Zambia\n",
      "out_edges, node: Zambia, relation: part of, target: East Africa\n",
      "out_edges, node: Zambia, relation: contains the administrative territorial entity, target: Copperbelt Province\n",
      "out_edges, node: Zambia, relation: lowest point, target: Zambezi River\n",
      "out_edges, node: Zambia, relation: capital, target: Lusaka\n",
      "in_edges, node: Interpol, relation: member of, target: Interpol\n",
      "out_edges, node: Turkmenistan, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Turkmenistan, relation: diplomatic relation, target: Serbia\n",
      "out_edges, node: Turkmenistan, relation: instance of, target: country\n",
      "out_edges, node: Turkmenistan, relation: different from, target: Turkestan\n",
      "out_edges, node: Turkmenistan, relation: shares border with, target: Iran\n",
      "out_edges, node: Turkmenistan, relation: capital, target: Ashgabat\n",
      "out_edges, node: Turkmenistan, relation: continent, target: Asia\n",
      "out_edges, node: Turkmenistan, relation: located in/on physical feature, target: Central Asia\n",
      "out_edges, node: Turkmenistan, relation: language used, target: Turkmen\n",
      "out_edges, node: Turkmenistan, relation: country, target: Turkmenistan\n",
      "out_edges, node: Turkmenistan, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Turkmenistan, relation: head of government, target: Gurbanguly Berdimuhamedow\n",
      "in_edges, node: World Meteorological Organization, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Wales, relation: contains the administrative territorial entity, target: Swansea\n",
      "out_edges, node: Wales, relation: official language, target: Welsh\n",
      "out_edges, node: Wales, relation: continent, target: Europe\n",
      "out_edges, node: Wales, relation: country, target: United Kingdom\n",
      "out_edges, node: Wales, relation: legislative body, target: Senedd\n",
      "out_edges, node: Wales, relation: archives at, target: National Library of Wales\n",
      "out_edges, node: Wales, relation: head of government, target: Mark Drakeford\n",
      "out_edges, node: Wales, relation: highest point, target: Yr Wyddfa\n",
      "out_edges, node: Wales, relation: currency, target: pound sterling\n",
      "out_edges, node: Wales, relation: shares border with, target: England\n",
      "out_edges, node: Wales, relation: executive body, target: Welsh Government\n",
      "out_edges, node: Wales, relation: instance of, target: country\n",
      "out_edges, node: Wales, relation: patron saint, target: Saint David\n",
      "out_edges, node: Wales, relation: capital, target: Cardiff\n",
      "in_edges, node: Swansea, relation: contains the administrative territorial entity, target: Swansea\n",
      "out_edges, node: Nepal, relation: diplomatic relation, target: India\n",
      "out_edges, node: Nepal, relation: language used, target: Nepali\n",
      "out_edges, node: Nepal, relation: continent, target: Asia\n",
      "out_edges, node: Nepal, relation: country, target: Nepal\n",
      "out_edges, node: Nepal, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Nepal, relation: twinned administrative body, target: Toyota\n",
      "out_edges, node: Nepal, relation: part of, target: South Asia\n",
      "out_edges, node: Nepal, relation: instance of, target: geographic region\n",
      "out_edges, node: Nepal, relation: highest point, target: Mount Everest\n",
      "out_edges, node: Nepal, relation: religion or worldview, target: Bon\n",
      "out_edges, node: Nepal, relation: capital, target: Kathmandu\n",
      "out_edges, node: Nepal, relation: located in/on physical feature, target: Himalayas\n",
      "out_edges, node: Nepal, relation: ethnic group, target: Muslim\n",
      "in_edges, node: Nepal, relation: shares border with, target: Nepal\n",
      "in_edges, node: Ljubljana, relation: capital, target: Ljubljana\n",
      "out_edges, node: Mauritania, relation: diplomatic relation, target: Spain\n",
      "out_edges, node: Mauritania, relation: part of, target: West Africa\n",
      "out_edges, node: Mauritania, relation: member of, target: Organisation of Islamic Cooperation\n",
      "out_edges, node: Mauritania, relation: contains the administrative territorial entity, target: Nouakchott\n",
      "out_edges, node: Mauritania, relation: shares border with, target: Senegal\n",
      "out_edges, node: Mauritania, relation: continent, target: Africa\n",
      "out_edges, node: Mauritania, relation: replaces, target: French West Africa\n",
      "out_edges, node: Mauritania, relation: country, target: Mauritania\n",
      "out_edges, node: Mauritania, relation: instance of, target: country\n",
      "out_edges, node: Mauritania, relation: official language, target: Arabic\n",
      "out_edges, node: Mauritania, relation: named after, target: Moors\n",
      "out_edges, node: Mauritania, relation: language used, target: Zenaga\n",
      "out_edges, node: Mauritania, relation: located in/on physical feature, target: Sahel\n",
      "out_edges, node: Mauritania, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "in_edges, node: African Union, relation: member of, target: African Union\n",
      "in_edges, node: African Union, relation: part of, target: African Union\n",
      "in_edges, node: African Union, relation: diplomatic relation, target: African Union\n",
      "in_edges, node: Christmas, relation: public holiday, target: Christmas\n",
      "in_edges, node: Lazio, relation: contains the administrative territorial entity, target: Lazio\n",
      "in_edges, node: Atlantic Ocean, relation: lowest point, target: Atlantic Ocean\n",
      "in_edges, node: Atlantic Ocean, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "in_edges, node: Asia, relation: continent, target: Asia\n",
      "out_edges, node: Namibia, relation: member of, target: Commonwealth of Nations\n",
      "out_edges, node: Namibia, relation: diplomatic relation, target: Zambia\n",
      "out_edges, node: Namibia, relation: part of, target: Southern Africa\n",
      "out_edges, node: Namibia, relation: language used, target: Ndonga\n",
      "out_edges, node: Namibia, relation: shares border with, target: South Africa\n",
      "out_edges, node: Namibia, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Namibia, relation: country, target: Namibia\n",
      "out_edges, node: Namibia, relation: continent, target: Africa\n",
      "out_edges, node: Namibia, relation: instance of, target: republic\n",
      "out_edges, node: Namibia, relation: named after, target: Namib\n",
      "out_edges, node: Namibia, relation: capital, target: Windhoek\n",
      "out_edges, node: Namibia, relation: head of state, target: Hage Gottfried Geingob\n",
      "in_edges, node: Commonwealth of Nations, relation: member of, target: Commonwealth of Nations\n",
      "in_edges, node: Commonwealth of Nations, relation: part of, target: Commonwealth of Nations\n",
      "out_edges, node: Saint Kitts and Nevis, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Saint Kitts and Nevis, relation: member of, target: Organisation of African, Caribbean and Pacific States\n",
      "out_edges, node: Saint Kitts and Nevis, relation: country, target: Saint Kitts and Nevis\n",
      "out_edges, node: Saint Kitts and Nevis, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Saint Kitts and Nevis, relation: shares border with, target: Venezuela\n",
      "out_edges, node: Saint Kitts and Nevis, relation: language used, target: English\n",
      "out_edges, node: Saint Kitts and Nevis, relation: capital, target: Basseterre\n",
      "out_edges, node: Saint Kitts and Nevis, relation: instance of, target: state\n",
      "out_edges, node: Saint Kitts and Nevis, relation: part of, target: Lesser Antilles\n",
      "out_edges, node: Saint Kitts and Nevis, relation: continent, target: North America\n",
      "out_edges, node: Saint Kitts and Nevis, relation: twinned administrative body, target: Miami-Dade County\n",
      "out_edges, node: Saint Kitts and Nevis, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Saint Kitts and Nevis, relation: basic form of government, target: constitutional monarchy\n",
      "in_edges, node: Caribbean Sea, relation: lowest point, target: Caribbean Sea\n",
      "in_edges, node: Caribbean Sea, relation: located in or next to body of water, target: Caribbean Sea\n",
      "out_edges, node: Tuvalu, relation: instance of, target: country\n",
      "out_edges, node: Tuvalu, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Tuvalu, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Tuvalu, relation: diplomatic relation, target: Cuba\n",
      "out_edges, node: Tuvalu, relation: official language, target: English\n",
      "out_edges, node: Tuvalu, relation: capital, target: Funafuti\n",
      "out_edges, node: Tuvalu, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Tuvalu, relation: part of, target: Polynesia\n",
      "out_edges, node: Tuvalu, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Tuvalu, relation: country, target: Tuvalu\n",
      "out_edges, node: Tuvalu, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Vanuatu, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Vanuatu, relation: member of, target: Organisation internationale de la Francophonie\n",
      "out_edges, node: Vanuatu, relation: replaces, target: New Hebrides\n",
      "out_edges, node: Vanuatu, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Vanuatu, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Vanuatu, relation: country, target: Vanuatu\n",
      "out_edges, node: Vanuatu, relation: language used, target: English\n",
      "out_edges, node: Vanuatu, relation: instance of, target: country\n",
      "out_edges, node: Vanuatu, relation: official language, target: French\n",
      "out_edges, node: Vanuatu, relation: capital, target: Port Vila\n",
      "out_edges, node: Vanuatu, relation: part of, target: Melanesia\n",
      "in_edges, node: Vanuatu, relation: shares border with, target: Vanuatu\n",
      "in_edges, node: Pacific Ocean, relation: lowest point, target: Pacific Ocean\n",
      "in_edges, node: Pacific Ocean, relation: located in or next to body of water, target: Pacific Ocean\n",
      "out_edges, node: Bulgaria, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Bulgaria, relation: lowest point, target: Black Sea\n",
      "out_edges, node: Bulgaria, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Bulgaria, relation: contains the administrative territorial entity, target: Plovdiv Province\n",
      "out_edges, node: Bulgaria, relation: official symbol, target: lion\n",
      "out_edges, node: Bulgaria, relation: language used, target: Romani\n",
      "out_edges, node: Bulgaria, relation: twinned administrative body, target: Toyoake\n",
      "out_edges, node: Bulgaria, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Bulgaria, relation: central bank, target: Bulgarian National Bank\n",
      "out_edges, node: Bulgaria, relation: part of, target: European Union\n",
      "out_edges, node: Bulgaria, relation: official language, target: Bulgarian\n",
      "out_edges, node: Bulgaria, relation: shares border with, target: Greece\n",
      "out_edges, node: Bulgaria, relation: continent, target: Europe\n",
      "out_edges, node: Bulgaria, relation: located in/on physical feature, target: Balkans\n",
      "out_edges, node: Bulgaria, relation: capital, target: Sofia\n",
      "out_edges, node: Bulgaria, relation: official observer status in organisation, target: European Space Agency\n",
      "out_edges, node: Bulgaria, relation: country, target: Bulgaria\n",
      "out_edges, node: Bulgaria, relation: instance of, target: country\n",
      "out_edges, node: Somalia, relation: diplomatic relation, target: Turkey\n",
      "out_edges, node: Somalia, relation: member of, target: African Development Bank\n",
      "out_edges, node: Somalia, relation: official language, target: Somali\n",
      "out_edges, node: Somalia, relation: instance of, target: republic\n",
      "out_edges, node: Somalia, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Somalia, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Somalia, relation: continent, target: Africa\n",
      "out_edges, node: Somalia, relation: capital, target: Mogadishu\n",
      "out_edges, node: Somalia, relation: part of, target: East Africa\n",
      "out_edges, node: Somalia, relation: country, target: Somalia\n",
      "in_edges, node: Somalia, relation: shares border with, target: Somalia\n",
      "out_edges, node: Senegal, relation: member of, target: UNESCO\n",
      "out_edges, node: Senegal, relation: head of government, target: Macky Sall\n",
      "out_edges, node: Senegal, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Senegal, relation: replaces, target: French West Africa\n",
      "out_edges, node: Senegal, relation: continent, target: Africa\n",
      "out_edges, node: Senegal, relation: instance of, target: republic\n",
      "out_edges, node: Senegal, relation: language used, target: Fula\n",
      "out_edges, node: Senegal, relation: country, target: Senegal\n",
      "out_edges, node: Senegal, relation: shares border with, target: Guinea\n",
      "out_edges, node: Senegal, relation: official language, target: Wolof\n",
      "out_edges, node: Senegal, relation: capital, target: Dakar\n",
      "out_edges, node: Senegal, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Senegal, relation: located in/on physical feature, target: West Africa\n",
      "in_edges, node: Minsk, relation: capital, target: Minsk\n",
      "out_edges, node: South Africa, relation: diplomatic relation, target: Botswana\n",
      "out_edges, node: South Africa, relation: member of, target: African Union\n",
      "out_edges, node: South Africa, relation: shares border with, target: Namibia\n",
      "out_edges, node: South Africa, relation: official language, target: Southern Ndebele\n",
      "out_edges, node: South Africa, relation: public holiday, target: Christmas\n",
      "out_edges, node: South Africa, relation: basic form of government, target: representative democracy\n",
      "out_edges, node: South Africa, relation: instance of, target: country\n",
      "out_edges, node: South Africa, relation: continent, target: Africa\n",
      "out_edges, node: South Africa, relation: language used, target: Swazi\n",
      "out_edges, node: South Africa, relation: different from, target: Southern Africa\n",
      "out_edges, node: South Africa, relation: capital, target: Pretoria\n",
      "out_edges, node: South Africa, relation: contains the administrative territorial entity, target: Western Cape\n",
      "out_edges, node: South Africa, relation: located in or next to body of water, target: Indian Ocean\n",
      "out_edges, node: South Africa, relation: named after, target: south\n",
      "out_edges, node: South Africa, relation: country, target: South Africa\n",
      "out_edges, node: South Africa, relation: head of government, target: Cyril Ramaphosa\n",
      "out_edges, node: South Africa, relation: central bank, target: South African Reserve Bank\n",
      "in_edges, node: Rēzekne, relation: contains the administrative territorial entity, target: Rēzekne\n",
      "in_edges, node: United States Minor Outlying Islands, relation: contains the statistical territorial entity, target: United States Minor Outlying Islands\n",
      "in_edges, node: Sinaloa, relation: contains the administrative territorial entity, target: Sinaloa\n",
      "out_edges, node: Maldives, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Maldives, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Maldives, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Maldives, relation: different from, target: Falkland Islands\n",
      "out_edges, node: Maldives, relation: capital, target: Malé\n",
      "out_edges, node: Maldives, relation: instance of, target: country\n",
      "out_edges, node: Maldives, relation: country, target: Maldives\n",
      "out_edges, node: Maldives, relation: religion or worldview, target: Sunni Islam\n",
      "out_edges, node: Maldives, relation: language used, target: English\n",
      "out_edges, node: Maldives, relation: shares border with, target: United Kingdom\n",
      "out_edges, node: Maldives, relation: located in/on physical feature, target: South Asia\n",
      "out_edges, node: Maldives, relation: continent, target: Asia\n",
      "out_edges, node: Maldives, relation: official religion, target: Islam\n",
      "in_edges, node: Baltic states, relation: located in/on physical feature, target: Baltic states\n",
      "in_edges, node: Baltic states, relation: part of, target: Baltic states\n",
      "in_edges, node: Si Sa Ket, relation: contains the administrative territorial entity, target: Si Sa Ket\n",
      "in_edges, node: Asian elephant, relation: official symbol, target: Asian elephant\n",
      "out_edges, node: Costa Rica, relation: located in or next to body of water, target: Pacific Ocean\n",
      "out_edges, node: Costa Rica, relation: country, target: Costa Rica\n",
      "out_edges, node: Costa Rica, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Costa Rica, relation: instance of, target: country\n",
      "out_edges, node: Costa Rica, relation: member of, target: World Health Organization\n",
      "out_edges, node: Costa Rica, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Costa Rica, relation: language used, target: Spanish\n",
      "out_edges, node: Costa Rica, relation: continent, target: North America\n",
      "out_edges, node: Costa Rica, relation: shares border with, target: Nicaragua\n",
      "out_edges, node: Costa Rica, relation: capital, target: San José\n",
      "out_edges, node: Costa Rica, relation: part of, target: Latin America\n",
      "in_edges, node: Encyclopædia Britannica 11th edition, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Kazakhstan, relation: twinned administrative body, target: Toyota\n",
      "out_edges, node: Kazakhstan, relation: member of, target: Asian Development Bank\n",
      "out_edges, node: Kazakhstan, relation: continent, target: Asia\n",
      "out_edges, node: Kazakhstan, relation: diplomatic relation, target: Uzbekistan\n",
      "out_edges, node: Kazakhstan, relation: contains the administrative territorial entity, target: Shymkent\n",
      "out_edges, node: Kazakhstan, relation: head of state, target: Kassym-Jomart Tokayev\n",
      "out_edges, node: Kazakhstan, relation: shares border with, target: Kyrgyzstan\n",
      "out_edges, node: Kazakhstan, relation: part of, target: Central Asia\n",
      "out_edges, node: Kazakhstan, relation: named after, target: Kazakhs\n",
      "out_edges, node: Kazakhstan, relation: language used, target: Russian\n",
      "out_edges, node: Kazakhstan, relation: country, target: Kazakhstan\n",
      "out_edges, node: Kazakhstan, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Kazakhstan, relation: instance of, target: country\n",
      "out_edges, node: Kazakhstan, relation: capital, target: Astana\n",
      "in_edges, node: Toyota, relation: twinned administrative body, target: Toyota\n",
      "in_edges, node: European Union, relation: diplomatic relation, target: European Union\n",
      "in_edges, node: European Union, relation: part of, target: European Union\n",
      "in_edges, node: European Union, relation: member of, target: European Union\n",
      "in_edges, node: European Union, relation: shares border with, target: European Union\n",
      "out_edges, node: Iran, relation: shares border with, target: Bahrain\n",
      "out_edges, node: Iran, relation: part of, target: Middle East\n",
      "out_edges, node: Iran, relation: official language, target: Persian\n",
      "out_edges, node: Iran, relation: diplomatic relation, target: Pakistan\n",
      "out_edges, node: Iran, relation: member of, target: Organisation of Islamic Cooperation\n",
      "out_edges, node: Iran, relation: head of state, target: Ali Khamenei\n",
      "out_edges, node: Iran, relation: language used, target: Armenian\n",
      "out_edges, node: Iran, relation: public holiday, target: Eid al-Fitr\n",
      "out_edges, node: Iran, relation: located in or next to body of water, target: Persian Gulf\n",
      "out_edges, node: Iran, relation: contains the administrative territorial entity, target: Mazandaran Province\n",
      "out_edges, node: Iran, relation: instance of, target: country\n",
      "out_edges, node: Iran, relation: official religion, target: Islam\n",
      "out_edges, node: Iran, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Iran, relation: country, target: Iran\n",
      "out_edges, node: Iran, relation: ethnic group, target: Kurds\n",
      "out_edges, node: Iran, relation: central bank, target: Central Bank of Iran\n",
      "out_edges, node: Iran, relation: lowest point, target: Caspian Sea\n",
      "out_edges, node: Iran, relation: capital, target: Tehran\n",
      "out_edges, node: Iran, relation: continent, target: Asia\n",
      "out_edges, node: Northern Mariana Islands, relation: official color, target: white\n",
      "out_edges, node: Northern Mariana Islands, relation: language used, target: English\n",
      "out_edges, node: Northern Mariana Islands, relation: named after, target: Mariana Islands\n",
      "out_edges, node: Northern Mariana Islands, relation: instance of, target: country\n",
      "out_edges, node: Northern Mariana Islands, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Northern Mariana Islands, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Northern Mariana Islands, relation: part of, target: Micronesia\n",
      "out_edges, node: Northern Mariana Islands, relation: country, target: United States of America\n",
      "out_edges, node: Northern Mariana Islands, relation: currency, target: United States dollar\n",
      "out_edges, node: Northern Mariana Islands, relation: has part(s) of the class, target: island\n",
      "in_edges, node: Northern Mariana Islands, relation: contains the administrative territorial entity, target: Northern Mariana Islands\n",
      "in_edges, node: white, relation: official color, target: white\n",
      "in_edges, node: UNESCO, relation: member of, target: UNESCO\n",
      "out_edges, node: Chad, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Chad, relation: diplomatic relation, target: European Union\n",
      "out_edges, node: Chad, relation: named after, target: Lake Chad\n",
      "out_edges, node: Chad, relation: head of government, target: Idriss Déby\n",
      "out_edges, node: Chad, relation: shares border with, target: Cameroon\n",
      "out_edges, node: Chad, relation: language used, target: Sango\n",
      "out_edges, node: Chad, relation: official language, target: Arabic\n",
      "out_edges, node: Chad, relation: located in/on physical feature, target: Sahel\n",
      "out_edges, node: Chad, relation: instance of, target: country\n",
      "out_edges, node: Chad, relation: contains the administrative territorial entity, target: N'Djamena\n",
      "out_edges, node: Chad, relation: country, target: Chad\n",
      "out_edges, node: Chad, relation: continent, target: Africa\n",
      "out_edges, node: Egypt, relation: member of, target: International Finance Corporation\n",
      "out_edges, node: Egypt, relation: continent, target: Asia\n",
      "out_edges, node: Egypt, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Egypt, relation: instance of, target: country\n",
      "out_edges, node: Egypt, relation: part of, target: North Africa\n",
      "out_edges, node: Egypt, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Egypt, relation: official language, target: Arabic\n",
      "out_edges, node: Egypt, relation: capital, target: Cairo\n",
      "out_edges, node: Egypt, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Egypt, relation: contains the administrative territorial entity, target: Sohag Governorate\n",
      "out_edges, node: Egypt, relation: basic form of government, target: republic\n",
      "out_edges, node: Egypt, relation: country, target: Egypt\n",
      "out_edges, node: Egypt, relation: language used, target: Coptic\n",
      "out_edges, node: Egypt, relation: shares border with, target: Gaza Strip\n",
      "out_edges, node: Egypt, relation: head of state, target: Abdel Fattah el-Miksiki\n",
      "out_edges, node: Egypt, relation: named after, target: Ptah\n",
      "in_edges, node: Isle of Man, relation: diplomatic relation, target: Isle of Man\n",
      "in_edges, node: Arabian Sea, relation: lowest point, target: Arabian Sea\n",
      "in_edges, node: Arabian Sea, relation: located in or next to body of water, target: Arabian Sea\n",
      "in_edges, node: Balochi, relation: language used, target: Balochi\n",
      "in_edges, node: Balochi, relation: official language, target: Balochi\n",
      "in_edges, node: Monaco, relation: diplomatic relation, target: Monaco\n",
      "in_edges, node: Catholicism, relation: religion or worldview, target: Catholicism\n",
      "out_edges, node: Haiti, relation: member of, target: UNESCO\n",
      "out_edges, node: Haiti, relation: shares border with, target: Dominican Republic\n",
      "out_edges, node: Haiti, relation: contains the administrative territorial entity, target: Artibonite\n",
      "out_edges, node: Haiti, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Haiti, relation: official language, target: French\n",
      "out_edges, node: Haiti, relation: part of, target: Caribbean\n",
      "out_edges, node: Haiti, relation: capital, target: Port-au-Prince\n",
      "out_edges, node: Haiti, relation: continent, target: North America\n",
      "out_edges, node: Haiti, relation: country, target: Haiti\n",
      "out_edges, node: Haiti, relation: instance of, target: country\n",
      "out_edges, node: Haiti, relation: lowest point, target: Caribbean Sea\n",
      "in_edges, node: Asian Development Bank, relation: member of, target: Asian Development Bank\n",
      "out_edges, node: Oman, relation: diplomatic relation, target: Bangladesh\n",
      "out_edges, node: Oman, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Oman, relation: shares border with, target: United Arab Emirates\n",
      "out_edges, node: Oman, relation: official language, target: Arabic\n",
      "out_edges, node: Oman, relation: part of, target: Middle East\n",
      "out_edges, node: Oman, relation: country, target: Oman\n",
      "out_edges, node: Oman, relation: continent, target: Asia\n",
      "out_edges, node: Oman, relation: instance of, target: country\n",
      "out_edges, node: Oman, relation: located in/on physical feature, target: Arabian Peninsula\n",
      "out_edges, node: Oman, relation: language used, target: English\n",
      "out_edges, node: Oman, relation: official religion, target: Islam\n",
      "out_edges, node: Oman, relation: capital, target: Muscat\n",
      "out_edges, node: Switzerland, relation: diplomatic relation, target: Argentina\n",
      "out_edges, node: Switzerland, relation: member of, target: Council of Europe\n",
      "out_edges, node: Switzerland, relation: lowest point, target: Lake Maggiore\n",
      "out_edges, node: Switzerland, relation: shares border with, target: Germany\n",
      "out_edges, node: Switzerland, relation: language used, target: French\n",
      "out_edges, node: Switzerland, relation: continent, target: Europe\n",
      "out_edges, node: Switzerland, relation: official language, target: Italian\n",
      "out_edges, node: Switzerland, relation: country, target: Switzerland\n",
      "out_edges, node: Switzerland, relation: contains the administrative territorial entity, target: Basel-Stadt\n",
      "out_edges, node: Switzerland, relation: named after, target: Schwyz\n",
      "out_edges, node: Switzerland, relation: executive body, target: Swiss Federal Council\n",
      "out_edges, node: Switzerland, relation: located in/on physical feature, target: Central Europe\n",
      "out_edges, node: Switzerland, relation: central bank, target: Swiss National Bank\n",
      "out_edges, node: Switzerland, relation: currency, target: Swiss franc\n",
      "out_edges, node: Switzerland, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Switzerland, relation: instance of, target: country\n",
      "out_edges, node: Switzerland, relation: public holiday, target: Swiss National Day\n",
      "out_edges, node: Argentina, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Argentina, relation: diplomatic relation, target: Bangladesh\n",
      "out_edges, node: Argentina, relation: contains the administrative territorial entity, target: Corrientes\n",
      "out_edges, node: Argentina, relation: part of, target: Latin America\n",
      "out_edges, node: Argentina, relation: language used, target: Spanish\n",
      "out_edges, node: Argentina, relation: central bank, target: Central Bank of Argentina\n",
      "out_edges, node: Argentina, relation: shares border with, target: Chile\n",
      "out_edges, node: Argentina, relation: capital, target: Buenos Aires\n",
      "out_edges, node: Argentina, relation: head of government, target: Alberto Fernández\n",
      "out_edges, node: Argentina, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Argentina, relation: instance of, target: nation\n",
      "out_edges, node: Argentina, relation: country, target: Argentina\n",
      "out_edges, node: Argentina, relation: named after, target: silver\n",
      "out_edges, node: Argentina, relation: highest point, target: Aconcagua\n",
      "out_edges, node: South Korea, relation: contains the administrative territorial entity, target: Seoul\n",
      "out_edges, node: South Korea, relation: diplomatic relation, target: Colombia\n",
      "out_edges, node: South Korea, relation: highest judicial authority, target: Constitutional Court of Korea\n",
      "out_edges, node: South Korea, relation: head of state, target: Yoon Suk Yeol\n",
      "out_edges, node: South Korea, relation: member of, target: UNESCO\n",
      "out_edges, node: South Korea, relation: shares border with, target: North Korea\n",
      "out_edges, node: South Korea, relation: central bank, target: Bank of Korea\n",
      "out_edges, node: South Korea, relation: instance of, target: country\n",
      "out_edges, node: South Korea, relation: public holiday, target: Gwangbokjeol\n",
      "out_edges, node: South Korea, relation: continent, target: Asia\n",
      "out_edges, node: South Korea, relation: located in or next to body of water, target: Yellow Sea\n",
      "out_edges, node: South Korea, relation: part of, target: East Asia\n",
      "out_edges, node: South Korea, relation: basic form of government, target: republic\n",
      "out_edges, node: South Korea, relation: official language, target: Korean\n",
      "out_edges, node: South Korea, relation: lowest point, target: Sea of Japan\n",
      "out_edges, node: South Korea, relation: country, target: South Korea\n",
      "out_edges, node: South Korea, relation: replaces, target: Korea\n",
      "out_edges, node: South Korea, relation: twinned administrative body, target: Toyota\n",
      "out_edges, node: South Korea, relation: religion or worldview, target: Protestantism\n",
      "in_edges, node: Universal Postal Union, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Azerbaijan, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Azerbaijan, relation: capital, target: Baku\n",
      "out_edges, node: Azerbaijan, relation: language used, target: Armenian\n",
      "out_edges, node: Azerbaijan, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Azerbaijan, relation: shares border with, target: Iran\n",
      "out_edges, node: Azerbaijan, relation: part of, target: Caucasus\n",
      "out_edges, node: Azerbaijan, relation: instance of, target: country\n",
      "out_edges, node: Azerbaijan, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Azerbaijan, relation: continent, target: Asia\n",
      "out_edges, node: Azerbaijan, relation: contains the administrative territorial entity, target: Lankaran\n",
      "out_edges, node: Azerbaijan, relation: head of state, target: Ilham Aliyev\n",
      "out_edges, node: Azerbaijan, relation: ethnic group, target: Armenians\n",
      "out_edges, node: Azerbaijan, relation: country, target: Azerbaijan\n",
      "out_edges, node: Azerbaijan, relation: basic form of government, target: republic\n",
      "out_edges, node: Azerbaijan, relation: lowest point, target: Caspian Sea\n",
      "out_edges, node: Azerbaijan, relation: official language, target: Azerbaijani\n",
      "in_edges, node: West Africa, relation: part of, target: West Africa\n",
      "in_edges, node: West Africa, relation: located in/on physical feature, target: West Africa\n",
      "in_edges, node: Organisation internationale de la Francophonie, relation: member of, target: Organisation internationale de la Francophonie\n",
      "out_edges, node: Chile, relation: part of, target: South America\n",
      "out_edges, node: Chile, relation: located in or next to body of water, target: Pacific Ocean\n",
      "out_edges, node: Chile, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Chile, relation: diplomatic relation, target: United Kingdom\n",
      "out_edges, node: Chile, relation: shares border with, target: Peru\n",
      "out_edges, node: Chile, relation: capital, target: Santiago\n",
      "out_edges, node: Chile, relation: head of government, target: Gabriel Boric\n",
      "out_edges, node: Chile, relation: country, target: Chile\n",
      "out_edges, node: Chile, relation: contains the administrative territorial entity, target: O'Higgins Region\n",
      "out_edges, node: Chile, relation: official language, target: Spanish\n",
      "out_edges, node: Chile, relation: instance of, target: nation\n",
      "out_edges, node: Chile, relation: language used, target: Mapudungun\n",
      "out_edges, node: Chile, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "in_edges, node: Doha, relation: has part(s), target: Doha\n",
      "out_edges, node: Madagascar, relation: member of, target: International Development Association\n",
      "out_edges, node: Madagascar, relation: official language, target: French\n",
      "out_edges, node: Madagascar, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Madagascar, relation: shares border with, target: Comoros\n",
      "out_edges, node: Madagascar, relation: part of, target: East Africa\n",
      "out_edges, node: Madagascar, relation: basic form of government, target: republic\n",
      "out_edges, node: Madagascar, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Madagascar, relation: language used, target: English\n",
      "out_edges, node: Madagascar, relation: named after, target: Mogadishu\n",
      "out_edges, node: Madagascar, relation: instance of, target: country\n",
      "out_edges, node: Madagascar, relation: head of state, target: Andry Rajoelina\n",
      "out_edges, node: Madagascar, relation: country, target: Madagascar\n",
      "out_edges, node: Madagascar, relation: capital, target: Antananarivo\n",
      "out_edges, node: Madagascar, relation: continent, target: Africa\n",
      "in_edges, node: International Development Association, relation: member of, target: International Development Association\n",
      "out_edges, node: Saint Lucia, relation: member of, target: Organization of American States\n",
      "out_edges, node: Saint Lucia, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Saint Lucia, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Saint Lucia, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Saint Lucia, relation: part of, target: Lesser Antilles\n",
      "out_edges, node: Saint Lucia, relation: instance of, target: country\n",
      "out_edges, node: Saint Lucia, relation: named after, target: Saint Lucy\n",
      "out_edges, node: Saint Lucia, relation: country, target: Saint Lucia\n",
      "out_edges, node: Saint Lucia, relation: official language, target: English\n",
      "out_edges, node: Saint Lucia, relation: continent, target: North America\n",
      "out_edges, node: Saint Lucia, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Saint Lucia, relation: capital, target: Castries\n",
      "out_edges, node: Saint Lucia, relation: shares border with, target: Venezuela\n",
      "in_edges, node: Organization of American States, relation: member of, target: Organization of American States\n",
      "in_edges, node: Kuala Lumpur, relation: capital, target: Kuala Lumpur\n",
      "out_edges, node: Ghana, relation: diplomatic relation, target: Burkina Faso\n",
      "out_edges, node: Ghana, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Ghana, relation: member of, target: Organisation of African, Caribbean and Pacific States\n",
      "out_edges, node: Ghana, relation: continent, target: Africa\n",
      "out_edges, node: Ghana, relation: language used, target: Akan\n",
      "out_edges, node: Ghana, relation: head of state, target: Nana Akufo-Addo\n",
      "out_edges, node: Ghana, relation: official language, target: English\n",
      "out_edges, node: Ghana, relation: shares border with, target: Ivory Coast\n",
      "out_edges, node: Ghana, relation: capital, target: Accra\n",
      "out_edges, node: Ghana, relation: country, target: Ghana\n",
      "out_edges, node: Ghana, relation: lowest point, target: Gulf of Guinea\n",
      "out_edges, node: Ghana, relation: part of, target: West Africa\n",
      "out_edges, node: Ghana, relation: basic form of government, target: democracy\n",
      "out_edges, node: Ghana, relation: instance of, target: country\n",
      "out_edges, node: Honduras, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Honduras, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Honduras, relation: continent, target: North America\n",
      "out_edges, node: Honduras, relation: part of, target: Latin America\n",
      "out_edges, node: Honduras, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Honduras, relation: country, target: Honduras\n",
      "out_edges, node: Honduras, relation: instance of, target: country\n",
      "out_edges, node: Honduras, relation: shares border with, target: Guatemala\n",
      "out_edges, node: Honduras, relation: capital, target: Tegucigalpa\n",
      "out_edges, node: Honduras, relation: language used, target: Spanish\n",
      "out_edges, node: Honduras, relation: basic form of government, target: republic\n",
      "out_edges, node: Vietnam, relation: official language, target: Vietnamese\n",
      "out_edges, node: Vietnam, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Vietnam, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Vietnam, relation: contains the administrative territorial entity, target: Haiphong\n",
      "out_edges, node: Vietnam, relation: capital, target: Hanoi\n",
      "out_edges, node: Vietnam, relation: lowest point, target: South China Sea\n",
      "out_edges, node: Vietnam, relation: part of, target: Indochina\n",
      "out_edges, node: Vietnam, relation: ethnic group, target: Hmong people\n",
      "out_edges, node: Vietnam, relation: religion or worldview, target: Catholicism\n",
      "out_edges, node: Vietnam, relation: follows, target: North Vietnam\n",
      "out_edges, node: Vietnam, relation: country, target: Vietnam\n",
      "out_edges, node: Vietnam, relation: continent, target: Asia\n",
      "out_edges, node: Vietnam, relation: language used, target: Khmer\n",
      "out_edges, node: Vietnam, relation: instance of, target: country\n",
      "in_edges, node: Vietnam, relation: shares border with, target: Vietnam\n",
      "out_edges, node: New Zealand, relation: diplomatic relation, target: Australia\n",
      "out_edges, node: New Zealand, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: New Zealand, relation: member of, target: United Nations\n",
      "out_edges, node: New Zealand, relation: contains the administrative territorial entity, target: Wellington Region\n",
      "out_edges, node: New Zealand, relation: official language, target: Māori\n",
      "out_edges, node: New Zealand, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: New Zealand, relation: central bank, target: Reserve Bank of New Zealand\n",
      "out_edges, node: New Zealand, relation: part of, target: Australasia\n",
      "out_edges, node: New Zealand, relation: instance of, target: country\n",
      "out_edges, node: New Zealand, relation: continent, target: Insular Oceania\n",
      "out_edges, node: New Zealand, relation: highest point, target: Aoraki / Mount Cook\n",
      "out_edges, node: New Zealand, relation: capital, target: Wellington\n",
      "out_edges, node: New Zealand, relation: country, target: New Zealand\n",
      "out_edges, node: New Zealand, relation: named after, target: Zeeland\n",
      "out_edges, node: Hungary, relation: contains the administrative territorial entity, target: Pest County\n",
      "out_edges, node: Hungary, relation: shares border with, target: Romania\n",
      "out_edges, node: Hungary, relation: diplomatic relation, target: Kazakhstan\n",
      "out_edges, node: Hungary, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Hungary, relation: part of, target: European Economic Area\n",
      "out_edges, node: Hungary, relation: language used, target: Slovene\n",
      "out_edges, node: Hungary, relation: head of government, target: Viktor Orbán\n",
      "out_edges, node: Hungary, relation: country, target: Hungary\n",
      "out_edges, node: Hungary, relation: official language, target: Hungarian\n",
      "out_edges, node: Hungary, relation: instance of, target: country\n",
      "out_edges, node: Hungary, relation: located in or next to body of water, target: Danube\n",
      "out_edges, node: Hungary, relation: continent, target: Europe\n",
      "out_edges, node: Hungary, relation: central bank, target: Hungarian National Bank\n",
      "out_edges, node: Hungary, relation: located in/on physical feature, target: Central Europe\n",
      "out_edges, node: Hungary, relation: named after, target: Hungarians\n",
      "out_edges, node: Hungary, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Hungary, relation: capital, target: Budapest\n",
      "in_edges, node: Pest County, relation: contains the administrative territorial entity, target: Pest County\n",
      "in_edges, node: Lee Hsien Loong, relation: head of government, target: Lee Hsien Loong\n",
      "in_edges, node: Council of Europe, relation: member of, target: Council of Europe\n",
      "in_edges, node: Southern Africa, relation: part of, target: Southern Africa\n",
      "in_edges, node: Southern Africa, relation: different from, target: Southern Africa\n",
      "in_edges, node: Southern Africa, relation: located in/on physical feature, target: Southern Africa\n",
      "in_edges, node: Latin American Economic System, relation: member of, target: Latin American Economic System\n",
      "out_edges, node: Saudi Arabia, relation: diplomatic relation, target: Greece\n",
      "out_edges, node: Saudi Arabia, relation: part of, target: West Asia\n",
      "out_edges, node: Saudi Arabia, relation: official language, target: Arabic\n",
      "out_edges, node: Saudi Arabia, relation: member of, target: UNESCO\n",
      "out_edges, node: Saudi Arabia, relation: located in/on physical feature, target: Arabian Peninsula\n",
      "out_edges, node: Saudi Arabia, relation: participant in, target: Gulf War\n",
      "out_edges, node: Saudi Arabia, relation: shares border with, target: Yemen\n",
      "out_edges, node: Saudi Arabia, relation: instance of, target: country\n",
      "out_edges, node: Saudi Arabia, relation: located in or next to body of water, target: Gulf of Aqaba\n",
      "out_edges, node: Saudi Arabia, relation: country, target: Saudi Arabia\n",
      "out_edges, node: Saudi Arabia, relation: public holiday, target: Eid al-Adha\n",
      "out_edges, node: Saudi Arabia, relation: head of government, target: Salman bin Abdulaziz Al Saud\n",
      "out_edges, node: Saudi Arabia, relation: official religion, target: Islam\n",
      "out_edges, node: Saudi Arabia, relation: continent, target: Asia\n",
      "out_edges, node: Saudi Arabia, relation: capital, target: Riyadh\n",
      "out_edges, node: Saudi Arabia, relation: official symbol, target: Falco\n",
      "out_edges, node: Saudi Arabia, relation: religion or worldview, target: Wahhabism\n",
      "in_edges, node: Europe, relation: continent, target: Europe\n",
      "in_edges, node: Europe, relation: part of, target: Europe\n",
      "out_edges, node: Albania, relation: diplomatic relation, target: Greece\n",
      "out_edges, node: Albania, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Albania, relation: language used, target: Aromanian\n",
      "out_edges, node: Albania, relation: capital, target: Tirana\n",
      "out_edges, node: Albania, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Albania, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Albania, relation: continent, target: Europe\n",
      "out_edges, node: Albania, relation: ethnic group, target: Greeks\n",
      "out_edges, node: Albania, relation: shares border with, target: European Union\n",
      "out_edges, node: Albania, relation: instance of, target: country\n",
      "out_edges, node: Albania, relation: located in/on physical feature, target: Balkans\n",
      "out_edges, node: Albania, relation: basic form of government, target: parliamentary system\n",
      "out_edges, node: Albania, relation: official language, target: Albanian\n",
      "out_edges, node: Albania, relation: head of government, target: Edi Rama\n",
      "out_edges, node: Albania, relation: country, target: Albania\n",
      "in_edges, node: Seoul, relation: contains the administrative territorial entity, target: Seoul\n",
      "in_edges, node: Amazonas, relation: contains the administrative territorial entity, target: Amazonas\n",
      "in_edges, node: opal, relation: official symbol, target: opal\n",
      "out_edges, node: Comoros, relation: member of, target: African Development Bank\n",
      "out_edges, node: Comoros, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Comoros, relation: continent, target: Africa\n",
      "out_edges, node: Comoros, relation: diplomatic relation, target: Georgia\n",
      "out_edges, node: Comoros, relation: part of, target: East Africa\n",
      "out_edges, node: Comoros, relation: language used, target: French\n",
      "out_edges, node: Comoros, relation: described by source, target: The World Factbook\n",
      "out_edges, node: Comoros, relation: contains the administrative territorial entity, target: Grande Comore\n",
      "out_edges, node: Comoros, relation: instance of, target: country\n",
      "out_edges, node: Comoros, relation: shares border with, target: Seychelles\n",
      "out_edges, node: Comoros, relation: country, target: Comoros\n",
      "out_edges, node: Comoros, relation: official religion, target: Islam\n",
      "out_edges, node: Comoros, relation: official language, target: Arabic\n",
      "out_edges, node: Comoros, relation: capital, target: Moroni\n",
      "in_edges, node: International Red Cross and Red Crescent Movement, relation: member of, target: International Red Cross and Red Crescent Movement\n",
      "in_edges, node: Eid al-Adha, relation: public holiday, target: Eid al-Adha\n",
      "out_edges, node: Transnistria, relation: official language, target: Ukrainian\n",
      "out_edges, node: Transnistria, relation: shares border with, target: Moldova\n",
      "out_edges, node: Transnistria, relation: instance of, target: Rechtsstaat\n",
      "out_edges, node: Transnistria, relation: diplomatic relation, target: South Ossetia\n",
      "out_edges, node: Transnistria, relation: country, target: Transnistria\n",
      "out_edges, node: Transnistria, relation: public holiday, target: International Workers' Day\n",
      "out_edges, node: Transnistria, relation: capital, target: Tiraspol\n",
      "out_edges, node: Transnistria, relation: official religion, target: Russian Orthodox Church\n",
      "out_edges, node: Transnistria, relation: contains the administrative territorial entity, target: Bender\n",
      "out_edges, node: Transnistria, relation: basic form of government, target: republic\n",
      "out_edges, node: Transnistria, relation: continent, target: Europe\n",
      "in_edges, node: Ukrainian, relation: official language, target: Ukrainian\n",
      "in_edges, node: Ukrainian, relation: language used, target: Ukrainian\n",
      "out_edges, node: Kenya, relation: shares border with, target: Ethiopia\n",
      "out_edges, node: Kenya, relation: diplomatic relation, target: Uganda\n",
      "out_edges, node: Kenya, relation: twinned administrative body, target: Cheltenham\n",
      "out_edges, node: Kenya, relation: language used, target: Gikuyu\n",
      "out_edges, node: Kenya, relation: ethnic group, target: African people\n",
      "out_edges, node: Kenya, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Kenya, relation: official language, target: English\n",
      "out_edges, node: Kenya, relation: part of, target: East Africa\n",
      "out_edges, node: Kenya, relation: instance of, target: country\n",
      "out_edges, node: Kenya, relation: capital, target: Nairobi\n",
      "out_edges, node: Kenya, relation: highest point, target: Mount Kenya\n",
      "out_edges, node: Kenya, relation: continent, target: Africa\n",
      "out_edges, node: Kenya, relation: country, target: Kenya\n",
      "out_edges, node: Kenya, relation: lowest point, target: Indian Ocean\n",
      "in_edges, node: South America, relation: part of, target: South America\n",
      "in_edges, node: South America, relation: continent, target: South America\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: official language, target: English\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: capital, target: Kingstown\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: continent, target: North America\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: country, target: Saint Vincent and the Grenadines\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: named after, target: Vincent of Saragossa\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: part of, target: Lesser Antilles\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: instance of, target: country\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: shares border with, target: Venezuela\n",
      "out_edges, node: Saint Vincent and the Grenadines, relation: basic form of government, target: constitutional monarchy\n",
      "in_edges, node: Iraq War, relation: significant event, target: Iraq War\n",
      "in_edges, node: Iraq War, relation: participant in, target: Iraq War\n",
      "in_edges, node: Malay, relation: official language, target: Malay\n",
      "in_edges, node: Malay, relation: language used, target: Malay\n",
      "out_edges, node: Sri Lanka, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Sri Lanka, relation: diplomatic relation, target: Japan\n",
      "out_edges, node: Sri Lanka, relation: instance of, target: republic\n",
      "out_edges, node: Sri Lanka, relation: religion or worldview, target: Hinduism\n",
      "out_edges, node: Sri Lanka, relation: country, target: Sri Lanka\n",
      "out_edges, node: Sri Lanka, relation: capital, target: Colombo\n",
      "out_edges, node: Sri Lanka, relation: located in/on physical feature, target: South Asia\n",
      "out_edges, node: Sri Lanka, relation: official language, target: Sinhala\n",
      "out_edges, node: Sri Lanka, relation: language used, target: English\n",
      "out_edges, node: Sri Lanka, relation: continent, target: Asia\n",
      "out_edges, node: Sri Lanka, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Sri Lanka, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Czech Republic, relation: diplomatic relation, target: Mexico\n",
      "out_edges, node: Czech Republic, relation: part of, target: European Union\n",
      "out_edges, node: Czech Republic, relation: member of, target: United Nations\n",
      "out_edges, node: Czech Republic, relation: central bank, target: Czech National Bank\n",
      "out_edges, node: Czech Republic, relation: public holiday, target: Good Friday\n",
      "out_edges, node: Czech Republic, relation: has part(s), target: Moravia\n",
      "out_edges, node: Czech Republic, relation: language used, target: Polish\n",
      "out_edges, node: Czech Republic, relation: head of state, target: Petr Pavel\n",
      "out_edges, node: Czech Republic, relation: contains the administrative territorial entity, target: Liberec Region\n",
      "out_edges, node: Czech Republic, relation: capital, target: Prague\n",
      "out_edges, node: Czech Republic, relation: continent, target: Europe\n",
      "out_edges, node: Czech Republic, relation: located in or next to body of water, target: Oder\n",
      "out_edges, node: Czech Republic, relation: religion or worldview, target: Catholic Church\n",
      "out_edges, node: Czech Republic, relation: shares border with, target: Germany\n",
      "out_edges, node: Czech Republic, relation: country, target: Czech Republic\n",
      "out_edges, node: Czech Republic, relation: ethnic group, target: Slovaks\n",
      "out_edges, node: Czech Republic, relation: archives at, target: National Archives\n",
      "out_edges, node: Czech Republic, relation: instance of, target: country\n",
      "out_edges, node: Czech Republic, relation: lowest point, target: Elbe\n",
      "out_edges, node: Czech Republic, relation: head of government, target: Petr Fiala\n",
      "out_edges, node: Czech Republic, relation: named after, target: Czechs\n",
      "in_edges, node: Czech Republic, relation: designated as terrorist by, target: Czech Republic\n",
      "in_edges, node: Germans, relation: ethnic group, target: Germans\n",
      "in_edges, node: Prayut Chan-o-cha, relation: head of government, target: Prayut Chan-o-cha\n",
      "out_edges, node: United Arab Emirates, relation: diplomatic relation, target: Australia\n",
      "out_edges, node: United Arab Emirates, relation: member of, target: Organisation of Islamic Cooperation\n",
      "out_edges, node: United Arab Emirates, relation: contains the administrative territorial entity, target: Emirate of Abu Dhabi\n",
      "out_edges, node: United Arab Emirates, relation: part of, target: West Asia\n",
      "out_edges, node: United Arab Emirates, relation: shares border with, target: Iran\n",
      "out_edges, node: United Arab Emirates, relation: continent, target: Asia\n",
      "out_edges, node: United Arab Emirates, relation: head of government, target: Mohammed bin Rashid Al Maktoum\n",
      "out_edges, node: United Arab Emirates, relation: official language, target: Arabic\n",
      "out_edges, node: United Arab Emirates, relation: country, target: United Arab Emirates\n",
      "out_edges, node: United Arab Emirates, relation: lowest point, target: Persian Gulf\n",
      "out_edges, node: United Arab Emirates, relation: language used, target: English\n",
      "out_edges, node: United Arab Emirates, relation: capital, target: Abu Dhabi\n",
      "out_edges, node: United Arab Emirates, relation: instance of, target: country\n",
      "out_edges, node: United Arab Emirates, relation: official religion, target: Islam\n",
      "out_edges, node: Dominican Republic, relation: member of, target: World Health Organization\n",
      "out_edges, node: Dominican Republic, relation: shares border with, target: Venezuela\n",
      "out_edges, node: Dominican Republic, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Dominican Republic, relation: part of, target: Caribbean\n",
      "out_edges, node: Dominican Republic, relation: language used, target: Spanish\n",
      "out_edges, node: Dominican Republic, relation: named after, target: Santo Domingo\n",
      "out_edges, node: Dominican Republic, relation: continent, target: North America\n",
      "out_edges, node: Dominican Republic, relation: different from, target: Dominica\n",
      "out_edges, node: Dominican Republic, relation: country, target: Dominican Republic\n",
      "out_edges, node: Dominican Republic, relation: instance of, target: country\n",
      "out_edges, node: Nicaragua, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Nicaragua, relation: head of state, target: Daniel Ortega\n",
      "out_edges, node: Nicaragua, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Nicaragua, relation: language used, target: Spanish\n",
      "out_edges, node: Nicaragua, relation: shares border with, target: Costa Rica\n",
      "out_edges, node: Nicaragua, relation: instance of, target: country\n",
      "out_edges, node: Nicaragua, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Nicaragua, relation: basic form of government, target: republic\n",
      "out_edges, node: Nicaragua, relation: country, target: Nicaragua\n",
      "out_edges, node: Nicaragua, relation: part of, target: Central America\n",
      "out_edges, node: Nicaragua, relation: capital, target: Managua\n",
      "out_edges, node: Nicaragua, relation: continent, target: North America\n",
      "in_edges, node: Nepali, relation: language used, target: Nepali\n",
      "in_edges, node: Baltic Sea, relation: located in or next to body of water, target: Baltic Sea\n",
      "in_edges, node: Baltic Sea, relation: lowest point, target: Baltic Sea\n",
      "in_edges, node: World Trade Organization, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Eritrea, relation: shares border with, target: Sudan\n",
      "out_edges, node: Eritrea, relation: member of, target: International Development Association\n",
      "out_edges, node: Eritrea, relation: head of government, target: Isaias Afwerki\n",
      "out_edges, node: Eritrea, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Eritrea, relation: language used, target: Afar\n",
      "out_edges, node: Eritrea, relation: official language, target: Tigrinya\n",
      "out_edges, node: Eritrea, relation: instance of, target: country\n",
      "out_edges, node: Eritrea, relation: capital, target: Asmara\n",
      "out_edges, node: Eritrea, relation: part of, target: East Africa\n",
      "out_edges, node: Eritrea, relation: named after, target: Red Sea\n",
      "out_edges, node: Eritrea, relation: continent, target: Africa\n",
      "out_edges, node: Eritrea, relation: country, target: Eritrea\n",
      "out_edges, node: Sudan, relation: member of, target: Arab League\n",
      "out_edges, node: Sudan, relation: shares border with, target: Egypt\n",
      "out_edges, node: Sudan, relation: instance of, target: country\n",
      "out_edges, node: Sudan, relation: diplomatic relation, target: South Sudan\n",
      "out_edges, node: Sudan, relation: country, target: Sudan\n",
      "out_edges, node: Sudan, relation: language used, target: English\n",
      "out_edges, node: Sudan, relation: capital, target: Khartoum\n",
      "out_edges, node: Sudan, relation: part of, target: North Africa\n",
      "out_edges, node: Sudan, relation: ethnic group, target: Fulbe people\n",
      "out_edges, node: Sudan, relation: continent, target: Africa\n",
      "out_edges, node: Sudan, relation: head of government, target: Abdalla Hamdok\n",
      "out_edges, node: Sudan, relation: located in/on physical feature, target: Sahel\n",
      "out_edges, node: Sudan, relation: official language, target: Arabic\n",
      "out_edges, node: Sudan, relation: lowest point, target: Red Sea\n",
      "in_edges, node: Sudan, relation: separated from, target: Sudan\n",
      "in_edges, node: Sea of Okhotsk, relation: located in or next to body of water, target: Sea of Okhotsk\n",
      "in_edges, node: Soviet Union, relation: diplomatic relation, target: Soviet Union\n",
      "in_edges, node: Soviet Union, relation: separated from, target: Soviet Union\n",
      "in_edges, node: Soviet Union, relation: shares border with, target: Soviet Union\n",
      "out_edges, node: The Gambia, relation: member of, target: World Trade Organization\n",
      "out_edges, node: The Gambia, relation: diplomatic relation, target: Philippines\n",
      "out_edges, node: The Gambia, relation: capital, target: Banjul\n",
      "out_edges, node: The Gambia, relation: language used, target: Mandinka\n",
      "out_edges, node: The Gambia, relation: shares border with, target: Senegal\n",
      "out_edges, node: The Gambia, relation: continent, target: Africa\n",
      "out_edges, node: The Gambia, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: The Gambia, relation: country, target: The Gambia\n",
      "out_edges, node: The Gambia, relation: basic form of government, target: republic\n",
      "out_edges, node: The Gambia, relation: located in/on physical feature, target: West Africa\n",
      "out_edges, node: The Gambia, relation: instance of, target: country\n",
      "out_edges, node: Guatemala, relation: diplomatic relation, target: India\n",
      "out_edges, node: Guatemala, relation: capital, target: Guatemala City\n",
      "out_edges, node: Guatemala, relation: member of, target: Interpol\n",
      "out_edges, node: Guatemala, relation: country, target: Guatemala\n",
      "out_edges, node: Guatemala, relation: language used, target: K’iche’\n",
      "out_edges, node: Guatemala, relation: continent, target: North America\n",
      "out_edges, node: Guatemala, relation: contains the administrative territorial entity, target: Petén Department\n",
      "out_edges, node: Guatemala, relation: part of, target: Central America\n",
      "out_edges, node: Guatemala, relation: located in or next to body of water, target: Pacific Ocean\n",
      "out_edges, node: Guatemala, relation: official language, target: Spanish\n",
      "out_edges, node: Guatemala, relation: shares border with, target: Belize\n",
      "out_edges, node: Guatemala, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Guatemala, relation: instance of, target: country\n",
      "in_edges, node: ASEAN, relation: member of, target: ASEAN\n",
      "in_edges, node: ASEAN, relation: shares border with, target: ASEAN\n",
      "in_edges, node: Baku, relation: capital, target: Baku\n",
      "in_edges, node: Corrientes, relation: contains the administrative territorial entity, target: Corrientes\n",
      "in_edges, node: Suriname, relation: diplomatic relation, target: Suriname\n",
      "in_edges, node: Suriname, relation: shares border with, target: Suriname\n",
      "in_edges, node: Rhode Island, relation: contains the administrative territorial entity, target: Rhode Island\n",
      "in_edges, node: Macky Sall, relation: head of government, target: Macky Sall\n",
      "in_edges, node: North Atlantic Treaty Organization, relation: designated as terrorist by, target: North Atlantic Treaty Organization\n",
      "in_edges, node: North Atlantic Treaty Organization, relation: member of, target: North Atlantic Treaty Organization\n",
      "out_edges, node: Kiribati, relation: diplomatic relation, target: Romania\n",
      "out_edges, node: Kiribati, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Kiribati, relation: located in/on physical feature, target: Micronesia\n",
      "out_edges, node: Kiribati, relation: shares border with, target: United States of America\n",
      "out_edges, node: Kiribati, relation: country, target: Kiribati\n",
      "out_edges, node: Kiribati, relation: language used, target: English\n",
      "out_edges, node: Kiribati, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Kiribati, relation: part of, target: Polynesia\n",
      "out_edges, node: Kiribati, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Kiribati, relation: instance of, target: country\n",
      "in_edges, node: Aust-Agder, relation: contains the administrative territorial entity, target: Aust-Agder\n",
      "out_edges, node: Estonia, relation: member of, target: League of Nations\n",
      "out_edges, node: Estonia, relation: part of, target: European Union\n",
      "out_edges, node: Estonia, relation: official language, target: Estonian\n",
      "out_edges, node: Estonia, relation: diplomatic relation, target: Uzbekistan\n",
      "out_edges, node: Estonia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Estonia, relation: shares border with, target: Russia\n",
      "out_edges, node: Estonia, relation: currency, target: euro\n",
      "out_edges, node: Estonia, relation: located in or next to body of water, target: Baltic Sea\n",
      "out_edges, node: Estonia, relation: contains the administrative territorial entity, target: Ida-Viru County\n",
      "out_edges, node: Estonia, relation: language used, target: Russian\n",
      "out_edges, node: Estonia, relation: head of government, target: Kaja Kallas\n",
      "out_edges, node: Estonia, relation: instance of, target: republic\n",
      "out_edges, node: Estonia, relation: continent, target: Europe\n",
      "out_edges, node: Estonia, relation: capital, target: Tallinn\n",
      "out_edges, node: Estonia, relation: country, target: Estonia\n",
      "in_edges, node: League of Nations, relation: member of, target: League of Nations\n",
      "out_edges, node: Trinidad and Tobago, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Trinidad and Tobago, relation: diplomatic relation, target: Japan\n",
      "out_edges, node: Trinidad and Tobago, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Trinidad and Tobago, relation: named after, target: Tobago\n",
      "out_edges, node: Trinidad and Tobago, relation: shares border with, target: Venezuela\n",
      "out_edges, node: Trinidad and Tobago, relation: located in/on physical feature, target: Caribbean\n",
      "out_edges, node: Trinidad and Tobago, relation: part of, target: Lesser Antilles\n",
      "out_edges, node: Trinidad and Tobago, relation: language used, target: English\n",
      "out_edges, node: Trinidad and Tobago, relation: contains the administrative territorial entity, target: Port of Spain\n",
      "out_edges, node: Trinidad and Tobago, relation: instance of, target: country\n",
      "out_edges, node: Trinidad and Tobago, relation: continent, target: North America\n",
      "out_edges, node: Trinidad and Tobago, relation: country, target: Trinidad and Tobago\n",
      "in_edges, node: Great Britain, relation: different from, target: Great Britain\n",
      "in_edges, node: Great Britain, relation: located in/on physical feature, target: Great Britain\n",
      "in_edges, node: Moldova, relation: diplomatic relation, target: Moldova\n",
      "in_edges, node: Moldova, relation: shares border with, target: Moldova\n",
      "in_edges, node: West Asia, relation: part of, target: West Asia\n",
      "in_edges, node: Andean Community, relation: member of, target: Andean Community\n",
      "out_edges, node: Botswana, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Botswana, relation: shares border with, target: Namibia\n",
      "out_edges, node: Botswana, relation: official language, target: English\n",
      "out_edges, node: Botswana, relation: instance of, target: country\n",
      "out_edges, node: Botswana, relation: member of, target: Southern African Development Community\n",
      "out_edges, node: Botswana, relation: part of, target: Southern Africa\n",
      "out_edges, node: Botswana, relation: separated from, target: United Kingdom\n",
      "out_edges, node: Botswana, relation: language used, target: Herero\n",
      "out_edges, node: Botswana, relation: basic form of government, target: republic\n",
      "out_edges, node: Botswana, relation: capital, target: Gaborone\n",
      "out_edges, node: Botswana, relation: country, target: Botswana\n",
      "out_edges, node: Botswana, relation: continent, target: Africa\n",
      "out_edges, node: Cyprus, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Cyprus, relation: diplomatic relation, target: Georgia\n",
      "out_edges, node: Cyprus, relation: language used, target: Turkish\n",
      "out_edges, node: Cyprus, relation: currency, target: euro\n",
      "out_edges, node: Cyprus, relation: official language, target: Modern Greek\n",
      "out_edges, node: Cyprus, relation: part of, target: European Economic Area\n",
      "out_edges, node: Cyprus, relation: located in/on physical feature, target: Cyprus\n",
      "out_edges, node: Cyprus, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Cyprus, relation: lowest point, target: Mediterranean Sea\n",
      "out_edges, node: Cyprus, relation: basic form of government, target: republic\n",
      "out_edges, node: Cyprus, relation: territory claimed by, target: Northern Cyprus\n",
      "out_edges, node: Cyprus, relation: shares border with, target: Israel\n",
      "out_edges, node: Cyprus, relation: instance of, target: country\n",
      "out_edges, node: Cyprus, relation: capital, target: Nicosia\n",
      "out_edges, node: Cyprus, relation: continent, target: Asia\n",
      "in_edges, node: Idaho, relation: contains the administrative territorial entity, target: Idaho\n",
      "in_edges, node: Nebraska, relation: contains the administrative territorial entity, target: Nebraska\n",
      "out_edges, node: Seychelles, relation: diplomatic relation, target: Taiwan\n",
      "out_edges, node: Seychelles, relation: member of, target: UNESCO\n",
      "out_edges, node: Seychelles, relation: country, target: Seychelles\n",
      "out_edges, node: Seychelles, relation: shares border with, target: Madagascar\n",
      "out_edges, node: Seychelles, relation: continent, target: Africa\n",
      "out_edges, node: Seychelles, relation: instance of, target: republic\n",
      "out_edges, node: Seychelles, relation: capital, target: Victoria\n",
      "out_edges, node: Seychelles, relation: language used, target: French\n",
      "out_edges, node: Seychelles, relation: part of, target: East Africa\n",
      "out_edges, node: Seychelles, relation: official language, target: English\n",
      "out_edges, node: Seychelles, relation: lowest point, target: Indian Ocean\n",
      "in_edges, node: Wolof, relation: language used, target: Wolof\n",
      "in_edges, node: Wolof, relation: official language, target: Wolof\n",
      "in_edges, node: presidential system, relation: instance of, target: presidential system\n",
      "in_edges, node: presidential system, relation: basic form of government, target: presidential system\n",
      "in_edges, node: Mauritius, relation: diplomatic relation, target: Mauritius\n",
      "in_edges, node: Mauritius, relation: shares border with, target: Mauritius\n",
      "in_edges, node: Turkmen, relation: language used, target: Turkmen\n",
      "in_edges, node: Manitoba, relation: contains the administrative territorial entity, target: Manitoba\n",
      "in_edges, node: Organisation of African, Caribbean and Pacific States, relation: member of, target: Organisation of African, Caribbean and Pacific States\n",
      "out_edges, node: Central African Republic, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Central African Republic, relation: official language, target: French\n",
      "out_edges, node: Central African Republic, relation: diplomatic relation, target: Sudan\n",
      "out_edges, node: Central African Republic, relation: country, target: Central African Republic\n",
      "out_edges, node: Central African Republic, relation: contains the administrative territorial entity, target: Bangui\n",
      "out_edges, node: Central African Republic, relation: language used, target: Sango\n",
      "out_edges, node: Central African Republic, relation: instance of, target: republic\n",
      "out_edges, node: Central African Republic, relation: located in/on physical feature, target: Central Africa\n",
      "out_edges, node: Central African Republic, relation: shares border with, target: Arab League\n",
      "out_edges, node: Central African Republic, relation: continent, target: Africa\n",
      "in_edges, node: Austria-Hungary, relation: replaces, target: Austria-Hungary\n",
      "out_edges, node: Kingdom of the Netherlands, relation: currency, target: euro\n",
      "out_edges, node: Kingdom of the Netherlands, relation: member of, target: United Nations\n",
      "out_edges, node: Kingdom of the Netherlands, relation: official language, target: Dutch\n",
      "out_edges, node: Kingdom of the Netherlands, relation: owner of, target: Royal Palace of Amsterdam\n",
      "out_edges, node: Kingdom of the Netherlands, relation: diplomatic relation, target: Belgium\n",
      "out_edges, node: Kingdom of the Netherlands, relation: capital, target: Amsterdam\n",
      "out_edges, node: Kingdom of the Netherlands, relation: shares border with, target: France\n",
      "out_edges, node: Kingdom of the Netherlands, relation: continent, target: Europe\n",
      "out_edges, node: Kingdom of the Netherlands, relation: contains the administrative territorial entity, target: Curaçao\n",
      "out_edges, node: Kingdom of the Netherlands, relation: part of, target: European Economic Area\n",
      "out_edges, node: Kingdom of the Netherlands, relation: language used, target: West Frisian\n",
      "out_edges, node: Kingdom of the Netherlands, relation: instance of, target: country\n",
      "out_edges, node: Kingdom of the Netherlands, relation: head of government, target: Mark Rutte\n",
      "out_edges, node: Kingdom of the Netherlands, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Kingdom of the Netherlands, relation: head of state, target: Willem-Alexander of the Netherlands\n",
      "in_edges, node: Kingdom of the Netherlands, relation: country, target: Kingdom of the Netherlands\n",
      "in_edges, node: Kingdom of the Netherlands, relation: located in the administrative territorial entity, target: Kingdom of the Netherlands\n",
      "out_edges, node: Iceland, relation: diplomatic relation, target: Russia\n",
      "out_edges, node: Iceland, relation: continent, target: Europe\n",
      "out_edges, node: Iceland, relation: member of, target: Council of Europe\n",
      "out_edges, node: Iceland, relation: shares border with, target: Greenland\n",
      "out_edges, node: Iceland, relation: part of, target: European Economic Area\n",
      "out_edges, node: Iceland, relation: public holiday, target: Feast of the Ascension\n",
      "out_edges, node: Iceland, relation: located in/on physical feature, target: Scandinavia\n",
      "out_edges, node: Iceland, relation: country, target: Iceland\n",
      "out_edges, node: Iceland, relation: instance of, target: country\n",
      "out_edges, node: Iceland, relation: language used, target: Icelandic\n",
      "out_edges, node: Iceland, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Iceland, relation: named after, target: ice\n",
      "out_edges, node: Iceland, relation: head of state, target: Guðni Jóhannesson\n",
      "out_edges, node: Iceland, relation: head of government, target: Katrín Jakobsdóttir\n",
      "out_edges, node: Iceland, relation: lowest point, target: Atlantic Ocean\n",
      "out_edges, node: Iceland, relation: taxon found at location, target: Vulpes lagopus\n",
      "out_edges, node: Iceland, relation: capital, target: Reykjavík\n",
      "out_edges, node: Togo, relation: capital, target: Lomé\n",
      "out_edges, node: Togo, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Togo, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Togo, relation: shares border with, target: Burkina Faso\n",
      "out_edges, node: Togo, relation: language used, target: French\n",
      "out_edges, node: Togo, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Togo, relation: head of state, target: Faure Essozimna Gnassingbé\n",
      "out_edges, node: Togo, relation: located in/on physical feature, target: West Africa\n",
      "out_edges, node: Togo, relation: instance of, target: republic\n",
      "out_edges, node: Togo, relation: continent, target: Africa\n",
      "out_edges, node: Togo, relation: country, target: Togo\n",
      "in_edges, node: Pashtuns, relation: ethnic group, target: Pashtuns\n",
      "in_edges, node: Black Sea, relation: lowest point, target: Black Sea\n",
      "in_edges, node: Black Sea, relation: located in or next to body of water, target: Black Sea\n",
      "out_edges, node: Northern Ireland, relation: different from, target: Ulster\n",
      "out_edges, node: Northern Ireland, relation: currency, target: pound sterling\n",
      "out_edges, node: Northern Ireland, relation: shares border with, target: Republic of Ireland\n",
      "out_edges, node: Northern Ireland, relation: patron saint, target: Saint Patrick\n",
      "out_edges, node: Northern Ireland, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Northern Ireland, relation: contains the administrative territorial entity, target: Belfast\n",
      "out_edges, node: Northern Ireland, relation: located in the administrative territorial entity, target: United Kingdom\n",
      "out_edges, node: Northern Ireland, relation: continent, target: Europe\n",
      "out_edges, node: Northern Ireland, relation: official language, target: English\n",
      "out_edges, node: Northern Ireland, relation: instance of, target: country\n",
      "in_edges, node: Ulster, relation: different from, target: Ulster\n",
      "in_edges, node: Ulster, relation: contains the administrative territorial entity, target: Ulster\n",
      "in_edges, node: Levant, relation: located in/on physical feature, target: Levant\n",
      "in_edges, node: Krasnoyarsk Krai, relation: contains the administrative territorial entity, target: Krasnoyarsk Krai\n",
      "in_edges, node: Artibonite, relation: contains the administrative territorial entity, target: Artibonite\n",
      "in_edges, node: east, relation: named after, target: east\n",
      "out_edges, node: Mongolia, relation: member of, target: International Finance Corporation\n",
      "out_edges, node: Mongolia, relation: diplomatic relation, target: Turkey\n",
      "out_edges, node: Mongolia, relation: capital, target: Ulaanbaatar\n",
      "out_edges, node: Mongolia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Mongolia, relation: shares border with, target: People's Republic of China\n",
      "out_edges, node: Mongolia, relation: located in/on physical feature, target: East Asia\n",
      "out_edges, node: Mongolia, relation: basic form of government, target: parliamentary system\n",
      "out_edges, node: Mongolia, relation: language used, target: Tuvan\n",
      "out_edges, node: Mongolia, relation: continent, target: Asia\n",
      "out_edges, node: Mongolia, relation: instance of, target: country\n",
      "out_edges, node: Mongolia, relation: named after, target: Mongols\n",
      "out_edges, node: Mongolia, relation: official observer status in organisation, target: Shanghai Cooperation Organisation\n",
      "out_edges, node: Mongolia, relation: official language, target: Mongolian\n",
      "out_edges, node: Mongolia, relation: country, target: Mongolia\n",
      "in_edges, node: representative democracy, relation: basic form of government, target: representative democracy\n",
      "out_edges, node: Bahrain, relation: official religion, target: Islam\n",
      "out_edges, node: Bahrain, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Bahrain, relation: member of, target: UNESCO\n",
      "out_edges, node: Bahrain, relation: instance of, target: archipelago\n",
      "out_edges, node: Bahrain, relation: lowest point, target: Persian Gulf\n",
      "out_edges, node: Bahrain, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Bahrain, relation: language used, target: English\n",
      "out_edges, node: Bahrain, relation: part of, target: West Asia\n",
      "out_edges, node: Bahrain, relation: continent, target: Asia\n",
      "out_edges, node: Bahrain, relation: shares border with, target: Iran\n",
      "out_edges, node: Bahrain, relation: capital, target: Manama\n",
      "out_edges, node: Bahrain, relation: head of state, target: Hamad II of Bahrain\n",
      "out_edges, node: Bahrain, relation: country, target: Bahrain\n",
      "out_edges, node: Bahrain, relation: official language, target: Arabic\n",
      "out_edges, node: Bahrain, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Bahrain, relation: located in/on physical feature, target: Middle East\n",
      "out_edges, node: Kyrgyzstan, relation: country, target: Kyrgyzstan\n",
      "out_edges, node: Kyrgyzstan, relation: language used, target: Kyrgyz\n",
      "out_edges, node: Kyrgyzstan, relation: continent, target: Asia\n",
      "out_edges, node: Kyrgyzstan, relation: member of, target: UNESCO\n",
      "out_edges, node: Kyrgyzstan, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Kyrgyzstan, relation: shares border with, target: People's Republic of China\n",
      "out_edges, node: Kyrgyzstan, relation: official language, target: Russian\n",
      "out_edges, node: Kyrgyzstan, relation: located in/on physical feature, target: Central Asia\n",
      "out_edges, node: Kyrgyzstan, relation: contains the administrative territorial entity, target: Osh\n",
      "out_edges, node: Kyrgyzstan, relation: capital, target: Bishkek\n",
      "out_edges, node: Kyrgyzstan, relation: instance of, target: republic\n",
      "out_edges, node: Laos, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Laos, relation: language used, target: Lao\n",
      "out_edges, node: Laos, relation: diplomatic relation, target: Philippines\n",
      "out_edges, node: Laos, relation: shares border with, target: People's Republic of China\n",
      "out_edges, node: Laos, relation: country, target: Laos\n",
      "out_edges, node: Laos, relation: continent, target: Asia\n",
      "out_edges, node: Laos, relation: lowest point, target: Mekong River\n",
      "out_edges, node: Laos, relation: capital, target: Vientiane\n",
      "out_edges, node: Laos, relation: instance of, target: country\n",
      "out_edges, node: Laos, relation: part of, target: Southeast Asia\n",
      "in_edges, node: Skolt Sami, relation: language used, target: Skolt Sami\n",
      "out_edges, node: Fiji, relation: member of, target: Commonwealth of Nations\n",
      "out_edges, node: Fiji, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Fiji, relation: country, target: Fiji\n",
      "out_edges, node: Fiji, relation: official language, target: Fijian\n",
      "out_edges, node: Fiji, relation: capital, target: Suva\n",
      "out_edges, node: Fiji, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Fiji, relation: located in/on physical feature, target: Melanesia\n",
      "out_edges, node: Fiji, relation: instance of, target: country\n",
      "out_edges, node: Fiji, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Fiji, relation: lowest point, target: Pacific Ocean\n",
      "in_edges, node: Fiji, relation: shares border with, target: Fiji\n",
      "in_edges, node: Armenian, relation: language used, target: Armenian\n",
      "in_edges, node: Armenian, relation: official language, target: Armenian\n",
      "in_edges, node: Assumption of Mary, relation: public holiday, target: Assumption of Mary\n",
      "out_edges, node: Myanmar, relation: member of, target: United Nations\n",
      "out_edges, node: Myanmar, relation: diplomatic relation, target: Russia\n",
      "out_edges, node: Myanmar, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Myanmar, relation: located in/on physical feature, target: Southeast Asia\n",
      "out_edges, node: Myanmar, relation: language used, target: Sylheti\n",
      "out_edges, node: Myanmar, relation: ethnic group, target: Karen people\n",
      "out_edges, node: Myanmar, relation: shares border with, target: India\n",
      "out_edges, node: Myanmar, relation: basic form of government, target: republic\n",
      "out_edges, node: Myanmar, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Myanmar, relation: instance of, target: country\n",
      "out_edges, node: Myanmar, relation: continent, target: Asia\n",
      "out_edges, node: Myanmar, relation: capital, target: Naypyidaw\n",
      "out_edges, node: Myanmar, relation: country, target: Myanmar\n",
      "in_edges, node: Adriatic Sea, relation: located in or next to body of water, target: Adriatic Sea\n",
      "in_edges, node: Adriatic Sea, relation: lowest point, target: Adriatic Sea\n",
      "in_edges, node: Awadhi, relation: language used, target: Awadhi\n",
      "in_edges, node: Zuzana Čaputová, relation: head of state, target: Zuzana Čaputová\n",
      "in_edges, node: Colorado, relation: contains the administrative territorial entity, target: Colorado\n",
      "in_edges, node: Hawaii, relation: contains the administrative territorial entity, target: Hawaii\n",
      "in_edges, node: Ceredigion, relation: contains the administrative territorial entity, target: Ceredigion\n",
      "in_edges, node: Haiphong, relation: contains the administrative territorial entity, target: Haiphong\n",
      "in_edges, node: New Hebrides, relation: replaces, target: New Hebrides\n",
      "in_edges, node: Ndonga, relation: language used, target: Ndonga\n",
      "out_edges, node: Syria, relation: shares border with, target: Israel\n",
      "out_edges, node: Syria, relation: diplomatic relation, target: Saudi Arabia\n",
      "out_edges, node: Syria, relation: member of, target: International Finance Corporation\n",
      "out_edges, node: Syria, relation: part of, target: Middle East\n",
      "out_edges, node: Syria, relation: located in/on physical feature, target: Levant\n",
      "out_edges, node: Syria, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Syria, relation: capital, target: Damascus\n",
      "out_edges, node: Syria, relation: continent, target: Asia\n",
      "out_edges, node: Syria, relation: instance of, target: country\n",
      "out_edges, node: Syria, relation: official language, target: Arabic\n",
      "out_edges, node: Syria, relation: country, target: Syria\n",
      "out_edges, node: Syria, relation: head of state, target: Bashar al-Assad\n",
      "out_edges, node: Syria, relation: lowest point, target: Sea of Galilee\n",
      "in_edges, node: European Economic Area, relation: part of, target: European Economic Area\n",
      "in_edges, node: European Economic Area, relation: member of, target: European Economic Area\n",
      "in_edges, node: Saint Anne, relation: patron saint, target: Saint Anne\n",
      "in_edges, node: Bank of Slovenia, relation: central bank, target: Bank of Slovenia\n",
      "in_edges, node: Daniel Ortega, relation: head of state, target: Daniel Ortega\n",
      "in_edges, node: Uyghur, relation: language used, target: Uyghur\n",
      "in_edges, node: Organisation of Islamic Cooperation, relation: member of, target: Organisation of Islamic Cooperation\n",
      "out_edges, node: Jamaica, relation: member of, target: Interpol\n",
      "out_edges, node: Jamaica, relation: instance of, target: country\n",
      "out_edges, node: Jamaica, relation: official language, target: English\n",
      "out_edges, node: Jamaica, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Jamaica, relation: diplomatic relation, target: Spain\n",
      "out_edges, node: Jamaica, relation: part of, target: Caribbean\n",
      "out_edges, node: Jamaica, relation: shares border with, target: United Kingdom\n",
      "out_edges, node: Jamaica, relation: country, target: Jamaica\n",
      "out_edges, node: Jamaica, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Jamaica, relation: capital, target: Kingston\n",
      "out_edges, node: Jamaica, relation: continent, target: North America\n",
      "in_edges, node: Hinduism, relation: religion or worldview, target: Hinduism\n",
      "in_edges, node: Tennessee, relation: contains the administrative territorial entity, target: Tennessee\n",
      "in_edges, node: Hong Kong, relation: diplomatic relation, target: Hong Kong\n",
      "in_edges, node: Hong Kong, relation: contains the administrative territorial entity, target: Hong Kong\n",
      "in_edges, node: Supreme Court of Japan, relation: highest judicial authority, target: Supreme Court of Japan\n",
      "out_edges, node: Bolivia, relation: member of, target: United Nations\n",
      "out_edges, node: Bolivia, relation: diplomatic relation, target: Hungary\n",
      "out_edges, node: Bolivia, relation: official language, target: Aymara\n",
      "out_edges, node: Bolivia, relation: country, target: Bolivia\n",
      "out_edges, node: Bolivia, relation: continent, target: South America\n",
      "out_edges, node: Bolivia, relation: named after, target: Simón Bolívar\n",
      "out_edges, node: Bolivia, relation: language used, target: Spanish\n",
      "out_edges, node: Bolivia, relation: described by source, target: The World Factbook\n",
      "out_edges, node: Bolivia, relation: capital, target: La Paz\n",
      "out_edges, node: Bolivia, relation: shares border with, target: Brazil\n",
      "out_edges, node: Bolivia, relation: instance of, target: country\n",
      "out_edges, node: Bolivia, relation: part of, target: Latin America\n",
      "out_edges, node: Antigua and Barbuda, relation: official language, target: English\n",
      "out_edges, node: Antigua and Barbuda, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Antigua and Barbuda, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Antigua and Barbuda, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Antigua and Barbuda, relation: contains the administrative territorial entity, target: Barbuda\n",
      "out_edges, node: Antigua and Barbuda, relation: diplomatic relation, target: Taiwan\n",
      "out_edges, node: Antigua and Barbuda, relation: part of, target: Caribbean\n",
      "out_edges, node: Antigua and Barbuda, relation: continent, target: North America\n",
      "out_edges, node: Antigua and Barbuda, relation: instance of, target: country\n",
      "out_edges, node: Antigua and Barbuda, relation: country, target: Antigua and Barbuda\n",
      "out_edges, node: Antigua and Barbuda, relation: capital, target: Saint John's\n",
      "out_edges, node: Antigua and Barbuda, relation: shares border with, target: United Kingdom\n",
      "in_edges, node: Oesterreichische Nationalbank, relation: central bank, target: Oesterreichische Nationalbank\n",
      "in_edges, node: Michoacán, relation: contains the administrative territorial entity, target: Michoacán\n",
      "in_edges, node: United States dollar, relation: currency, target: United States dollar\n",
      "in_edges, node: Reserve Bank of India, relation: central bank, target: Reserve Bank of India\n",
      "in_edges, node: Central Bank of Brazil, relation: central bank, target: Central Bank of Brazil\n",
      "in_edges, node: Indians, relation: ethnic group, target: Indians\n",
      "out_edges, node: Uzbekistan, relation: language used, target: Tajik\n",
      "out_edges, node: Uzbekistan, relation: continent, target: Asia\n",
      "out_edges, node: Uzbekistan, relation: ethnic group, target: Tajiks\n",
      "out_edges, node: Uzbekistan, relation: member of, target: Organisation of Islamic Cooperation\n",
      "out_edges, node: Uzbekistan, relation: diplomatic relation, target: Turkmenistan\n",
      "out_edges, node: Uzbekistan, relation: basic form of government, target: presidential system\n",
      "out_edges, node: Uzbekistan, relation: shares border with, target: Tajikistan\n",
      "out_edges, node: Uzbekistan, relation: part of, target: Central Asia\n",
      "out_edges, node: Uzbekistan, relation: capital, target: Tashkent\n",
      "out_edges, node: Uzbekistan, relation: instance of, target: country\n",
      "out_edges, node: Uzbekistan, relation: country, target: Uzbekistan\n",
      "out_edges, node: Uzbekistan, relation: head of state, target: Shavkat Mirziyoyev\n",
      "in_edges, node: Tajik, relation: language used, target: Tajik\n",
      "in_edges, node: Tajik, relation: official language, target: Tajik\n",
      "in_edges, node: Congo, relation: named after, target: Congo\n",
      "out_edges, node: Andorra, relation: shares border with, target: Spain\n",
      "out_edges, node: Andorra, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Andorra, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Andorra, relation: member of, target: Organisation for the Prohibition of Chemical Weapons\n",
      "out_edges, node: Andorra, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Andorra, relation: instance of, target: country\n",
      "out_edges, node: Andorra, relation: ethnic group, target: Spaniards\n",
      "out_edges, node: Andorra, relation: head of state, target: Emmanuel Macron\n",
      "out_edges, node: Andorra, relation: currency, target: euro\n",
      "out_edges, node: Andorra, relation: official language, target: Catalan\n",
      "out_edges, node: Andorra, relation: located in/on physical feature, target: Iberian Peninsula\n",
      "out_edges, node: Andorra, relation: country, target: Andorra\n",
      "out_edges, node: Andorra, relation: continent, target: Europe\n",
      "out_edges, node: Andorra, relation: contains the administrative territorial entity, target: Canillo\n",
      "out_edges, node: Andorra, relation: capital, target: Andorra la Vella\n",
      "in_edges, node: Arab League, relation: member of, target: Arab League\n",
      "in_edges, node: Arab League, relation: shares border with, target: Arab League\n",
      "in_edges, node: Arab League, relation: diplomatic relation, target: Arab League\n",
      "in_edges, node: Chinese New Year, relation: public holiday, target: Chinese New Year\n",
      "in_edges, node: Non-Aligned Movement, relation: member of, target: Non-Aligned Movement\n",
      "in_edges, node: Non-Aligned Movement, relation: part of, target: Non-Aligned Movement\n",
      "in_edges, node: Jan Mayen, relation: contains the administrative territorial entity, target: Jan Mayen\n",
      "in_edges, node: State of Palestine, relation: diplomatic relation, target: State of Palestine\n",
      "in_edges, node: Moscow, relation: contains the administrative territorial entity, target: Moscow\n",
      "in_edges, node: President of the United States, relation: office held by head of government, target: President of the United States\n",
      "in_edges, node: Knesset, relation: legislative body, target: Knesset\n",
      "in_edges, node: Slovene, relation: language used, target: Slovene\n",
      "in_edges, node: Slovene, relation: official language, target: Slovene\n",
      "in_edges, node: Gorontalo, relation: contains the administrative territorial entity, target: Gorontalo\n",
      "in_edges, node: United States Constitution, relation: main regulatory text, target: United States Constitution\n",
      "in_edges, node: Esperanto, relation: language used, target: Esperanto\n",
      "in_edges, node: Middle East, relation: part of, target: Middle East\n",
      "in_edges, node: Middle East, relation: shares border with, target: Middle East\n",
      "in_edges, node: Middle East, relation: located in/on physical feature, target: Middle East\n",
      "in_edges, node: Urdu, relation: language used, target: Urdu\n",
      "in_edges, node: Urdu, relation: official language, target: Urdu\n",
      "in_edges, node: Islamabad, relation: capital, target: Islamabad\n",
      "out_edges, node: Cuba, relation: diplomatic relation, target: Bolivia\n",
      "out_edges, node: Cuba, relation: twinned administrative body, target: Santa Fe\n",
      "out_edges, node: Cuba, relation: lowest point, target: Caribbean Sea\n",
      "out_edges, node: Cuba, relation: official language, target: Spanish\n",
      "out_edges, node: Cuba, relation: shares border with, target: United Kingdom\n",
      "out_edges, node: Cuba, relation: country, target: Cuba\n",
      "out_edges, node: Cuba, relation: member of, target: United Nations\n",
      "out_edges, node: Cuba, relation: continent, target: North America\n",
      "out_edges, node: Cuba, relation: capital, target: Havana\n",
      "out_edges, node: Cuba, relation: instance of, target: country\n",
      "out_edges, node: Cuba, relation: part of, target: Caribbean\n",
      "out_edges, node: Cuba, relation: contains the administrative territorial entity, target: Isla de la Juventud\n",
      "out_edges, node: Cuba, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Cuba, relation: located in or next to body of water, target: Gulf of Mexico\n",
      "in_edges, node: Eid al-Fitr, relation: public holiday, target: Eid al-Fitr\n",
      "in_edges, node: Persian, relation: official language, target: Persian\n",
      "in_edges, node: Persian, relation: language used, target: Persian\n",
      "in_edges, node: Hubei, relation: contains the administrative territorial entity, target: Hubei\n",
      "in_edges, node: Ireland, relation: located in/on physical feature, target: Ireland\n",
      "in_edges, node: Ireland, relation: named after, target: Ireland\n",
      "in_edges, node: Ireland, relation: different from, target: Ireland\n",
      "in_edges, node: Sahrawi Arab Democratic Republic, relation: diplomatic relation, target: Sahrawi Arab Democratic Republic\n",
      "out_edges, node: Niger, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Niger, relation: shares border with, target: Burkina Faso\n",
      "out_edges, node: Niger, relation: member of, target: World Meteorological Organization\n",
      "out_edges, node: Niger, relation: located in/on physical feature, target: Sahel\n",
      "out_edges, node: Niger, relation: continent, target: Africa\n",
      "out_edges, node: Niger, relation: replaces, target: French West Africa\n",
      "out_edges, node: Niger, relation: country, target: Niger\n",
      "out_edges, node: Niger, relation: instance of, target: country\n",
      "out_edges, node: Niger, relation: part of, target: West Africa\n",
      "out_edges, node: Niger, relation: language used, target: Hausa\n",
      "out_edges, node: Niger, relation: named after, target: Niger River\n",
      "out_edges, node: Niger, relation: capital, target: Niamey\n",
      "in_edges, node: Isaias Afwerki, relation: head of government, target: Isaias Afwerki\n",
      "out_edges, node: Lesotho, relation: capital, target: Maseru\n",
      "out_edges, node: Lesotho, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Lesotho, relation: named after, target: Sesotho\n",
      "out_edges, node: Lesotho, relation: shares border with, target: South Africa\n",
      "out_edges, node: Lesotho, relation: country, target: Lesotho\n",
      "out_edges, node: Lesotho, relation: instance of, target: country\n",
      "out_edges, node: Lesotho, relation: language used, target: Xhosa\n",
      "out_edges, node: Lesotho, relation: diplomatic relation, target: Russia\n",
      "out_edges, node: Lesotho, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Lesotho, relation: located in/on physical feature, target: Southern Africa\n",
      "out_edges, node: Lesotho, relation: twinned administrative body, target: Gummersbach\n",
      "out_edges, node: Lesotho, relation: continent, target: Africa\n",
      "in_edges, node: Maseru, relation: capital, target: Maseru\n",
      "out_edges, node: Tunisia, relation: member of, target: Interpol\n",
      "out_edges, node: Tunisia, relation: official language, target: Arabic\n",
      "out_edges, node: Tunisia, relation: ethnic group, target: Arabs\n",
      "out_edges, node: Tunisia, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Tunisia, relation: capital, target: Tunis\n",
      "out_edges, node: Tunisia, relation: shares border with, target: Libya\n",
      "out_edges, node: Tunisia, relation: instance of, target: country\n",
      "out_edges, node: Tunisia, relation: public holiday, target: Eid al-Adha\n",
      "out_edges, node: Tunisia, relation: country, target: Tunisia\n",
      "out_edges, node: Tunisia, relation: twinned administrative body, target: Seto\n",
      "out_edges, node: Tunisia, relation: part of, target: North Africa\n",
      "out_edges, node: Tunisia, relation: language used, target: French\n",
      "out_edges, node: Tunisia, relation: continent, target: Africa\n",
      "out_edges, node: Tunisia, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "in_edges, node: International Women's Day, relation: public holiday, target: International Women's Day\n",
      "out_edges, node: Mozambique, relation: diplomatic relation, target: Australia\n",
      "out_edges, node: Mozambique, relation: instance of, target: republic\n",
      "out_edges, node: Mozambique, relation: language used, target: Portuguese\n",
      "out_edges, node: Mozambique, relation: shares border with, target: Comoros\n",
      "out_edges, node: Mozambique, relation: part of, target: East Africa\n",
      "out_edges, node: Mozambique, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Mozambique, relation: country, target: Mozambique\n",
      "out_edges, node: Mozambique, relation: located in/on physical feature, target: Southern Africa\n",
      "out_edges, node: Mozambique, relation: ethnic group, target: African people\n",
      "out_edges, node: Mozambique, relation: capital, target: Maputo\n",
      "out_edges, node: Mozambique, relation: continent, target: Africa\n",
      "out_edges, node: Marshall Islands, relation: language used, target: Marshallese\n",
      "out_edges, node: Marshall Islands, relation: member of, target: World Health Organization\n",
      "out_edges, node: Marshall Islands, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Marshall Islands, relation: contains the administrative territorial entity, target: Majuro\n",
      "out_edges, node: Marshall Islands, relation: located in/on physical feature, target: Micronesia\n",
      "out_edges, node: Marshall Islands, relation: official language, target: English\n",
      "out_edges, node: Marshall Islands, relation: instance of, target: country\n",
      "out_edges, node: Marshall Islands, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Marshall Islands, relation: currency, target: United States dollar\n",
      "out_edges, node: Marshall Islands, relation: country, target: Marshall Islands\n",
      "out_edges, node: Marshall Islands, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Marshall Islands, relation: shares border with, target: Nauru\n",
      "in_edges, node: Marshallese, relation: language used, target: Marshallese\n",
      "in_edges, node: Irish, relation: language used, target: Irish\n",
      "in_edges, node: Irish, relation: official language, target: Irish\n",
      "in_edges, node: Georgia, relation: diplomatic relation, target: Georgia\n",
      "in_edges, node: Georgia, relation: shares border with, target: Georgia\n",
      "in_edges, node: Czech National Bank, relation: central bank, target: Czech National Bank\n",
      "out_edges, node: Aruba, relation: part of, target: Lesser Antilles\n",
      "out_edges, node: Aruba, relation: member of, target: UNESCO\n",
      "out_edges, node: Aruba, relation: head of state, target: Willem-Alexander of the Netherlands\n",
      "out_edges, node: Aruba, relation: language used, target: English\n",
      "out_edges, node: Aruba, relation: country, target: Aruba\n",
      "out_edges, node: Aruba, relation: official language, target: Papiamento\n",
      "out_edges, node: Aruba, relation: located in the administrative territorial entity, target: Kingdom of the Netherlands\n",
      "out_edges, node: Aruba, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Aruba, relation: continent, target: South America\n",
      "out_edges, node: Aruba, relation: patron saint, target: Saint David\n",
      "out_edges, node: Aruba, relation: capital, target: Oranjestad\n",
      "out_edges, node: Aruba, relation: instance of, target: country\n",
      "in_edges, node: Aruba, relation: contains the administrative territorial entity, target: Aruba\n",
      "in_edges, node: Lesser Antilles, relation: part of, target: Lesser Antilles\n",
      "in_edges, node: Lesser Antilles, relation: located in/on physical feature, target: Lesser Antilles\n",
      "in_edges, node: democracy, relation: basic form of government, target: democracy\n",
      "in_edges, node: democracy, relation: has quality, target: democracy\n",
      "in_edges, node: Parliament of Canada, relation: legislative body, target: Parliament of Canada\n",
      "in_edges, node: Viktor Orbán, relation: head of government, target: Viktor Orbán\n",
      "in_edges, node: United Nations Conference on Trade and Development, relation: member of, target: United Nations Conference on Trade and Development\n",
      "in_edges, node: Pennsylvania, relation: contains the administrative territorial entity, target: Pennsylvania\n",
      "in_edges, node: La Marseillaise, relation: anthem, target: La Marseillaise\n",
      "in_edges, node: Bratislava, relation: capital, target: Bratislava\n",
      "in_edges, node: Maldivian, relation: language used, target: Maldivian\n",
      "in_edges, node: Macau, relation: contains the administrative territorial entity, target: Macau\n",
      "in_edges, node: Macau, relation: diplomatic relation, target: Macau\n",
      "in_edges, node: Nonthaburi, relation: contains the administrative territorial entity, target: Nonthaburi\n",
      "in_edges, node: Afar, relation: language used, target: Afar\n",
      "in_edges, node: Commonwealth of Independent States, relation: member of, target: Commonwealth of Independent States\n",
      "out_edges, node: Sierra Leone, relation: member of, target: World Trade Organization\n",
      "out_edges, node: Sierra Leone, relation: language used, target: Temne\n",
      "out_edges, node: Sierra Leone, relation: capital, target: Freetown\n",
      "out_edges, node: Sierra Leone, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Sierra Leone, relation: instance of, target: republic\n",
      "out_edges, node: Sierra Leone, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Sierra Leone, relation: continent, target: Africa\n",
      "out_edges, node: Sierra Leone, relation: discoverer or inventor, target: Pedro de Sintra\n",
      "out_edges, node: Sierra Leone, relation: country, target: Sierra Leone\n",
      "out_edges, node: Sierra Leone, relation: shares border with, target: Guinea\n",
      "out_edges, node: Sierra Leone, relation: part of, target: West Africa\n",
      "out_edges, node: Djibouti, relation: member of, target: Organisation of African, Caribbean and Pacific States\n",
      "out_edges, node: Djibouti, relation: country, target: Djibouti\n",
      "out_edges, node: Djibouti, relation: official language, target: Arabic\n",
      "out_edges, node: Djibouti, relation: diplomatic relation, target: Somalia\n",
      "out_edges, node: Djibouti, relation: language used, target: Afar\n",
      "out_edges, node: Djibouti, relation: part of, target: East Africa\n",
      "out_edges, node: Djibouti, relation: continent, target: Africa\n",
      "out_edges, node: Djibouti, relation: instance of, target: country\n",
      "in_edges, node: International Civil Aviation Organization, relation: member of, target: International Civil Aviation Organization\n",
      "in_edges, node: Dire Dawa, relation: contains the administrative territorial entity, target: Dire Dawa\n",
      "in_edges, node: Lake Maggiore, relation: lowest point, target: Lake Maggiore\n",
      "out_edges, node: Liechtenstein, relation: diplomatic relation, target: United States of America\n",
      "out_edges, node: Liechtenstein, relation: member of, target: Universal Postal Union\n",
      "out_edges, node: Liechtenstein, relation: located in or next to body of water, target: Rhine\n",
      "out_edges, node: Liechtenstein, relation: shares border with, target: European Union\n",
      "out_edges, node: Liechtenstein, relation: capital, target: Vaduz\n",
      "out_edges, node: Liechtenstein, relation: continent, target: Europe\n",
      "out_edges, node: Liechtenstein, relation: head of state, target: Hans-Adam II, Prince of Liechtenstein\n",
      "out_edges, node: Liechtenstein, relation: language used, target: German\n",
      "out_edges, node: Liechtenstein, relation: contains the administrative territorial entity, target: Triesen\n",
      "out_edges, node: Liechtenstein, relation: currency, target: Swiss franc\n",
      "out_edges, node: Liechtenstein, relation: public holiday, target: New Year's Day\n",
      "out_edges, node: Liechtenstein, relation: country, target: Liechtenstein\n",
      "out_edges, node: Liechtenstein, relation: basic form of government, target: constitutional monarchy\n",
      "out_edges, node: Liechtenstein, relation: central bank, target: Swiss National Bank\n",
      "out_edges, node: Liechtenstein, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Liechtenstein, relation: instance of, target: country\n",
      "out_edges, node: Liechtenstein, relation: part of, target: German Confederation\n",
      "in_edges, node: Balinese, relation: language used, target: Balinese\n",
      "in_edges, node: Ali Khamenei, relation: head of state, target: Ali Khamenei\n",
      "in_edges, node: Dutch, relation: ethnic group, target: Dutch\n",
      "in_edges, node: Dutch, relation: language used, target: Dutch\n",
      "in_edges, node: Dutch, relation: official language, target: Dutch\n",
      "in_edges, node: Good Friday, relation: public holiday, target: Good Friday\n",
      "in_edges, node: Charles III of the United Kingdom, relation: head of state, target: Charles III of the United Kingdom\n",
      "in_edges, node: Constitutional Court of Korea, relation: highest judicial authority, target: Constitutional Court of Korea\n",
      "in_edges, node: Buddhism, relation: religion or worldview, target: Buddhism\n",
      "in_edges, node: Assamese, relation: language used, target: Assamese\n",
      "in_edges, node: Chumphon, relation: contains the administrative territorial entity, target: Chumphon\n",
      "in_edges, node: Turkish, relation: language used, target: Turkish\n",
      "in_edges, node: Turkish, relation: official language, target: Turkish\n",
      "in_edges, node: International Labour Organization, relation: member of, target: International Labour Organization\n",
      "out_edges, node: Sint Maarten, relation: official language, target: English\n",
      "out_edges, node: Sint Maarten, relation: language used, target: Dutch\n",
      "out_edges, node: Sint Maarten, relation: different from, target: Saint Martin\n",
      "out_edges, node: Sint Maarten, relation: twinned administrative body, target: Tallahassee\n",
      "out_edges, node: Sint Maarten, relation: instance of, target: country\n",
      "out_edges, node: Sint Maarten, relation: country, target: Kingdom of the Netherlands\n",
      "out_edges, node: Sint Maarten, relation: patron saint, target: Martin of Tours\n",
      "out_edges, node: Sint Maarten, relation: member of, target: Interpol\n",
      "out_edges, node: Sint Maarten, relation: part of, target: Caribbean\n",
      "out_edges, node: Sint Maarten, relation: shares border with, target: France\n",
      "out_edges, node: Sint Maarten, relation: continent, target: North America\n",
      "out_edges, node: Sint Maarten, relation: head of state, target: Willem-Alexander of the Netherlands\n",
      "in_edges, node: Sint Maarten, relation: contains the administrative territorial entity, target: Sint Maarten\n",
      "in_edges, node: Granada, relation: different from, target: Granada\n",
      "in_edges, node: Kyrgyz, relation: language used, target: Kyrgyz\n",
      "in_edges, node: Yoon Suk Yeol, relation: head of state, target: Yoon Suk Yeol\n",
      "out_edges, node: Tanzania, relation: member of, target: International Finance Corporation\n",
      "out_edges, node: Tanzania, relation: diplomatic relation, target: Malaysia\n",
      "out_edges, node: Tanzania, relation: official language, target: English\n",
      "out_edges, node: Tanzania, relation: country, target: Tanzania\n",
      "out_edges, node: Tanzania, relation: shares border with, target: Burundi\n",
      "out_edges, node: Tanzania, relation: contains the administrative territorial entity, target: Mwanza\n",
      "out_edges, node: Tanzania, relation: language used, target: Sandawe\n",
      "out_edges, node: Tanzania, relation: instance of, target: republic\n",
      "out_edges, node: Tanzania, relation: lowest point, target: Indian Ocean\n",
      "out_edges, node: Tanzania, relation: capital, target: Dodoma\n",
      "out_edges, node: Tanzania, relation: ethnic group, target: Bantu people\n",
      "out_edges, node: Tanzania, relation: named after, target: Zanzibar\n",
      "out_edges, node: Tanzania, relation: part of, target: East Africa\n",
      "out_edges, node: Tanzania, relation: continent, target: Africa\n",
      "out_edges, node: Papua New Guinea, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Papua New Guinea, relation: different from, target: Guinea\n",
      "out_edges, node: Papua New Guinea, relation: official language, target: Hiri Motu\n",
      "out_edges, node: Papua New Guinea, relation: language used, target: Tok Pisin\n",
      "out_edges, node: Papua New Guinea, relation: twinned administrative body, target: Toyota\n",
      "out_edges, node: Papua New Guinea, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Papua New Guinea, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Papua New Guinea, relation: shares border with, target: Australia\n",
      "out_edges, node: Papua New Guinea, relation: country, target: Papua New Guinea\n",
      "out_edges, node: Papua New Guinea, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Papua New Guinea, relation: capital, target: Port Moresby\n",
      "out_edges, node: Papua New Guinea, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Papua New Guinea, relation: contains the administrative territorial entity, target: Autonomous Region of Bougainville\n",
      "out_edges, node: Papua New Guinea, relation: instance of, target: country\n",
      "out_edges, node: Papua New Guinea, relation: located in/on physical feature, target: New Guinea\n",
      "in_edges, node: Kirundi, relation: named after, target: Kirundi\n",
      "in_edges, node: Lomé, relation: capital, target: Lomé\n",
      "in_edges, node: Northern Cyprus, relation: diplomatic relation, target: Northern Cyprus\n",
      "in_edges, node: Northern Cyprus, relation: territory claimed by, target: Northern Cyprus\n",
      "in_edges, node: Arctic Ocean, relation: located in or next to body of water, target: Arctic Ocean\n",
      "in_edges, node: Arctic Ocean, relation: lowest point, target: Arctic Ocean\n",
      "in_edges, node: Joseph, relation: patron saint, target: Joseph\n",
      "out_edges, node: England, relation: instance of, target: country\n",
      "out_edges, node: England, relation: language used, target: English\n",
      "out_edges, node: England, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: England, relation: capital, target: London\n",
      "out_edges, node: England, relation: shares border with, target: Wales\n",
      "out_edges, node: England, relation: continent, target: Europe\n",
      "out_edges, node: England, relation: country, target: United Kingdom\n",
      "out_edges, node: England, relation: patron saint, target: Saint George\n",
      "out_edges, node: England, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: England, relation: currency, target: pound sterling\n",
      "in_edges, node: England, relation: contains the administrative territorial entity, target: England\n",
      "in_edges, node: Jēkabpils, relation: contains the administrative territorial entity, target: Jēkabpils\n",
      "in_edges, node: Caribbean, relation: part of, target: Caribbean\n",
      "in_edges, node: Caribbean, relation: located in/on physical feature, target: Caribbean\n",
      "in_edges, node: Santa Fe, relation: twinned administrative body, target: Santa Fe\n",
      "in_edges, node: Telemark, relation: contains the administrative territorial entity, target: Telemark\n",
      "in_edges, node: Welsh, relation: official language, target: Welsh\n",
      "in_edges, node: Welsh, relation: language used, target: Welsh\n",
      "in_edges, node: Indian Ocean, relation: lowest point, target: Indian Ocean\n",
      "in_edges, node: Indian Ocean, relation: located in or next to body of water, target: Indian Ocean\n",
      "in_edges, node: Albanian, relation: official language, target: Albanian\n",
      "in_edges, node: Albanian, relation: language used, target: Albanian\n",
      "in_edges, node: Kyriakos Mitsotakis, relation: head of government, target: Kyriakos Mitsotakis\n",
      "in_edges, node: Asia-Pacific Economic Cooperation, relation: member of, target: Asia-Pacific Economic Cooperation\n",
      "in_edges, node: Eswatini, relation: diplomatic relation, target: Eswatini\n",
      "in_edges, node: Eswatini, relation: shares border with, target: Eswatini\n",
      "in_edges, node: Adyghe, relation: language used, target: Adyghe\n",
      "in_edges, node: Lipetsk Oblast, relation: contains the administrative territorial entity, target: Lipetsk Oblast\n",
      "in_edges, node: Kuwait, relation: diplomatic relation, target: Kuwait\n",
      "in_edges, node: Kuwait, relation: shares border with, target: Kuwait\n",
      "in_edges, node: Nynorsk, relation: official language, target: Nynorsk\n",
      "in_edges, node: Shanxi, relation: contains the administrative territorial entity, target: Shanxi\n",
      "in_edges, node: Greek, relation: language used, target: Greek\n",
      "in_edges, node: Hausa people, relation: ethnic group, target: Hausa people\n",
      "in_edges, node: Madeira, relation: contains the administrative territorial entity, target: Madeira\n",
      "out_edges, node: Solomon Islands, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Solomon Islands, relation: shares border with, target: Australia\n",
      "out_edges, node: Solomon Islands, relation: country, target: Solomon Islands\n",
      "out_edges, node: Solomon Islands, relation: member of, target: UNESCO\n",
      "out_edges, node: Solomon Islands, relation: capital, target: Honiara\n",
      "out_edges, node: Solomon Islands, relation: instance of, target: country\n",
      "out_edges, node: Solomon Islands, relation: diplomatic relation, target: Taiwan\n",
      "out_edges, node: Solomon Islands, relation: language used, target: English\n",
      "out_edges, node: Solomon Islands, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Solomon Islands, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Solomon Islands, relation: named after, target: Solomon\n",
      "out_edges, node: Solomon Islands, relation: located in/on physical feature, target: Melanesia\n",
      "in_edges, node: Évora, relation: contains the administrative territorial entity, target: Évora\n",
      "in_edges, node: Jelgava, relation: contains the administrative territorial entity, target: Jelgava\n",
      "in_edges, node: Ladakh, relation: contains the administrative territorial entity, target: Ladakh\n",
      "in_edges, node: Ivanovo Oblast, relation: contains the administrative territorial entity, target: Ivanovo Oblast\n",
      "out_edges, node: Palau, relation: twinned administrative body, target: Mie Prefecture\n",
      "out_edges, node: Palau, relation: diplomatic relation, target: Australia\n",
      "out_edges, node: Palau, relation: member of, target: World Health Organization\n",
      "out_edges, node: Palau, relation: shares border with, target: Federated States of Micronesia\n",
      "out_edges, node: Palau, relation: official language, target: Japanese\n",
      "out_edges, node: Palau, relation: located in/on physical feature, target: Micronesia\n",
      "out_edges, node: Palau, relation: language used, target: English\n",
      "out_edges, node: Palau, relation: currency, target: United States dollar\n",
      "out_edges, node: Palau, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Palau, relation: located in or next to body of water, target: Pacific Ocean\n",
      "out_edges, node: Palau, relation: instance of, target: country\n",
      "out_edges, node: Palau, relation: country, target: Palau\n",
      "in_edges, node: Mie Prefecture, relation: twinned administrative body, target: Mie Prefecture\n",
      "in_edges, node: Mie Prefecture, relation: contains the administrative territorial entity, target: Mie Prefecture\n",
      "in_edges, node: Kursk Oblast, relation: contains the administrative territorial entity, target: Kursk Oblast\n",
      "in_edges, node: Estonian, relation: official language, target: Estonian\n",
      "in_edges, node: Estonian, relation: language used, target: Estonian\n",
      "out_edges, node: Liberia, relation: member of, target: African Union\n",
      "out_edges, node: Liberia, relation: diplomatic relation, target: North Korea\n",
      "out_edges, node: Liberia, relation: instance of, target: republic\n",
      "out_edges, node: Liberia, relation: language used, target: English\n",
      "out_edges, node: Liberia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Liberia, relation: located in or next to body of water, target: Atlantic Ocean\n",
      "out_edges, node: Liberia, relation: continent, target: Africa\n",
      "out_edges, node: Liberia, relation: located in/on physical feature, target: West Africa\n",
      "out_edges, node: Liberia, relation: head of government, target: George Weah\n",
      "out_edges, node: Liberia, relation: country, target: Liberia\n",
      "out_edges, node: Liberia, relation: shares border with, target: Ivory Coast\n",
      "out_edges, node: Liberia, relation: capital, target: Monrovia\n",
      "in_edges, node: Washington, D.C., relation: capital, target: Washington, D.C.\n",
      "in_edges, node: New Mexico, relation: contains the administrative territorial entity, target: New Mexico\n",
      "in_edges, node: Grand Est, relation: contains the administrative territorial entity, target: Grand Est\n",
      "in_edges, node: olive, relation: production statistics, target: olive\n",
      "in_edges, node: Jammu and Kashmir, relation: contains the administrative territorial entity, target: Jammu and Kashmir\n",
      "in_edges, node: Bosnia and Herzegovina, relation: shares border with, target: Bosnia and Herzegovina\n",
      "in_edges, node: Bosnia and Herzegovina, relation: diplomatic relation, target: Bosnia and Herzegovina\n",
      "in_edges, node: Adygea, relation: contains the administrative territorial entity, target: Adygea\n",
      "in_edges, node: French West Africa, relation: replaces, target: French West Africa\n",
      "in_edges, node: Tigrinya, relation: language used, target: Tigrinya\n",
      "in_edges, node: Tigrinya, relation: official language, target: Tigrinya\n",
      "in_edges, node: Cheltenham, relation: twinned administrative body, target: Cheltenham\n",
      "out_edges, node: Cook Islands, relation: member of, target: World Health Organization\n",
      "out_edges, node: Cook Islands, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Cook Islands, relation: instance of, target: country\n",
      "out_edges, node: Cook Islands, relation: official language, target: English\n",
      "out_edges, node: Cook Islands, relation: language used, target: Cook Islands Maori\n",
      "out_edges, node: Cook Islands, relation: diplomatic relation, target: Taiwan\n",
      "out_edges, node: Cook Islands, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Cook Islands, relation: country, target: Cook Islands\n",
      "out_edges, node: Cook Islands, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Cook Islands, relation: twinned administrative body, target: Auckland\n",
      "out_edges, node: Cook Islands, relation: located in/on physical feature, target: Polynesia\n",
      "in_edges, node: Palikir, relation: capital, target: Palikir\n",
      "in_edges, node: International Workers' Day, relation: public holiday, target: International Workers' Day\n",
      "in_edges, node: Bashkortostan, relation: contains the administrative territorial entity, target: Bashkortostan\n",
      "in_edges, node: European Southern Observatory, relation: member of, target: European Southern Observatory\n",
      "in_edges, node: Maha Sarakham, relation: contains the administrative territorial entity, target: Maha Sarakham\n",
      "in_edges, node: Karachay-Balkar, relation: language used, target: Karachay-Balkar\n",
      "in_edges, node: Ulaanbaatar, relation: capital, target: Ulaanbaatar\n",
      "in_edges, node: Ukraine, relation: diplomatic relation, target: Ukraine\n",
      "in_edges, node: Ukraine, relation: designated as terrorist by, target: Ukraine\n",
      "in_edges, node: Ukraine, relation: shares border with, target: Ukraine\n",
      "in_edges, node: Serbian, relation: language used, target: Serbian\n",
      "in_edges, node: Serbian, relation: official language, target: Serbian\n",
      "in_edges, node: constitutional monarchy, relation: basic form of government, target: constitutional monarchy\n",
      "in_edges, node: constitutional monarchy, relation: instance of, target: constitutional monarchy\n",
      "in_edges, node: Queensland, relation: contains the administrative territorial entity, target: Queensland\n",
      "in_edges, node: Food and Agriculture Organization, relation: member of, target: Food and Agriculture Organization\n",
      "in_edges, node: Southern Sami, relation: language used, target: Southern Sami\n",
      "in_edges, node: Tripoli, relation: capital, target: Tripoli\n",
      "in_edges, node: North Macedonia, relation: diplomatic relation, target: North Macedonia\n",
      "in_edges, node: North Macedonia, relation: shares border with, target: North Macedonia\n",
      "in_edges, node: Arabs, relation: diplomatic relation, target: Arabs\n",
      "in_edges, node: Arabs, relation: ethnic group, target: Arabs\n",
      "in_edges, node: Micronesia, relation: located in/on physical feature, target: Micronesia\n",
      "in_edges, node: Micronesia, relation: different from, target: Micronesia\n",
      "in_edges, node: Micronesia, relation: part of, target: Micronesia\n",
      "in_edges, node: Plovdiv Province, relation: contains the administrative territorial entity, target: Plovdiv Province\n",
      "in_edges, node: State of Mexico, relation: contains the administrative territorial entity, target: State of Mexico\n",
      "out_edges, node: Malawi, relation: member of, target: International Telecommunication Union\n",
      "out_edges, node: Malawi, relation: instance of, target: country\n",
      "out_edges, node: Malawi, relation: official language, target: Chewa\n",
      "out_edges, node: Malawi, relation: shares border with, target: Tanzania\n",
      "out_edges, node: Malawi, relation: diplomatic relation, target: Rwanda\n",
      "out_edges, node: Malawi, relation: capital, target: Lilongwe\n",
      "out_edges, node: Malawi, relation: language used, target: Tumbuka\n",
      "out_edges, node: Malawi, relation: located in/on physical feature, target: Southern Africa\n",
      "out_edges, node: Malawi, relation: named after, target: Lake Malawi\n",
      "out_edges, node: Malawi, relation: part of, target: East Africa\n",
      "out_edges, node: Malawi, relation: continent, target: Africa\n",
      "out_edges, node: Malawi, relation: country, target: Malawi\n",
      "in_edges, node: Martin Luther King Jr. Day, relation: public holiday, target: Martin Luther King Jr. Day\n",
      "in_edges, node: Murmansk Oblast, relation: contains the administrative territorial entity, target: Murmansk Oblast\n",
      "in_edges, node: Arctic Council, relation: member of, target: Arctic Council\n",
      "in_edges, node: Bucharest, relation: capital, target: Bucharest\n",
      "in_edges, node: Oromia Region, relation: contains the administrative territorial entity, target: Oromia Region\n",
      "in_edges, node: Temne, relation: language used, target: Temne\n",
      "in_edges, node: COMECON, relation: member of, target: COMECON\n",
      "in_edges, node: Gikuyu, relation: language used, target: Gikuyu\n",
      "in_edges, node: Mariana Islands, relation: named after, target: Mariana Islands\n",
      "in_edges, node: International Energy Agency, relation: member of, target: International Energy Agency\n",
      "in_edges, node: lion, relation: official symbol, target: lion\n",
      "in_edges, node: Tibetan, relation: language used, target: Tibetan\n",
      "in_edges, node: Tyumen Oblast, relation: contains the administrative territorial entity, target: Tyumen Oblast\n",
      "in_edges, node: Luxembourg, relation: diplomatic relation, target: Luxembourg\n",
      "in_edges, node: Luxembourg, relation: shares border with, target: Luxembourg\n",
      "in_edges, node: Telugu, relation: language used, target: Telugu\n",
      "in_edges, node: Union of South American Nations, relation: member of, target: Union of South American Nations\n",
      "out_edges, node: El Salvador, relation: continent, target: North America\n",
      "out_edges, node: El Salvador, relation: currency, target: United States dollar\n",
      "out_edges, node: El Salvador, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: El Salvador, relation: head of government, target: Nayib Bukele\n",
      "out_edges, node: El Salvador, relation: diplomatic relation, target: Georgia\n",
      "out_edges, node: El Salvador, relation: part of, target: Central America\n",
      "out_edges, node: El Salvador, relation: language used, target: Spanish\n",
      "out_edges, node: El Salvador, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: El Salvador, relation: country, target: El Salvador\n",
      "out_edges, node: El Salvador, relation: shares border with, target: Honduras\n",
      "out_edges, node: El Salvador, relation: capital, target: San Salvador\n",
      "out_edges, node: El Salvador, relation: instance of, target: country\n",
      "in_edges, node: North America, relation: continent, target: North America\n",
      "in_edges, node: North America, relation: part of, target: North America\n",
      "in_edges, node: Southern Ndebele, relation: official language, target: Southern Ndebele\n",
      "in_edges, node: Kansas, relation: contains the administrative territorial entity, target: Kansas\n",
      "in_edges, node: Shona, relation: official language, target: Shona\n",
      "in_edges, node: Shona, relation: language used, target: Shona\n",
      "in_edges, node: Paraná River, relation: located in or next to body of water, target: Paraná River\n",
      "in_edges, node: Dead Sea, relation: lowest point, target: Dead Sea\n",
      "in_edges, node: Dead Sea, relation: located in or next to body of water, target: Dead Sea\n",
      "in_edges, node: Shymkent, relation: contains the administrative territorial entity, target: Shymkent\n",
      "in_edges, node: Greeks, relation: named after, target: Greeks\n",
      "in_edges, node: Greeks, relation: ethnic group, target: Greeks\n",
      "in_edges, node: Bihor County, relation: contains the administrative territorial entity, target: Bihor County\n",
      "in_edges, node: Pays de la Loire, relation: contains the administrative territorial entity, target: Pays de la Loire\n",
      "in_edges, node: Mateusz Morawiecki, relation: head of government, target: Mateusz Morawiecki\n",
      "in_edges, node: Samsun Province, relation: contains the administrative territorial entity, target: Samsun Province\n",
      "in_edges, node: Mahāyāna, relation: religion or worldview, target: Mahāyāna\n",
      "out_edges, node: Scotland, relation: highest point, target: Ben Nevis\n",
      "out_edges, node: Scotland, relation: language used, target: Scottish Gaelic\n",
      "out_edges, node: Scotland, relation: continent, target: Europe\n",
      "out_edges, node: Scotland, relation: contains the administrative territorial entity, target: Orkney Islands\n",
      "out_edges, node: Scotland, relation: patron saint, target: Andrew the Apostle\n",
      "out_edges, node: Scotland, relation: located in/on physical feature, target: Great Britain\n",
      "out_edges, node: Scotland, relation: capital, target: Edinburgh\n",
      "out_edges, node: Scotland, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Scotland, relation: official language, target: Scots\n",
      "out_edges, node: Scotland, relation: currency, target: pound sterling\n",
      "out_edges, node: Scotland, relation: shares border with, target: England\n",
      "out_edges, node: Scotland, relation: executive body, target: Scottish Government\n",
      "out_edges, node: Scotland, relation: country, target: United Kingdom\n",
      "out_edges, node: Scotland, relation: legislative body, target: Scottish Parliament\n",
      "out_edges, node: Scotland, relation: instance of, target: country\n",
      "in_edges, node: Ben Nevis, relation: highest point, target: Ben Nevis\n",
      "in_edges, node: Magahi, relation: language used, target: Magahi\n",
      "in_edges, node: Insular Oceania, relation: continent, target: Insular Oceania\n",
      "in_edges, node: West Sumatra, relation: contains the administrative territorial entity, target: West Sumatra\n",
      "in_edges, node: Mediterranean Sea, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "in_edges, node: Mediterranean Sea, relation: lowest point, target: Mediterranean Sea\n",
      "in_edges, node: Ossetian, relation: language used, target: Ossetian\n",
      "in_edges, node: Nara Prefecture, relation: contains the administrative territorial entity, target: Nara Prefecture\n",
      "in_edges, node: Latin America, relation: part of, target: Latin America\n",
      "in_edges, node: Latin America, relation: diplomatic relation, target: Latin America\n",
      "in_edges, node: Russian, relation: official language, target: Russian\n",
      "in_edges, node: Russian, relation: language used, target: Russian\n",
      "out_edges, node: Samoa, relation: member of, target: International Bank for Reconstruction and Development\n",
      "out_edges, node: Samoa, relation: diplomatic relation, target: Germany\n",
      "out_edges, node: Samoa, relation: continent, target: Insular Oceania\n",
      "out_edges, node: Samoa, relation: instance of, target: country\n",
      "out_edges, node: Samoa, relation: twinned administrative body, target: Auckland\n",
      "out_edges, node: Samoa, relation: language used, target: Samoan\n",
      "out_edges, node: Samoa, relation: part of, target: Polynesia\n",
      "out_edges, node: Samoa, relation: country, target: Samoa\n",
      "out_edges, node: Samoa, relation: edition or translation of, target: Snow White\n",
      "out_edges, node: Samoa, relation: official language, target: English\n",
      "out_edges, node: Samoa, relation: lowest point, target: Pacific Ocean\n",
      "out_edges, node: Samoa, relation: capital, target: Apia\n",
      "in_edges, node: Organization for Economic Cooperation and Development, relation: member of, target: Organization for Economic Cooperation and Development\n",
      "in_edges, node: Tswana, relation: official language, target: Tswana\n",
      "in_edges, node: Tswana, relation: language used, target: Tswana\n",
      "in_edges, node: Organization of the Petroleum Exporting Countries, relation: member of, target: Organization of the Petroleum Exporting Countries\n",
      "in_edges, node: Kalmar County, relation: contains the administrative territorial entity, target: Kalmar County\n",
      "in_edges, node: Łódź Voivodeship, relation: contains the administrative territorial entity, target: Łódź Voivodeship\n",
      "out_edges, node: Niue, relation: head of state, target: Charles III of the United Kingdom\n",
      "out_edges, node: Niue, relation: country, target: Niue\n",
      "out_edges, node: Niue, relation: member of, target: World Health Organization\n",
      "out_edges, node: Niue, relation: diplomatic relation, target: People's Republic of China\n",
      "out_edges, node: Niue, relation: official language, target: Niuean\n",
      "out_edges, node: Niue, relation: language used, target: English\n",
      "out_edges, node: Niue, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Niue, relation: instance of, target: country\n",
      "out_edges, node: Niue, relation: part of, target: Polynesia\n",
      "out_edges, node: Niue, relation: continent, target: Insular Oceania\n",
      "in_edges, node: Burushaski, relation: language used, target: Burushaski\n",
      "in_edges, node: Aymara, relation: official language, target: Aymara\n",
      "in_edges, node: Northern Sami, relation: language used, target: Northern Sami\n",
      "in_edges, node: Benelux, relation: member of, target: Benelux\n",
      "in_edges, node: International Fund for Agricultural Development, relation: member of, target: International Fund for Agricultural Development\n",
      "in_edges, node: Bank of Japan, relation: central bank, target: Bank of Japan\n",
      "in_edges, node: Brussels-Capital Region, relation: has part(s), target: Brussels-Capital Region\n",
      "in_edges, node: Tajiks, relation: ethnic group, target: Tajiks\n",
      "in_edges, node: Fryslân, relation: contains the administrative territorial entity, target: Fryslân\n",
      "in_edges, node: Uusimaa, relation: contains the administrative territorial entity, target: Uusimaa\n",
      "in_edges, node: East Africa, relation: part of, target: East Africa\n",
      "in_edges, node: Southeast Sulawesi, relation: contains the administrative territorial entity, target: Southeast Sulawesi\n",
      "in_edges, node: Gangwon State, relation: contains the administrative territorial entity, target: Gangwon State\n",
      "in_edges, node: Lao, relation: language used, target: Lao\n",
      "in_edges, node: Hedmark, relation: contains the administrative territorial entity, target: Hedmark\n",
      "in_edges, node: G20, relation: member of, target: G20\n",
      "in_edges, node: Persian Gulf, relation: located in or next to body of water, target: Persian Gulf\n",
      "in_edges, node: Persian Gulf, relation: lowest point, target: Persian Gulf\n",
      "in_edges, node: North American Free Trade Agreement, relation: member of, target: North American Free Trade Agreement\n",
      "in_edges, node: Kinyarwanda, relation: language used, target: Kinyarwanda\n",
      "in_edges, node: Kinyarwanda, relation: official language, target: Kinyarwanda\n",
      "in_edges, node: Lesser Poland Voivodeship, relation: contains the administrative territorial entity, target: Lesser Poland Voivodeship\n",
      "in_edges, node: Southeast Asia, relation: part of, target: Southeast Asia\n",
      "in_edges, node: Southeast Asia, relation: located in/on physical feature, target: Southeast Asia\n",
      "in_edges, node: Indonesian, relation: language used, target: Indonesian\n",
      "in_edges, node: Spanish, relation: language used, target: Spanish\n",
      "in_edges, node: Spanish, relation: official language, target: Spanish\n",
      "in_edges, node: Erzurum Province, relation: contains the administrative territorial entity, target: Erzurum Province\n",
      "out_edges, node: Catalonia, relation: capital, target: Barcelona\n",
      "out_edges, node: Catalonia, relation: country, target: Spain\n",
      "out_edges, node: Catalonia, relation: contains the administrative territorial entity, target: Province of Girona\n",
      "out_edges, node: Catalonia, relation: shares border with, target: Andorra\n",
      "out_edges, node: Catalonia, relation: language used, target: Spanish\n",
      "out_edges, node: Catalonia, relation: patron saint, target: Saint George\n",
      "out_edges, node: Catalonia, relation: significant event, target: Peninsular War\n",
      "out_edges, node: Catalonia, relation: continent, target: Europe\n",
      "out_edges, node: Catalonia, relation: instance of, target: country\n",
      "out_edges, node: Catalonia, relation: official language, target: Occitan\n",
      "out_edges, node: Catalonia, relation: described by source, target: Encyclopædia Britannica 11th edition\n",
      "out_edges, node: Catalonia, relation: basic form of government, target: parliamentary system\n",
      "out_edges, node: Catalonia, relation: currency, target: euro\n",
      "out_edges, node: Catalonia, relation: located in/on physical feature, target: Iberian Peninsula\n",
      "out_edges, node: Catalonia, relation: located in or next to body of water, target: Mediterranean Sea\n",
      "out_edges, node: Catalonia, relation: named after, target: Catalan people\n",
      "in_edges, node: Barcelona, relation: capital, target: Barcelona\n",
      "in_edges, node: Molise, relation: contains the administrative territorial entity, target: Molise\n",
      "in_edges, node: Rondônia, relation: contains the administrative territorial entity, target: Rondônia\n",
      "in_edges, node: Region Zealand, relation: contains the administrative territorial entity, target: Region Zealand\n",
      "in_edges, node: Central Kalimantan, relation: contains the administrative territorial entity, target: Central Kalimantan\n",
      "in_edges, node: Moravia, relation: has part(s), target: Moravia\n",
      "in_edges, node: Emmanuel Macron, relation: head of state, target: Emmanuel Macron\n",
      "in_edges, node: Sanaa, relation: capital, target: Sanaa\n",
      "in_edges, node: Vatican City, relation: diplomatic relation, target: Vatican City\n",
      "in_edges, node: Vatican City, relation: shares border with, target: Vatican City\n",
      "in_edges, node: Filipino, relation: language used, target: Filipino\n",
      "in_edges, node: Lakshadweep, relation: contains the administrative territorial entity, target: Lakshadweep\n",
      "in_edges, node: Denbighshire, relation: contains the administrative territorial entity, target: Denbighshire\n",
      "in_edges, node: Chihuahua, relation: contains the administrative territorial entity, target: Chihuahua\n",
      "in_edges, node: Kanuri, relation: language used, target: Kanuri\n",
      "in_edges, node: United Nations Security Council, relation: member of, target: United Nations Security Council\n",
      "in_edges, node: Cantonese, relation: language used, target: Cantonese\n",
      "in_edges, node: Bengali, relation: language used, target: Bengali\n",
      "in_edges, node: Lisbon, relation: capital, target: Lisbon\n",
      "in_edges, node: Timiș County, relation: contains the administrative territorial entity, target: Timiș County\n",
      "in_edges, node: Rivers State, relation: contains the administrative territorial entity, target: Rivers State\n",
      "in_edges, node: Hakka, relation: language used, target: Hakka\n",
      "in_edges, node: Saraburi, relation: contains the administrative territorial entity, target: Saraburi\n",
      "in_edges, node: Miyazaki Prefecture, relation: contains the administrative territorial entity, target: Miyazaki Prefecture\n",
      "in_edges, node: Dushanbe, relation: capital, target: Dushanbe\n",
      "in_edges, node: Satakunta, relation: contains the administrative territorial entity, target: Satakunta\n",
      "in_edges, node: Bali, relation: contains the administrative territorial entity, target: Bali\n",
      "in_edges, node: Punjab, relation: contains the administrative territorial entity, target: Punjab\n",
      "in_edges, node: Punjab, relation: named after, target: Punjab\n",
      "in_edges, node: Hungarian, relation: language used, target: Hungarian\n",
      "in_edges, node: Hungarian, relation: official language, target: Hungarian\n",
      "in_edges, node: Zaire, relation: replaces, target: Zaire\n",
      "in_edges, node: Poles, relation: ethnic group, target: Poles\n",
      "in_edges, node: Willem-Alexander of the Netherlands, relation: head of state, target: Willem-Alexander of the Netherlands\n",
      "in_edges, node: Greenland, relation: shares border with, target: Greenland\n",
      "in_edges, node: Scottish Gaelic, relation: language used, target: Scottish Gaelic\n",
      "in_edges, node: Lake Chad, relation: named after, target: Lake Chad\n",
      "in_edges, node: Lake Chad, relation: located in or next to body of water, target: Lake Chad\n",
      "in_edges, node: Wakayama Prefecture, relation: twinned administrative body, target: Wakayama Prefecture\n",
      "in_edges, node: Wakayama Prefecture, relation: contains the administrative territorial entity, target: Wakayama Prefecture\n",
      "in_edges, node: Chewa, relation: official language, target: Chewa\n",
      "in_edges, node: Chewa, relation: language used, target: Chewa\n",
      "in_edges, node: Italian, relation: official language, target: Italian\n",
      "in_edges, node: Italian, relation: language used, target: Italian\n",
      "in_edges, node: Adana Province, relation: contains the administrative territorial entity, target: Adana Province\n",
      "in_edges, node: World War I, relation: participant in, target: World War I\n",
      "in_edges, node: World War I, relation: significant event, target: World War I\n",
      "in_edges, node: archipelago, relation: instance of, target: archipelago\n",
      "in_edges, node: National Archives of Japan, relation: archives at, target: National Archives of Japan\n",
      "in_edges, node: Tabasco, relation: contains the administrative territorial entity, target: Tabasco\n",
      "in_edges, node: Lopburi, relation: contains the administrative territorial entity, target: Lopburi\n",
      "in_edges, node: Gyeonggi, relation: contains the administrative territorial entity, target: Gyeonggi\n",
      "in_edges, node: European Economic Community, relation: member of, target: European Economic Community\n",
      "in_edges, node: Turkestan, relation: different from, target: Turkestan\n",
      "in_edges, node: Mazandaran Province, relation: contains the administrative territorial entity, target: Mazandaran Province\n",
      "in_edges, node: Eurasia, relation: continent, target: Eurasia\n",
      "in_edges, node: Eurasia, relation: located in/on physical feature, target: Eurasia\n",
      "in_edges, node: Nakhon Si Thammarat, relation: contains the administrative territorial entity, target: Nakhon Si Thammarat\n",
      "in_edges, node: Fars Province, relation: contains the administrative territorial entity, target: Fars Province\n",
      "in_edges, node: South Ossetia, relation: shares border with, target: South Ossetia\n",
      "in_edges, node: South Ossetia, relation: diplomatic relation, target: South Ossetia\n",
      "in_edges, node: Quechua, relation: official language, target: Quechua\n",
      "in_edges, node: Odia, relation: language used, target: Odia\n",
      "in_edges, node: Southern African Development Community, relation: member of, target: Southern African Development Community\n",
      "in_edges, node: Romani, relation: language used, target: Romani\n",
      "in_edges, node: Sindh, relation: named after, target: Sindh\n",
      "in_edges, node: Sesotho, relation: named after, target: Sesotho\n",
      "in_edges, node: Sesotho, relation: official language, target: Sesotho\n",
      "in_edges, node: African people, relation: ethnic group, target: African people\n",
      "in_edges, node: Sinhala, relation: language used, target: Sinhala\n",
      "in_edges, node: Sinhala, relation: official language, target: Sinhala\n",
      "in_edges, node: Tierra del Fuego, Antarctica and South Atlantic Islands Province, relation: contains the administrative territorial entity, target: Tierra del Fuego, Antarctica and South Atlantic Islands Province\n",
      "in_edges, node: Vladimir Putin, relation: head of state, target: Vladimir Putin\n",
      "in_edges, node: Salzburg, relation: contains the administrative territorial entity, target: Salzburg\n",
      "in_edges, node: Toyoake, relation: twinned administrative body, target: Toyoake\n",
      "in_edges, node: Muğla Province, relation: contains the administrative territorial entity, target: Muğla Province\n",
      "in_edges, node: International Maritime Organization, relation: member of, target: International Maritime Organization\n",
      "in_edges, node: Buryatia, relation: contains the administrative territorial entity, target: Buryatia\n",
      "in_edges, node: David Ben-Gurion, relation: founded by, target: David Ben-Gurion\n",
      "in_edges, node: Judaism, relation: official religion, target: Judaism\n",
      "in_edges, node: Iowa, relation: contains the administrative territorial entity, target: Iowa\n",
      "in_edges, node: Arabian Peninsula, relation: located in/on physical feature, target: Arabian Peninsula\n",
      "in_edges, node: Southern Min, relation: language used, target: Southern Min\n",
      "in_edges, node: Central Bank of Argentina, relation: central bank, target: Central Bank of Argentina\n",
      "in_edges, node: Warsaw Pact, relation: member of, target: Warsaw Pact\n",
      "in_edges, node: Hazaras, relation: ethnic group, target: Hazaras\n",
      "in_edges, node: Rhine, relation: located in or next to body of water, target: Rhine\n",
      "in_edges, node: Somali, relation: official language, target: Somali\n",
      "in_edges, node: Somali, relation: language used, target: Somali\n",
      "in_edges, node: Ashgabat, relation: capital, target: Ashgabat\n",
      "in_edges, node: Tamil, relation: language used, target: Tamil\n",
      "in_edges, node: Tamil, relation: official language, target: Tamil\n",
      "in_edges, node: Drenthe, relation: contains the administrative territorial entity, target: Drenthe\n",
      "in_edges, node: Province of Girona, relation: contains the administrative territorial entity, target: Province of Girona\n",
      "in_edges, node: Nayib Bukele, relation: head of government, target: Nayib Bukele\n",
      "in_edges, node: Swahili, relation: official language, target: Swahili\n",
      "in_edges, node: Swahili, relation: language used, target: Swahili\n",
      "in_edges, node: Uttaradit, relation: contains the administrative territorial entity, target: Uttaradit\n",
      "in_edges, node: Sverdlovsk Oblast, relation: contains the administrative territorial entity, target: Sverdlovsk Oblast\n",
      "in_edges, node: Japanese, relation: language used, target: Japanese\n",
      "in_edges, node: Japanese, relation: official language, target: Japanese\n",
      "in_edges, node: Gifu Prefecture, relation: contains the administrative territorial entity, target: Gifu Prefecture\n",
      "in_edges, node: Cornish, relation: language used, target: Cornish\n",
      "in_edges, node: Korean, relation: language used, target: Korean\n",
      "in_edges, node: Korean, relation: official language, target: Korean\n",
      "in_edges, node: Church Slavonic, relation: language used, target: Church Slavonic\n",
      "in_edges, node: Tagalog, relation: language used, target: Tagalog\n",
      "in_edges, node: Americas, relation: different from, target: Americas\n",
      "in_edges, node: Americas, relation: part of, target: Americas\n",
      "in_edges, node: Wellington Region, relation: contains the administrative territorial entity, target: Wellington Region\n",
      "in_edges, node: Tuscany, relation: contains the administrative territorial entity, target: Tuscany\n",
      "in_edges, node: Umbria, relation: contains the administrative territorial entity, target: Umbria\n",
      "in_edges, node: Borneo, relation: located in/on physical feature, target: Borneo\n",
      "in_edges, node: Romanian, relation: official language, target: Romanian\n",
      "in_edges, node: Romanian, relation: language used, target: Romanian\n",
      "in_edges, node: International Organization for Standardization, relation: member of, target: International Organization for Standardization\n",
      "in_edges, node: Vilnius County, relation: contains the administrative territorial entity, target: Vilnius County\n",
      "in_edges, node: New Jersey, relation: contains the administrative territorial entity, target: New Jersey\n",
      "in_edges, node: International Criminal Court, relation: member of, target: International Criminal Court\n",
      "in_edges, node: CERN, relation: member of, target: CERN\n",
      "in_edges, node: CERN, relation: official observer status in organisation, target: CERN\n",
      "in_edges, node: administrative territorial entity, relation: instance of, target: administrative territorial entity\n",
      "in_edges, node: Balkans, relation: located in/on physical feature, target: Balkans\n",
      "in_edges, node: Balkans, relation: location, target: Balkans\n",
      "in_edges, node: Balkans, relation: part of, target: Balkans\n",
      "in_edges, node: Harald V of Norway, relation: head of state, target: Harald V of Norway\n",
      "in_edges, node: County Monaghan, relation: contains the administrative territorial entity, target: County Monaghan\n",
      "in_edges, node: Catalan, relation: language used, target: Catalan\n",
      "in_edges, node: Catalan, relation: official language, target: Catalan\n",
      "in_edges, node: Nouakchott, relation: contains the administrative territorial entity, target: Nouakchott\n",
      "in_edges, node: Feast of the Ascension, relation: public holiday, target: Feast of the Ascension\n",
      "in_edges, node: European Free Trade Association, relation: member of, target: European Free Trade Association\n",
      "in_edges, node: Sahel, relation: located in/on physical feature, target: Sahel\n",
      "in_edges, node: Turks and Caicos Islands, relation: shares border with, target: Turks and Caicos Islands\n",
      "in_edges, node: Confucianism, relation: religion or worldview, target: Confucianism\n",
      "in_edges, node: Southern Denmark, relation: contains the administrative territorial entity, target: Southern Denmark\n",
      "in_edges, node: Caucasus, relation: part of, target: Caucasus\n",
      "in_edges, node: Czech, relation: language used, target: Czech\n",
      "in_edges, node: Tuva, relation: contains the administrative territorial entity, target: Tuva\n",
      "in_edges, node: Salva Kiir Mayardit, relation: head of government, target: Salva Kiir Mayardit\n",
      "in_edges, node: Newfoundland and Labrador, relation: contains the administrative territorial entity, target: Newfoundland and Labrador\n",
      "in_edges, node: Hiri Motu, relation: official language, target: Hiri Motu\n",
      "in_edges, node: Athens, relation: capital, target: Athens\n",
      "in_edges, node: Community of Portuguese Language Countries, relation: member of, target: Community of Portuguese Language Countries\n",
      "in_edges, node: Ancient Greek, relation: language used, target: Ancient Greek\n",
      "in_edges, node: North Africa, relation: located in/on physical feature, target: North Africa\n",
      "in_edges, node: North Africa, relation: part of, target: North Africa\n",
      "in_edges, node: Shanghai Cooperation Organisation, relation: official observer status in organisation, target: Shanghai Cooperation Organisation\n",
      "in_edges, node: Shanghai Cooperation Organisation, relation: member of, target: Shanghai Cooperation Organisation\n",
      "in_edges, node: European Coal and Steel Community, relation: member of, target: European Coal and Steel Community\n",
      "in_edges, node: Tok Pisin, relation: language used, target: Tok Pisin\n",
      "in_edges, node: Asmara, relation: capital, target: Asmara\n",
      "in_edges, node: County Laois, relation: contains the administrative territorial entity, target: County Laois\n",
      "in_edges, node: Aromanian, relation: language used, target: Aromanian\n",
      "in_edges, node: Tigray Region, relation: contains the administrative territorial entity, target: Tigray Region\n",
      "in_edges, node: Tunis, relation: capital, target: Tunis\n",
      "in_edges, node: Rio de Janeiro, relation: contains the administrative territorial entity, target: Rio de Janeiro\n",
      "in_edges, node: South Jeolla, relation: contains the administrative territorial entity, target: South Jeolla\n",
      "in_edges, node: Durango, relation: contains the administrative territorial entity, target: Durango\n",
      "in_edges, node: Great Zimbabwe, relation: named after, target: Great Zimbabwe\n",
      "in_edges, node: Tsai Ing-wen, relation: head of state, target: Tsai Ing-wen\n",
      "in_edges, node: Shavuot, relation: public holiday, target: Shavuot\n",
      "in_edges, node: nation, relation: instance of, target: nation\n",
      "in_edges, node: Hun Sen, relation: head of government, target: Hun Sen\n",
      "in_edges, node: Kashubian, relation: language used, target: Kashubian\n",
      "in_edges, node: Jharkhand, relation: contains the administrative territorial entity, target: Jharkhand\n",
      "in_edges, node: Emirate of Abu Dhabi, relation: contains the administrative territorial entity, target: Emirate of Abu Dhabi\n",
      "in_edges, node: Perm Krai, relation: contains the administrative territorial entity, target: Perm Krai\n",
      "in_edges, node: Guatemala City, relation: capital, target: Guatemala City\n",
      "in_edges, node: Royal Palace of Amsterdam, relation: owner of, target: Royal Palace of Amsterdam\n",
      "in_edges, node: Occitania, relation: contains the administrative territorial entity, target: Occitania\n",
      "in_edges, node: Occitania, relation: shares border with, target: Occitania\n",
      "in_edges, node: Normandy, relation: contains the administrative territorial entity, target: Normandy\n",
      "in_edges, node: Central Africa, relation: located in/on physical feature, target: Central Africa\n",
      "in_edges, node: Central Denmark Region, relation: contains the administrative territorial entity, target: Central Denmark Region\n",
      "in_edges, node: Provence-Alpes-Côte d'Azur, relation: contains the administrative territorial entity, target: Provence-Alpes-Côte d'Azur\n",
      "in_edges, node: Utrecht, relation: contains the administrative territorial entity, target: Utrecht\n",
      "in_edges, node: Khyber Pakhtunkhwa, relation: contains the administrative territorial entity, target: Khyber Pakhtunkhwa\n",
      "in_edges, node: Artvin Province, relation: contains the administrative territorial entity, target: Artvin Province\n",
      "in_edges, node: Inari Sami, relation: language used, target: Inari Sami\n",
      "in_edges, node: Saint Martin, relation: different from, target: Saint Martin\n",
      "in_edges, node: Saint Martin, relation: contains the administrative territorial entity, target: Saint Martin\n",
      "in_edges, node: Hindi, relation: official language, target: Hindi\n",
      "in_edges, node: Hindi, relation: language used, target: Hindi\n",
      "in_edges, node: Benjamin Netanyahu, relation: head of government, target: Benjamin Netanyahu\n",
      "in_edges, node: Breton, relation: language used, target: Breton\n",
      "in_edges, node: Christianity, relation: religion or worldview, target: Christianity\n",
      "in_edges, node: Christianity, relation: official religion, target: Christianity\n",
      "in_edges, node: United Nations Institute for Training and Research, relation: member of, target: United Nations Institute for Training and Research\n",
      "in_edges, node: Algiers Province, relation: contains the administrative territorial entity, target: Algiers Province\n",
      "in_edges, node: County Sligo, relation: contains the administrative territorial entity, target: County Sligo\n",
      "in_edges, node: Oslo, relation: contains the administrative territorial entity, target: Oslo\n",
      "in_edges, node: Modern Greek, relation: official language, target: Modern Greek\n",
      "in_edges, node: Modern Greek, relation: language used, target: Modern Greek\n",
      "in_edges, node: South Asia, relation: part of, target: South Asia\n",
      "in_edges, node: South Asia, relation: located in/on physical feature, target: South Asia\n",
      "in_edges, node: Orkney Islands, relation: contains the administrative territorial entity, target: Orkney Islands\n",
      "in_edges, node: Bank of Korea, relation: central bank, target: Bank of Korea\n",
      "in_edges, node: boot, relation: shape, target: boot\n",
      "in_edges, node: Constitutional Council of France, relation: highest judicial authority, target: Constitutional Council of France\n",
      "in_edges, node: Greater Poland Voivodeship, relation: contains the administrative territorial entity, target: Greater Poland Voivodeship\n",
      "in_edges, node: Kalmykia, relation: contains the administrative territorial entity, target: Kalmykia\n",
      "in_edges, node: Liguria, relation: contains the administrative territorial entity, target: Liguria\n",
      "in_edges, node: Turks, relation: ethnic group, target: Turks\n",
      "in_edges, node: pound sterling, relation: currency, target: pound sterling\n",
      "in_edges, node: Puerto Rico, relation: contains the administrative territorial entity, target: Puerto Rico\n",
      "in_edges, node: Mogilev Region, relation: contains the administrative territorial entity, target: Mogilev Region\n",
      "in_edges, node: Parliament of Sweden, relation: legislative body, target: Parliament of Sweden\n",
      "in_edges, node: Bonaire, relation: contains the administrative territorial entity, target: Bonaire\n",
      "in_edges, node: La Pampa, relation: contains the administrative territorial entity, target: La Pampa\n",
      "in_edges, node: Yoruba people, relation: ethnic group, target: Yoruba people\n",
      "in_edges, node: Tokushima Prefecture, relation: contains the administrative territorial entity, target: Tokushima Prefecture\n",
      "in_edges, node: Paul Kagame, relation: head of state, target: Paul Kagame\n",
      "in_edges, node: Socialist Federal Republic of Yugoslavia, relation: replaces, target: Socialist Federal Republic of Yugoslavia\n",
      "in_edges, node: International Confederation of Free Trade Unions, relation: member of, target: International Confederation of Free Trade Unions\n",
      "in_edges, node: Central Finland, relation: contains the administrative territorial entity, target: Central Finland\n",
      "in_edges, node: Balıkesir Province, relation: contains the administrative territorial entity, target: Balıkesir Province\n",
      "in_edges, node: Andrew the Apostle, relation: patron saint, target: Andrew the Apostle\n",
      "in_edges, node: Guangdong, relation: contains the administrative territorial entity, target: Guangdong\n",
      "in_edges, node: Kashmiri, relation: language used, target: Kashmiri\n",
      "in_edges, node: Vaduz, relation: capital, target: Vaduz\n",
      "in_edges, node: Kagoshima Prefecture, relation: contains the administrative territorial entity, target: Kagoshima Prefecture\n",
      "in_edges, node: Kuyavian-Pomeranian Voivodeship, relation: contains the administrative territorial entity, target: Kuyavian-Pomeranian Voivodeship\n",
      "in_edges, node: Akershus, relation: contains the administrative territorial entity, target: Akershus\n",
      "in_edges, node: Yasothon, relation: contains the administrative territorial entity, target: Yasothon\n",
      "in_edges, node: Bosnian, relation: language used, target: Bosnian\n",
      "in_edges, node: Francis of Assisi, relation: patron saint, target: Francis of Assisi\n",
      "in_edges, node: National Gallery of Victoria, relation: has works in the collection, target: National Gallery of Victoria\n",
      "in_edges, node: Prince Edward Island, relation: contains the administrative territorial entity, target: Prince Edward Island\n",
      "in_edges, node: German, relation: language used, target: German\n",
      "in_edges, node: German, relation: official language, target: German\n",
      "in_edges, node: Mercosur, relation: member of, target: Mercosur\n",
      "in_edges, node: Mercosur, relation: diplomatic relation, target: Mercosur\n",
      "in_edges, node: Chaiyaphum, relation: contains the administrative territorial entity, target: Chaiyaphum\n",
      "in_edges, node: Saskatchewan, relation: contains the administrative territorial entity, target: Saskatchewan\n",
      "in_edges, node: Bangui, relation: contains the administrative territorial entity, target: Bangui\n",
      "in_edges, node: Equatorial Guinea, relation: diplomatic relation, target: Equatorial Guinea\n",
      "in_edges, node: Equatorial Guinea, relation: shares border with, target: Equatorial Guinea\n",
      "in_edges, node: United States Declaration of Independence, relation: foundational text, target: United States Declaration of Independence\n",
      "in_edges, node: Tallahassee, relation: twinned administrative body, target: Tallahassee\n",
      "in_edges, node: Dzongkha, relation: language used, target: Dzongkha\n",
      "in_edges, node: Aghul, relation: language used, target: Aghul\n",
      "in_edges, node: Tirana, relation: capital, target: Tirana\n",
      "in_edges, node: Hyogo Prefecture, relation: contains the administrative territorial entity, target: Hyogo Prefecture\n",
      "in_edges, node: Hyogo Prefecture, relation: twinned administrative body, target: Hyogo Prefecture\n",
      "in_edges, node: Tatarstan, relation: contains the administrative territorial entity, target: Tatarstan\n",
      "in_edges, node: Luba-Kasai, relation: language used, target: Luba-Kasai\n",
      "in_edges, node: Johor, relation: contains the administrative territorial entity, target: Johor\n",
      "in_edges, node: Economic Community of West African States, relation: member of, target: Economic Community of West African States\n",
      "in_edges, node: Tamaulipas State, relation: contains the administrative territorial entity, target: Tamaulipas State\n",
      "in_edges, node: Phang Nga, relation: contains the administrative territorial entity, target: Phang Nga\n",
      "in_edges, node: Ilam Province, relation: contains the administrative territorial entity, target: Ilam Province\n",
      "in_edges, node: Barbuda, relation: contains the administrative territorial entity, target: Barbuda\n",
      "in_edges, node: West Pomeranian Voivodeship, relation: contains the administrative territorial entity, target: West Pomeranian Voivodeship\n",
      "in_edges, node: Chiba Prefecture, relation: contains the administrative territorial entity, target: Chiba Prefecture\n",
      "in_edges, node: Belize, relation: diplomatic relation, target: Belize\n",
      "in_edges, node: Belize, relation: shares border with, target: Belize\n",
      "in_edges, node: Freetown, relation: capital, target: Freetown\n",
      "in_edges, node: New Year, relation: public holiday, target: New Year\n",
      "in_edges, node: Barents Sea, relation: located in or next to body of water, target: Barents Sea\n",
      "in_edges, node: French Guiana, relation: contains the administrative territorial entity, target: French Guiana\n",
      "in_edges, node: French Guiana, relation: shares border with, target: French Guiana\n",
      "in_edges, node: French Guiana, relation: different from, target: French Guiana\n",
      "in_edges, node: West Java, relation: contains the administrative territorial entity, target: West Java\n",
      "in_edges, node: Great Lakes, relation: located in or next to body of water, target: Great Lakes\n",
      "in_edges, node: Alaska, relation: contains the administrative territorial entity, target: Alaska\n",
      "in_edges, node: Polish, relation: language used, target: Polish\n",
      "in_edges, node: Polish, relation: official language, target: Polish\n",
      "in_edges, node: All Souls' Day, relation: public holiday, target: All Souls' Day\n",
      "in_edges, node: Riga, relation: contains the administrative territorial entity, target: Riga\n",
      "in_edges, node: Kalmyk, relation: language used, target: Kalmyk\n",
      "in_edges, node: Niuean, relation: official language, target: Niuean\n",
      "in_edges, node: Élisabeth Borne, relation: head of government, target: Élisabeth Borne\n",
      "in_edges, node: Mont Blanc, relation: highest point, target: Mont Blanc\n",
      "in_edges, node: Mongolian, relation: language used, target: Mongolian\n",
      "in_edges, node: Mongolian, relation: official language, target: Mongolian\n",
      "in_edges, node: Basel-Stadt, relation: contains the administrative territorial entity, target: Basel-Stadt\n",
      "in_edges, node: Tobago, relation: named after, target: Tobago\n",
      "in_edges, node: Interparliamentary Union, relation: member of, target: Interparliamentary Union\n",
      "in_edges, node: Nordic Council, relation: member of, target: Nordic Council\n",
      "in_edges, node: Somogy County, relation: contains the administrative territorial entity, target: Somogy County\n",
      "in_edges, node: Bulgarian National Bank, relation: central bank, target: Bulgarian National Bank\n",
      "in_edges, node: Justin Trudeau, relation: head of government, target: Justin Trudeau\n",
      "in_edges, node: Hordaland, relation: contains the administrative territorial entity, target: Hordaland\n",
      "in_edges, node: Carl XVI Gustaf of Sweden, relation: head of state, target: Carl XVI Gustaf of Sweden\n",
      "in_edges, node: International Federation of Red Cross and Red Crescent Societies, relation: member of, target: International Federation of Red Cross and Red Crescent Societies\n",
      "in_edges, node: Tonga, relation: diplomatic relation, target: Tonga\n",
      "in_edges, node: Obwalden, relation: contains the administrative territorial entity, target: Obwalden\n",
      "in_edges, node: Dagestan, relation: contains the administrative territorial entity, target: Dagestan\n",
      "in_edges, node: Norwegian, relation: language used, target: Norwegian\n",
      "in_edges, node: Norwegian, relation: official language, target: Norwegian\n",
      "in_edges, node: Ranong province, relation: contains the administrative territorial entity, target: Ranong province\n",
      "in_edges, node: Epiphany, relation: public holiday, target: Epiphany\n",
      "in_edges, node: Louisiana, relation: contains the administrative territorial entity, target: Louisiana\n",
      "in_edges, node: Central Europe, relation: part of, target: Central Europe\n",
      "in_edges, node: Central Europe, relation: located in/on physical feature, target: Central Europe\n",
      "in_edges, node: Yoruba, relation: language used, target: Yoruba\n",
      "in_edges, node: Hebrew, relation: official language, target: Hebrew\n",
      "in_edges, node: Afrikaans, relation: language used, target: Afrikaans\n",
      "in_edges, node: Afrikaans, relation: official language, target: Afrikaans\n",
      "in_edges, node: Jūrmala, relation: contains the administrative territorial entity, target: Jūrmala\n",
      "in_edges, node: European Bank for Reconstruction and Development, relation: member of, target: European Bank for Reconstruction and Development\n",
      "in_edges, node: Sichuan, relation: contains the administrative territorial entity, target: Sichuan\n",
      "in_edges, node: Santiago, relation: capital, target: Santiago\n",
      "in_edges, node: Vorarlberg, relation: contains the administrative territorial entity, target: Vorarlberg\n",
      "in_edges, node: Luiz Inácio Lula da Silva, relation: head of state, target: Luiz Inácio Lula da Silva\n",
      "in_edges, node: Lithuanian, relation: official language, target: Lithuanian\n",
      "in_edges, node: Lithuanian, relation: language used, target: Lithuanian\n",
      "in_edges, node: Szabolcs-Szatmár-Bereg County, relation: contains the administrative territorial entity, target: Szabolcs-Szatmár-Bereg County\n",
      "in_edges, node: Neuchâtel, relation: contains the administrative territorial entity, target: Neuchâtel\n",
      "in_edges, node: Tlaxcala, relation: contains the administrative territorial entity, target: Tlaxcala\n",
      "in_edges, node: Ankara Province, relation: contains the administrative territorial entity, target: Ankara Province\n",
      "in_edges, node: Manipur, relation: contains the administrative territorial entity, target: Manipur\n",
      "in_edges, node: Paul Biya, relation: head of state, target: Paul Biya\n",
      "in_edges, node: Korea, relation: replaces, target: Korea\n",
      "in_edges, node: Akan, relation: language used, target: Akan\n",
      "in_edges, node: Đắk Lắk, relation: contains the administrative territorial entity, target: Đắk Lắk\n",
      "in_edges, node: Rechtsstaat, relation: instance of, target: Rechtsstaat\n",
      "in_edges, node: Tsonga, relation: official language, target: Tsonga\n",
      "in_edges, node: Tsonga, relation: language used, target: Tsonga\n",
      "in_edges, node: Chuvash Republic, relation: contains the administrative territorial entity, target: Chuvash Republic\n",
      "in_edges, node: Nevada, relation: contains the administrative territorial entity, target: Nevada\n",
      "in_edges, node: Stockholm, relation: capital, target: Stockholm\n",
      "in_edges, node: Schwyz, relation: named after, target: Schwyz\n",
      "in_edges, node: Miami-Dade County, relation: twinned administrative body, target: Miami-Dade County\n",
      "in_edges, node: Kaluga Oblast, relation: contains the administrative territorial entity, target: Kaluga Oblast\n",
      "in_edges, node: Asturias, relation: shares border with, target: Asturias\n",
      "in_edges, node: Mwanza, relation: contains the administrative territorial entity, target: Mwanza\n",
      "in_edges, node: Daejeon, relation: contains the administrative territorial entity, target: Daejeon\n",
      "in_edges, node: Sa Kaeo, relation: contains the administrative territorial entity, target: Sa Kaeo\n",
      "in_edges, node: António Costa, relation: head of government, target: António Costa\n",
      "in_edges, node: Manchu, relation: language used, target: Manchu\n",
      "in_edges, node: Panama, relation: diplomatic relation, target: Panama\n",
      "in_edges, node: Panama, relation: shares border with, target: Panama\n",
      "in_edges, node: Caribbean Community, relation: member of, target: Caribbean Community\n",
      "in_edges, node: Kassym-Jomart Tokayev, relation: head of state, target: Kassym-Jomart Tokayev\n",
      "in_edges, node: Harare, relation: capital, target: Harare\n",
      "in_edges, node: Samara Oblast, relation: contains the administrative territorial entity, target: Samara Oblast\n",
      "in_edges, node: Honiara, relation: capital, target: Honiara\n",
      "in_edges, node: De Nederlandsche Bank, relation: central bank, target: De Nederlandsche Bank\n",
      "in_edges, node: Central America, relation: part of, target: Central America\n",
      "in_edges, node: Northern Ndebele, relation: official language, target: Northern Ndebele\n",
      "in_edges, node: Northern Ndebele, relation: language used, target: Northern Ndebele\n",
      "in_edges, node: Bank for International Settlements, relation: member of, target: Bank for International Settlements\n",
      "in_edges, node: Angus, relation: contains the administrative territorial entity, target: Angus\n",
      "in_edges, node: island, relation: instance of, target: island\n",
      "in_edges, node: island, relation: has part(s) of the class, target: island\n",
      "in_edges, node: Idriss Déby, relation: head of government, target: Idriss Déby\n",
      "in_edges, node: Majuro, relation: contains the administrative territorial entity, target: Majuro\n",
      "in_edges, node: Baja California Sur, relation: contains the administrative territorial entity, target: Baja California Sur\n",
      "in_edges, node: Swazi, relation: language used, target: Swazi\n",
      "in_edges, node: World War II, relation: significant event, target: World War II\n",
      "in_edges, node: World War II, relation: participant in, target: World War II\n",
      "in_edges, node: Swiss Federal Council, relation: executive body, target: Swiss Federal Council\n",
      "in_edges, node: Maghreb, relation: member of, target: Maghreb\n",
      "in_edges, node: Zulu, relation: language used, target: Zulu\n",
      "in_edges, node: Sucre, relation: contains the administrative territorial entity, target: Sucre\n",
      "in_edges, node: Sucre, relation: capital, target: Sucre\n",
      "in_edges, node: Amsterdam, relation: capital, target: Amsterdam\n",
      "in_edges, node: Perth and Kinross, relation: contains the administrative territorial entity, target: Perth and Kinross\n",
      "in_edges, node: Nordland, relation: contains the administrative territorial entity, target: Nordland\n",
      "in_edges, node: Thai, relation: official language, target: Thai\n",
      "in_edges, node: Thai, relation: language used, target: Thai\n",
      "in_edges, node: Prishtina, relation: capital, target: Prishtina\n",
      "in_edges, node: Joko Widodo, relation: head of state, target: Joko Widodo\n",
      "in_edges, node: Sango, relation: language used, target: Sango\n",
      "in_edges, node: All Saints' Day, relation: public holiday, target: All Saints' Day\n",
      "in_edges, node: Benin City, relation: different from, target: Benin City\n",
      "in_edges, node: Győr-Moson-Sopron County, relation: contains the administrative territorial entity, target: Győr-Moson-Sopron County\n",
      "in_edges, node: Tamil Nadu, relation: contains the administrative territorial entity, target: Tamil Nadu\n",
      "in_edges, node: International Monetary Fund, relation: member of, target: International Monetary Fund\n",
      "in_edges, node: Ida-Viru County, relation: contains the administrative territorial entity, target: Ida-Viru County\n",
      "in_edges, node: Bambara, relation: language used, target: Bambara\n",
      "in_edges, node: Maundy Thursday, relation: public holiday, target: Maundy Thursday\n",
      "in_edges, node: North Chungcheong, relation: contains the administrative territorial entity, target: North Chungcheong\n",
      "in_edges, node: Oryol Oblast, relation: contains the administrative territorial entity, target: Oryol Oblast\n",
      "in_edges, node: Xhosa, relation: official language, target: Xhosa\n",
      "in_edges, node: Xhosa, relation: language used, target: Xhosa\n",
      "in_edges, node: Sindhi, relation: language used, target: Sindhi\n",
      "in_edges, node: World Tourism Organization, relation: member of, target: World Tourism Organization\n",
      "in_edges, node: International Olympic Committee, relation: member of, target: International Olympic Committee\n",
      "in_edges, node: Amapá, relation: contains the administrative territorial entity, target: Amapá\n",
      "in_edges, node: Styria, relation: contains the administrative territorial entity, target: Styria\n",
      "in_edges, node: Dadra and Nagar Haveli district, relation: contains the administrative territorial entity, target: Dadra and Nagar Haveli district\n",
      "in_edges, node: Christmas Island, relation: contains the administrative territorial entity, target: Christmas Island\n",
      "in_edges, node: Ouagadougou, relation: capital, target: Ouagadougou\n",
      "in_edges, node: Oppland, relation: contains the administrative territorial entity, target: Oppland\n",
      "in_edges, node: Pretoria, relation: capital, target: Pretoria\n",
      "in_edges, node: Kemerovo Oblast, relation: contains the administrative territorial entity, target: Kemerovo Oblast\n",
      "in_edges, node: Mount Ararat, relation: highest point, target: Mount Ararat\n",
      "in_edges, node: Pattani, relation: contains the administrative territorial entity, target: Pattani\n",
      "in_edges, node: Grisons, relation: contains the administrative territorial entity, target: Grisons\n",
      "in_edges, node: Tomsk Oblast, relation: contains the administrative territorial entity, target: Tomsk Oblast\n",
      "in_edges, node: Cooperation Council for the Arab States of the Gulf, relation: member of, target: Cooperation Council for the Arab States of the Gulf\n",
      "in_edges, node: White House, relation: owner of, target: White House\n",
      "in_edges, node: Igbo, relation: language used, target: Igbo\n",
      "in_edges, node: Falkland Islands, relation: different from, target: Falkland Islands\n",
      "in_edges, node: Bangkok, relation: contains the administrative territorial entity, target: Bangkok\n",
      "in_edges, node: Lapland, relation: contains the administrative territorial entity, target: Lapland\n",
      "in_edges, node: Guerrero, relation: contains the administrative territorial entity, target: Guerrero\n",
      "in_edges, node: City of Brussels, relation: capital, target: City of Brussels\n",
      "in_edges, node: Russian Soviet Federative Socialist Republic, relation: replaces, target: Russian Soviet Federative Socialist Republic\n",
      "in_edges, node: Komi, relation: language used, target: Komi\n",
      "in_edges, node: Kurdish, relation: official language, target: Kurdish\n",
      "in_edges, node: Kurdish, relation: language used, target: Kurdish\n",
      "in_edges, node: Karelian, relation: language used, target: Karelian\n",
      "in_edges, node: Buenos Aires Province, relation: contains the administrative territorial entity, target: Buenos Aires Province\n",
      "in_edges, node: French Polynesia, relation: contains the administrative territorial entity, target: French Polynesia\n",
      "in_edges, node: French Polynesia, relation: diplomatic relation, target: French Polynesia\n",
      "in_edges, node: Petr Pavel, relation: head of state, target: Petr Pavel\n",
      "in_edges, node: Brasília, relation: capital, target: Brasília\n",
      "in_edges, node: Edirne Province, relation: contains the administrative territorial entity, target: Edirne Province\n",
      "in_edges, node: Western Cape, relation: contains the administrative territorial entity, target: Western Cape\n",
      "in_edges, node: Campania, relation: contains the administrative territorial entity, target: Campania\n",
      "in_edges, node: Guangxi, relation: contains the administrative territorial entity, target: Guangxi\n",
      "in_edges, node: Inter-American Development Bank, relation: member of, target: Inter-American Development Bank\n",
      "in_edges, node: Bank of Canada, relation: central bank, target: Bank of Canada\n",
      "in_edges, node: Yamoussoukro, relation: capital, target: Yamoussoukro\n",
      "in_edges, node: Sovereign Military Order of Malta, relation: diplomatic relation, target: Sovereign Military Order of Malta\n",
      "in_edges, node: Riau, relation: contains the administrative territorial entity, target: Riau\n",
      "in_edges, node: Banjul, relation: capital, target: Banjul\n",
      "in_edges, node: Bemba, relation: language used, target: Bemba\n",
      "in_edges, node: Russian Civil War, relation: participant in, target: Russian Civil War\n",
      "in_edges, node: Tokyo, relation: capital, target: Tokyo\n",
      "in_edges, node: Jean de Brébeuf, relation: patron saint, target: Jean de Brébeuf\n",
      "in_edges, node: Santali, relation: language used, target: Santali\n",
      "in_edges, node: Russian Empire, relation: diplomatic relation, target: Russian Empire\n",
      "in_edges, node: Russian Empire, relation: different from, target: Russian Empire\n",
      "in_edges, node: International Atomic Energy Agency, relation: member of, target: International Atomic Energy Agency\n",
      "in_edges, node: Saint George, relation: patron saint, target: Saint George\n",
      "in_edges, node: Croatian, relation: official language, target: Croatian\n",
      "in_edges, node: Croatian, relation: language used, target: Croatian\n",
      "in_edges, node: Podlaskie Voivodeship, relation: contains the administrative territorial entity, target: Podlaskie Voivodeship\n",
      "in_edges, node: Toyama Prefecture, relation: contains the administrative territorial entity, target: Toyama Prefecture\n",
      "in_edges, node: Kingstown, relation: capital, target: Kingstown\n",
      "in_edges, node: South Sumatra, relation: contains the administrative territorial entity, target: South Sumatra\n",
      "in_edges, node: Oregon, relation: contains the administrative territorial entity, target: Oregon\n",
      "in_edges, node: Sandawe, relation: language used, target: Sandawe\n",
      "in_edges, node: New Caledonia, relation: contains the administrative territorial entity, target: New Caledonia\n",
      "in_edges, node: New Caledonia, relation: diplomatic relation, target: New Caledonia\n",
      "in_edges, node: Colombo, relation: capital, target: Colombo\n",
      "in_edges, node: International Organization for Migration, relation: member of, target: International Organization for Migration\n",
      "in_edges, node: Scandinavia, relation: located in/on physical feature, target: Scandinavia\n",
      "in_edges, node: Northern Sotho, relation: official language, target: Northern Sotho\n",
      "in_edges, node: Beja, relation: language used, target: Beja\n",
      "in_edges, node: Beja, relation: contains the administrative territorial entity, target: Beja\n",
      "in_edges, node: East Ayrshire, relation: contains the administrative territorial entity, target: East Ayrshire\n",
      "in_edges, node: Spaniards, relation: ethnic group, target: Spaniards\n",
      "in_edges, node: Rosh Hashanah, relation: public holiday, target: Rosh Hashanah\n",
      "in_edges, node: Abaza, relation: language used, target: Abaza\n",
      "in_edges, node: Nong Khai, relation: contains the administrative territorial entity, target: Nong Khai\n",
      "in_edges, node: Mae Hong Son, relation: contains the administrative territorial entity, target: Mae Hong Son\n",
      "in_edges, node: County Longford, relation: contains the administrative territorial entity, target: County Longford\n",
      "in_edges, node: Liberec Region, relation: contains the administrative territorial entity, target: Liberec Region\n",
      "in_edges, node: ABN AMRO, relation: owner of, target: ABN AMRO\n",
      "in_edges, node: Alemannic, relation: language used, target: Alemannic\n",
      "in_edges, node: Hanoi, relation: capital, target: Hanoi\n",
      "in_edges, node: County Dublin, relation: contains the administrative territorial entity, target: County Dublin\n",
      "in_edges, node: Khabarovsk Krai, relation: contains the administrative territorial entity, target: Khabarovsk Krai\n",
      "in_edges, node: The World Factbook, relation: described by source, target: The World Factbook\n",
      "in_edges, node: Martin of Tours, relation: patron saint, target: Martin of Tours\n",
      "in_edges, node: Gabriel Boric, relation: head of government, target: Gabriel Boric\n",
      "in_edges, node: Pardubice Region, relation: contains the administrative territorial entity, target: Pardubice Region\n",
      "in_edges, node: Damascus, relation: capital, target: Damascus\n",
      "in_edges, node: Saint Casimir, relation: patron saint, target: Saint Casimir\n",
      "in_edges, node: South Gyeongsang, relation: contains the administrative territorial entity, target: South Gyeongsang\n",
      "in_edges, node: Taichung, relation: contains the administrative territorial entity, target: Taichung\n",
      "in_edges, node: Goiás, relation: contains the administrative territorial entity, target: Goiás\n",
      "in_edges, node: Neuquén, relation: contains the administrative territorial entity, target: Neuquén\n",
      "in_edges, node: Maharashtra, relation: contains the administrative territorial entity, target: Maharashtra\n",
      "in_edges, node: Rio Grande do Norte, relation: contains the administrative territorial entity, target: Rio Grande do Norte\n",
      "in_edges, node: Gwangbokjeol, relation: public holiday, target: Gwangbokjeol\n",
      "in_edges, node: granite, relation: official symbol, target: granite\n",
      "in_edges, node: Henry, relation: patron saint, target: Henry\n",
      "in_edges, node: Nagasaki Prefecture, relation: contains the administrative territorial entity, target: Nagasaki Prefecture\n",
      "in_edges, node: Ewe, relation: language used, target: Ewe\n",
      "in_edges, node: Central Asia, relation: part of, target: Central Asia\n",
      "in_edges, node: Central Asia, relation: located in/on physical feature, target: Central Asia\n",
      "in_edges, node: Peninsular War, relation: significant event, target: Peninsular War\n",
      "in_edges, node: Bengkulu, relation: contains the administrative territorial entity, target: Bengkulu\n",
      "in_edges, node: Phayao, relation: contains the administrative territorial entity, target: Phayao\n",
      "in_edges, node: Arkhangelsk Oblast, relation: contains the administrative territorial entity, target: Arkhangelsk Oblast\n",
      "in_edges, node: Plzeň Region, relation: contains the administrative territorial entity, target: Plzeň Region\n",
      "in_edges, node: Zabaykalsky Krai, relation: contains the administrative territorial entity, target: Zabaykalsky Krai\n",
      "in_edges, node: Gulf War, relation: participant in, target: Gulf War\n",
      "in_edges, node: Gulf War, relation: significant event, target: Gulf War\n",
      "in_edges, node: Malé, relation: capital, target: Malé\n",
      "in_edges, node: Acholi, relation: language used, target: Acholi\n",
      "in_edges, node: Civilization V, relation: present in work, target: Civilization V\n",
      "in_edges, node: Mari El, relation: contains the administrative territorial entity, target: Mari El\n",
      "in_edges, node: Anwar Ibrahim, relation: head of government, target: Anwar Ibrahim\n",
      "in_edges, node: West Lothian, relation: contains the administrative territorial entity, target: West Lothian\n",
      "in_edges, node: Māori, relation: official language, target: Māori\n",
      "in_edges, node: Västernorrland County, relation: contains the administrative territorial entity, target: Västernorrland County\n",
      "in_edges, node: West Virginia, relation: contains the administrative territorial entity, target: West Virginia\n",
      "in_edges, node: K’iche’, relation: language used, target: K’iche’\n",
      "in_edges, node: Chechen, relation: language used, target: Chechen\n",
      "in_edges, node: Dili, relation: capital, target: Dili\n",
      "in_edges, node: Bhojpuri, relation: language used, target: Bhojpuri\n",
      "in_edges, node: Tetum, relation: language used, target: Tetum\n",
      "in_edges, node: Haryana, relation: contains the administrative territorial entity, target: Haryana\n",
      "in_edges, node: Primorsky Krai, relation: contains the administrative territorial entity, target: Primorsky Krai\n",
      "in_edges, node: Sundanese, relation: language used, target: Sundanese\n",
      "in_edges, node: Mato Grosso, relation: contains the administrative territorial entity, target: Mato Grosso\n",
      "in_edges, node: Pyongyang, relation: contains the administrative territorial entity, target: Pyongyang\n",
      "in_edges, node: Pyongyang, relation: twinned administrative body, target: Pyongyang\n",
      "in_edges, node: Mato Grosso do Sul, relation: contains the administrative territorial entity, target: Mato Grosso do Sul\n",
      "in_edges, node: Nairobi, relation: capital, target: Nairobi\n",
      "in_edges, node: history of Australia, relation: history of topic, target: history of Australia\n",
      "in_edges, node: Chinese people, relation: ethnic group, target: Chinese people\n",
      "in_edges, node: National Education Association, relation: member of, target: National Education Association\n",
      "in_edges, node: Amu Darya, relation: lowest point, target: Amu Darya\n",
      "in_edges, node: BRICS, relation: member of, target: BRICS\n",
      "in_edges, node: Buenos Aires, relation: capital, target: Buenos Aires\n",
      "in_edges, node: Phetchaburi, relation: contains the administrative territorial entity, target: Phetchaburi\n",
      "in_edges, node: Sardinian, relation: language used, target: Sardinian\n",
      "in_edges, node: Syriac, relation: language used, target: Syriac\n",
      "in_edges, node: Khakas, relation: language used, target: Khakas\n",
      "in_edges, node: Querétaro, relation: contains the administrative territorial entity, target: Querétaro\n",
      "in_edges, node: Krasnodar Krai, relation: contains the administrative territorial entity, target: Krasnodar Krai\n",
      "in_edges, node: Uthai Thani, relation: contains the administrative territorial entity, target: Uthai Thani\n",
      "in_edges, node: Buskerud, relation: contains the administrative territorial entity, target: Buskerud\n",
      "in_edges, node: Auckland, relation: twinned administrative body, target: Auckland\n",
      "in_edges, node: Avar, relation: language used, target: Avar\n",
      "in_edges, node: Chrysanthemum morifolium, relation: official symbol, target: Chrysanthemum morifolium\n",
      "in_edges, node: Metro Manila, relation: contains the administrative territorial entity, target: Metro Manila\n",
      "in_edges, node: Kabardian, relation: language used, target: Kabardian\n",
      "in_edges, node: Paris, relation: capital, target: Paris\n",
      "in_edges, node: Yiddish, relation: language used, target: Yiddish\n",
      "in_edges, node: New Hampshire, relation: contains the administrative territorial entity, target: New Hampshire\n",
      "in_edges, node: Marathi, relation: language used, target: Marathi\n",
      "in_edges, node: United Nations Industrial Development Organization, relation: member of, target: United Nations Industrial Development Organization\n",
      "in_edges, node: Kazakhs, relation: ethnic group, target: Kazakhs\n",
      "in_edges, node: Kazakhs, relation: named after, target: Kazakhs\n",
      "in_edges, node: Simón Bolívar, relation: named after, target: Simón Bolívar\n",
      "in_edges, node: Andhra Pradesh, relation: contains the administrative territorial entity, target: Andhra Pradesh\n",
      "in_edges, node: Danmarks Nationalbank, relation: central bank, target: Danmarks Nationalbank\n",
      "in_edges, node: Zhejiang, relation: contains the administrative territorial entity, target: Zhejiang\n",
      "in_edges, node: American Civil War, relation: significant event, target: American Civil War\n",
      "in_edges, node: K2, relation: highest point, target: K2\n",
      "in_edges, node: Narodowy Bank Polski, relation: central bank, target: Narodowy Bank Polski\n",
      "in_edges, node: East China Sea, relation: located in or next to body of water, target: East China Sea\n",
      "in_edges, node: Sabah, relation: contains the administrative territorial entity, target: Sabah\n",
      "in_edges, node: Slovak, relation: official language, target: Slovak\n",
      "in_edges, node: Slovak, relation: language used, target: Slovak\n",
      "in_edges, node: Kazakh, relation: language used, target: Kazakh\n",
      "in_edges, node: County Galway, relation: contains the administrative territorial entity, target: County Galway\n",
      "in_edges, node: federal government of the United States, relation: executive body, target: federal government of the United States\n",
      "in_edges, node: Sámi, relation: language used, target: Sámi\n",
      "in_edges, node: Sámi, relation: official language, target: Sámi\n",
      "in_edges, node: Sylheti, relation: language used, target: Sylheti\n",
      "in_edges, node: Mandinka, relation: language used, target: Mandinka\n",
      "in_edges, node: Gibraltar, relation: twinned administrative body, target: Gibraltar\n",
      "in_edges, node: Prague, relation: capital, target: Prague\n",
      "in_edges, node: Red Sea, relation: named after, target: Red Sea\n",
      "in_edges, node: Red Sea, relation: located in or next to body of water, target: Red Sea\n",
      "in_edges, node: Red Sea, relation: lowest point, target: Red Sea\n",
      "in_edges, node: Petén Department, relation: contains the administrative territorial entity, target: Petén Department\n",
      "in_edges, node: Beirut, relation: capital, target: Beirut\n",
      "in_edges, node: West Kalimantan, relation: contains the administrative territorial entity, target: West Kalimantan\n",
      "in_edges, node: Addis Ababa, relation: capital, target: Addis Ababa\n",
      "in_edges, node: Haitian Creole, relation: official language, target: Haitian Creole\n",
      "in_edges, node: Eastern Europe, relation: part of, target: Eastern Europe\n",
      "in_edges, node: County Clare, relation: contains the administrative territorial entity, target: County Clare\n",
      "in_edges, node: Oder, relation: located in or next to body of water, target: Oder\n",
      "in_edges, node: Flevoland, relation: contains the administrative territorial entity, target: Flevoland\n",
      "in_edges, node: Faro, relation: contains the administrative territorial entity, target: Faro\n",
      "in_edges, node: Occitan, relation: language used, target: Occitan\n",
      "in_edges, node: Occitan, relation: official language, target: Occitan\n",
      "in_edges, node: Uri, relation: contains the administrative territorial entity, target: Uri\n",
      "in_edges, node: Papiamento, relation: official language, target: Papiamento\n",
      "in_edges, node: Arab world, relation: part of, target: Arab world\n",
      "in_edges, node: Organisation of African Unity, relation: member of, target: Organisation of African Unity\n",
      "in_edges, node: Saratov Oblast, relation: contains the administrative territorial entity, target: Saratov Oblast\n",
      "in_edges, node: São Tomé, relation: capital, target: São Tomé\n",
      "in_edges, node: German Democratic Republic, relation: diplomatic relation, target: German Democratic Republic\n",
      "in_edges, node: Saint Patrick, relation: patron saint, target: Saint Patrick\n",
      "in_edges, node: New Delhi, relation: capital, target: New Delhi\n",
      "in_edges, node: Banco de Portugal, relation: central bank, target: Banco de Portugal\n",
      "in_edges, node: Oaxaca, relation: contains the administrative territorial entity, target: Oaxaca\n",
      "in_edges, node: Phra Nakhon Si Ayutthaya, relation: contains the administrative territorial entity, target: Phra Nakhon Si Ayutthaya\n",
      "in_edges, node: Sikkim, relation: contains the administrative territorial entity, target: Sikkim\n",
      "in_edges, node: Liepāja, relation: contains the administrative territorial entity, target: Liepāja\n",
      "in_edges, node: Papaver rhoeas, relation: official symbol, target: Papaver rhoeas\n",
      "in_edges, node: Niger State, relation: contains the administrative territorial entity, target: Niger State\n",
      "in_edges, node: Ukrainians, relation: ethnic group, target: Ukrainians\n",
      "in_edges, node: Nagano Prefecture, relation: contains the administrative territorial entity, target: Nagano Prefecture\n",
      "in_edges, node: Stavropol Krai, relation: contains the administrative territorial entity, target: Stavropol Krai\n",
      "in_edges, node: Gustavo Petro, relation: head of state, target: Gustavo Petro\n",
      "in_edges, node: Madurese, relation: language used, target: Madurese\n",
      "in_edges, node: Lozi, relation: language used, target: Lozi\n",
      "in_edges, node: Kainuu, relation: contains the administrative territorial entity, target: Kainuu\n",
      "in_edges, node: Ourense Province, relation: contains the administrative territorial entity, target: Ourense Province\n",
      "in_edges, node: Øresund, relation: located in or next to body of water, target: Øresund\n",
      "in_edges, node: Whooper Swan, relation: official symbol, target: Whooper Swan\n",
      "in_edges, node: Manama, relation: capital, target: Manama\n",
      "in_edges, node: Guarani, relation: language used, target: Guarani\n",
      "in_edges, node: Guarani, relation: official language, target: Guarani\n",
      "in_edges, node: Alagoas, relation: contains the administrative territorial entity, target: Alagoas\n",
      "in_edges, node: Lankaran, relation: contains the administrative territorial entity, target: Lankaran\n",
      "in_edges, node: National Bank of Belgium, relation: central bank, target: National Bank of Belgium\n",
      "in_edges, node: Bogotá, relation: capital, target: Bogotá\n",
      "in_edges, node: Fijian, relation: official language, target: Fijian\n",
      "in_edges, node: Reserve Bank of New Zealand, relation: central bank, target: Reserve Bank of New Zealand\n",
      "in_edges, node: Himachal Pradesh, relation: contains the administrative territorial entity, target: Himachal Pradesh\n",
      "in_edges, node: Khasi, relation: language used, target: Khasi\n",
      "in_edges, node: Hamad II of Bahrain, relation: head of state, target: Hamad II of Bahrain\n",
      "in_edges, node: Bank of Greece, relation: central bank, target: Bank of Greece\n",
      "in_edges, node: Marcelo Rebelo de Sousa, relation: head of state, target: Marcelo Rebelo de Sousa\n",
      "in_edges, node: Swedish, relation: language used, target: Swedish\n",
      "in_edges, node: Hans-Adam II, Prince of Liechtenstein, relation: head of state, target: Hans-Adam II, Prince of Liechtenstein\n",
      "in_edges, node: Latvian, relation: language used, target: Latvian\n",
      "in_edges, node: Druze, relation: official religion, target: Druze\n",
      "in_edges, node: Fula, relation: language used, target: Fula\n",
      "in_edges, node: Nederlandse Spoorwegen, relation: owner of, target: Nederlandse Spoorwegen\n",
      "in_edges, node: Nana Akufo-Addo, relation: head of state, target: Nana Akufo-Addo\n",
      "in_edges, node: Cook Islands Maori, relation: language used, target: Cook Islands Maori\n",
      "in_edges, node: Shimane Prefecture, relation: contains the administrative territorial entity, target: Shimane Prefecture\n",
      "in_edges, node: Koreans, relation: ethnic group, target: Koreans\n",
      "in_edges, node: Jiangxi, relation: contains the administrative territorial entity, target: Jiangxi\n",
      "in_edges, node: Thimphu, relation: capital, target: Thimphu\n",
      "in_edges, node: Komi Republic, relation: contains the administrative territorial entity, target: Komi Republic\n",
      "in_edges, node: Malagasy, relation: official language, target: Malagasy\n",
      "in_edges, node: Prešov Region, relation: contains the administrative territorial entity, target: Prešov Region\n",
      "in_edges, node: London, relation: capital, target: London\n",
      "in_edges, node: Fang, relation: language used, target: Fang\n",
      "in_edges, node: Alberto Fernández, relation: head of government, target: Alberto Fernández\n",
      "in_edges, node: Central Java, relation: contains the administrative territorial entity, target: Central Java\n",
      "in_edges, node: Missouri, relation: contains the administrative territorial entity, target: Missouri\n",
      "in_edges, node: Hidalgo, relation: contains the administrative territorial entity, target: Hidalgo\n",
      "in_edges, node: Bujumbura, relation: headquarters location, target: Bujumbura\n",
      "in_edges, node: Meath, relation: contains the administrative territorial entity, target: Meath\n",
      "in_edges, node: Bloemfontein, relation: capital, target: Bloemfontein\n",
      "in_edges, node: Edinburgh, relation: capital, target: Edinburgh\n",
      "in_edges, node: Minnesota, relation: contains the administrative territorial entity, target: Minnesota\n",
      "in_edges, node: Tavastia Proper, relation: contains the administrative territorial entity, target: Tavastia Proper\n",
      "in_edges, node: Mekong River, relation: lowest point, target: Mekong River\n",
      "in_edges, node: South Lanarkshire, relation: contains the administrative territorial entity, target: South Lanarkshire\n",
      "in_edges, node: Osh, relation: contains the administrative territorial entity, target: Osh\n",
      "in_edges, node: Santiago del Estero, relation: contains the administrative territorial entity, target: Santiago del Estero\n",
      "in_edges, node: Recep Tayyip Erdoğan, relation: head of government, target: Recep Tayyip Erdoğan\n",
      "in_edges, node: Het Loo Palace, relation: owner of, target: Het Loo Palace\n",
      "in_edges, node: Karen people, relation: ethnic group, target: Karen people\n",
      "in_edges, node: Flora Shaw, relation: named by, target: Flora Shaw\n",
      "in_edges, node: Catholic Church, relation: religion or worldview, target: Catholic Church\n",
      "in_edges, node: Chiang Mai, relation: contains the administrative territorial entity, target: Chiang Mai\n",
      "in_edges, node: North Jeolla, relation: contains the administrative territorial entity, target: North Jeolla\n",
      "in_edges, node: North West Province, relation: contains the administrative territorial entity, target: North West Province\n",
      "in_edges, node: Limburg, relation: contains the administrative territorial entity, target: Limburg\n",
      "in_edges, node: Bulgarian, relation: official language, target: Bulgarian\n",
      "in_edges, node: Bulgarian, relation: language used, target: Bulgarian\n",
      "in_edges, node: Kristianstad, relation: lowest point, target: Kristianstad\n",
      "in_edges, node: Venda, relation: official language, target: Venda\n",
      "in_edges, node: Venda, relation: language used, target: Venda\n",
      "in_edges, node: Old Prussian, relation: language used, target: Old Prussian\n",
      "in_edges, node: Vilnius, relation: capital, target: Vilnius\n",
      "in_edges, node: Kamba, relation: language used, target: Kamba\n",
      "in_edges, node: Ido, relation: language used, target: Ido\n",
      "in_edges, node: Faure Essozimna Gnassingbé, relation: head of state, target: Faure Essozimna Gnassingbé\n",
      "in_edges, node: Bald Eagle, relation: official symbol, target: Bald Eagle\n",
      "in_edges, node: County Carlow, relation: contains the administrative territorial entity, target: County Carlow\n",
      "in_edges, node: Joe Biden, relation: head of government, target: Joe Biden\n",
      "in_edges, node: Indus River, relation: named after, target: Indus River\n",
      "in_edges, node: carnival, relation: public holiday, target: carnival\n",
      "in_edges, node: Twi, relation: language used, target: Twi\n",
      "in_edges, node: Nayarit, relation: contains the administrative territorial entity, target: Nayarit\n",
      "in_edges, node: Vienna, relation: contains the administrative territorial entity, target: Vienna\n",
      "in_edges, node: Gilgit-Baltistan, relation: contains the administrative territorial entity, target: Gilgit-Baltistan\n",
      "in_edges, node: La Rioja, relation: contains the administrative territorial entity, target: La Rioja\n",
      "in_edges, node: Morelos, relation: contains the administrative territorial entity, target: Morelos\n",
      "in_edges, node: Sveriges Riksbank, relation: central bank, target: Sveriges Riksbank\n",
      "in_edges, node: Chukotka Autonomous Okrug, relation: contains the administrative territorial entity, target: Chukotka Autonomous Okrug\n",
      "in_edges, node: Lleida Province, relation: contains the administrative territorial entity, target: Lleida Province\n",
      "in_edges, node: Prahova County, relation: contains the administrative territorial entity, target: Prahova County\n",
      "in_edges, node: Castile and León, relation: shares border with, target: Castile and León\n",
      "in_edges, node: Dar es Salaam, relation: contains the administrative territorial entity, target: Dar es Salaam\n",
      "in_edges, node: Gomel Region, relation: contains the administrative territorial entity, target: Gomel Region\n",
      "in_edges, node: Czechoslovakia, relation: diplomatic relation, target: Czechoslovakia\n",
      "in_edges, node: Udon Thani, relation: contains the administrative territorial entity, target: Udon Thani\n",
      "in_edges, node: Bourgogne-Franche-Comté, relation: contains the administrative territorial entity, target: Bourgogne-Franche-Comté\n",
      "in_edges, node: Kedah, relation: contains the administrative territorial entity, target: Kedah\n",
      "in_edges, node: Kedah, relation: shares border with, target: Kedah\n",
      "in_edges, node: Intelsat, relation: member of, target: Intelsat\n",
      "in_edges, node: Lilongwe, relation: capital, target: Lilongwe\n",
      "in_edges, node: Gulf of Aqaba, relation: located in or next to body of water, target: Gulf of Aqaba\n",
      "in_edges, node: Federal Reserve System, relation: central bank, target: Federal Reserve System\n",
      "in_edges, node: Rishi Sunak, relation: head of government, target: Rishi Sunak\n",
      "in_edges, node: geographic region, relation: instance of, target: geographic region\n",
      "in_edges, node: Hatay Province, relation: contains the administrative territorial entity, target: Hatay Province\n",
      "in_edges, node: Astrakhan Oblast, relation: contains the administrative territorial entity, target: Astrakhan Oblast\n",
      "in_edges, node: Vysočina Region, relation: contains the administrative territorial entity, target: Vysočina Region\n",
      "in_edges, node: Marrakesh, relation: named after, target: Marrakesh\n",
      "in_edges, node: Grande Comore, relation: contains the administrative territorial entity, target: Grande Comore\n",
      "in_edges, node: Tainan, relation: contains the administrative territorial entity, target: Tainan\n",
      "in_edges, node: Kocaeli Province, relation: contains the administrative territorial entity, target: Kocaeli Province\n",
      "in_edges, node: International Chamber of Commerce, relation: member of, target: International Chamber of Commerce\n",
      "in_edges, node: Opole Voivodeship, relation: contains the administrative territorial entity, target: Opole Voivodeship\n",
      "in_edges, node: Yellow Sea, relation: located in or next to body of water, target: Yellow Sea\n",
      "in_edges, node: Carib, relation: language used, target: Carib\n",
      "in_edges, node: Yamanashi Prefecture, relation: contains the administrative territorial entity, target: Yamanashi Prefecture\n",
      "in_edges, node: Javanese, relation: language used, target: Javanese\n",
      "in_edges, node: Brazzaville, relation: capital, target: Brazzaville\n",
      "in_edges, node: Bandar Seri Begawan, relation: capital, target: Bandar Seri Begawan\n",
      "in_edges, node: Mette Frederiksen, relation: head of government, target: Mette Frederiksen\n",
      "in_edges, node: Saint Patrick's Day, relation: public holiday, target: Saint Patrick's Day\n",
      "in_edges, node: United Nations Economic Commission for Africa, relation: member of, target: United Nations Economic Commission for Africa\n",
      "in_edges, node: Denis Sassou-Nguesso, relation: head of state, target: Denis Sassou-Nguesso\n",
      "in_edges, node: Miyagi Prefecture, relation: contains the administrative territorial entity, target: Miyagi Prefecture\n",
      "in_edges, node: Utah, relation: contains the administrative territorial entity, target: Utah\n",
      "in_edges, node: Vietnam War, relation: participant in, target: Vietnam War\n",
      "in_edges, node: Meghalaya, relation: contains the administrative territorial entity, target: Meghalaya\n",
      "in_edges, node: Finnmark, relation: contains the administrative territorial entity, target: Finnmark\n",
      "in_edges, node: Sakhalin Oblast, relation: contains the administrative territorial entity, target: Sakhalin Oblast\n",
      "in_edges, node: Funafuti, relation: capital, target: Funafuti\n",
      "in_edges, node: Nuevo León, relation: contains the administrative territorial entity, target: Nuevo León\n",
      "in_edges, node: Magadan Oblast, relation: contains the administrative territorial entity, target: Magadan Oblast\n",
      "in_edges, node: North Sea, relation: located in or next to body of water, target: North Sea\n",
      "in_edges, node: Scottish people, relation: ethnic group, target: Scottish people\n",
      "in_edges, node: St. George's, relation: capital, target: St. George's\n",
      "in_edges, node: Piedmont, relation: contains the administrative territorial entity, target: Piedmont\n",
      "in_edges, node: Yoweri Museveni, relation: head of state, target: Yoweri Museveni\n",
      "in_edges, node: Central Ostrobothnia, relation: contains the administrative territorial entity, target: Central Ostrobothnia\n",
      "in_edges, node: Mizoram, relation: contains the administrative territorial entity, target: Mizoram\n",
      "in_edges, node: Ilham Aliyev, relation: head of state, target: Ilham Aliyev\n",
      "in_edges, node: Samoan, relation: language used, target: Samoan\n",
      "in_edges, node: Saga Prefecture, relation: contains the administrative territorial entity, target: Saga Prefecture\n",
      "in_edges, node: Catamarca, relation: contains the administrative territorial entity, target: Catamarca\n",
      "in_edges, node: Burgenland, relation: contains the administrative territorial entity, target: Burgenland\n",
      "in_edges, node: Hausa, relation: language used, target: Hausa\n",
      "in_edges, node: Passover, relation: public holiday, target: Passover\n",
      "in_edges, node: Pentecost, relation: public holiday, target: Pentecost\n",
      "in_edges, node: Jainism, relation: religion or worldview, target: Jainism\n",
      "in_edges, node: Katanga Province, relation: contains the administrative territorial entity, target: Katanga Province\n",
      "in_edges, node: Harju County, relation: contains the administrative territorial entity, target: Harju County\n",
      "in_edges, node: Lomé Convention, relation: member of, target: Lomé Convention\n",
      "in_edges, node: Lampang, relation: contains the administrative territorial entity, target: Lampang\n",
      "in_edges, node: Chechnya, relation: contains the administrative territorial entity, target: Chechnya\n",
      "in_edges, node: Cairo, relation: capital, target: Cairo\n",
      "in_edges, node: Kigali, relation: capital, target: Kigali\n",
      "in_edges, node: Chelyabinsk Oblast, relation: contains the administrative territorial entity, target: Chelyabinsk Oblast\n",
      "in_edges, node: Yaoundé, relation: capital, target: Yaoundé\n",
      "in_edges, node: Mansi, relation: language used, target: Mansi\n",
      "in_edges, node: Michigan, relation: contains the administrative territorial entity, target: Michigan\n",
      "in_edges, node: Santo Domingo, relation: named after, target: Santo Domingo\n",
      "in_edges, node: south, relation: named after, target: south\n",
      "in_edges, node: Kinshasa, relation: contains the administrative territorial entity, target: Kinshasa\n",
      "in_edges, node: road, relation: named after, target: road\n",
      "in_edges, node: Vincent of Saragossa, relation: named after, target: Vincent of Saragossa\n",
      "in_edges, node: South Moravian Region, relation: contains the administrative territorial entity, target: South Moravian Region\n",
      "in_edges, node: federation, relation: basic form of government, target: federation\n",
      "in_edges, node: federation, relation: instance of, target: federation\n",
      "in_edges, node: Bokmål, relation: official language, target: Bokmål\n",
      "in_edges, node: Epirus Region, relation: contains the administrative territorial entity, target: Epirus Region\n",
      "in_edges, node: Bissau, relation: capital, target: Bissau\n",
      "in_edges, node: Bay of Bengal, relation: lowest point, target: Bay of Bengal\n",
      "in_edges, node: Bay of Bengal, relation: located in or next to body of water, target: Bay of Bengal\n",
      "in_edges, node: Zapotec, relation: language used, target: Zapotec\n",
      "in_edges, node: Amazon, relation: located in or next to body of water, target: Amazon\n",
      "in_edges, node: Ohio, relation: contains the administrative territorial entity, target: Ohio\n",
      "in_edges, node: La Paz, relation: capital, target: La Paz\n",
      "in_edges, node: Attack on Pearl Harbor, relation: significant event, target: Attack on Pearl Harbor\n",
      "in_edges, node: north, relation: named after, target: north\n",
      "in_edges, node: Tatar, relation: language used, target: Tatar\n",
      "in_edges, node: Manila, relation: capital, target: Manila\n",
      "in_edges, node: honey, relation: named after, target: honey\n",
      "in_edges, node: Khartoum, relation: capital, target: Khartoum\n",
      "in_edges, node: Nakhon Phanom, relation: contains the administrative territorial entity, target: Nakhon Phanom\n",
      "in_edges, node: Denali, relation: highest point, target: Denali\n",
      "in_edges, node: Polynesia, relation: part of, target: Polynesia\n",
      "in_edges, node: Polynesia, relation: located in/on physical feature, target: Polynesia\n",
      "in_edges, node: East Asia, relation: part of, target: East Asia\n",
      "in_edges, node: East Asia, relation: located in/on physical feature, target: East Asia\n",
      "in_edges, node: Moksha, relation: language used, target: Moksha\n",
      "in_edges, node: Betula pendula, relation: official symbol, target: Betula pendula\n",
      "in_edges, node: Tambov Oblast, relation: contains the administrative territorial entity, target: Tambov Oblast\n",
      "in_edges, node: Yukon, relation: contains the administrative territorial entity, target: Yukon\n",
      "in_edges, node: Akita Prefecture, relation: contains the administrative territorial entity, target: Akita Prefecture\n",
      "in_edges, node: Danube, relation: located in or next to body of water, target: Danube\n",
      "in_edges, node: Belarusian, relation: language used, target: Belarusian\n",
      "in_edges, node: Belarusian, relation: official language, target: Belarusian\n",
      "in_edges, node: Native Americans in the United States, relation: ethnic group, target: Native Americans in the United States\n",
      "in_edges, node: Chukchi, relation: language used, target: Chukchi\n",
      "in_edges, node: Barbados, relation: diplomatic relation, target: Barbados\n",
      "in_edges, node: Kostroma Oblast, relation: contains the administrative territorial entity, target: Kostroma Oblast\n",
      "in_edges, node: Tigre, relation: language used, target: Tigre\n",
      "in_edges, node: renminbi, relation: currency, target: renminbi\n",
      "in_edges, node: Conakry, relation: capital, target: Conakry\n",
      "in_edges, node: Havana, relation: capital, target: Havana\n",
      "in_edges, node: Şanlıurfa Province, relation: contains the administrative territorial entity, target: Şanlıurfa Province\n",
      "in_edges, node: Amharic, relation: language used, target: Amharic\n",
      "in_edges, node: Dari, relation: language used, target: Dari\n",
      "in_edges, node: San Marino, relation: shares border with, target: San Marino\n",
      "in_edges, node: San Marino, relation: diplomatic relation, target: San Marino\n",
      "in_edges, node: South Sulawesi, relation: contains the administrative territorial entity, target: South Sulawesi\n",
      "in_edges, node: South Carolina, relation: contains the administrative territorial entity, target: South Carolina\n",
      "in_edges, node: O'Higgins Region, relation: contains the administrative territorial entity, target: O'Higgins Region\n",
      "in_edges, node: Tumbuka, relation: language used, target: Tumbuka\n",
      "in_edges, node: Torfaen, relation: contains the administrative territorial entity, target: Torfaen\n",
      "in_edges, node: Tuareg, relation: language used, target: Tuareg\n",
      "in_edges, node: Suphan Buri, relation: contains the administrative territorial entity, target: Suphan Buri\n",
      "in_edges, node: Nunavut, relation: contains the administrative territorial entity, target: Nunavut\n",
      "in_edges, node: Connacht, relation: contains the administrative territorial entity, target: Connacht\n",
      "in_edges, node: Basilicata, relation: contains the administrative territorial entity, target: Basilicata\n",
      "in_edges, node: Samut Prakan, relation: contains the administrative territorial entity, target: Samut Prakan\n",
      "in_edges, node: Baja California, relation: contains the administrative territorial entity, target: Baja California\n",
      "in_edges, node: Bamako, relation: contains the administrative territorial entity, target: Bamako\n",
      "in_edges, node: Senedd, relation: legislative body, target: Senedd\n",
      "in_edges, node: New Aquitaine, relation: contains the administrative territorial entity, target: New Aquitaine\n",
      "in_edges, node: Global Biodiversity Information Facility, relation: member of, target: Global Biodiversity Information Facility\n",
      "in_edges, node: Aegean Sea, relation: located in or next to body of water, target: Aegean Sea\n",
      "in_edges, node: Lublin Voivodeship, relation: contains the administrative territorial entity, target: Lublin Voivodeship\n",
      "in_edges, node: Abdullah II of Jordan, relation: head of state, target: Abdullah II of Jordan\n",
      "in_edges, node: National Library of Wales, relation: archives at, target: National Library of Wales\n",
      "in_edges, node: Busan, relation: contains the administrative territorial entity, target: Busan\n",
      "in_edges, node: Acacia pycnantha, relation: official symbol, target: Acacia pycnantha\n",
      "in_edges, node: José Ramos-Horta, relation: head of state, target: José Ramos-Horta\n",
      "in_edges, node: Aceh, relation: contains the administrative territorial entity, target: Aceh\n",
      "in_edges, node: Surin, relation: contains the administrative territorial entity, target: Surin\n",
      "in_edges, node: Tamim bin Hamad Al Thani, relation: head of state, target: Tamim bin Hamad Al Thani\n",
      "in_edges, node: Skagerrak, relation: located in or next to body of water, target: Skagerrak\n",
      "in_edges, node: United Nations High Commissioner for Refugees, relation: member of, target: United Nations High Commissioner for Refugees\n",
      "in_edges, node: Phnom Penh, relation: contains the administrative territorial entity, target: Phnom Penh\n",
      "in_edges, node: Icelandic, relation: language used, target: Icelandic\n",
      "in_edges, node: Zulia, relation: contains the administrative territorial entity, target: Zulia\n",
      "in_edges, node: Mount Kenya, relation: highest point, target: Mount Kenya\n",
      "in_edges, node: Ventspils, relation: contains the administrative territorial entity, target: Ventspils\n",
      "in_edges, node: Gauteng, relation: contains the administrative territorial entity, target: Gauteng\n",
      "in_edges, node: South Asian Association for Regional Cooperation, relation: member of, target: South Asian Association for Regional Cooperation\n",
      "in_edges, node: Norodom Sihamoni, relation: head of state, target: Norodom Sihamoni\n",
      "in_edges, node: Kievan Rus', relation: named after, target: Kievan Rus'\n",
      "in_edges, node: Yao, relation: language used, target: Yao\n",
      "in_edges, node: Aydın Province, relation: contains the administrative territorial entity, target: Aydın Province\n",
      "in_edges, node: Protestantism, relation: religion or worldview, target: Protestantism\n",
      "in_edges, node: New Brunswick, relation: contains the administrative territorial entity, target: New Brunswick\n",
      "in_edges, node: Renfrewshire, relation: contains the administrative territorial entity, target: Renfrewshire\n",
      "in_edges, node: Lorestan Province, relation: contains the administrative territorial entity, target: Lorestan Province\n",
      "in_edges, node: Kayseri Province, relation: contains the administrative territorial entity, target: Kayseri Province\n",
      "in_edges, node: Republic of Upper Volta, relation: replaces, target: Republic of Upper Volta\n",
      "in_edges, node: Confederate States of America, relation: replaces, target: Confederate States of America\n",
      "in_edges, node: Caracas, relation: capital, target: Caracas\n",
      "in_edges, node: Dumfries and Galloway, relation: contains the administrative territorial entity, target: Dumfries and Galloway\n",
      "in_edges, node: Tasman Sea, relation: located in or next to body of water, target: Tasman Sea\n",
      "in_edges, node: Caesalpinia echinata, relation: named after, target: Caesalpinia echinata\n",
      "in_edges, node: Bolívar, relation: contains the administrative territorial entity, target: Bolívar\n",
      "in_edges, node: Central Bank of Venezuela, relation: central bank, target: Central Bank of Venezuela\n",
      "in_edges, node: Herero, relation: language used, target: Herero\n",
      "in_edges, node: Chaco, relation: contains the administrative territorial entity, target: Chaco\n",
      "in_edges, node: Wyoming, relation: contains the administrative territorial entity, target: Wyoming\n",
      "in_edges, node: Rome, relation: capital, target: Rome\n",
      "in_edges, node: South China Sea, relation: lowest point, target: South China Sea\n",
      "in_edges, node: South China Sea, relation: located in or next to body of water, target: South China Sea\n",
      "in_edges, node: Chiapas, relation: contains the administrative territorial entity, target: Chiapas\n",
      "in_edges, node: Cuban Missile Crisis, relation: significant event, target: Cuban Missile Crisis\n",
      "in_edges, node: Andrej Plenković, relation: head of government, target: Andrej Plenković\n",
      "in_edges, node: United Nations Economic Commission for Europe, relation: member of, target: United Nations Economic Commission for Europe\n",
      "in_edges, node: President of Russia, relation: office held by head of state, target: President of Russia\n",
      "in_edges, node: Kwanyama, relation: language used, target: Kwanyama\n",
      "in_edges, node: War in Afghanistan, relation: participant in, target: War in Afghanistan\n",
      "in_edges, node: War in Afghanistan, relation: significant event, target: War in Afghanistan\n",
      "in_edges, node: Maasai, relation: language used, target: Maasai\n",
      "in_edges, node: Nord-Est, relation: contains the administrative territorial entity, target: Nord-Est\n",
      "in_edges, node: Easter, relation: public holiday, target: Easter\n",
      "in_edges, node: Anthony Albanese, relation: head of government, target: Anthony Albanese\n",
      "in_edges, node: empire, relation: instance of, target: empire\n",
      "in_edges, node: Bern, relation: contains the administrative territorial entity, target: Bern\n",
      "in_edges, node: Rogaland, relation: contains the administrative territorial entity, target: Rogaland\n",
      "in_edges, node: Basseterre, relation: capital, target: Basseterre\n",
      "in_edges, node: Saint Lucy, relation: named after, target: Saint Lucy\n",
      "in_edges, node: Indochina, relation: part of, target: Indochina\n",
      "in_edges, node: Cundinamarca Department, relation: contains the administrative territorial entity, target: Cundinamarca Department\n",
      "in_edges, node: Punjabi, relation: language used, target: Punjabi\n",
      "in_edges, node: Italians, relation: ethnic group, target: Italians\n",
      "in_edges, node: Southeast Asia Treaty Organization, relation: member of, target: Southeast Asia Treaty Organization\n",
      "in_edges, node: Phrae, relation: contains the administrative territorial entity, target: Phrae\n",
      "in_edges, node: Mount Everest, relation: highest point, target: Mount Everest\n",
      "in_edges, node: Quebec, relation: contains the administrative territorial entity, target: Quebec\n",
      "in_edges, node: Veterans Day, relation: public holiday, target: Veterans Day\n",
      "in_edges, node: west, relation: named after, target: west\n",
      "in_edges, node: Suva, relation: capital, target: Suva\n",
      "in_edges, node: Kongo, relation: language used, target: Kongo\n",
      "in_edges, node: Van Province, relation: contains the administrative territorial entity, target: Van Province\n",
      "in_edges, node: Lingala, relation: language used, target: Lingala\n",
      "in_edges, node: Lake Victoria, relation: shares border with, target: Lake Victoria\n",
      "in_edges, node: Central Bank of Ireland, relation: central bank, target: Central Bank of Ireland\n",
      "in_edges, node: United States Virgin Islands, relation: contains the administrative territorial entity, target: United States Virgin Islands\n",
      "in_edges, node: Perak, relation: contains the administrative territorial entity, target: Perak\n",
      "in_edges, node: Alto Paraguay, relation: contains the administrative territorial entity, target: Alto Paraguay\n",
      "in_edges, node: Victoria, relation: capital, target: Victoria\n",
      "in_edges, node: Victoria, relation: contains the administrative territorial entity, target: Victoria\n",
      "in_edges, node: Anjouan, relation: contains the administrative territorial entity, target: Anjouan\n",
      "in_edges, node: Nicolás Maduro, relation: head of state, target: Nicolás Maduro\n",
      "in_edges, node: Aargau, relation: contains the administrative territorial entity, target: Aargau\n",
      "in_edges, node: Orenburg Oblast, relation: contains the administrative territorial entity, target: Orenburg Oblast\n",
      "in_edges, node: Grand'Anse, relation: contains the administrative territorial entity, target: Grand'Anse\n",
      "in_edges, node: Meitei, relation: language used, target: Meitei\n",
      "in_edges, node: Irkutsk Oblast, relation: contains the administrative territorial entity, target: Irkutsk Oblast\n",
      "in_edges, node: Independence Day, relation: public holiday, target: Independence Day\n",
      "in_edges, node: Braga, relation: contains the administrative territorial entity, target: Braga\n",
      "in_edges, node: Jalisco, relation: contains the administrative territorial entity, target: Jalisco\n",
      "in_edges, node: Iberian Peninsula, relation: located in/on physical feature, target: Iberian Peninsula\n",
      "in_edges, node: Istanbul Province, relation: contains the administrative territorial entity, target: Istanbul Province\n",
      "in_edges, node: Byelorussian Soviet Socialist Republic, relation: replaces, target: Byelorussian Soviet Socialist Republic\n",
      "in_edges, node: Azerbaijani, relation: language used, target: Azerbaijani\n",
      "in_edges, node: Azerbaijani, relation: official language, target: Azerbaijani\n",
      "in_edges, node: Norwegian Sea, relation: lowest point, target: Norwegian Sea\n",
      "in_edges, node: Macedonian, relation: language used, target: Macedonian\n",
      "in_edges, node: Emilia-Romagna, relation: contains the administrative territorial entity, target: Emilia-Romagna\n",
      "in_edges, node: Argyll and Bute, relation: contains the administrative territorial entity, target: Argyll and Bute\n",
      "in_edges, node: Nógrád County, relation: contains the administrative territorial entity, target: Nógrád County\n",
      "in_edges, node: Gilan Province, relation: contains the administrative territorial entity, target: Gilan Province\n",
      "in_edges, node: Ajdovščina Municipality, relation: contains the administrative territorial entity, target: Ajdovščina Municipality\n",
      "in_edges, node: Mark Drakeford, relation: head of government, target: Mark Drakeford\n",
      "in_edges, node: Romani people, relation: ethnic group, target: Romani people\n",
      "in_edges, node: Tochigi Prefecture, relation: contains the administrative territorial entity, target: Tochigi Prefecture\n",
      "in_edges, node: Chechens, relation: ethnic group, target: Chechens\n",
      "in_edges, node: Romansh, relation: language used, target: Romansh\n",
      "in_edges, node: Belfast, relation: contains the administrative territorial entity, target: Belfast\n",
      "in_edges, node: Wallonia, relation: has part(s), target: Wallonia\n",
      "in_edges, node: Amur Oblast, relation: contains the administrative territorial entity, target: Amur Oblast\n",
      "in_edges, node: Phatthalung, relation: contains the administrative territorial entity, target: Phatthalung\n",
      "in_edges, node: KwaZulu-Natal, relation: contains the administrative territorial entity, target: KwaZulu-Natal\n",
      "in_edges, node: Jambi, relation: contains the administrative territorial entity, target: Jambi\n",
      "in_edges, node: Zhuang, relation: language used, target: Zhuang\n",
      "in_edges, node: Nogai, relation: language used, target: Nogai\n",
      "in_edges, node: Roi Et, relation: contains the administrative territorial entity, target: Roi Et\n",
      "in_edges, node: Finnish, relation: language used, target: Finnish\n",
      "in_edges, node: Finnish, relation: official language, target: Finnish\n",
      "in_edges, node: Balochistan, relation: named after, target: Balochistan\n",
      "in_edges, node: Swiss National Bank, relation: central bank, target: Swiss National Bank\n",
      "in_edges, node: Nong Bua Lamphu, relation: contains the administrative territorial entity, target: Nong Bua Lamphu\n",
      "in_edges, node: September 11 attacks, relation: participant in, target: September 11 attacks\n",
      "in_edges, node: September 11 attacks, relation: significant event, target: September 11 attacks\n",
      "in_edges, node: Norges Bank, relation: central bank, target: Norges Bank\n",
      "in_edges, node: County Cork, relation: contains the administrative territorial entity, target: County Cork\n",
      "in_edges, node: County Cavan, relation: contains the administrative territorial entity, target: County Cavan\n",
      "in_edges, node: Khoisan, relation: official language, target: Khoisan\n",
      "in_edges, node: Hmong people, relation: ethnic group, target: Hmong people\n",
      "in_edges, node: Khmer, relation: official language, target: Khmer\n",
      "in_edges, node: Khmer, relation: language used, target: Khmer\n",
      "in_edges, node: Port-au-Prince, relation: capital, target: Port-au-Prince\n",
      "in_edges, node: Portalegre, relation: contains the administrative territorial entity, target: Portalegre\n",
      "in_edges, node: Caspian Sea, relation: located in or next to body of water, target: Caspian Sea\n",
      "in_edges, node: Caspian Sea, relation: lowest point, target: Caspian Sea\n",
      "in_edges, node: Khanty, relation: language used, target: Khanty\n",
      "in_edges, node: Convallaria majalis, relation: official symbol, target: Convallaria majalis\n",
      "in_edges, node: Scots, relation: language used, target: Scots\n",
      "in_edges, node: Scots, relation: official language, target: Scots\n",
      "in_edges, node: state, relation: instance of, target: state\n",
      "in_edges, node: County Offaly, relation: contains the administrative territorial entity, target: County Offaly\n",
      "in_edges, node: Abiy Ahmed Ali, relation: head of government, target: Abiy Ahmed Ali\n",
      "in_edges, node: Faroese, relation: language used, target: Faroese\n",
      "in_edges, node: Heves County, relation: contains the administrative territorial entity, target: Heves County\n",
      "in_edges, node: Pahang, relation: contains the administrative territorial entity, target: Pahang\n",
      "in_edges, node: Kars Province, relation: contains the administrative territorial entity, target: Kars Province\n",
      "in_edges, node: Groningen, relation: contains the administrative territorial entity, target: Groningen\n",
      "in_edges, node: Russians, relation: ethnic group, target: Russians\n",
      "in_edges, node: Azad Kashmir, relation: contains the administrative territorial entity, target: Azad Kashmir\n",
      "in_edges, node: Sohag Governorate, relation: contains the administrative territorial entity, target: Sohag Governorate\n",
      "in_edges, node: Chamorro, relation: language used, target: Chamorro\n",
      "in_edges, node: Permyak, relation: language used, target: Permyak\n",
      "in_edges, node: Corsica, relation: contains the administrative territorial entity, target: Corsica\n",
      "in_edges, node: Kaja Kallas, relation: head of government, target: Kaja Kallas\n",
      "in_edges, node: Okayama Prefecture, relation: contains the administrative territorial entity, target: Okayama Prefecture\n",
      "in_edges, node: Yerevan, relation: capital, target: Yerevan\n",
      "in_edges, node: Canterbury Region, relation: contains the administrative territorial entity, target: Canterbury Region\n",
      "in_edges, node: Faiyum Governorate, relation: contains the administrative territorial entity, target: Faiyum Governorate\n",
      "in_edges, node: Prachuap Khiri Khan, relation: contains the administrative territorial entity, target: Prachuap Khiri Khan\n",
      "in_edges, node: Accra, relation: capital, target: Accra\n",
      "in_edges, node: Taipei, relation: contains the administrative territorial entity, target: Taipei\n",
      "in_edges, node: Flintshire, relation: contains the administrative territorial entity, target: Flintshire\n",
      "in_edges, node: Ilocano, relation: language used, target: Ilocano\n",
      "in_edges, node: Bataan, relation: different from, target: Bataan\n",
      "in_edges, node: Kamchatka Krai, relation: contains the administrative territorial entity, target: Kamchatka Krai\n",
      "in_edges, node: Tottori Prefecture, relation: contains the administrative territorial entity, target: Tottori Prefecture\n",
      "in_edges, node: Haji Hassanal Bolkiah, relation: head of state, target: Haji Hassanal Bolkiah\n",
      "in_edges, node: Australasia, relation: part of, target: Australasia\n",
      "in_edges, node: Burmese, relation: language used, target: Burmese\n",
      "in_edges, node: Lucerne, relation: contains the administrative territorial entity, target: Lucerne\n",
      "in_edges, node: Jönköping County, relation: contains the administrative territorial entity, target: Jönköping County\n",
      "in_edges, node: Appenzell Innerrhoden, relation: contains the administrative territorial entity, target: Appenzell Innerrhoden\n",
      "in_edges, node: Guadeloupe, relation: contains the administrative territorial entity, target: Guadeloupe\n",
      "in_edges, node: Triesen, relation: contains the administrative territorial entity, target: Triesen\n",
      "in_edges, node: Nidwalden, relation: contains the administrative territorial entity, target: Nidwalden\n",
      "in_edges, node: Nikol Pashinyan, relation: head of government, target: Nikol Pashinyan\n",
      "in_edges, node: Warmian-Masurian Voivodeship, relation: contains the administrative territorial entity, target: Warmian-Masurian Voivodeship\n",
      "in_edges, node: Trat, relation: contains the administrative territorial entity, target: Trat\n",
      "in_edges, node: Skåne County, relation: contains the administrative territorial entity, target: Skåne County\n",
      "in_edges, node: Kumamoto Prefecture, relation: contains the administrative territorial entity, target: Kumamoto Prefecture\n",
      "in_edges, node: Flemish, relation: language used, target: Flemish\n",
      "in_edges, node: Ottoman Empire, relation: replaces, target: Ottoman Empire\n",
      "in_edges, node: Wadden Sea, relation: located in or next to body of water, target: Wadden Sea\n",
      "in_edges, node: Shiga Prefecture, relation: contains the administrative territorial entity, target: Shiga Prefecture\n",
      "in_edges, node: Pernambuco, relation: contains the administrative territorial entity, target: Pernambuco\n",
      "in_edges, node: Karaim, relation: language used, target: Karaim\n",
      "in_edges, node: daylight saving time, relation: located in time zone, target: daylight saving time\n",
      "in_edges, node: Karelia, relation: contains the administrative territorial entity, target: Karelia\n",
      "in_edges, node: ice, relation: named after, target: ice\n",
      "in_edges, node: Santa Fe Province, relation: contains the administrative territorial entity, target: Santa Fe Province\n",
      "in_edges, node: Nakhon Nayok, relation: contains the administrative territorial entity, target: Nakhon Nayok\n",
      "in_edges, node: Värmland County, relation: contains the administrative territorial entity, target: Värmland County\n",
      "in_edges, node: Swiss franc, relation: currency, target: Swiss franc\n",
      "in_edges, node: Tiraspol, relation: capital, target: Tiraspol\n",
      "in_edges, node: cider, relation: production statistics, target: cider\n",
      "in_edges, node: Rize Province, relation: contains the administrative territorial entity, target: Rize Province\n",
      "in_edges, node: Russian Orthodox Church, relation: official religion, target: Russian Orthodox Church\n",
      "in_edges, node: Jeju, relation: contains the administrative territorial entity, target: Jeju\n",
      "in_edges, node: IJsselmeer, relation: located in or next to body of water, target: IJsselmeer\n",
      "in_edges, node: British Columbia, relation: contains the administrative territorial entity, target: British Columbia\n",
      "in_edges, node: Niigata Prefecture, relation: contains the administrative territorial entity, target: Niigata Prefecture\n",
      "in_edges, node: Nahuatl, relation: official language, target: Nahuatl\n",
      "in_edges, node: Curaçao, relation: contains the administrative territorial entity, target: Curaçao\n",
      "in_edges, node: Dublin, relation: capital, target: Dublin\n",
      "in_edges, node: Parliament of Finland, relation: legislative body, target: Parliament of Finland\n",
      "in_edges, node: Sheikh Hasina, relation: head of government, target: Sheikh Hasina\n",
      "in_edges, node: Gaborone, relation: capital, target: Gaborone\n",
      "in_edges, node: Dodoma, relation: capital, target: Dodoma\n",
      "in_edges, node: Shetland Islands, relation: contains the administrative territorial entity, target: Shetland Islands\n",
      "in_edges, node: Irish people, relation: ethnic group, target: Irish people\n",
      "in_edges, node: 2020 Summer Olympics, relation: significant event, target: 2020 Summer Olympics\n",
      "in_edges, node: Nord-Ouest, relation: contains the administrative territorial entity, target: Nord-Ouest\n",
      "in_edges, node: Samut Sakhon, relation: contains the administrative territorial entity, target: Samut Sakhon\n",
      "in_edges, node: World Intellectual Property Organization, relation: member of, target: World Intellectual Property Organization\n",
      "in_edges, node: Lampung, relation: contains the administrative territorial entity, target: Lampung\n",
      "in_edges, node: Incheon, relation: contains the administrative territorial entity, target: Incheon\n",
      "in_edges, node: Lake Malawi, relation: named after, target: Lake Malawi\n",
      "in_edges, node: Ibaraki Prefecture, relation: contains the administrative territorial entity, target: Ibaraki Prefecture\n",
      "in_edges, node: South Kalimantan, relation: contains the administrative territorial entity, target: South Kalimantan\n",
      "in_edges, node: Kaliningrad Oblast, relation: contains the administrative territorial entity, target: Kaliningrad Oblast\n",
      "in_edges, node: Veps, relation: language used, target: Veps\n",
      "in_edges, node: Ancash department, relation: contains the administrative territorial entity, target: Ancash department\n",
      "in_edges, node: Troms, relation: contains the administrative territorial entity, target: Troms\n",
      "in_edges, node: North Vietnam, relation: follows, target: North Vietnam\n",
      "in_edges, node: Amman, relation: capital, target: Amman\n",
      "in_edges, node: San José, relation: capital, target: San José\n",
      "in_edges, node: Serbia and Montenegro, relation: replaces, target: Serbia and Montenegro\n",
      "in_edges, node: Palestinian National Authority, relation: diplomatic relation, target: Palestinian National Authority\n",
      "in_edges, node: Nassau, relation: capital, target: Nassau\n",
      "in_edges, node: East Renfrewshire, relation: contains the administrative territorial entity, target: East Renfrewshire\n",
      "in_edges, node: parliamentary system, relation: basic form of government, target: parliamentary system\n",
      "in_edges, node: Vest-Agder, relation: contains the administrative territorial entity, target: Vest-Agder\n",
      "in_edges, node: Moscow Oblast, relation: contains the administrative territorial entity, target: Moscow Oblast\n",
      "in_edges, node: South Holland, relation: contains the administrative territorial entity, target: South Holland\n",
      "in_edges, node: Piauí, relation: contains the administrative territorial entity, target: Piauí\n",
      "in_edges, node: Bon, relation: religion or worldview, target: Bon\n",
      "in_edges, node: Bragança, relation: contains the administrative territorial entity, target: Bragança\n",
      "in_edges, node: Fukuoka Prefecture, relation: contains the administrative territorial entity, target: Fukuoka Prefecture\n",
      "in_edges, node: Kabyle, relation: language used, target: Kabyle\n",
      "in_edges, node: Armenians, relation: ethnic group, target: Armenians\n",
      "in_edges, node: Hudson Bay, relation: located in or next to body of water, target: Hudson Bay\n",
      "in_edges, node: Moldovan, relation: official language, target: Moldovan\n",
      "in_edges, node: Mohammed bin Rashid Al Maktoum, relation: head of government, target: Mohammed bin Rashid Al Maktoum\n",
      "in_edges, node: Kingston, relation: capital, target: Kingston\n",
      "in_edges, node: Slovaks, relation: ethnic group, target: Slovaks\n",
      "in_edges, node: São Paulo, relation: contains the administrative territorial entity, target: São Paulo\n",
      "in_edges, node: County Donegal, relation: contains the administrative territorial entity, target: County Donegal\n",
      "in_edges, node: Tyrrhenian Sea, relation: located in or next to body of water, target: Tyrrhenian Sea\n",
      "in_edges, node: Emirate of Ajman, relation: contains the administrative territorial entity, target: Emirate of Ajman\n",
      "in_edges, node: Blaenau Gwent County Borough, relation: contains the administrative territorial entity, target: Blaenau Gwent County Borough\n",
      "in_edges, node: Port of Spain, relation: contains the administrative territorial entity, target: Port of Spain\n",
      "in_edges, node: Faroe Islands, relation: shares border with, target: Faroe Islands\n",
      "in_edges, node: Mississippi, relation: contains the administrative territorial entity, target: Mississippi\n",
      "in_edges, node: Nagaland, relation: contains the administrative territorial entity, target: Nagaland\n",
      "in_edges, node: Yunnan, relation: contains the administrative territorial entity, target: Yunnan\n",
      "in_edges, node: Bender, relation: contains the administrative territorial entity, target: Bender\n",
      "in_edges, node: Umbundu, relation: language used, target: Umbundu\n",
      "in_edges, node: Canadians, relation: ethnic group, target: Canadians\n",
      "in_edges, node: Zürich, relation: contains the administrative territorial entity, target: Zürich\n",
      "in_edges, node: Rostov Oblast, relation: contains the administrative territorial entity, target: Rostov Oblast\n",
      "in_edges, node: Pedro Álvares Cabral, relation: discoverer or inventor, target: Pedro Álvares Cabral\n",
      "in_edges, node: Moravian-Silesian Region, relation: contains the administrative territorial entity, target: Moravian-Silesian Region\n",
      "in_edges, node: Sunni Islam, relation: religion or worldview, target: Sunni Islam\n",
      "in_edges, node: animism, relation: religion or worldview, target: animism\n",
      "in_edges, node: Thurgau, relation: contains the administrative territorial entity, target: Thurgau\n",
      "in_edges, node: Yamalo-Nenets Autonomous Okrug, relation: contains the administrative territorial entity, target: Yamalo-Nenets Autonomous Okrug\n",
      "in_edges, node: Kathmandu, relation: capital, target: Kathmandu\n",
      "in_edges, node: Lezgian, relation: language used, target: Lezgian\n",
      "in_edges, node: Ankara, relation: capital, target: Ankara\n",
      "in_edges, node: Lääne-Viru County, relation: contains the administrative territorial entity, target: Lääne-Viru County\n",
      "in_edges, node: Örebro County, relation: contains the administrative territorial entity, target: Örebro County\n",
      "in_edges, node: Maithili, relation: language used, target: Maithili\n",
      "in_edges, node: South Karelia, relation: contains the administrative territorial entity, target: South Karelia\n",
      "in_edges, node: Uttar Pradesh, relation: contains the administrative territorial entity, target: Uttar Pradesh\n",
      "in_edges, node: São Francisco River, relation: located in or next to body of water, target: São Francisco River\n",
      "in_edges, node: Bantu people, relation: ethnic group, target: Bantu people\n",
      "in_edges, node: Galician, relation: language used, target: Galician\n",
      "in_edges, node: Neman, relation: lowest point, target: Neman\n",
      "in_edges, node: Guelma Province, relation: contains the administrative territorial entity, target: Guelma Province\n",
      "in_edges, node: Hebei, relation: contains the administrative territorial entity, target: Hebei\n",
      "in_edges, node: Songkhla, relation: contains the administrative territorial entity, target: Songkhla\n",
      "in_edges, node: Campeche, relation: contains the administrative territorial entity, target: Campeche\n",
      "in_edges, node: Ottawa, relation: capital, target: Ottawa\n",
      "in_edges, node: Yr Wyddfa, relation: highest point, target: Yr Wyddfa\n",
      "in_edges, node: Narendra Modi, relation: head of government, target: Narendra Modi\n",
      "in_edges, node: Chanthaburi, relation: contains the administrative territorial entity, target: Chanthaburi\n",
      "in_edges, node: Misiones, relation: contains the administrative territorial entity, target: Misiones\n",
      "in_edges, node: Liaoning, relation: contains the administrative territorial entity, target: Liaoning\n",
      "in_edges, node: Lima, relation: capital, target: Lima\n",
      "in_edges, node: Lima, relation: named after, target: Lima\n",
      "in_edges, node: Municipality of Tolmin, relation: contains the administrative territorial entity, target: Municipality of Tolmin\n",
      "in_edges, node: Sunday, relation: named after, target: Sunday\n",
      "in_edges, node: Tuvan, relation: language used, target: Tuvan\n",
      "in_edges, node: Kyrgyz people, relation: ethnic group, target: Kyrgyz people\n",
      "in_edges, node: Hurricane Katrina, relation: significant event, target: Hurricane Katrina\n",
      "in_edges, node: Kerman Province, relation: contains the administrative territorial entity, target: Kerman Province\n",
      "in_edges, node: Sumgait, relation: contains the administrative territorial entity, target: Sumgait\n",
      "in_edges, node: Réunion, relation: contains the administrative territorial entity, target: Réunion\n",
      "in_edges, node: Da Nang, relation: contains the administrative territorial entity, target: Da Nang\n",
      "in_edges, node: North Sumatra, relation: contains the administrative territorial entity, target: North Sumatra\n",
      "in_edges, node: Emomali Rahmon, relation: head of state, target: Emomali Rahmon\n",
      "in_edges, node: Ehime Prefecture, relation: contains the administrative territorial entity, target: Ehime Prefecture\n",
      "in_edges, node: Kirov Oblast, relation: contains the administrative territorial entity, target: Kirov Oblast\n",
      "in_edges, node: Guarda, relation: contains the administrative territorial entity, target: Guarda\n",
      "in_edges, node: Cần Thơ, relation: contains the administrative territorial entity, target: Cần Thơ\n",
      "in_edges, node: Zoran Milanović, relation: head of state, target: Zoran Milanović\n",
      "in_edges, node: Tasmania, relation: contains the administrative territorial entity, target: Tasmania\n",
      "in_edges, node: Mount Fuji, relation: highest point, target: Mount Fuji\n",
      "in_edges, node: Aichi Prefecture, relation: contains the administrative territorial entity, target: Aichi Prefecture\n",
      "in_edges, node: Sea of Japan, relation: located in or next to body of water, target: Sea of Japan\n",
      "in_edges, node: Sea of Japan, relation: lowest point, target: Sea of Japan\n",
      "in_edges, node: Mexico City, relation: capital, target: Mexico City\n",
      "in_edges, node: Zanzibar, relation: named after, target: Zanzibar\n",
      "in_edges, node: Illinois, relation: contains the administrative territorial entity, target: Illinois\n",
      "in_edges, node: Pará, relation: contains the administrative territorial entity, target: Pará\n",
      "in_edges, node: Gävleborg County, relation: contains the administrative territorial entity, target: Gävleborg County\n",
      "in_edges, node: National Archives, relation: archives at, target: National Archives\n",
      "in_edges, node: Ulsan, relation: contains the administrative territorial entity, target: Ulsan\n",
      "in_edges, node: Beijing, relation: capital, target: Beijing\n",
      "in_edges, node: Paraná, relation: contains the administrative territorial entity, target: Paraná\n",
      "in_edges, node: Aceh language, relation: language used, target: Aceh language\n",
      "in_edges, node: Helsinki, relation: capital, target: Helsinki\n",
      "in_edges, node: Western Australia, relation: contains the administrative territorial entity, target: Western Australia\n",
      "in_edges, node: Aoraki / Mount Cook, relation: highest point, target: Aoraki / Mount Cook\n",
      "in_edges, node: Åland, relation: contains the administrative territorial entity, target: Åland\n",
      "in_edges, node: Benghazi, relation: contains the administrative territorial entity, target: Benghazi\n",
      "in_edges, node: Uppsala County, relation: contains the administrative territorial entity, target: Uppsala County\n",
      "in_edges, node: Rajasthan, relation: contains the administrative territorial entity, target: Rajasthan\n",
      "in_edges, node: Tashkent, relation: capital, target: Tashkent\n",
      "in_edges, node: Jämtland County, relation: contains the administrative territorial entity, target: Jämtland County\n",
      "in_edges, node: Niger River, relation: named after, target: Niger River\n",
      "in_edges, node: Norrbotten County, relation: contains the administrative territorial entity, target: Norrbotten County\n",
      "in_edges, node: Østfold, relation: contains the administrative territorial entity, target: Østfold\n",
      "in_edges, node: United States Congress, relation: legislative body, target: United States Congress\n",
      "in_edges, node: Friulian, relation: language used, target: Friulian\n",
      "in_edges, node: Abuja, relation: capital, target: Abuja\n",
      "in_edges, node: Nymphaea nouchali, relation: official symbol, target: Nymphaea nouchali\n",
      "in_edges, node: Sevastopol, relation: contains the administrative territorial entity, target: Sevastopol\n",
      "in_edges, node: Kentucky, relation: contains the administrative territorial entity, target: Kentucky\n",
      "in_edges, node: Muhammadu Buhari, relation: head of government, target: Muhammadu Buhari\n",
      "in_edges, node: Melanesia, relation: located in/on physical feature, target: Melanesia\n",
      "in_edges, node: Melanesia, relation: part of, target: Melanesia\n",
      "in_edges, node: Melanesia, relation: different from, target: Melanesia\n",
      "in_edges, node: Kannada, relation: language used, target: Kannada\n",
      "in_edges, node: Sukkot, relation: public holiday, target: Sukkot\n",
      "in_edges, node: Ontario, relation: contains the administrative territorial entity, target: Ontario\n",
      "in_edges, node: Udmurt Republic, relation: contains the administrative territorial entity, target: Udmurt Republic\n",
      "in_edges, node: Mordovia, relation: contains the administrative territorial entity, target: Mordovia\n",
      "in_edges, node: Walloon, relation: language used, target: Walloon\n",
      "in_edges, node: Votic, relation: language used, target: Votic\n",
      "in_edges, node: Viana do Castelo, relation: contains the administrative territorial entity, target: Viana do Castelo\n",
      "in_edges, node: Acre, relation: contains the administrative territorial entity, target: Acre\n",
      "in_edges, node: Podgorica, relation: capital, target: Podgorica\n",
      "in_edges, node: Saint Petersburg, relation: contains the administrative territorial entity, target: Saint Petersburg\n",
      "in_edges, node: Kütahya Province, relation: contains the administrative territorial entity, target: Kütahya Province\n",
      "in_edges, node: West Bengal, relation: contains the administrative territorial entity, target: West Bengal\n",
      "in_edges, node: Chagatai, relation: language used, target: Chagatai\n",
      "in_edges, node: Andrés Manuel López Obrador, relation: head of government, target: Andrés Manuel López Obrador\n",
      "in_edges, node: English people, relation: ethnic group, target: English people\n",
      "in_edges, node: Wu Language, relation: language used, target: Wu Language\n",
      "in_edges, node: Upper Austria, relation: contains the administrative territorial entity, target: Upper Austria\n",
      "in_edges, node: Mari, relation: language used, target: Mari\n",
      "in_edges, node: Uzbek, relation: language used, target: Uzbek\n",
      "in_edges, node: Uzbek, relation: official language, target: Uzbek\n",
      "in_edges, node: Cebuano, relation: language used, target: Cebuano\n",
      "in_edges, node: Anemone coronaria, relation: official symbol, target: Anemone coronaria\n",
      "in_edges, node: Stockholm County, relation: contains the administrative territorial entity, target: Stockholm County\n",
      "in_edges, node: Bermuda, relation: diplomatic relation, target: Bermuda\n",
      "in_edges, node: Lower Silesian Voivodeship, relation: contains the administrative territorial entity, target: Lower Silesian Voivodeship\n",
      "in_edges, node: Southern Europe, relation: located in/on physical feature, target: Southern Europe\n",
      "in_edges, node: Karachay-Cherkessia, relation: contains the administrative territorial entity, target: Karachay-Cherkessia\n",
      "in_edges, node: Heilongjiang, relation: contains the administrative territorial entity, target: Heilongjiang\n",
      "in_edges, node: Selangor, relation: contains the administrative territorial entity, target: Selangor\n",
      "in_edges, node: Kymenlaakso, relation: contains the administrative territorial entity, target: Kymenlaakso\n",
      "in_edges, node: Sarawak, relation: contains the administrative territorial entity, target: Sarawak\n",
      "in_edges, node: Dinka, relation: language used, target: Dinka\n",
      "in_edges, node: Taoism, relation: religion or worldview, target: Taoism\n",
      "in_edges, node: Satun, relation: contains the administrative territorial entity, target: Satun\n",
      "in_edges, node: Erzya, relation: language used, target: Erzya\n",
      "in_edges, node: Norfolk Island, relation: contains the administrative territorial entity, target: Norfolk Island\n",
      "in_edges, node: Tegucigalpa, relation: capital, target: Tegucigalpa\n",
      "in_edges, node: Guðni Jóhannesson, relation: head of state, target: Guðni Jóhannesson\n",
      "in_edges, node: Olomouc Region, relation: contains the administrative territorial entity, target: Olomouc Region\n",
      "in_edges, node: Kanchanaburi, relation: contains the administrative territorial entity, target: Kanchanaburi\n",
      "in_edges, node: Olea europaea, relation: official symbol, target: Olea europaea\n",
      "in_edges, node: Milo Đukanović, relation: head of state, target: Milo Đukanović\n",
      "in_edges, node: Southern Savonia, relation: contains the administrative territorial entity, target: Southern Savonia\n",
      "in_edges, node: Zeeland, relation: contains the administrative territorial entity, target: Zeeland\n",
      "in_edges, node: Zeeland, relation: named after, target: Zeeland\n",
      "in_edges, node: West Frisian, relation: language used, target: West Frisian\n",
      "in_edges, node: Borno State, relation: contains the administrative territorial entity, target: Borno State\n",
      "in_edges, node: Japanese people, relation: ethnic group, target: Japanese people\n",
      "in_edges, node: Holly Blue, relation: official symbol, target: Holly Blue\n",
      "in_edges, node: War of 1812, relation: significant event, target: War of 1812\n",
      "in_edges, node: Green Pheasant, relation: official symbol, target: Green Pheasant\n",
      "in_edges, node: Vajiralongkorn, relation: head of state, target: Vajiralongkorn\n",
      "in_edges, node: Ostrobothnia, relation: contains the administrative territorial entity, target: Ostrobothnia\n",
      "in_edges, node: Saint Pierre and Miquelon, relation: contains the administrative territorial entity, target: Saint Pierre and Miquelon\n",
      "in_edges, node: Kelantan, relation: contains the administrative territorial entity, target: Kelantan\n",
      "in_edges, node: Qinghai, relation: contains the administrative territorial entity, target: Qinghai\n",
      "in_edges, node: Franco-Provençal, relation: language used, target: Franco-Provençal\n",
      "in_edges, node: Gunma Prefecture, relation: contains the administrative territorial entity, target: Gunma Prefecture\n",
      "in_edges, node: Khon Kaen, relation: contains the administrative territorial entity, target: Khon Kaen\n",
      "in_edges, node: Gulf of Guinea, relation: lowest point, target: Gulf of Guinea\n",
      "in_edges, node: Gulf of Guinea, relation: located in or next to body of water, target: Gulf of Guinea\n",
      "in_edges, node: Seto, relation: twinned administrative body, target: Seto\n",
      "in_edges, node: Ceará, relation: contains the administrative territorial entity, target: Ceará\n",
      "in_edges, node: Tisza, relation: located in or next to body of water, target: Tisza\n",
      "in_edges, node: A Coruña Province, relation: contains the administrative territorial entity, target: A Coruña Province\n",
      "in_edges, node: Winter War, relation: participant in, target: Winter War\n",
      "in_edges, node: Topkapı Palace, relation: owner of, target: Topkapı Palace\n",
      "in_edges, node: South Yemen, relation: diplomatic relation, target: South Yemen\n",
      "in_edges, node: South Yemen, relation: replaces, target: South Yemen\n",
      "in_edges, node: Bank of England, relation: central bank, target: Bank of England\n",
      "in_edges, node: Samut Songkhram, relation: contains the administrative territorial entity, target: Samut Songkhram\n",
      "in_edges, node: Sakon Nakhon, relation: contains the administrative territorial entity, target: Sakon Nakhon\n",
      "in_edges, node: Rio Grande do Sul, relation: contains the administrative territorial entity, target: Rio Grande do Sul\n",
      "in_edges, node: canton St. Gallen, relation: contains the administrative territorial entity, target: canton St. Gallen\n",
      "in_edges, node: Wellington, relation: capital, target: Wellington\n",
      "in_edges, node: Phu Yen, relation: contains the administrative territorial entity, target: Phu Yen\n",
      "in_edges, node: Ali Bongo Ondimba, relation: head of state, target: Ali Bongo Ondimba\n",
      "in_edges, node: Drava, relation: located in or next to body of water, target: Drava\n",
      "in_edges, node: Elbe, relation: lowest point, target: Elbe\n",
      "in_edges, node: Shandong, relation: contains the administrative territorial entity, target: Shandong\n",
      "in_edges, node: Windward Islands, relation: part of, target: Windward Islands\n",
      "in_edges, node: Sea of Galilee, relation: located in or next to body of water, target: Sea of Galilee\n",
      "in_edges, node: Sea of Galilee, relation: lowest point, target: Sea of Galilee\n",
      "in_edges, node: Saitama Prefecture, relation: contains the administrative territorial entity, target: Saitama Prefecture\n",
      "in_edges, node: Roraima, relation: contains the administrative territorial entity, target: Roraima\n",
      "in_edges, node: Luganda, relation: language used, target: Luganda\n",
      "in_edges, node: Leo Varadkar, relation: head of government, target: Leo Varadkar\n",
      "in_edges, node: Aden, relation: capital, target: Aden\n",
      "in_edges, node: Pedro de Sintra, relation: discoverer or inventor, target: Pedro de Sintra\n",
      "in_edges, node: Spanish–American War, relation: significant event, target: Spanish–American War\n",
      "in_edges, node: Trang, relation: contains the administrative territorial entity, target: Trang\n",
      "in_edges, node: Tianjin, relation: contains the administrative territorial entity, target: Tianjin\n",
      "in_edges, node: Mukdahan, relation: contains the administrative territorial entity, target: Mukdahan\n",
      "in_edges, node: Tocantins, relation: contains the administrative territorial entity, target: Tocantins\n",
      "in_edges, node: Somalis, relation: ethnic group, target: Somalis\n",
      "in_edges, node: Schaffhausen, relation: contains the administrative territorial entity, target: Schaffhausen\n",
      "in_edges, node: Pathum Thani, relation: contains the administrative territorial entity, target: Pathum Thani\n",
      "in_edges, node: Scottish Government, relation: executive body, target: Scottish Government\n",
      "in_edges, node: Santa Cruz, relation: contains the administrative territorial entity, target: Santa Cruz\n",
      "in_edges, node: Sofia, relation: capital, target: Sofia\n",
      "in_edges, node: Heard Island and McDonald Islands, relation: contains the administrative territorial entity, target: Heard Island and McDonald Islands\n",
      "in_edges, node: Dakar, relation: capital, target: Dakar\n",
      "in_edges, node: Kurds, relation: ethnic group, target: Kurds\n",
      "in_edges, node: Maltese, relation: language used, target: Maltese\n",
      "in_edges, node: Prachin Buri, relation: contains the administrative territorial entity, target: Prachin Buri\n",
      "in_edges, node: District of Columbia, relation: contains the administrative territorial entity, target: District of Columbia\n",
      "in_edges, node: Avestan, relation: language used, target: Avestan\n",
      "in_edges, node: Västmanland County, relation: contains the administrative territorial entity, target: Västmanland County\n",
      "in_edges, node: Maputo, relation: capital, target: Maputo\n",
      "in_edges, node: Bishkek, relation: capital, target: Bishkek\n",
      "in_edges, node: Yaroslavl Oblast, relation: contains the administrative territorial entity, target: Yaroslavl Oblast\n",
      "in_edges, node: Wrexham County Borough, relation: contains the administrative territorial entity, target: Wrexham County Borough\n",
      "in_edges, node: Scottish Borders, relation: contains the administrative territorial entity, target: Scottish Borders\n",
      "in_edges, node: Mon, relation: language used, target: Mon\n",
      "in_edges, node: Newport, relation: contains the administrative territorial entity, target: Newport\n",
      "in_edges, node: Tula Oblast, relation: contains the administrative territorial entity, target: Tula Oblast\n",
      "in_edges, node: Pali, relation: language used, target: Pali\n",
      "in_edges, node: Anglesey, relation: contains the administrative territorial entity, target: Anglesey\n",
      "in_edges, node: Yucatán, relation: contains the administrative territorial entity, target: Yucatán\n",
      "in_edges, node: Shinto, relation: religion or worldview, target: Shinto\n",
      "in_edges, node: Tripura, relation: contains the administrative territorial entity, target: Tripura\n",
      "in_edges, node: Georgetown, relation: capital, target: Georgetown\n",
      "in_edges, node: Sergio Mattarella, relation: head of state, target: Sergio Mattarella\n",
      "in_edges, node: Reserve Bank of Australia, relation: central bank, target: Reserve Bank of Australia\n",
      "in_edges, node: Palestine, relation: partially coincident with, target: Palestine\n",
      "in_edges, node: Delaware, relation: contains the administrative territorial entity, target: Delaware\n",
      "in_edges, node: Buri Ram, relation: contains the administrative territorial entity, target: Buri Ram\n",
      "in_edges, node: Hanukkah, relation: public holiday, target: Hanukkah\n",
      "in_edges, node: Bongbong Marcos, relation: head of state, target: Bongbong Marcos\n",
      "in_edges, node: Western Sahara, relation: shares border with, target: Western Sahara\n",
      "in_edges, node: Ang Thong, relation: contains the administrative territorial entity, target: Ang Thong\n",
      "in_edges, node: New York, relation: contains the administrative territorial entity, target: New York\n",
      "in_edges, node: Petr Fiala, relation: head of government, target: Petr Fiala\n",
      "in_edges, node: Novgorod Oblast, relation: contains the administrative territorial entity, target: Novgorod Oblast\n",
      "in_edges, node: Gujarat, relation: contains the administrative territorial entity, target: Gujarat\n",
      "in_edges, node: Quito, relation: capital, target: Quito\n",
      "in_edges, node: Bács-Kiskun County, relation: contains the administrative territorial entity, target: Bács-Kiskun County\n",
      "in_edges, node: Coptic, relation: language used, target: Coptic\n",
      "in_edges, node: Parliament of the United Kingdom, relation: legislative body, target: Parliament of the United Kingdom\n",
      "in_edges, node: Gwangju, relation: contains the administrative territorial entity, target: Gwangju\n",
      "in_edges, node: history of Russia, relation: history of topic, target: history of Russia\n",
      "in_edges, node: Vologda Oblast, relation: contains the administrative territorial entity, target: Vologda Oblast\n",
      "in_edges, node: Phichit, relation: contains the administrative territorial entity, target: Phichit\n",
      "in_edges, node: Pomeranian Voivodeship, relation: contains the administrative territorial entity, target: Pomeranian Voivodeship\n",
      "in_edges, node: California, relation: contains the administrative territorial entity, target: California\n",
      "in_edges, node: Nueva Esparta, relation: contains the administrative territorial entity, target: Nueva Esparta\n",
      "in_edges, node: Maryland, relation: contains the administrative territorial entity, target: Maryland\n",
      "in_edges, node: Chubut, relation: contains the administrative territorial entity, target: Chubut\n",
      "in_edges, node: Guam, relation: contains the administrative territorial entity, target: Guam\n",
      "in_edges, node: Gitanas Nausėda, relation: head of state, target: Gitanas Nausėda\n",
      "in_edges, node: Chachoengsao, relation: contains the administrative territorial entity, target: Chachoengsao\n",
      "in_edges, node: Northwest Territories, relation: contains the administrative territorial entity, target: Northwest Territories\n",
      "in_edges, node: Malayalam, relation: language used, target: Malayalam\n",
      "in_edges, node: Copenhagen, relation: capital, target: Copenhagen\n",
      "in_edges, node: Belgorod Oblast, relation: contains the administrative territorial entity, target: Belgorod Oblast\n",
      "in_edges, node: George Weah, relation: head of government, target: George Weah\n",
      "in_edges, node: Connecticut, relation: contains the administrative territorial entity, target: Connecticut\n",
      "in_edges, node: Otago Region, relation: contains the administrative territorial entity, target: Otago Region\n",
      "in_edges, node: Conseil d'État, relation: highest judicial authority, target: Conseil d'État\n",
      "in_edges, node: Chon Buri, relation: contains the administrative territorial entity, target: Chon Buri\n",
      "in_edges, node: Limburgish, relation: language used, target: Limburgish\n",
      "in_edges, node: Leningrad Oblast, relation: contains the administrative territorial entity, target: Leningrad Oblast\n",
      "in_edges, node: coast, relation: named after, target: coast\n",
      "in_edges, node: Glarus, relation: contains the administrative territorial entity, target: Glarus\n",
      "in_edges, node: Araucanía Region, relation: contains the administrative territorial entity, target: Araucanía Region\n",
      "in_edges, node: Appenzell Ausserrhoden, relation: contains the administrative territorial entity, target: Appenzell Ausserrhoden\n",
      "in_edges, node: Assam, relation: contains the administrative territorial entity, target: Assam\n",
      "in_edges, node: Vientiane, relation: capital, target: Vientiane\n",
      "in_edges, node: Klaus Iohannis, relation: head of state, target: Klaus Iohannis\n",
      "in_edges, node: Wallis and Futuna, relation: contains the administrative territorial entity, target: Wallis and Futuna\n",
      "in_edges, node: Çanakkale Province, relation: contains the administrative territorial entity, target: Çanakkale Province\n",
      "in_edges, node: Hunan, relation: contains the administrative territorial entity, target: Hunan\n",
      "in_edges, node: Economic and Monetary Union of the European Union, relation: member of, target: Economic and Monetary Union of the European Union\n",
      "in_edges, node: Bangka Belitung Islands, relation: contains the administrative territorial entity, target: Bangka Belitung Islands\n",
      "in_edges, node: children's day, relation: public holiday, target: children's day\n",
      "in_edges, node: Namib, relation: named after, target: Namib\n",
      "in_edges, node: Madhya Pradesh, relation: contains the administrative territorial entity, target: Madhya Pradesh\n",
      "in_edges, node: Snow White, relation: edition or translation of, target: Snow White\n",
      "in_edges, node: Carmarthenshire, relation: contains the administrative territorial entity, target: Carmarthenshire\n",
      "in_edges, node: Papua, relation: contains the administrative territorial entity, target: Papua\n",
      "in_edges, node: brown bear, relation: official symbol, target: brown bear\n",
      "in_edges, node: Apulia, relation: contains the administrative territorial entity, target: Apulia\n",
      "in_edges, node: Managua, relation: capital, target: Managua\n",
      "in_edges, node: Zagreb, relation: contains the administrative territorial entity, target: Zagreb\n",
      "in_edges, node: yen, relation: currency, target: yen\n",
      "in_edges, node: Yala, relation: contains the administrative territorial entity, target: Yala\n",
      "in_edges, node: Tyrol, relation: contains the administrative territorial entity, target: Tyrol\n",
      "in_edges, node: Monmouthshire, relation: contains the administrative territorial entity, target: Monmouthshire\n",
      "in_edges, node: Västerbotten County, relation: contains the administrative territorial entity, target: Västerbotten County\n",
      "in_edges, node: Altai Krai, relation: contains the administrative territorial entity, target: Altai Krai\n",
      "in_edges, node: Eurasian Hoopoe, relation: official symbol, target: Eurasian Hoopoe\n",
      "in_edges, node: Ingrian, relation: language used, target: Ingrian\n",
      "in_edges, node: Evangelical Lutheran Church of Finland, relation: religion or worldview, target: Evangelical Lutheran Church of Finland\n",
      "in_edges, node: Port Moresby, relation: capital, target: Port Moresby\n",
      "in_edges, node: Mediterranean Basin, relation: located in/on physical feature, target: Mediterranean Basin\n",
      "in_edges, node: Mediterranean Basin, relation: part of, target: Mediterranean Basin\n",
      "in_edges, node: Bank of France, relation: central bank, target: Bank of France\n",
      "in_edges, node: Yamaguchi Prefecture, relation: contains the administrative territorial entity, target: Yamaguchi Prefecture\n",
      "in_edges, node: Hunedoara County, relation: contains the administrative territorial entity, target: Hunedoara County\n",
      "in_edges, node: Ainu, relation: language used, target: Ainu\n",
      "in_edges, node: champagne, relation: production statistics, target: champagne\n",
      "in_edges, node: Arizona, relation: contains the administrative territorial entity, target: Arizona\n",
      "in_edges, node: Surat Thani, relation: contains the administrative territorial entity, target: Surat Thani\n",
      "in_edges, node: Yamagata Prefecture, relation: contains the administrative territorial entity, target: Yamagata Prefecture\n",
      "in_edges, node: Vladimir Oblast, relation: contains the administrative territorial entity, target: Vladimir Oblast\n",
      "in_edges, node: Mogadishu, relation: capital, target: Mogadishu\n",
      "in_edges, node: Mogadishu, relation: named after, target: Mogadishu\n",
      "in_edges, node: Penang, relation: contains the administrative territorial entity, target: Penang\n",
      "in_edges, node: Cayman Islands, relation: diplomatic relation, target: Cayman Islands\n",
      "in_edges, node: Nile, relation: located in or next to body of water, target: Nile\n",
      "in_edges, node: Ingush, relation: language used, target: Ingush\n",
      "in_edges, node: Central Bank of Iran, relation: central bank, target: Central Bank of Iran\n",
      "in_edges, node: Overijssel, relation: contains the administrative territorial entity, target: Overijssel\n",
      "in_edges, node: Jordan River, relation: named after, target: Jordan River\n",
      "in_edges, node: Jordan River, relation: located in or next to body of water, target: Jordan River\n",
      "in_edges, node: Tatars, relation: ethnic group, target: Tatars\n",
      "in_edges, node: Gaza Strip, relation: shares border with, target: Gaza Strip\n",
      "in_edges, node: Rabat, relation: capital, target: Rabat\n",
      "in_edges, node: Serbo-Croatian, relation: language used, target: Serbo-Croatian\n",
      "in_edges, node: Australian Capital Territory, relation: contains the administrative territorial entity, target: Australian Capital Territory\n",
      "in_edges, node: Mongols, relation: named after, target: Mongols\n",
      "in_edges, node: Daman and Diu, relation: contains the administrative territorial entity, target: Daman and Diu\n",
      "in_edges, node: North Brabant, relation: contains the administrative territorial entity, target: North Brabant\n",
      "in_edges, node: Margrethe II of Denmark, relation: head of state, target: Margrethe II of Denmark\n",
      "in_edges, node: Dutch East Indies, relation: replaces, target: Dutch East Indies\n",
      "in_edges, node: Southern Ostrobothnia, relation: contains the administrative territorial entity, target: Southern Ostrobothnia\n",
      "in_edges, node: Coahuila, relation: contains the administrative territorial entity, target: Coahuila\n",
      "in_edges, node: Sonora, relation: contains the administrative territorial entity, target: Sonora\n",
      "in_edges, node: Philip II of Spain, relation: named after, target: Philip II of Spain\n",
      "in_edges, node: Allier, relation: twinned administrative body, target: Allier\n",
      "in_edges, node: Oromo, relation: language used, target: Oromo\n",
      "in_edges, node: Ganja, relation: contains the administrative territorial entity, target: Ganja\n",
      "in_edges, node: Western Europe, relation: part of, target: Western Europe\n",
      "in_edges, node: Western Europe, relation: located in/on physical feature, target: Western Europe\n",
      "in_edges, node: Odisha, relation: contains the administrative territorial entity, target: Odisha\n",
      "in_edges, node: Nakhon Ratchasima, relation: contains the administrative territorial entity, target: Nakhon Ratchasima\n",
      "in_edges, node: South Bohemian Region, relation: contains the administrative territorial entity, target: South Bohemian Region\n",
      "in_edges, node: An Giang, relation: contains the administrative territorial entity, target: An Giang\n",
      "in_edges, node: Chhattisgarh, relation: contains the administrative territorial entity, target: Chhattisgarh\n",
      "in_edges, node: Welsh Government, relation: executive body, target: Welsh Government\n",
      "in_edges, node: Krabi, relation: contains the administrative territorial entity, target: Krabi\n",
      "in_edges, node: Mexican-American War, relation: significant event, target: Mexican-American War\n",
      "in_edges, node: County Roscommon, relation: contains the administrative territorial entity, target: County Roscommon\n",
      "in_edges, node: Östergötland County, relation: contains the administrative territorial entity, target: Östergötland County\n",
      "in_edges, node: Chongqing, relation: contains the administrative territorial entity, target: Chongqing\n",
      "in_edges, node: canton Vaud, relation: contains the administrative territorial entity, target: canton Vaud\n",
      "in_edges, node: Apollo 11, relation: significant event, target: Apollo 11\n",
      "in_edges, node: Yugoslavia, relation: diplomatic relation, target: Yugoslavia\n",
      "in_edges, node: Naruhito, relation: head of state, target: Naruhito\n",
      "in_edges, node: Gotland, relation: has part(s), target: Gotland\n",
      "in_edges, node: Halland County, relation: contains the administrative territorial entity, target: Halland County\n",
      "in_edges, node: Hiroshima Prefecture, relation: contains the administrative territorial entity, target: Hiroshima Prefecture\n",
      "in_edges, node: Ho Chi Minh City, relation: contains the administrative territorial entity, target: Ho Chi Minh City\n",
      "in_edges, node: Nauruan, relation: official language, target: Nauruan\n",
      "in_edges, node: Alexander Lukashenko, relation: head of state, target: Alexander Lukashenko\n",
      "in_edges, node: Lule Sami, relation: language used, target: Lule Sami\n",
      "in_edges, node: Phetchabun, relation: contains the administrative territorial entity, target: Phetchabun\n",
      "in_edges, node: Aosta Valley, relation: contains the administrative territorial entity, target: Aosta Valley\n",
      "in_edges, node: Praia, relation: capital, target: Praia\n",
      "in_edges, node: Świętokrzyskie Voivodeship, relation: contains the administrative territorial entity, target: Świętokrzyskie Voivodeship\n",
      "in_edges, node: blue, relation: official color, target: blue\n",
      "in_edges, node: Narathiwat, relation: contains the administrative territorial entity, target: Narathiwat\n",
      "in_edges, node: Kampala, relation: capital, target: Kampala\n",
      "in_edges, node: Mount Elbrus, relation: highest point, target: Mount Elbrus\n",
      "in_edges, node: Ingushetia, relation: contains the administrative territorial entity, target: Ingushetia\n",
      "in_edges, node: Luanda, relation: capital, target: Luanda\n",
      "in_edges, node: Cyril Ramaphosa, relation: head of government, target: Cyril Ramaphosa\n",
      "in_edges, node: Hauts-de-France, relation: contains the administrative territorial entity, target: Hauts-de-France\n",
      "in_edges, node: Sanskrit, relation: language used, target: Sanskrit\n",
      "in_edges, node: Delhi, relation: contains the administrative territorial entity, target: Delhi\n",
      "in_edges, node: Clipperton Island, relation: contains the administrative territorial entity, target: Clipperton Island\n",
      "in_edges, node: Ryazan Oblast, relation: contains the administrative territorial entity, target: Ryazan Oblast\n",
      "in_edges, node: Massachusetts, relation: contains the administrative territorial entity, target: Massachusetts\n",
      "in_edges, node: Silesian Voivodeship, relation: contains the administrative territorial entity, target: Silesian Voivodeship\n",
      "in_edges, node: San Luis Province, relation: contains the administrative territorial entity, target: San Luis Province\n",
      "in_edges, node: Ionian Sea, relation: located in or next to body of water, target: Ionian Sea\n",
      "in_edges, node: silver, relation: named after, target: silver\n",
      "in_edges, node: Bank of Mexico, relation: central bank, target: Bank of Mexico\n",
      "in_edges, node: Kurgan Oblast, relation: contains the administrative territorial entity, target: Kurgan Oblast\n",
      "in_edges, node: Nakhon Sawan, relation: contains the administrative territorial entity, target: Nakhon Sawan\n",
      "in_edges, node: Lake Tanganyika, relation: lowest point, target: Lake Tanganyika\n",
      "in_edges, node: Northern Ostrobothnia, relation: contains the administrative territorial entity, target: Northern Ostrobothnia\n",
      "in_edges, node: Himalayas, relation: located in/on physical feature, target: Himalayas\n",
      "in_edges, node: East Kalimantan, relation: contains the administrative territorial entity, target: East Kalimantan\n",
      "in_edges, node: West Bank, relation: shares border with, target: West Bank\n",
      "in_edges, node: Castelo Branco, relation: contains the administrative territorial entity, target: Castelo Branco\n",
      "in_edges, node: Chimborazo, relation: highest point, target: Chimborazo\n",
      "in_edges, node: Supreme Court of the United States, relation: highest judicial authority, target: Supreme Court of the United States\n",
      "in_edges, node: Kalasin, relation: contains the administrative territorial entity, target: Kalasin\n",
      "in_edges, node: Montevideo, relation: capital, target: Montevideo\n",
      "in_edges, node: Salta, relation: contains the administrative territorial entity, target: Salta\n",
      "in_edges, node: Inner Mongolia, relation: contains the administrative territorial entity, target: Inner Mongolia\n",
      "in_edges, node: Nicosia, relation: capital, target: Nicosia\n",
      "in_edges, node: cocoa bean, relation: production statistics, target: cocoa bean\n",
      "in_edges, node: Osaka Prefecture, relation: contains the administrative territorial entity, target: Osaka Prefecture\n",
      "in_edges, node: Niamey, relation: capital, target: Niamey\n",
      "in_edges, node: Kattegat, relation: located in or next to body of water, target: Kattegat\n",
      "in_edges, node: Río de la Plata, relation: located in or next to body of water, target: Río de la Plata\n",
      "in_edges, node: Jewish Autonomous Oblast, relation: contains the administrative territorial entity, target: Jewish Autonomous Oblast\n",
      "in_edges, node: Fulbe people, relation: ethnic group, target: Fulbe people\n",
      "in_edges, node: Federal District, relation: contains the administrative territorial entity, target: Federal District\n",
      "in_edges, node: Australian Government, relation: executive body, target: Australian Government\n",
      "in_edges, node: Sing Buri, relation: contains the administrative territorial entity, target: Sing Buri\n",
      "in_edges, node: Bislama, relation: official language, target: Bislama\n",
      "in_edges, node: Pskov Oblast, relation: contains the administrative territorial entity, target: Pskov Oblast\n",
      "in_edges, node: Setúbal, relation: contains the administrative territorial entity, target: Setúbal\n",
      "in_edges, node: Muslim, relation: ethnic group, target: Muslim\n",
      "in_edges, node: Santa Catarina, relation: contains the administrative territorial entity, target: Santa Catarina\n",
      "in_edges, node: Fribourg, relation: contains the administrative territorial entity, target: Fribourg\n",
      "in_edges, node: Port Vila, relation: capital, target: Port Vila\n",
      "in_edges, node: Tehran, relation: capital, target: Tehran\n",
      "in_edges, node: Munster, relation: contains the administrative territorial entity, target: Munster\n",
      "in_edges, node: Vattenfall, relation: owner of, target: Vattenfall\n",
      "in_edges, node: Amnat Charoen, relation: contains the administrative territorial entity, target: Amnat Charoen\n",
      "in_edges, node: European perch, relation: official symbol, target: European perch\n",
      "in_edges, node: Chuvash, relation: language used, target: Chuvash\n",
      "in_edges, node: Alexander Van der Bellen, relation: head of state, target: Alexander Van der Bellen\n",
      "in_edges, node: Tarragona Province, relation: contains the administrative territorial entity, target: Tarragona Province\n",
      "in_edges, node: Corsican, relation: language used, target: Corsican\n",
      "in_edges, node: Saint David, relation: patron saint, target: Saint David\n",
      "in_edges, node: North Sulawesi, relation: contains the administrative territorial entity, target: North Sulawesi\n",
      "in_edges, node: South Dakota, relation: contains the administrative territorial entity, target: South Dakota\n",
      "in_edges, node: Kronoberg County, relation: contains the administrative territorial entity, target: Kronoberg County\n",
      "in_edges, node: Windhoek, relation: capital, target: Windhoek\n",
      "in_edges, node: North Karelia, relation: contains the administrative territorial entity, target: North Karelia\n",
      "in_edges, node: East Java, relation: contains the administrative territorial entity, target: East Java\n",
      "in_edges, node: North American beaver, relation: official symbol, target: North American beaver\n",
      "in_edges, node: Iván Duque, relation: head of government, target: Iván Duque\n",
      "in_edges, node: Hungarian National Bank, relation: central bank, target: Hungarian National Bank\n",
      "in_edges, node: Misrata, relation: contains the administrative territorial entity, target: Misrata\n",
      "in_edges, node: Aberdeenshire, relation: contains the administrative territorial entity, target: Aberdeenshire\n",
      "in_edges, node: Old Church Slavonic, relation: language used, target: Old Church Slavonic\n",
      "in_edges, node: Auvergne-Rhône-Alpes, relation: contains the administrative territorial entity, target: Auvergne-Rhône-Alpes\n",
      "in_edges, node: Kabardino-Balkaria, relation: contains the administrative territorial entity, target: Kabardino-Balkaria\n",
      "in_edges, node: Oranjestad, relation: capital, target: Oranjestad\n",
      "in_edges, node: Isaac Herzog, relation: head of state, target: Isaac Herzog\n",
      "in_edges, node: Bratislava Region, relation: contains the administrative territorial entity, target: Bratislava Region\n",
      "in_edges, node: Mendoza Province, relation: contains the administrative territorial entity, target: Mendoza Province\n",
      "in_edges, node: United Nations Convention on the Law of the Sea, relation: member of, target: United Nations Convention on the Law of the Sea\n",
      "in_edges, node: Île-de-France, relation: contains the administrative territorial entity, target: Île-de-France\n",
      "in_edges, node: Franks, relation: named after, target: Franks\n",
      "in_edges, node: city, relation: instance of, target: city\n",
      "in_edges, node: Leinster, relation: contains the administrative territorial entity, target: Leinster\n",
      "in_edges, node: European Investment Bank, relation: member of, target: European Investment Bank\n",
      "in_edges, node: Salman bin Abdulaziz Al Saud, relation: head of government, target: Salman bin Abdulaziz Al Saud\n",
      "in_edges, node: Luxembourgish, relation: language used, target: Luxembourgish\n",
      "in_edges, node: Päijänne Tavastia, relation: contains the administrative territorial entity, target: Päijänne Tavastia\n",
      "in_edges, node: Bahia, relation: contains the administrative territorial entity, target: Bahia\n",
      "in_edges, node: Mark Rutte, relation: head of government, target: Mark Rutte\n",
      "in_edges, node: Kabul, relation: capital, target: Kabul\n",
      "in_edges, node: Alberta, relation: contains the administrative territorial entity, target: Alberta\n",
      "in_edges, node: Autonomous Region of Bougainville, relation: contains the administrative territorial entity, target: Autonomous Region of Bougainville\n",
      "in_edges, node: Pembrokeshire, relation: contains the administrative territorial entity, target: Pembrokeshire\n",
      "in_edges, node: N'Djamena, relation: contains the administrative territorial entity, target: N'Djamena\n",
      "in_edges, node: Andrzej Duda, relation: head of state, target: Andrzej Duda\n",
      "in_edges, node: Balzers, relation: contains the administrative territorial entity, target: Balzers\n",
      "in_edges, node: Venice, relation: named after, target: Venice\n",
      "in_edges, node: Helmand, relation: contains the administrative territorial entity, target: Helmand\n",
      "in_edges, node: Kim Jong-un, relation: head of state, target: Kim Jong-un\n",
      "in_edges, node: Alexandria Governorate, relation: contains the administrative territorial entity, target: Alexandria Governorate\n",
      "in_edges, node: North Gyeongsang, relation: contains the administrative territorial entity, target: North Gyeongsang\n",
      "in_edges, node: County Louth, relation: contains the administrative territorial entity, target: County Louth\n",
      "in_edges, node: Schaan, relation: contains the administrative territorial entity, target: Schaan\n",
      "in_edges, node: Andaman and Nicobar Islands, relation: contains the administrative territorial entity, target: Andaman and Nicobar Islands\n",
      "in_edges, node: Maranhão, relation: contains the administrative territorial entity, target: Maranhão\n",
      "in_edges, node: Roseau, relation: capital, target: Roseau\n",
      "in_edges, node: Mariana of Austria, relation: named after, target: Mariana of Austria\n",
      "in_edges, node: Government of Russia, relation: executive body, target: Government of Russia\n",
      "in_edges, node: Kurdistan Region, relation: diplomatic relation, target: Kurdistan Region\n",
      "in_edges, node: Kurukh, relation: language used, target: Kurukh\n",
      "in_edges, node: Pirkanmaa, relation: contains the administrative territorial entity, target: Pirkanmaa\n",
      "in_edges, node: Ge'ez, relation: language used, target: Ge'ez\n",
      "in_edges, node: Michael D. Higgins, relation: head of state, target: Michael D. Higgins\n",
      "in_edges, node: Bryansk Oblast, relation: contains the administrative territorial entity, target: Bryansk Oblast\n",
      "in_edges, node: Svalbard, relation: contains the administrative territorial entity, target: Svalbard\n",
      "in_edges, node: Svalbard, relation: shares border with, target: Svalbard\n",
      "in_edges, node: Fife, relation: contains the administrative territorial entity, target: Fife\n",
      "in_edges, node: Alassane Ouattara, relation: head of state, target: Alassane Ouattara\n",
      "in_edges, node: Schellenberg, relation: contains the administrative territorial entity, target: Schellenberg\n",
      "in_edges, node: Virginia, relation: contains the administrative territorial entity, target: Virginia\n",
      "in_edges, node: Central Bank of Ecuador, relation: central bank, target: Central Bank of Ecuador\n",
      "in_edges, node: North Denmark Region, relation: contains the administrative territorial entity, target: North Denmark Region\n",
      "in_edges, node: Kagawa Prefecture, relation: contains the administrative territorial entity, target: Kagawa Prefecture\n",
      "in_edges, node: Hage Gottfried Geingob, relation: head of state, target: Hage Gottfried Geingob\n",
      "in_edges, node: Jilin, relation: contains the administrative territorial entity, target: Jilin\n",
      "in_edges, node: Canton of Geneva, relation: contains the administrative territorial entity, target: Canton of Geneva\n",
      "in_edges, node: Fukui Prefecture, relation: contains the administrative territorial entity, target: Fukui Prefecture\n",
      "in_edges, node: Kaohsiung, relation: contains the administrative territorial entity, target: Kaohsiung\n",
      "in_edges, node: Sogn og Fjordane, relation: contains the administrative territorial entity, target: Sogn og Fjordane\n",
      "in_edges, node: Blagoevgrad Province, relation: contains the administrative territorial entity, target: Blagoevgrad Province\n",
      "in_edges, node: Ratchaburi, relation: contains the administrative territorial entity, target: Ratchaburi\n",
      "in_edges, node: Swiss National Day, relation: public holiday, target: Swiss National Day\n",
      "in_edges, node: Tver Oblast, relation: contains the administrative territorial entity, target: Tver Oblast\n",
      "in_edges, node: Arkansas, relation: contains the administrative territorial entity, target: Arkansas\n",
      "in_edges, node: Shavkat Mirziyoyev, relation: head of state, target: Shavkat Mirziyoyev\n",
      "in_edges, node: United Nations Economic Commission for Latin America and the Caribbean, relation: member of, target: United Nations Economic Commission for Latin America and the Caribbean\n",
      "in_edges, node: Abruzzo, relation: contains the administrative territorial entity, target: Abruzzo\n",
      "in_edges, node: Martinique, relation: contains the administrative territorial entity, target: Martinique\n",
      "in_edges, node: San Luis Potosí, relation: contains the administrative territorial entity, target: San Luis Potosí\n",
      "in_edges, node: sumo, relation: official symbol, target: sumo\n",
      "in_edges, node: Evenki, relation: language used, target: Evenki\n",
      "in_edges, node: Vermont, relation: contains the administrative territorial entity, target: Vermont\n",
      "in_edges, node: Eastern Front (World War II), relation: participant in, target: Eastern Front (World War II)\n",
      "in_edges, node: Florida Department, relation: contains the administrative territorial entity, target: Florida Department\n",
      "in_edges, node: Pyrenees, relation: located in/on physical feature, target: Pyrenees\n",
      "in_edges, node: Canberra, relation: capital, target: Canberra\n",
      "in_edges, node: Adıyaman Province, relation: contains the administrative territorial entity, target: Adıyaman Province\n",
      "in_edges, node: Okapia johnstoni, relation: official symbol, target: Okapia johnstoni\n",
      "in_edges, node: city-state, relation: instance of, target: city-state\n",
      "in_edges, node: Abdalla Hamdok, relation: head of government, target: Abdalla Hamdok\n",
      "in_edges, node: Phitsanulok, relation: contains the administrative territorial entity, target: Phitsanulok\n",
      "in_edges, node: Bulawayo, relation: contains the administrative territorial entity, target: Bulawayo\n",
      "in_edges, node: Aconcagua, relation: highest point, target: Aconcagua\n",
      "in_edges, node: Mapudungun, relation: language used, target: Mapudungun\n",
      "in_edges, node: Ainu people, relation: ethnic group, target: Ainu people\n",
      "in_edges, node: American Revolutionary War, relation: participant in, target: American Revolutionary War\n",
      "in_edges, node: North Ossetia–Alania, relation: contains the administrative territorial entity, target: North Ossetia–Alania\n",
      "in_edges, node: Almaty, relation: contains the administrative territorial entity, target: Almaty\n",
      "in_edges, node: Nakhon Pathom, relation: contains the administrative territorial entity, target: Nakhon Pathom\n",
      "in_edges, node: Yakut, relation: language used, target: Yakut\n",
      "in_edges, node: Central Bohemian Region, relation: contains the administrative territorial entity, target: Central Bohemian Region\n",
      "in_edges, node: Indiana, relation: contains the administrative territorial entity, target: Indiana\n",
      "in_edges, node: Veneto, relation: contains the administrative territorial entity, target: Veneto\n",
      "in_edges, node: English Channel, relation: located in or next to body of water, target: English Channel\n",
      "in_edges, node: Tây Ninh, relation: contains the administrative territorial entity, target: Tây Ninh\n",
      "in_edges, node: Han Chinese people, relation: ethnic group, target: Han Chinese people\n",
      "in_edges, node: Ústí nad Labem Region, relation: contains the administrative territorial entity, target: Ústí nad Labem Region\n",
      "in_edges, node: Chai Nat, relation: contains the administrative territorial entity, target: Chai Nat\n",
      "in_edges, node: Cape Town, relation: capital, target: Cape Town\n",
      "in_edges, node: Trentino-South Tyrol, relation: contains the administrative territorial entity, target: Trentino-South Tyrol\n",
      "in_edges, node: Naypyidaw, relation: capital, target: Naypyidaw\n",
      "in_edges, node: Bacău, relation: twinned administrative body, target: Bacău\n",
      "in_edges, node: Minas Gerais, relation: contains the administrative territorial entity, target: Minas Gerais\n",
      "in_edges, node: Russian invasion of Ukraine, relation: participant in, target: Russian invasion of Ukraine\n",
      "in_edges, node: Nowruz, relation: public holiday, target: Nowruz\n",
      "in_edges, node: Eastern Cape, relation: contains the administrative territorial entity, target: Eastern Cape\n",
      "in_edges, node: Moors, relation: named after, target: Moors\n",
      "in_edges, node: Negeri Sembilan, relation: contains the administrative territorial entity, target: Negeri Sembilan\n",
      "in_edges, node: Masovian Voivodeship, relation: contains the administrative territorial entity, target: Masovian Voivodeship\n",
      "in_edges, node: Loire Valley, relation: has part(s), target: Loire Valley\n",
      "in_edges, node: Netherlands Antilles, relation: diplomatic relation, target: Netherlands Antilles\n",
      "in_edges, node: Netherlands Antilles, relation: part of, target: Netherlands Antilles\n",
      "in_edges, node: Chinese, relation: official language, target: Chinese\n",
      "in_edges, node: Chinese, relation: language used, target: Chinese\n",
      "in_edges, node: Guizhou, relation: contains the administrative territorial entity, target: Guizhou\n",
      "in_edges, node: Georgian, relation: language used, target: Georgian\n",
      "in_edges, node: Central Bank of Russia, relation: central bank, target: Central Bank of Russia\n",
      "in_edges, node: Luba-Katanga, relation: language used, target: Luba-Katanga\n",
      "in_edges, node: Daerah Istimewa Yogyakarta, relation: contains the administrative territorial entity, target: Daerah Istimewa Yogyakarta\n",
      "in_edges, node: Strait of Gibraltar, relation: located in or next to body of water, target: Strait of Gibraltar\n",
      "in_edges, node: Inuit, relation: ethnic group, target: Inuit\n",
      "in_edges, node: Gelderland, relation: contains the administrative territorial entity, target: Gelderland\n",
      "in_edges, node: Xinjiang, relation: contains the administrative territorial entity, target: Xinjiang\n",
      "in_edges, node: Copperbelt Province, relation: contains the administrative territorial entity, target: Copperbelt Province\n",
      "in_edges, node: Kanagawa Prefecture, relation: contains the administrative territorial entity, target: Kanagawa Prefecture\n",
      "in_edges, node: Christopher Columbus, relation: named after, target: Christopher Columbus\n",
      "in_edges, node: Batna Province, relation: contains the administrative territorial entity, target: Batna Province\n",
      "in_edges, node: Hainan, relation: contains the administrative territorial entity, target: Hainan\n",
      "in_edges, node: Gujarati, relation: language used, target: Gujarati\n",
      "in_edges, node: Ahmadiyya, relation: religion or worldview, target: Ahmadiyya\n",
      "in_edges, node: Finland Proper, relation: contains the administrative territorial entity, target: Finland Proper\n",
      "in_edges, node: Telangana, relation: contains the administrative territorial entity, target: Telangana\n",
      "in_edges, node: Andry Rajoelina, relation: head of state, target: Andry Rajoelina\n",
      "in_edges, node: Texas, relation: contains the administrative territorial entity, target: Texas\n",
      "in_edges, node: Austrians, relation: ethnic group, target: Austrians\n",
      "in_edges, node: Puducherry, relation: contains the administrative territorial entity, target: Puducherry\n",
      "in_edges, node: Porto, relation: contains the administrative territorial entity, target: Porto\n",
      "in_edges, node: Bohemia, relation: has part(s), target: Bohemia\n",
      "in_edges, node: Powys, relation: contains the administrative territorial entity, target: Powys\n",
      "in_edges, node: Jonas Gahr Støre, relation: head of government, target: Jonas Gahr Støre\n",
      "in_edges, node: Bashar al-Assad, relation: head of state, target: Bashar al-Assad\n",
      "in_edges, node: Aragon, relation: shares border with, target: Aragon\n",
      "in_edges, node: Saba, relation: contains the administrative territorial entity, target: Saba\n",
      "in_edges, node: Zambezi River, relation: lowest point, target: Zambezi River\n",
      "in_edges, node: New South Wales, relation: contains the administrative territorial entity, target: New South Wales\n",
      "in_edges, node: Moray, relation: contains the administrative territorial entity, target: Moray\n",
      "in_edges, node: Santiago de Compostela, relation: capital, target: Santiago de Compostela\n",
      "in_edges, node: Edo State, relation: contains the administrative territorial entity, target: Edo State\n",
      "in_edges, node: Ancient Rome, relation: named after, target: Ancient Rome\n",
      "in_edges, node: Altai Republic, relation: contains the administrative territorial entity, target: Altai Republic\n",
      "in_edges, node: Capital Region of Denmark, relation: contains the administrative territorial entity, target: Capital Region of Denmark\n",
      "in_edges, node: Chinese Communist Party, relation: founded by, target: Chinese Communist Party\n",
      "in_edges, node: Córdoba Province, relation: contains the administrative territorial entity, target: Córdoba Province\n",
      "in_edges, node: Castries, relation: capital, target: Castries\n",
      "in_edges, node: Australian National Maritime Museum, relation: has works in the collection, target: Australian National Maritime Museum\n",
      "in_edges, node: Vale of Glamorgan, relation: contains the administrative territorial entity, target: Vale of Glamorgan\n",
      "in_edges, node: Ulf Kristersson, relation: head of government, target: Ulf Kristersson\n",
      "in_edges, node: Ladin, relation: language used, target: Ladin\n",
      "in_edges, node: Katrín Jakobsdóttir, relation: head of government, target: Katrín Jakobsdóttir\n",
      "in_edges, node: Warsaw, relation: capital, target: Warsaw\n",
      "in_edges, node: Edi Rama, relation: head of government, target: Edi Rama\n",
      "in_edges, node: theocracy, relation: basic form of government, target: theocracy\n",
      "in_edges, node: Solomon, relation: named after, target: Solomon\n",
      "in_edges, node: Gwynedd, relation: contains the administrative territorial entity, target: Gwynedd\n",
      "in_edges, node: Basque, relation: language used, target: Basque\n",
      "in_edges, node: Zenaga, relation: language used, target: Zenaga\n",
      "in_edges, node: British Isles, relation: located in/on physical feature, target: British Isles\n",
      "in_edges, node: Cassia fistula, relation: official symbol, target: Cassia fistula\n",
      "in_edges, node: Purim, relation: public holiday, target: Purim\n",
      "in_edges, node: Mayotte, relation: contains the administrative territorial entity, target: Mayotte\n",
      "in_edges, node: Espírito Santo, relation: contains the administrative territorial entity, target: Espírito Santo\n",
      "in_edges, node: Leiria, relation: contains the administrative territorial entity, target: Leiria\n",
      "in_edges, node: Scottish Parliament, relation: legislative body, target: Scottish Parliament\n",
      "in_edges, node: Kamphaeng Phet, relation: contains the administrative territorial entity, target: Kamphaeng Phet\n",
      "in_edges, node: Philippe I of Belgium, relation: head of state, target: Philippe I of Belgium\n",
      "in_edges, node: Borsod-Abaúj-Zemplén County, relation: contains the administrative territorial entity, target: Borsod-Abaúj-Zemplén County\n",
      "in_edges, node: Wisconsin, relation: contains the administrative territorial entity, target: Wisconsin\n",
      "in_edges, node: Antananarivo, relation: capital, target: Antananarivo\n",
      "in_edges, node: Monrovia, relation: capital, target: Monrovia\n",
      "in_edges, node: Jiangsu, relation: contains the administrative territorial entity, target: Jiangsu\n",
      "in_edges, node: Mount Kilimanjaro, relation: contains the administrative territorial entity, target: Mount Kilimanjaro\n",
      "in_edges, node: Hokkaido, relation: contains the administrative territorial entity, target: Hokkaido\n",
      "in_edges, node: Western European Union, relation: member of, target: Western European Union\n",
      "in_edges, node: Penza Oblast, relation: contains the administrative territorial entity, target: Penza Oblast\n",
      "in_edges, node: Konkani, relation: language used, target: Konkani\n",
      "in_edges, node: Călărași County, relation: contains the administrative territorial entity, target: Călărași County\n",
      "in_edges, node: Baghdad, relation: capital, target: Baghdad\n",
      "in_edges, node: Rapa Nui, relation: language used, target: Rapa Nui\n",
      "in_edges, node: Brittany, relation: contains the administrative territorial entity, target: Brittany\n",
      "in_edges, node: İzmir Province, relation: contains the administrative territorial entity, target: İzmir Province\n",
      "in_edges, node: Washington, relation: contains the administrative territorial entity, target: Washington\n",
      "in_edges, node: Bank of Finland, relation: central bank, target: Bank of Finland\n",
      "in_edges, node: Hungarians, relation: named after, target: Hungarians\n",
      "in_edges, node: Fumio Kishida, relation: head of government, target: Fumio Kishida\n",
      "in_edges, node: Bouvet Island, relation: contains the administrative territorial entity, target: Bouvet Island\n",
      "in_edges, node: Isla de la Juventud, relation: contains the administrative territorial entity, target: Isla de la Juventud\n",
      "in_edges, node: Daugavpils, relation: contains the administrative territorial entity, target: Daugavpils\n",
      "in_edges, node: Montana, relation: contains the administrative territorial entity, target: Montana\n",
      "in_edges, node: Jerusalem, relation: capital, target: Jerusalem\n",
      "in_edges, node: Selkup, relation: language used, target: Selkup\n",
      "in_edges, node: Suceava County, relation: contains the administrative territorial entity, target: Suceava County\n",
      "in_edges, node: Limpopo, relation: contains the administrative territorial entity, target: Limpopo\n",
      "in_edges, node: Sicily, relation: contains the administrative territorial entity, target: Sicily\n",
      "in_edges, node: Perlis, relation: contains the administrative territorial entity, target: Perlis\n",
      "in_edges, node: Luanda Province, relation: contains the administrative territorial entity, target: Luanda Province\n",
      "in_edges, node: Tallinn, relation: capital, target: Tallinn\n",
      "in_edges, node: Jujuy, relation: contains the administrative territorial entity, target: Jujuy\n",
      "in_edges, node: Karnataka, relation: contains the administrative territorial entity, target: Karnataka\n",
      "in_edges, node: Catalan people, relation: named after, target: Catalan people\n",
      "in_edges, node: South Ayrshire, relation: contains the administrative territorial entity, target: South Ayrshire\n",
      "in_edges, node: Zlín Region, relation: contains the administrative territorial entity, target: Zlín Region\n",
      "in_edges, node: National Aeronautics and Space Administration, relation: member of, target: National Aeronautics and Space Administration\n",
      "in_edges, node: Emirate of Dubai, relation: contains the administrative territorial entity, target: Emirate of Dubai\n",
      "in_edges, node: Dalarna County, relation: contains the administrative territorial entity, target: Dalarna County\n",
      "in_edges, node: Karlovy Vary Region, relation: contains the administrative territorial entity, target: Karlovy Vary Region\n",
      "in_edges, node: Santarém, relation: contains the administrative territorial entity, target: Santarém\n",
      "in_edges, node: Budapest, relation: capital, target: Budapest\n",
      "in_edges, node: Coccinella septempunctata, relation: official symbol, target: Coccinella septempunctata\n",
      "in_edges, node: Stepanakert, relation: contains the administrative territorial entity, target: Stepanakert\n",
      "in_edges, node: Lombardy, relation: contains the administrative territorial entity, target: Lombardy\n",
      "in_edges, node: Newar, relation: language used, target: Newar\n",
      "in_edges, node: Florida, relation: contains the administrative territorial entity, target: Florida\n",
      "in_edges, node: Riyadh, relation: capital, target: Riyadh\n",
      "in_edges, node: Lamphun, relation: contains the administrative territorial entity, target: Lamphun\n",
      "in_edges, node: Aguascalientes, relation: contains the administrative territorial entity, target: Aguascalientes\n",
      "in_edges, node: Albanians, relation: ethnic group, target: Albanians\n",
      "in_edges, node: Omsk Oblast, relation: contains the administrative territorial entity, target: Omsk Oblast\n",
      "in_edges, node: Rayong, relation: contains the administrative territorial entity, target: Rayong\n",
      "in_edges, node: Valais, relation: contains the administrative territorial entity, target: Valais\n",
      "in_edges, node: Clackmannanshire, relation: contains the administrative territorial entity, target: Clackmannanshire\n",
      "in_edges, node: Antalya Province, relation: contains the administrative territorial entity, target: Antalya Province\n",
      "in_edges, node: Kwajalein, relation: contains the administrative territorial entity, target: Kwajalein\n",
      "in_edges, node: Västergötland, relation: has part(s), target: Västergötland\n",
      "in_edges, node: Northern Territory, relation: contains the administrative territorial entity, target: Northern Territory\n",
      "in_edges, node: Gummersbach, relation: twinned administrative body, target: Gummersbach\n",
      "in_edges, node: Malacca, relation: contains the administrative territorial entity, target: Malacca\n",
      "in_edges, node: Sardinia, relation: contains the administrative territorial entity, target: Sardinia\n",
      "in_edges, node: Algerian War, relation: significant event, target: Algerian War\n",
      "in_edges, node: Okinawa Prefecture, relation: contains the administrative territorial entity, target: Okinawa Prefecture\n",
      "in_edges, node: Putrajaya, relation: contains the administrative territorial entity, target: Putrajaya\n",
      "in_edges, node: Chandigarh, relation: contains the administrative territorial entity, target: Chandigarh\n",
      "in_edges, node: Valletta, relation: capital, target: Valletta\n",
      "in_edges, node: Novosibirsk Oblast, relation: contains the administrative territorial entity, target: Novosibirsk Oblast\n",
      "in_edges, node: Henan, relation: contains the administrative territorial entity, target: Henan\n",
      "in_edges, node: Shaanxi, relation: contains the administrative territorial entity, target: Shaanxi\n",
      "in_edges, node: Greenlandic, relation: language used, target: Greenlandic\n",
      "in_edges, node: Satu Mare County, relation: contains the administrative territorial entity, target: Satu Mare County\n",
      "in_edges, node: San Salvador, relation: capital, target: San Salvador\n",
      "in_edges, node: Fujian, relation: contains the administrative territorial entity, target: Fujian\n",
      "in_edges, node: Goa, relation: contains the administrative territorial entity, target: Goa\n",
      "in_edges, node: Sakha, relation: contains the administrative territorial entity, target: Sakha\n",
      "in_edges, node: Anhui, relation: contains the administrative territorial entity, target: Anhui\n",
      "in_edges, node: Oklahoma, relation: contains the administrative territorial entity, target: Oklahoma\n",
      "in_edges, node: Barcelona Province, relation: contains the administrative territorial entity, target: Barcelona Province\n",
      "in_edges, node: Nova Scotia, relation: contains the administrative territorial entity, target: Nova Scotia\n",
      "in_edges, node: Canillo, relation: contains the administrative territorial entity, target: Canillo\n",
      "in_edges, node: Entre Ríos Province, relation: contains the administrative territorial entity, target: Entre Ríos Province\n",
      "in_edges, node: Zacatecas, relation: contains the administrative territorial entity, target: Zacatecas\n",
      "in_edges, node: Shanghai, relation: contains the administrative territorial entity, target: Shanghai\n",
      "in_edges, node: African Americans, relation: ethnic group, target: African Americans\n",
      "in_edges, node: Sergipe, relation: contains the administrative territorial entity, target: Sergipe\n",
      "in_edges, node: Ticino, relation: contains the administrative territorial entity, target: Ticino\n",
      "in_edges, node: Libreville, relation: capital, target: Libreville\n",
      "in_edges, node: Sikhism, relation: religion or worldview, target: Sikhism\n",
      "in_edges, node: Bihar, relation: contains the administrative territorial entity, target: Bihar\n",
      "in_edges, node: Nenets, relation: language used, target: Nenets\n",
      "in_edges, node: State Council of the People's Republic of China, relation: executive body, target: State Council of the People's Republic of China\n",
      "in_edges, node: Khakassia, relation: contains the administrative territorial entity, target: Khakassia\n",
      "in_edges, node: Kimbundu, relation: language used, target: Kimbundu\n",
      "in_edges, node: Muscat, relation: capital, target: Muscat\n",
      "in_edges, node: Asunción, relation: capital, target: Asunción\n",
      "in_edges, node: Czechs, relation: named after, target: Czechs\n",
      "in_edges, node: Inn, relation: located in or next to body of water, target: Inn\n",
      "in_edges, node: Outer Hebrides, relation: contains the administrative territorial entity, target: Outer Hebrides\n",
      "in_edges, node: Lubusz Voivodeship, relation: contains the administrative territorial entity, target: Lubusz Voivodeship\n",
      "in_edges, node: Triglav, relation: highest point, target: Triglav\n",
      "in_edges, node: Abdel Fattah el-Miksiki, relation: head of state, target: Abdel Fattah el-Miksiki\n",
      "in_edges, node: East Lothian, relation: contains the administrative territorial entity, target: East Lothian\n",
      "in_edges, node: Asian Americans, relation: ethnic group, target: Asian Americans\n",
      "in_edges, node: Mohammed VI, relation: head of state, target: Mohammed VI\n",
      "in_edges, node: Viseu, relation: contains the administrative territorial entity, target: Viseu\n",
      "in_edges, node: Lake Constance, relation: located in or next to body of water, target: Lake Constance\n",
      "in_edges, node: Falkirk, relation: contains the administrative territorial entity, target: Falkirk\n",
      "in_edges, node: Tibet, relation: contains the administrative territorial entity, target: Tibet\n",
      "in_edges, node: Bartın Province, relation: contains the administrative territorial entity, target: Bartın Province\n",
      "in_edges, node: equator, relation: named after, target: equator\n",
      "in_edges, node: Cocos Islands, relation: contains the administrative territorial entity, target: Cocos Islands\n",
      "in_edges, node: Andorra la Vella, relation: capital, target: Andorra la Vella\n",
      "in_edges, node: Cardiff, relation: capital, target: Cardiff\n",
      "in_edges, node: Jakarta, relation: capital, target: Jakarta\n",
      "in_edges, node: Guanajuato, relation: contains the administrative territorial entity, target: Guanajuato\n",
      "in_edges, node: Giorgia Meloni, relation: head of government, target: Giorgia Meloni\n",
      "in_edges, node: Sauli Niinistö, relation: head of state, target: Sauli Niinistö\n",
      "in_edges, node: Saint John's, relation: capital, target: Saint John's\n",
      "in_edges, node: Centre-Val de Loire, relation: contains the administrative territorial entity, target: Centre-Val de Loire\n",
      "in_edges, node: Lugo Province, relation: contains the administrative territorial entity, target: Lugo Province\n",
      "in_edges, node: Free State, relation: contains the administrative territorial entity, target: Free State\n",
      "in_edges, node: Smolensk Oblast, relation: contains the administrative territorial entity, target: Smolensk Oblast\n",
      "in_edges, node: Livonian, relation: language used, target: Livonian\n",
      "in_edges, node: Kyoto Prefecture, relation: contains the administrative territorial entity, target: Kyoto Prefecture\n",
      "in_edges, node: Valmiera, relation: contains the administrative territorial entity, target: Valmiera\n",
      "in_edges, node: Oita Prefecture, relation: contains the administrative territorial entity, target: Oita Prefecture\n",
      "in_edges, node: Friuli Venezia Giulia, relation: contains the administrative territorial entity, target: Friuli Venezia Giulia\n",
      "in_edges, node: Uttarakhand, relation: contains the administrative territorial entity, target: Uttarakhand\n",
      "in_edges, node: Iwate Prefecture, relation: contains the administrative territorial entity, target: Iwate Prefecture\n",
      "in_edges, node: Khuzestan Province, relation: contains the administrative territorial entity, target: Khuzestan Province\n",
      "in_edges, node: Vulpes lagopus, relation: taxon found at location, target: Vulpes lagopus\n",
      "in_edges, node: Los Lagos Region, relation: contains the administrative territorial entity, target: Los Lagos Region\n",
      "in_edges, node: Imo State, relation: contains the administrative territorial entity, target: Imo State\n",
      "in_edges, node: Skikda Province, relation: contains the administrative territorial entity, target: Skikda Province\n",
      "in_edges, node: Lusaka, relation: capital, target: Lusaka\n",
      "in_edges, node: South Australia, relation: contains the administrative territorial entity, target: South Australia\n",
      "in_edges, node: Aomori Prefecture, relation: contains the administrative territorial entity, target: Aomori Prefecture\n",
      "in_edges, node: Mikhail Mishustin, relation: head of government, target: Mikhail Mishustin\n",
      "in_edges, node: Veracruz, relation: contains the administrative territorial entity, target: Veracruz\n",
      "in_edges, node: Bastille Day, relation: public holiday, target: Bastille Day\n",
      "in_edges, node: Jász-Nagykun-Szolnok County, relation: contains the administrative territorial entity, target: Jász-Nagykun-Szolnok County\n",
      "in_edges, node: Gulf of Mexico, relation: located in or next to body of water, target: Gulf of Mexico\n",
      "in_edges, node: Solothurn, relation: contains the administrative territorial entity, target: Solothurn\n",
      "in_edges, node: Astana, relation: capital, target: Astana\n",
      "in_edges, node: Gurbanguly Berdimuhamedow, relation: head of government, target: Gurbanguly Berdimuhamedow\n",
      "in_edges, node: Khoekhoe, relation: language used, target: Khoekhoe\n",
      "in_edges, node: Bank of Italy, relation: central bank, target: Bank of Italy\n",
      "in_edges, node: Syrian Civil War, relation: participant in, target: Syrian Civil War\n",
      "in_edges, node: Buryat, relation: language used, target: Buryat\n",
      "in_edges, node: Ishikawa Prefecture, relation: contains the administrative territorial entity, target: Ishikawa Prefecture\n",
      "in_edges, node: Daegu, relation: contains the administrative territorial entity, target: Daegu\n",
      "in_edges, node: Mount Olympus, relation: highest point, target: Mount Olympus\n",
      "in_edges, node: Labuan, relation: contains the administrative territorial entity, target: Labuan\n",
      "in_edges, node: Abu Dhabi, relation: capital, target: Abu Dhabi\n",
      "in_edges, node: Aveiro, relation: contains the administrative territorial entity, target: Aveiro\n",
      "in_edges, node: Ningxia, relation: contains the administrative territorial entity, target: Ningxia\n",
      "in_edges, node: Kerala, relation: contains the administrative territorial entity, target: Kerala\n",
      "in_edges, node: Chiang Rai, relation: contains the administrative territorial entity, target: Chiang Rai\n",
      "in_edges, node: Jura, relation: contains the administrative territorial entity, target: Jura\n",
      "in_edges, node: Paraíba, relation: contains the administrative territorial entity, target: Paraíba\n",
      "in_edges, node: Azores, relation: contains the administrative territorial entity, target: Azores\n",
      "in_edges, node: Shizuoka Prefecture, relation: contains the administrative territorial entity, target: Shizuoka Prefecture\n",
      "in_edges, node: Moroni, relation: capital, target: Moroni\n",
      "in_edges, node: Xi Jinping, relation: head of state, target: Xi Jinping\n",
      "in_edges, node: Catherine of Siena, relation: patron saint, target: Catherine of Siena\n",
      "in_edges, node: National People's Congress, relation: legislative body, target: National People's Congress\n",
      "in_edges, node: Parliamentary Assembly of the Council of Europe, relation: designated as terrorist by, target: Parliamentary Assembly of the Council of Europe\n",
      "in_edges, node: Konya Province, relation: contains the administrative territorial entity, target: Konya Province\n",
      "in_edges, node: Court of Audit, relation: highest judicial authority, target: Court of Audit\n",
      "in_edges, node: Colima, relation: contains the administrative territorial entity, target: Colima\n",
      "in_edges, node: Sint Eustatius, relation: contains the administrative territorial entity, target: Sint Eustatius\n",
      "in_edges, node: Midlothian, relation: contains the administrative territorial entity, target: Midlothian\n",
      "in_edges, node: Porto-Novo, relation: capital, target: Porto-Novo\n",
      "in_edges, node: Bashkir, relation: language used, target: Bashkir\n",
      "in_edges, node: South Chungcheong, relation: contains the administrative territorial entity, target: South Chungcheong\n",
      "in_edges, node: Río Negro Province, relation: contains the administrative territorial entity, target: Río Negro Province\n",
      "in_edges, node: crematorium, relation: has facility, target: crematorium\n",
      "in_edges, node: Dhaka, relation: capital, target: Dhaka\n",
      "in_edges, node: Indigenous Australians, relation: ethnic group, target: Indigenous Australians\n",
      "in_edges, node: Volgograd Oblast, relation: contains the administrative territorial entity, target: Volgograd Oblast\n",
      "in_edges, node: Nizhny Novgorod Oblast, relation: contains the administrative territorial entity, target: Nizhny Novgorod Oblast\n",
      "in_edges, node: Sukhothai, relation: contains the administrative territorial entity, target: Sukhothai\n",
      "in_edges, node: Mpumalanga, relation: contains the administrative territorial entity, target: Mpumalanga\n",
      "in_edges, node: Phuket, relation: contains the administrative territorial entity, target: Phuket\n",
      "in_edges, node: Udmurt, relation: language used, target: Udmurt\n",
      "in_edges, node: Somaliland, relation: shares border with, target: Somaliland\n",
      "in_edges, node: green, relation: official symbol, target: green\n",
      "in_edges, node: Mérida, relation: contains the administrative territorial entity, target: Mérida\n",
      "in_edges, node: Lower Austria, relation: contains the administrative territorial entity, target: Lower Austria\n",
      "in_edges, node: Blekinge County, relation: contains the administrative territorial entity, target: Blekinge County\n",
      "in_edges, node: Reykjavík, relation: capital, target: Reykjavík\n",
      "in_edges, node: Adalbert of Prague, relation: patron saint, target: Adalbert of Prague\n",
      "in_edges, node: Bikini Atoll, relation: contains the administrative territorial entity, target: Bikini Atoll\n",
      "in_edges, node: County Mayo, relation: contains the administrative territorial entity, target: County Mayo\n",
      "in_edges, node: Zug, relation: contains the administrative territorial entity, target: Zug\n",
      "in_edges, node: County Limerick, relation: contains the administrative territorial entity, target: County Limerick\n",
      "in_edges, node: Basel-Landschaft, relation: contains the administrative territorial entity, target: Basel-Landschaft\n",
      "in_edges, node: Palm Sunday, relation: public holiday, target: Palm Sunday\n",
      "in_edges, node: Juba, relation: capital, target: Juba\n",
      "in_edges, node: Hradec Králové Region, relation: contains the administrative territorial entity, target: Hradec Králové Region\n",
      "in_edges, node: Alabama, relation: contains the administrative territorial entity, target: Alabama\n",
      "in_edges, node: Blida Province, relation: contains the administrative territorial entity, target: Blida Province\n",
      "in_edges, node: Low German, relation: language used, target: Low German\n",
      "in_edges, node: Fukushima Prefecture, relation: contains the administrative territorial entity, target: Fukushima Prefecture\n",
      "in_edges, node: Maine, relation: contains the administrative territorial entity, target: Maine\n",
      "in_edges, node: European Parliament, relation: designated as terrorist by, target: European Parliament\n",
      "in_edges, node: German Confederation, relation: part of, target: German Confederation\n",
      "in_edges, node: Ubon Ratchathani, relation: contains the administrative territorial entity, target: Ubon Ratchathani\n",
      "in_edges, node: Highland, relation: contains the administrative territorial entity, target: Highland\n",
      "in_edges, node: County Wicklow, relation: contains the administrative territorial entity, target: County Wicklow\n",
      "in_edges, node: Södermanland County, relation: contains the administrative territorial entity, target: Södermanland County\n",
      "in_edges, node: Cabinda Province, relation: contains the administrative territorial entity, target: Cabinda Province\n",
      "in_edges, node: Kōchi Prefecture, relation: contains the administrative territorial entity, target: Kōchi Prefecture\n",
      "in_edges, node: Elizabeth II, relation: head of state, target: Elizabeth II\n",
      "in_edges, node: Falco, relation: official symbol, target: Falco\n",
      "in_edges, node: North Carolina, relation: contains the administrative territorial entity, target: North Carolina\n",
      "in_edges, node: Tucumán Province, relation: contains the administrative territorial entity, target: Tucumán Province\n",
      "in_edges, node: Gansu, relation: contains the administrative territorial entity, target: Gansu\n",
      "in_edges, node: North Dakota, relation: contains the administrative territorial entity, target: North Dakota\n",
      "in_edges, node: Coral Sea, relation: located in or next to body of water, target: Coral Sea\n",
      "in_edges, node: French Southern and Antarctic Lands, relation: contains the administrative territorial entity, target: French Southern and Antarctic Lands\n",
      "in_edges, node: Calabria, relation: contains the administrative territorial entity, target: Calabria\n",
      "in_edges, node: Subcarpathian Voivodeship, relation: contains the administrative territorial entity, target: Subcarpathian Voivodeship\n",
      "in_edges, node: Flanders, relation: has part(s), target: Flanders\n",
      "in_edges, node: Marche, relation: contains the administrative territorial entity, target: Marche\n",
      "in_edges, node: South African Reserve Bank, relation: central bank, target: South African Reserve Bank\n",
      "in_edges, node: Gotland County, relation: contains the administrative territorial entity, target: Gotland County\n",
      "in_edges, node: Wahhabism, relation: religion or worldview, target: Wahhabism\n",
      "in_edges, node: Voronezh Oblast, relation: contains the administrative territorial entity, target: Voronezh Oblast\n",
      "in_edges, node: American Samoa, relation: contains the administrative territorial entity, target: American Samoa\n",
      "in_edges, node: Arunachal Pradesh, relation: contains the administrative territorial entity, target: Arunachal Pradesh\n",
      "in_edges, node: Quintana Roo, relation: contains the administrative territorial entity, target: Quintana Roo\n",
      "in_edges, node: Pico de Orizaba, relation: highest point, target: Pico de Orizaba\n",
      "in_edges, node: bitcoin, relation: currency, target: bitcoin\n",
      "in_edges, node: Valencian Community, relation: shares border with, target: Valencian Community\n",
      "in_edges, node: North Holland, relation: contains the administrative territorial entity, target: North Holland\n",
      "in_edges, node: New Guinea, relation: located in/on physical feature, target: New Guinea\n",
      "in_edges, node: Kanchenjunga, relation: highest point, target: Kanchenjunga\n",
      "in_edges, node: Great Australian Bight, relation: located in or next to body of water, target: Great Australian Bight\n",
      "in_edges, node: Nan, relation: contains the administrative territorial entity, target: Nan\n",
      "in_edges, node: Västra Götaland County, relation: contains the administrative territorial entity, target: Västra Götaland County\n",
      "in_edges, node: Algiers, relation: capital, target: Algiers\n",
      "in_edges, node: Ptah, relation: named after, target: Ptah\n",
      "in_edges, node: Rusyn, relation: language used, target: Rusyn\n",
      "in_edges, node: Lake Balaton, relation: located in or next to body of water, target: Lake Balaton\n",
      "in_edges, node: Apia, relation: capital, target: Apia\n",
      "in_edges, node: Central Bank of the Republic of Turkey, relation: central bank, target: Central Bank of the Republic of Turkey\n",
      "in_edges, node: Vestfold, relation: contains the administrative territorial entity, target: Vestfold\n",
      "in_edges, node: Bengal, relation: different from, target: Bengal\n",
      "Saving to ../data/questions/triplets/places_country.csv. Triplets size: 3416\n"
     ]
    }
   ],
   "source": [
    "remove_relation = [\"topic's main category\", \"topic's main template\", \"described by source\", \"Commons category\", \"on focus list of Wikimedia project\"]\n",
    "\n",
    "def generate_triplets(topic):\n",
    "    with open(f'../data/graph/{topic}.gpickle', 'rb') as f:\n",
    "        directed_graph = pickle.load(f)\n",
    "\n",
    "    remove_pairs = set()\n",
    "    triplets, all_pairs = [], []\n",
    "    for node in directed_graph.nodes():\n",
    "        for _, target, edge_data in directed_graph.out_edges(node, data=True):\n",
    "            relation = edge_data['label']\n",
    "            if (node, relation) in all_pairs:\n",
    "                remove_pairs.add((node, relation))\n",
    "            else:\n",
    "                all_pairs.append((node, relation))\n",
    "                print(f\"out_edges, node: {node}, relation: {relation}, target: {target}\")\n",
    "\n",
    "        for _, target_in, edge_data in directed_graph.in_edges(node, data=True):\n",
    "            relation = edge_data['label']\n",
    "            if (node, relation) in all_pairs:\n",
    "                remove_pairs.add((node, relation))\n",
    "            else:\n",
    "                all_pairs.append((node, relation))\n",
    "                print(f\"in_edges, node: {node}, relation: {relation}, target: {target_in}\")\n",
    "        \n",
    "\n",
    "    for node in directed_graph.nodes():\n",
    "        for _, target, edge_data in directed_graph.out_edges(node, data=True):\n",
    "            relation = edge_data['label']\n",
    "            if (node, relation) not in remove_pairs:\n",
    "                triplets.append({\"subject\": node, \"relation\": relation, \"object\": target})\n",
    "\n",
    "        for _, target, edge_data in directed_graph.in_edges(node, data=True):\n",
    "            relation = edge_data['label']\n",
    "            if (node, relation) not in remove_pairs:\n",
    "                triplets.append({\"subject\": node, \"relation\": relation, \"object\": target})\n",
    "\n",
    "    df_triplets = pd.DataFrame(triplets)\n",
    "    # df_triplets.to_csv(f\"../data/questions/triplets/tmp_{topic}.csv\", index=False, encoding='utf-8')\n",
    "    print(f\"Saving to ../data/questions/triplets/{topic}.csv. Triplets size: {len(df_triplets)}\")\n",
    "    # return df_triplets\n",
    "\n",
    "\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith('.gpickle'):\n",
    "#         df_triplets = generate_triplets(filename.split('.')[0])\n",
    "# generate_triplets('entertainment_anime')\n",
    "generate_triplets('places_country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958, 684, 214, 154, 0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = set(remove_pairs_in).intersection(set(remove_pairs_out))\n",
    "len(remove_pairs_in), len(remove_pairs_out), len(set(remove_pairs_in)), len(set(remove_pairs_out)), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_gen_question = \"\"\"Given a subject and a relation, generate a question that asks about the subject. \\\n",
    "Output the result in JSON format as demonstrated in the following example.\n",
    "Example input: subject: Interstellar, relation: director\n",
    "Example output: {\"question\": \"Who is the director of Interstellar?\"}\"\"\"\n",
    "client = AzureOpenAI(api_key=load_api_key('api_key_n_central_us'), api_version='2023-05-15', azure_endpoint=\"https://n-central-us.openai.azure.com/\")\n",
    "# client = AzureOpenAI(api_key=load_api_key('api_key_east_us'), api_version='2023-05-15', azure_endpoint=\"https://east-us-one.openai.azure.com/\")\n",
    "questions = []\n",
    "\n",
    "def generate_questions(df):\n",
    "    for i in df.index[:]:\n",
    "        subject, relation = df.loc[i, 'subject'], df.loc[i, 'relation']\n",
    "        prompt_gen_question = f\"subject: {subject}, relation: {relation}\"\n",
    "        raw_response = get_gpt_response(client, system_msg_gen_question, prompt_gen_question)\n",
    "        json_obj = json.loads(raw_response.choices[0].message.content)\n",
    "        questions.append(json_obj['paraphrased_question'])\n",
    "        print(json_obj)\n",
    "\n",
    "generate_questions(df_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from data_prep import *\n",
    "from urllib.error import HTTPError\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: llama-3.1 will end answers with period '.'. mistral-v0.3 often end with new line '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c694c14cbc4f8e85bd80ba3baaf88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa63d73d3b7f4c7a97ac5e0ee0c44251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_qa = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "model_qa = transformers.AutoModelForCausalLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'question': 'Which city is twinned with Novokuznetsk?', 'options': {'A': 'Haifa', 'B': 'Kemerovo', 'C': 'Moscow', 'D': 'Saint Petersburg'}, 'ground_truth': 'A'}\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"../data/questions/wh_only/hallucination_only/meta_llama_3.1_8b_instruct_places_city.csv\")\n",
    "n = 2  #len(df)\n",
    "targets = df['label'].tolist()[:n]\n",
    "subjects = df['subject'].tolist()[:n]\n",
    "questions = df['question'].tolist()[:n]\n",
    "ls = df['multiple_choice_question'].tolist()\n",
    "# ls = [json.loads(i.replace(\"'\", '\"')) for i in ls]\n",
    "json.loads(\"\"\"{\"question\": \"Which city\"s official symbol is Ceiba speciosa?\"}\"\"\")\n",
    "\n",
    "# # load string as json, fix JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
    "# ls_mc = [json.loads(i.replace(\"'\", '\"')) for i in df['multiple_choice_question'].tolist()]\n",
    "# ls_mc_q = [i['question'] for i in ls_mc]\n",
    "# ls_mc_a = [i['ground_truth']+'. '+target for i, target in zip(ls_mc, targets)]\n",
    "ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'question': 'Which city is twinned with Novokuznetsk?', 'options': 'A': 'Haifa', 'B': 'Kemerovo', 'C': 'Moscow', 'D': 'Saint Petersburg' \n",
      " 'A'\n"
     ]
    }
   ],
   "source": [
    "e = ls[0]\n",
    "# split the e into 2 parts: question + opeions and ground_truth\n",
    "idx = e.find(\", 'ground_truth': \")\n",
    "question = e[:idx].replace(\"{\", '').replace(\"}\", '')\n",
    "ground_truth = e[idx+len(\", 'ground_truth': \"):].replace(\"}\", '', )\n",
    "print(question, '\\n', ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"instance of\": \"city\"}\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/topic/topic_places_city.json', 'r', encoding='utf-8') as topics_file:\n",
    "    topics = topics_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'relation': {'type': 'uri',\n",
       "   'value': 'http://www.wikidata.org/prop/direct/P31'},\n",
       "  'subjectLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'Phoroctenia vittata'},\n",
       "  'objectLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'insect'}},\n",
       " {'relation': {'type': 'uri',\n",
       "   'value': 'http://www.wikidata.org/prop/direct/P105'},\n",
       "  'subjectLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'Phoroctenia vittata'},\n",
       "  'objectLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'species'}},\n",
       " {'relation': {'type': 'uri',\n",
       "   'value': 'http://www.wikidata.org/prop/direct/P31'},\n",
       "  'subjectLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'Phoroctenia vittata'},\n",
       "  'objectLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'taxon'}},\n",
       " {'relation': {'type': 'uri',\n",
       "   'value': 'http://www.wikidata.org/prop/direct/P171'},\n",
       "  'subjectLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'Phoroctenia vittata'},\n",
       "  'objectLabel': {'xml:lang': 'en',\n",
       "   'type': 'literal',\n",
       "   'value': 'Phoroctenia'}}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']['bindings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than 500 \n",
    "food_and_drink_topics = [\"video game genre\", \"dish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_names = [\"disease\", \"chemical compound\", \"painting\", \"astronomical object\", \"scientific article\", \"gene\", \"mountain\", \"album\", \"scientific journal\"] # not tried\n",
    "\n",
    "topic_names = []\n",
    "for topic in topic_names:\n",
    "    topic_id = identifier_conversion(topic)\n",
    "    print(f\"\\nTopic: {topic}\")\n",
    "    check_topic(topic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: television series\n",
      "Topic Q5398426 has 50000 relations.\n",
      "\n",
      "Topic: music genre\n",
      "Topic Q188451 has 2988 relations.\n",
      "\n",
      "Topic: video game\n",
      "Error querying for Q7889: EndPointInternalError: The endpoint returned the HTTP status code 500. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?subjectLabel ?relation ?objectLabel WHERE {\\n      ?subject wdt:P31 wd:Q7889.\\n      ?subject ?relation ?object.\\n      ?subject wikibase:identifiers ?subject_identifierCount.\\n      ?object wikibase:identifiers ?object_identifierCount.\\n      FILTER (?subject_identifierCount >= 8 && ?object_identifierCount >= 5) .  \\n      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\\n    }\\n    LIMIT 50000\\n    \\njava.util.concurrent.TimeoutException\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:123)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n'\n",
      "\n",
      "Topic: airline\n",
      "Topic Q46970 has 2847 relations.\n",
      "\n",
      "Topic: train\n",
      "Topic Q870 has 0 relations.\n",
      "\n",
      "Topic: song\n",
      "Topic Q7366 has 5153 relations.\n"
     ]
    }
   ],
   "source": [
    "topic_ls = [\"television series\", \"music genre\", \"anime\", \"music festival\", \"airline\", \"song\", \"scientific theory\", \"chemical element\", \"software\", \"ocean\", \"forest\", \n",
    "            \"disease\", \"university\", \"food\", \"desert\", \"plateau\", \"archipelago\", \"glacier\", \"canyon\"]\n",
    "for topic in topic_ls:\n",
    "    topic_id = identifier_conversion(topic)\n",
    "    print(f\"\\nTopic: {topic}\")\n",
    "    check_topic(topic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('On the Chersonese', 'oration', {'label': 'instance of'}), ('Against Timocrates', 'oration', {'label': 'instance of'}), ('On the Crown', 'oration', {'label': 'instance of'}), ('First Philippic', 'oration', {'label': 'instance of'}), ('Against Meidias', 'oration', {'label': 'instance of'}), ('On the False Embassy', 'oration', {'label': 'instance of'}), ('Against Aristocrates', 'oration', {'label': 'instance of'}), ('To Nicocles', 'oration', {'label': 'instance of'}), ('On the Murder of Eratosthenes', 'oration', {'label': 'instance of'}), ('Antidosis', 'oration', {'label': 'instance of'}), ('On the Peace', 'oration', {'label': 'instance of'}), ('Against Ctesiphon', 'oration', {'label': 'instance of'}), ('Panegyricus', 'oration', {'label': 'instance of'}), ('Catiline Orations', 'oration', {'label': 'genre'}), ('Against Leptines', 'oration', {'label': 'instance of'}), ('On the Embassy', 'oration', {'label': 'instance of'})]\n",
      "[('The Restaurant at the End of the Universe', 'Life, the Universe and Everything', {'label': 'followed by'}), ('So Long, and Thanks for All the Fish', 'Life, the Universe and Everything', {'label': 'follows'})]\n",
      "[('Life, the Universe and Everything', 'fantasy', {'label': 'genre'}), ('The Vampire Prince', 'fantasy', {'label': 'genre'}), ('Life of Pi', 'fantasy', {'label': 'genre'}), ('Spook Country', 'fantasy', {'label': 'genre'}), ('White Night', 'fantasy', {'label': 'genre'}), ('The Last Olympian', 'fantasy', {'label': 'genre'}), ('The Name of the Wind', 'fantasy', {'label': 'genre'}), ('Dragons of Autumn Twilight', 'fantasy', {'label': 'genre'}), ('The High Lord', 'fantasy', {'label': 'genre'}), ('A Clash of Kings', 'fantasy', {'label': 'genre'}), ('Harry Potter and the Prisoner of Azkaban', 'fantasy', {'label': 'genre'}), ('The Princess Bride', 'fantasy', {'label': 'genre'}), ('Prince Caspian', 'fantasy', {'label': 'genre'}), ('The Wonderful Wizard of Oz', 'fantasy', {'label': 'genre'}), ('The Witches', 'fantasy', {'label': 'genre'}), ('Artemis Fowl and the Arctic Incident', 'fantasy', {'label': 'genre'}), ('Artemis Fowl and the Eternity Code', 'fantasy', {'label': 'genre'}), ('The Lost Hero', 'fantasy', {'label': 'genre'}), ('The Sword of Shannara', 'fantasy', {'label': 'genre'}), ('The Screwtape Letters', 'fantasy', {'label': 'genre'}), ('The Witches of Eastwick', 'fantasy', {'label': 'genre'}), ('The Return of the King', 'fantasy', {'label': 'genre'}), (\"Gulliver's Travels\", 'fantasy', {'label': 'genre'}), ('Orlando: A Biography', 'fantasy', {'label': 'genre'}), ('The Silmarillion', 'fantasy', {'label': 'genre'}), ('The Hobbit', 'fantasy', {'label': 'genre'}), ('Witches Abroad', 'fantasy', {'label': 'genre'}), ('Neverwhere', 'fantasy', {'label': 'genre'}), ('Carmilla', 'fantasy', {'label': 'genre'}), ('The Novice', 'fantasy', {'label': 'genre'}), ('Wrath of a Mad God', 'fantasy', {'label': 'genre'}), ('The Red Pyramid', 'fantasy', {'label': 'genre'}), ('The Stand', 'fantasy', {'label': 'genre'}), ('Harry Potter and the Deathly Hallows', 'fantasy', {'label': 'genre'}), (\"The Serpent's Shadow\", 'fantasy', {'label': 'genre'}), ('The Martian Chronicles', 'fantasy', {'label': 'genre'}), ('Mort', 'fantasy', {'label': 'genre'}), ('The Ocean at the End of the Lane', 'fantasy', {'label': 'genre'}), ('A True Story', 'fantasy', {'label': 'genre'}), ('Hard-Boiled Wonderland and the End of the World', 'fantasy', {'label': 'genre'}), ('The Mists of Avalon', 'fantasy', {'label': 'genre'}), ('The Two Towers', 'fantasy', {'label': 'genre'}), (\"Harry Potter and the Philosopher's Stone\", 'fantasy', {'label': 'genre'}), ('Twenty Thousand Leagues Under the Sea', 'fantasy', {'label': 'genre'}), ('The Children of Húrin', 'fantasy', {'label': 'genre'}), ('The Alchemist', 'fantasy', {'label': 'genre'}), ('The Last Battle', 'fantasy', {'label': 'genre'}), ('Unfinished Tales', 'fantasy', {'label': 'genre'}), ('Through the Looking-Glass', 'fantasy', {'label': 'genre'}), ('Frankenstein; or, The Modern Prometheus', 'fantasy', {'label': 'genre'}), ('The Scions of Shannara', 'fantasy', {'label': 'genre'}), ('The Phantom of the Opera', 'fantasy', {'label': 'genre'}), ('Inkspell', 'fantasy', {'label': 'genre'}), ('Crown Duel', 'fantasy', {'label': 'genre'}), ('Stormrider', 'fantasy', {'label': 'genre'}), (\"The Titan's Curse\", 'fantasy', {'label': 'genre'}), ('Artemis Fowl', 'fantasy', {'label': 'genre'}), ('The Farthest Shore', 'fantasy', {'label': 'genre'}), ('She: A History of Adventure', 'fantasy', {'label': 'genre'}), ('The Marble Faun', 'fantasy', {'label': 'genre'}), ('A Wrinkle in Time', 'fantasy', {'label': 'genre'}), (\"The Magician's Nephew\", 'fantasy', {'label': 'genre'}), ('Abhorsen', 'fantasy', {'label': 'genre'}), ('Beyond the Deepwoods', 'fantasy', {'label': 'genre'}), ('The Lion, the Witch, and the Wardrobe', 'fantasy', {'label': 'genre'}), (\"The Devil's Elixirs\", 'fantasy', {'label': 'genre'}), ('The Slow Regard of Silent Things', 'fantasy', {'label': 'genre'}), ('The Dark Tower III: The Waste Lands', 'fantasy', {'label': 'genre'}), ('The Light Fantastic', 'fantasy', {'label': 'genre'}), ('The Carpet People', 'fantasy', {'label': 'genre'}), ('The Phantom Tollbooth', 'fantasy', {'label': 'genre'}), ('The Mysteries of Udolpho', 'fantasy', {'label': 'genre'}), ('Artemis Fowl and the Lost Colony', 'fantasy', {'label': 'genre'}), ('Pippi Longstocking', 'fantasy', {'label': 'genre'}), ('The Tales of Beedle the Bard', 'fantasy', {'label': 'genre'}), ('Dragons of Winter Night', 'fantasy', {'label': 'genre'}), ('The Short Second Life of Bree Tanner', 'fantasy', {'label': 'genre'}), ('Blue Moon', 'fantasy', {'label': 'genre'}), ('Harry Potter and the Half-Blood Prince', 'fantasy', {'label': 'genre'}), ('Tehanu', 'fantasy', {'label': 'genre'}), ('City of Bones', 'fantasy', {'label': 'genre'}), ('The Subtle Knife', 'fantasy', {'label': 'genre'}), ('The Sea of Monsters', 'fantasy', {'label': 'genre'}), ('A Wizard of Earthsea', 'fantasy', {'label': 'genre'}), ('Harry Potter and the Chamber of Secrets', 'fantasy', {'label': 'genre'}), ('The Battle of the Labyrinth', 'fantasy', {'label': 'genre'}), ('Harry Potter and the Order of the Phoenix', 'fantasy', {'label': 'genre'}), ('The Devil in Love', 'fantasy', {'label': 'genre'}), ('Bridge to Terabithia', 'fantasy', {'label': 'genre'}), ('Artemis Fowl and the Opal Deception', 'fantasy', {'label': 'genre'}), ('Jarka Ruus', 'fantasy', {'label': 'genre'}), ('Good Omens', 'fantasy', {'label': 'genre'}), ('Roverandom', 'fantasy', {'label': 'genre'}), ('The Eyes of the Dragon', 'fantasy', {'label': 'genre'}), ('Last Watch', 'fantasy', {'label': 'genre'}), (\"The Golem's Eye\", 'fantasy', {'label': 'genre'}), ('The Neverending Story', 'fantasy', {'label': 'genre'}), ('The Turn of the Screw', 'fantasy', {'label': 'genre'}), ('Harry Potter and the Goblet of Fire', 'fantasy', {'label': 'genre'}), ('Something Wicked This Way Comes', 'fantasy', {'label': 'genre'}), ('Reaper Man', 'fantasy', {'label': 'genre'}), ('The Tangle Box', 'fantasy', {'label': 'genre'}), (\"Assassin's Apprentice\", 'fantasy', {'label': 'genre'}), ('Grave Peril', 'fantasy', {'label': 'genre'}), ('New Atlantis', 'fantasy', {'label': 'genre'}), ('The Once and Future King', 'fantasy', {'label': 'genre'}), ('The Library of Babel', 'fantasy', {'label': 'genre'}), (\"Cugel's Saga\", 'fantasy', {'label': 'genre'}), ('Men at Arms', 'fantasy', {'label': 'genre'}), ('The Halloween Tree', 'fantasy', {'label': 'genre'}), ('The House of Hades', 'fantasy', {'label': 'genre'}), (\"Kiki's Delivery Service\", 'fantasy', {'label': 'genre'}), ('The Amber Spyglass', 'fantasy', {'label': 'genre'}), (\"Howl's Moving Castle\", 'fantasy', {'label': 'genre'}), ('Journey to the West', 'fantasy', {'label': 'genre'}), ('The Graveyard Book', 'fantasy', {'label': 'genre'}), ('The Mysterious Island', 'fantasy', {'label': 'genre'}), ('American Gods', 'fantasy', {'label': 'genre'}), ('The Fellowship of the Ring', 'fantasy', {'label': 'genre'}), (\"Lyra's Oxford\", 'fantasy', {'label': 'genre'}), ('A Game of Thrones', 'fantasy', {'label': 'genre'}), (\"A Connecticut Yankee in King Arthur's Court\", 'fantasy', {'label': 'genre'}), ('The Lightning Thief', 'fantasy', {'label': 'genre'}), ('The Lovely Bones', 'fantasy', {'label': 'genre'}), ('The Last Hero', 'fantasy', {'label': 'genre'}), ('Eclipse', 'fantasy', {'label': 'genre'}), ('Anansi Boys', 'fantasy', {'label': 'genre'}), ('Charlie and the Chocolate Factory', 'fantasy', {'label': 'genre'}), (\"Ronia the Robber's Daughter\", 'fantasy', {'label': 'genre'}), ('Ghost Story', 'fantasy', {'label': 'genre'}), ('The Dark Tower VII: The Dark Tower', 'fantasy', {'label': 'genre'}), ('Where the Wild Things Are', 'fantasy', {'label': 'genre'}), ('Watership Down', 'fantasy', {'label': 'genre'}), ('A Storm of Swords', 'fantasy', {'label': 'genre'})]\n",
      "[('Northangep Abbek', 'Persuasion', {'label': 'followed by'}), ('Persuasion', 'Persuasion', {'label': 'derivative work'})]\n",
      "[('Persuasion', 'United Kingdom', {'label': 'country of origin'}), (\"The Magician's Nephew\", 'United Kingdom', {'label': 'place of publication'}), ('Harry Potter and the Half-Blood Prince', 'United Kingdom', {'label': 'country of origin'}), ('Northern Lights', 'United Kingdom', {'label': 'country of origin'}), ('The Jungle Book', 'United Kingdom', {'label': 'place of publication'}), ('The Light Fantastic', 'United Kingdom', {'label': 'country of origin'}), ('The Picture of Dorian Gray', 'United Kingdom', {'label': 'narrative location'}), ('Till We Have Faces', 'United Kingdom', {'label': 'country of origin'}), ('Emma', 'United Kingdom', {'label': 'country of origin'}), (\"Lyra's Oxford\", 'United Kingdom', {'label': 'country of origin'}), ('Fifty Shades of Grey', 'United Kingdom', {'label': 'place of publication'}), ('She: A History of Adventure', 'United Kingdom', {'label': 'country of origin'}), ('Oliver Twist', 'United Kingdom', {'label': 'country of origin'}), ('Great Expectations', 'United Kingdom', {'label': 'country of origin'}), ('Lord of the Flies', 'United Kingdom', {'label': 'country of origin'}), ('The Wide Window', 'United Kingdom', {'label': 'country of origin'}), ('The Island of Dr Moreau', 'United Kingdom', {'label': 'country of origin'}), ('I, Claudius', 'United Kingdom', {'label': 'place of publication'}), ('The Hobbit', 'United Kingdom', {'label': 'country of origin'}), ('The Carpet People', 'United Kingdom', {'label': 'place of publication'}), ('Prince Caspian', 'United Kingdom', {'label': 'country of origin'}), ('The Return of the King', 'United Kingdom', {'label': 'country of origin'}), ('Treasure Island', 'United Kingdom', {'label': 'country of origin'}), ('Use of Weapons', 'United Kingdom', {'label': 'country of origin'}), ('Infernal Devices', 'United Kingdom', {'label': 'country of origin'}), ('The Scarlet Pimpernel', 'United Kingdom', {'label': 'country of origin'}), (\"Montezuma's Daughter\", 'United Kingdom', {'label': 'country of origin'}), ('Class Warfare', 'United Kingdom', {'label': 'country of origin'}), ('Things as They Are; or, The Adventures of Caleb Williams', 'United Kingdom', {'label': 'country of origin'}), ('Never Let Me Go', 'United Kingdom', {'label': 'country of origin'}), ('Absolute Friends', 'United Kingdom', {'label': 'country of origin'}), ('A Study in Scarlet', 'United Kingdom', {'label': 'place of publication'}), ('The Spy Who Came in from the Cold', 'United Kingdom', {'label': 'country of origin'}), ('Monsignor Quixote', 'United Kingdom', {'label': 'country of origin'}), ('Strange Case of Dr Jekyll and Mr Hyde', 'United Kingdom', {'label': 'country of origin'}), ('The Wind in the Willows', 'United Kingdom', {'label': 'country of origin'}), (\"Childhood's End\", 'United Kingdom', {'label': 'country of origin'}), ('Witches Abroad', 'United Kingdom', {'label': 'country of origin'}), (\"Howl's Moving Castle\", 'United Kingdom', {'label': 'country of origin'}), ('Brave New World', 'United Kingdom', {'label': 'country of origin'}), ('The Book of Dave', 'United Kingdom', {'label': 'country of origin'}), ('The Invisible Man', 'United Kingdom', {'label': 'country of origin'}), ('Rendezvous with Rama', 'United Kingdom', {'label': 'country of origin'}), (\"Harry Potter and the Philosopher's Stone\", 'United Kingdom', {'label': 'narrative location'}), ('Anthills of the Savannah', 'United Kingdom', {'label': 'place of publication'}), ('2061: Odyssey Three', 'United Kingdom', {'label': 'country of origin'}), ('The Songs of Distant Earth', 'United Kingdom', {'label': 'country of origin'}), ('Mansfield Park', 'United Kingdom', {'label': 'country of origin'}), ('The Fellowship of the Ring', 'United Kingdom', {'label': 'country of origin'}), ('Through the Looking-Glass', 'United Kingdom', {'label': 'country of origin'}), ('Reaper Man', 'United Kingdom', {'label': 'country of origin'}), ('The Amber Spyglass', 'United Kingdom', {'label': 'country of origin'}), ('Harry Potter and the Order of the Phoenix', 'United Kingdom', {'label': 'country of origin'}), ('Nineteen Eighty-Four', 'United Kingdom', {'label': 'country of origin'}), ('Good Omens', 'United Kingdom', {'label': 'country of origin'}), ('Tinker Tailor Soldier Spy', 'United Kingdom', {'label': 'country of origin'}), ('Harry Potter and the Goblet of Fire', 'United Kingdom', {'label': 'country of origin'}), ('Roverandom', 'United Kingdom', {'label': 'country of origin'}), ('Stormrider', 'United Kingdom', {'label': 'country of origin'}), ('Guinness World Records', 'United Kingdom', {'label': 'country of origin'}), ('Mostly Harmless', 'United Kingdom', {'label': 'country of origin'}), ('The Sign of Four', 'United Kingdom', {'label': 'country of origin'}), ('The Last Battle', 'United Kingdom', {'label': 'place of publication'}), ('The Memoirs of a Survivor', 'United Kingdom', {'label': 'country of origin'}), ('Ivanhoe', 'United Kingdom', {'label': 'country of origin'}), ('The Lion, the Witch, and the Wardrobe', 'United Kingdom', {'label': 'country of origin'}), ('The City and the Stars', 'United Kingdom', {'label': 'country of origin'}), ('Coraline', 'United Kingdom', {'label': 'country of origin'}), ('The Player of Games', 'United Kingdom', {'label': 'country of origin'}), ('The Last Days of Pompeii', 'United Kingdom', {'label': 'country of origin'}), ('Matilda', 'United Kingdom', {'label': 'country of origin'}), ('The Power and the Glory', 'United Kingdom', {'label': 'country of origin'}), ('Rama II', 'United Kingdom', {'label': 'country of origin'}), (\"The Golem's Eye\", 'United Kingdom', {'label': 'country of origin'}), ('The Silmarillion', 'United Kingdom', {'label': 'country of origin'}), ('Fanny Hill', 'United Kingdom', {'label': 'country of origin'}), ('A Clockwork Orange', 'United Kingdom', {'label': 'country of origin'}), ('Island', 'United Kingdom', {'label': 'country of origin'}), ('Moon Over Soho', 'United Kingdom', {'label': 'country of origin'}), ('The Mill on the Floss', 'United Kingdom', {'label': 'country of origin'}), ('Sense and Sensibility', 'United Kingdom', {'label': 'country of origin'}), (\"King Solomon's Mines\", 'United Kingdom', {'label': 'place of publication'}), ('Three Men in a Boat', 'United Kingdom', {'label': 'country of origin'}), ('The Last Hero', 'United Kingdom', {'label': 'country of origin'}), ('Orlando: A Biography', 'United Kingdom', {'label': 'country of origin'}), ('Carmilla', 'United Kingdom', {'label': 'country of origin'}), ('Beyond the Deepwoods', 'United Kingdom', {'label': 'country of origin'}), ('Harry Potter and the Chamber of Secrets', 'United Kingdom', {'label': 'country of origin'}), ('Political Justice', 'United Kingdom', {'label': 'country of origin'}), (\"Time's Arrow\", 'United Kingdom', {'label': 'country of origin'}), ('Men at Arms', 'United Kingdom', {'label': 'country of origin'}), ('Altered Carbon', 'United Kingdom', {'label': 'place of publication'}), ('Kidnapped', 'United Kingdom', {'label': 'country of origin'}), ('The Screwtape Letters', 'United Kingdom', {'label': 'country of origin'}), ('The Graveyard Book', 'United Kingdom', {'label': 'country of origin'}), ('Anansi Boys', 'United Kingdom', {'label': 'country of origin'}), ('Architectural Stained Glass', 'United Kingdom', {'label': 'country of origin'}), ('The Dark Side of the Sun', 'United Kingdom', {'label': 'country of origin'}), ('Vanity Fair', 'United Kingdom', {'label': 'country of origin'}), ('So Long, and Thanks for All the Fish', 'United Kingdom', {'label': 'country of origin'}), ('Charlie and the Chocolate Factory', 'United Kingdom', {'label': 'country of origin'}), ('The Time Machine: An Invention', 'United Kingdom', {'label': 'country of origin'}), ('Neverwhere', 'United Kingdom', {'label': 'country of origin'}), ('The Hound of the Baskervilles', 'United Kingdom', {'label': 'country of origin'}), ('Good-Bye to All That', 'United Kingdom', {'label': 'country of origin'}), ('The Mysterious Affair at Styles', 'United Kingdom', {'label': 'country of origin'}), (\"Gulliver's Travels\", 'United Kingdom', {'label': 'country of origin'}), ('Harry Potter and the Deathly Hallows', 'United Kingdom', {'label': 'country of origin'}), ('The War of the Worlds', 'United Kingdom', {'label': 'country of origin'}), ('Black Beauty', 'United Kingdom', {'label': 'country of origin'}), ('The Witches', 'United Kingdom', {'label': 'country of origin'}), ('Mrs Dalloway', 'United Kingdom', {'label': 'country of origin'}), ('Pride and Prejudice', 'United Kingdom', {'label': 'country of origin'}), ('And Then There Were None', 'United Kingdom', {'label': 'place of publication'}), ('Out of Africa', 'United Kingdom', {'label': 'country of origin'}), ('The Subtle Knife', 'United Kingdom', {'label': 'country of origin'}), ('The Turn of the Screw', 'United Kingdom', {'label': 'country of origin'}), ('Fatherland', 'United Kingdom', {'label': 'country of origin'}), ('The Secret Garden', 'United Kingdom', {'label': 'country of origin'}), ('The Two Towers', 'United Kingdom', {'label': 'country of origin'}), ('Darkness at Noon', 'United Kingdom', {'label': 'country of origin'}), ('My Philosophical Development', 'United Kingdom', {'label': 'country of origin'}), (\"Alice's Adventures in Wonderland\", 'United Kingdom', {'label': 'country of origin'}), ('Mort', 'United Kingdom', {'label': 'country of origin'}), ('The Vampire Prince', 'United Kingdom', {'label': 'country of origin'}), ('Life, the Universe and Everything', 'United Kingdom', {'label': 'country of origin'}), ('The Tales of Beedle the Bard', 'United Kingdom', {'label': 'country of origin'}), ('The Once and Future King', 'United Kingdom', {'label': 'country of origin'}), ('The Worst Journey in the World', 'United Kingdom', {'label': 'country of origin'}), ('Northangep Abbek', 'United Kingdom', {'label': 'country of origin'}), ('Harry Potter and the Prisoner of Azkaban', 'United Kingdom', {'label': 'country of origin'}), ('Unfinished Tales', 'United Kingdom', {'label': 'country of origin'}), ('The Mysteries of Udolpho', 'United Kingdom', {'label': 'country of origin'}), ('The Day of the Triffids', 'United Kingdom', {'label': 'place of publication'}), ('A Dream of John Ball', 'United Kingdom', {'label': 'country of origin'}), (\"Midnight's Children\", 'United Kingdom', {'label': 'country of origin'}), ('American Gods', 'United Kingdom', {'label': 'country of origin'}), ('The Children of Húrin', 'United Kingdom', {'label': 'country of origin'}), ('The Restaurant at the End of the Universe', 'United Kingdom', {'label': 'country of origin'}), ('Point Counter Point', 'United Kingdom', {'label': 'country of origin'}), ('A Tale of Two Cities', 'United Kingdom', {'label': 'country of origin'}), ('The Expedition of Humphry Clinker', 'United Kingdom', {'label': 'country of origin'}), (\"All Tomorrow's Parties\", 'United Kingdom', {'label': 'country of origin'}), ('New Atlantis', 'United Kingdom', {'label': 'country of origin'}), ('The Casual Vacancy', 'United Kingdom', {'label': 'country of origin'}), (\"Dirk Gently's Holistic Detective Agency\", 'United Kingdom', {'label': 'country of origin'}), ('Star Maker', 'United Kingdom', {'label': 'country of origin'}), (\"The Razor's Edge\", 'United Kingdom', {'label': 'country of origin'}), ('Robinson Crusoe', 'United Kingdom', {'label': 'country of origin'})]\n",
      "[]\n",
      "[('De Alexandri Magni fortuna aut virtute', 'Plutarch', {'label': 'author'}), ('De liberis educandis', 'Plutarch', {'label': 'author'}), ('De facie in orbe Lunae', 'Plutarch', {'label': 'author'}), ('Isis and Osiris', 'Plutarch', {'label': 'author'}), ('Quaestiones convivales', 'Plutarch', {'label': 'author'}), ('Quomodo adolescens poetas audire debeat', 'Plutarch', {'label': 'author'}), ('De mulierum virtutibus', 'Plutarch', {'label': 'author'}), ('Parallel Lives', 'Plutarch', {'label': 'author'}), ('Moralia', 'Plutarch', {'label': 'author'})]\n",
      "[]\n",
      "[('Perry Rhodan', 'Walter Ernsting', {'label': 'author'})]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instance of': [('On the Chersonese', 'oration', {'label': 'instance of'}),\n",
       "  ('Against Timocrates', 'oration', {'label': 'instance of'}),\n",
       "  ('On the Crown', 'oration', {'label': 'instance of'}),\n",
       "  ('First Philippic', 'oration', {'label': 'instance of'}),\n",
       "  ('Against Meidias', 'oration', {'label': 'instance of'}),\n",
       "  ('On the False Embassy', 'oration', {'label': 'instance of'}),\n",
       "  ('Against Aristocrates', 'oration', {'label': 'instance of'}),\n",
       "  ('To Nicocles', 'oration', {'label': 'instance of'}),\n",
       "  ('On the Murder of Eratosthenes', 'oration', {'label': 'instance of'}),\n",
       "  ('Antidosis', 'oration', {'label': 'instance of'}),\n",
       "  ('On the Peace', 'oration', {'label': 'instance of'}),\n",
       "  ('Against Ctesiphon', 'oration', {'label': 'instance of'}),\n",
       "  ('Panegyricus', 'oration', {'label': 'instance of'}),\n",
       "  ('Against Leptines', 'oration', {'label': 'instance of'}),\n",
       "  ('On the Embassy', 'oration', {'label': 'instance of'})],\n",
       " 'genre': [('Catiline Orations', 'oration', {'label': 'genre'}),\n",
       "  ('Life, the Universe and Everything', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Vampire Prince', 'fantasy', {'label': 'genre'}),\n",
       "  ('Life of Pi', 'fantasy', {'label': 'genre'}),\n",
       "  ('Spook Country', 'fantasy', {'label': 'genre'}),\n",
       "  ('White Night', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Last Olympian', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Name of the Wind', 'fantasy', {'label': 'genre'}),\n",
       "  ('Dragons of Autumn Twilight', 'fantasy', {'label': 'genre'}),\n",
       "  ('The High Lord', 'fantasy', {'label': 'genre'}),\n",
       "  ('A Clash of Kings', 'fantasy', {'label': 'genre'}),\n",
       "  ('Harry Potter and the Prisoner of Azkaban', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Princess Bride', 'fantasy', {'label': 'genre'}),\n",
       "  ('Prince Caspian', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Wonderful Wizard of Oz', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Witches', 'fantasy', {'label': 'genre'}),\n",
       "  ('Artemis Fowl and the Arctic Incident', 'fantasy', {'label': 'genre'}),\n",
       "  ('Artemis Fowl and the Eternity Code', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Lost Hero', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Sword of Shannara', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Screwtape Letters', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Witches of Eastwick', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Return of the King', 'fantasy', {'label': 'genre'}),\n",
       "  (\"Gulliver's Travels\", 'fantasy', {'label': 'genre'}),\n",
       "  ('Orlando: A Biography', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Silmarillion', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Hobbit', 'fantasy', {'label': 'genre'}),\n",
       "  ('Witches Abroad', 'fantasy', {'label': 'genre'}),\n",
       "  ('Neverwhere', 'fantasy', {'label': 'genre'}),\n",
       "  ('Carmilla', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Novice', 'fantasy', {'label': 'genre'}),\n",
       "  ('Wrath of a Mad God', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Red Pyramid', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Stand', 'fantasy', {'label': 'genre'}),\n",
       "  ('Harry Potter and the Deathly Hallows', 'fantasy', {'label': 'genre'}),\n",
       "  (\"The Serpent's Shadow\", 'fantasy', {'label': 'genre'}),\n",
       "  ('The Martian Chronicles', 'fantasy', {'label': 'genre'}),\n",
       "  ('Mort', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Ocean at the End of the Lane', 'fantasy', {'label': 'genre'}),\n",
       "  ('A True Story', 'fantasy', {'label': 'genre'}),\n",
       "  ('Hard-Boiled Wonderland and the End of the World',\n",
       "   'fantasy',\n",
       "   {'label': 'genre'}),\n",
       "  ('The Mists of Avalon', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Two Towers', 'fantasy', {'label': 'genre'}),\n",
       "  (\"Harry Potter and the Philosopher's Stone\", 'fantasy', {'label': 'genre'}),\n",
       "  ('Twenty Thousand Leagues Under the Sea', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Children of Húrin', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Alchemist', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Last Battle', 'fantasy', {'label': 'genre'}),\n",
       "  ('Unfinished Tales', 'fantasy', {'label': 'genre'}),\n",
       "  ('Through the Looking-Glass', 'fantasy', {'label': 'genre'}),\n",
       "  ('Frankenstein; or, The Modern Prometheus', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Scions of Shannara', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Phantom of the Opera', 'fantasy', {'label': 'genre'}),\n",
       "  ('Inkspell', 'fantasy', {'label': 'genre'}),\n",
       "  ('Crown Duel', 'fantasy', {'label': 'genre'}),\n",
       "  ('Stormrider', 'fantasy', {'label': 'genre'}),\n",
       "  (\"The Titan's Curse\", 'fantasy', {'label': 'genre'}),\n",
       "  ('Artemis Fowl', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Farthest Shore', 'fantasy', {'label': 'genre'}),\n",
       "  ('She: A History of Adventure', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Marble Faun', 'fantasy', {'label': 'genre'}),\n",
       "  ('A Wrinkle in Time', 'fantasy', {'label': 'genre'}),\n",
       "  (\"The Magician's Nephew\", 'fantasy', {'label': 'genre'}),\n",
       "  ('Abhorsen', 'fantasy', {'label': 'genre'}),\n",
       "  ('Beyond the Deepwoods', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Lion, the Witch, and the Wardrobe', 'fantasy', {'label': 'genre'}),\n",
       "  (\"The Devil's Elixirs\", 'fantasy', {'label': 'genre'}),\n",
       "  ('The Slow Regard of Silent Things', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Dark Tower III: The Waste Lands', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Light Fantastic', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Carpet People', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Phantom Tollbooth', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Mysteries of Udolpho', 'fantasy', {'label': 'genre'}),\n",
       "  ('Artemis Fowl and the Lost Colony', 'fantasy', {'label': 'genre'}),\n",
       "  ('Pippi Longstocking', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Tales of Beedle the Bard', 'fantasy', {'label': 'genre'}),\n",
       "  ('Dragons of Winter Night', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Short Second Life of Bree Tanner', 'fantasy', {'label': 'genre'}),\n",
       "  ('Blue Moon', 'fantasy', {'label': 'genre'}),\n",
       "  ('Harry Potter and the Half-Blood Prince', 'fantasy', {'label': 'genre'}),\n",
       "  ('Tehanu', 'fantasy', {'label': 'genre'}),\n",
       "  ('City of Bones', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Subtle Knife', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Sea of Monsters', 'fantasy', {'label': 'genre'}),\n",
       "  ('A Wizard of Earthsea', 'fantasy', {'label': 'genre'}),\n",
       "  ('Harry Potter and the Chamber of Secrets', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Battle of the Labyrinth', 'fantasy', {'label': 'genre'}),\n",
       "  ('Harry Potter and the Order of the Phoenix', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Devil in Love', 'fantasy', {'label': 'genre'}),\n",
       "  ('Bridge to Terabithia', 'fantasy', {'label': 'genre'}),\n",
       "  ('Artemis Fowl and the Opal Deception', 'fantasy', {'label': 'genre'}),\n",
       "  ('Jarka Ruus', 'fantasy', {'label': 'genre'}),\n",
       "  ('Good Omens', 'fantasy', {'label': 'genre'}),\n",
       "  ('Roverandom', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Eyes of the Dragon', 'fantasy', {'label': 'genre'}),\n",
       "  ('Last Watch', 'fantasy', {'label': 'genre'}),\n",
       "  (\"The Golem's Eye\", 'fantasy', {'label': 'genre'}),\n",
       "  ('The Neverending Story', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Turn of the Screw', 'fantasy', {'label': 'genre'}),\n",
       "  ('Harry Potter and the Goblet of Fire', 'fantasy', {'label': 'genre'}),\n",
       "  ('Something Wicked This Way Comes', 'fantasy', {'label': 'genre'}),\n",
       "  ('Reaper Man', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Tangle Box', 'fantasy', {'label': 'genre'}),\n",
       "  (\"Assassin's Apprentice\", 'fantasy', {'label': 'genre'}),\n",
       "  ('Grave Peril', 'fantasy', {'label': 'genre'}),\n",
       "  ('New Atlantis', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Once and Future King', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Library of Babel', 'fantasy', {'label': 'genre'}),\n",
       "  (\"Cugel's Saga\", 'fantasy', {'label': 'genre'}),\n",
       "  ('Men at Arms', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Halloween Tree', 'fantasy', {'label': 'genre'}),\n",
       "  ('The House of Hades', 'fantasy', {'label': 'genre'}),\n",
       "  (\"Kiki's Delivery Service\", 'fantasy', {'label': 'genre'}),\n",
       "  ('The Amber Spyglass', 'fantasy', {'label': 'genre'}),\n",
       "  (\"Howl's Moving Castle\", 'fantasy', {'label': 'genre'}),\n",
       "  ('Journey to the West', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Graveyard Book', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Mysterious Island', 'fantasy', {'label': 'genre'}),\n",
       "  ('American Gods', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Fellowship of the Ring', 'fantasy', {'label': 'genre'}),\n",
       "  (\"Lyra's Oxford\", 'fantasy', {'label': 'genre'}),\n",
       "  ('A Game of Thrones', 'fantasy', {'label': 'genre'}),\n",
       "  (\"A Connecticut Yankee in King Arthur's Court\",\n",
       "   'fantasy',\n",
       "   {'label': 'genre'}),\n",
       "  ('The Lightning Thief', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Lovely Bones', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Last Hero', 'fantasy', {'label': 'genre'}),\n",
       "  ('Eclipse', 'fantasy', {'label': 'genre'}),\n",
       "  ('Anansi Boys', 'fantasy', {'label': 'genre'}),\n",
       "  ('Charlie and the Chocolate Factory', 'fantasy', {'label': 'genre'}),\n",
       "  (\"Ronia the Robber's Daughter\", 'fantasy', {'label': 'genre'}),\n",
       "  ('Ghost Story', 'fantasy', {'label': 'genre'}),\n",
       "  ('The Dark Tower VII: The Dark Tower', 'fantasy', {'label': 'genre'}),\n",
       "  ('Where the Wild Things Are', 'fantasy', {'label': 'genre'}),\n",
       "  ('Watership Down', 'fantasy', {'label': 'genre'}),\n",
       "  ('A Storm of Swords', 'fantasy', {'label': 'genre'})],\n",
       " 'followed by': [('The Restaurant at the End of the Universe',\n",
       "   'Life, the Universe and Everything',\n",
       "   {'label': 'followed by'}),\n",
       "  ('Northangep Abbek', 'Persuasion', {'label': 'followed by'})],\n",
       " 'follows': [('So Long, and Thanks for All the Fish',\n",
       "   'Life, the Universe and Everything',\n",
       "   {'label': 'follows'})],\n",
       " 'derivative work': [('Persuasion',\n",
       "   'Persuasion',\n",
       "   {'label': 'derivative work'})],\n",
       " 'country of origin': [('Persuasion',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Harry Potter and the Half-Blood Prince',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Northern Lights', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Light Fantastic', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Till We Have Faces', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Emma', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  (\"Lyra's Oxford\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('She: A History of Adventure',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Oliver Twist', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Great Expectations', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Lord of the Flies', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Wide Window', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Island of Dr Moreau',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The Hobbit', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Prince Caspian', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Return of the King', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Treasure Island', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Use of Weapons', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Infernal Devices', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Scarlet Pimpernel', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  (\"Montezuma's Daughter\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Class Warfare', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Things as They Are; or, The Adventures of Caleb Williams',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Never Let Me Go', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Absolute Friends', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Spy Who Came in from the Cold',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Monsignor Quixote', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Strange Case of Dr Jekyll and Mr Hyde',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The Wind in the Willows',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  (\"Childhood's End\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Witches Abroad', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  (\"Howl's Moving Castle\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Brave New World', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Book of Dave', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Invisible Man', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Rendezvous with Rama', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('2061: Odyssey Three', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Songs of Distant Earth',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Mansfield Park', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Fellowship of the Ring',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Through the Looking-Glass',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Reaper Man', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Amber Spyglass', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Harry Potter and the Order of the Phoenix',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Nineteen Eighty-Four', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Good Omens', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Tinker Tailor Soldier Spy',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Harry Potter and the Goblet of Fire',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Roverandom', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Stormrider', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Guinness World Records', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Mostly Harmless', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Sign of Four', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Memoirs of a Survivor',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Ivanhoe', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Lion, the Witch, and the Wardrobe',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The City and the Stars', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Coraline', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Player of Games', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Last Days of Pompeii',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Matilda', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Power and the Glory',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Rama II', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  (\"The Golem's Eye\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Silmarillion', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Fanny Hill', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('A Clockwork Orange', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Island', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Moon Over Soho', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Mill on the Floss', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Sense and Sensibility', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Three Men in a Boat', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Last Hero', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Orlando: A Biography', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Carmilla', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Beyond the Deepwoods', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Harry Potter and the Chamber of Secrets',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Political Justice', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  (\"Time's Arrow\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Men at Arms', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Kidnapped', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Screwtape Letters', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Graveyard Book', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Anansi Boys', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Architectural Stained Glass',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The Dark Side of the Sun',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Vanity Fair', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('So Long, and Thanks for All the Fish',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Charlie and the Chocolate Factory',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The Time Machine: An Invention',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Neverwhere', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Hound of the Baskervilles',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Good-Bye to All That', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Mysterious Affair at Styles',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  (\"Gulliver's Travels\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Harry Potter and the Deathly Hallows',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The War of the Worlds', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Black Beauty', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Witches', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Mrs Dalloway', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Pride and Prejudice', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Out of Africa', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Subtle Knife', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Turn of the Screw', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Fatherland', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Secret Garden', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Two Towers', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Darkness at Noon', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('My Philosophical Development',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  (\"Alice's Adventures in Wonderland\",\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Mort', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Vampire Prince', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Life, the Universe and Everything',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The Tales of Beedle the Bard',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The Once and Future King',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('The Worst Journey in the World',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Northangep Abbek', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Harry Potter and the Prisoner of Azkaban',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Unfinished Tales', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Mysteries of Udolpho',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('A Dream of John Ball', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  (\"Midnight's Children\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('American Gods', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Children of Húrin', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Restaurant at the End of the Universe',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Point Counter Point', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('A Tale of Two Cities', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Expedition of Humphry Clinker',\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  (\"All Tomorrow's Parties\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('New Atlantis', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('The Casual Vacancy', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  (\"Dirk Gently's Holistic Detective Agency\",\n",
       "   'United Kingdom',\n",
       "   {'label': 'country of origin'}),\n",
       "  ('Star Maker', 'United Kingdom', {'label': 'country of origin'}),\n",
       "  (\"The Razor's Edge\", 'United Kingdom', {'label': 'country of origin'}),\n",
       "  ('Robinson Crusoe', 'United Kingdom', {'label': 'country of origin'})],\n",
       " 'place of publication': [(\"The Magician's Nephew\",\n",
       "   'United Kingdom',\n",
       "   {'label': 'place of publication'}),\n",
       "  ('The Jungle Book', 'United Kingdom', {'label': 'place of publication'}),\n",
       "  ('Fifty Shades of Grey',\n",
       "   'United Kingdom',\n",
       "   {'label': 'place of publication'}),\n",
       "  ('I, Claudius', 'United Kingdom', {'label': 'place of publication'}),\n",
       "  ('The Carpet People', 'United Kingdom', {'label': 'place of publication'}),\n",
       "  ('A Study in Scarlet', 'United Kingdom', {'label': 'place of publication'}),\n",
       "  ('Anthills of the Savannah',\n",
       "   'United Kingdom',\n",
       "   {'label': 'place of publication'}),\n",
       "  ('The Last Battle', 'United Kingdom', {'label': 'place of publication'}),\n",
       "  (\"King Solomon's Mines\",\n",
       "   'United Kingdom',\n",
       "   {'label': 'place of publication'}),\n",
       "  ('Altered Carbon', 'United Kingdom', {'label': 'place of publication'}),\n",
       "  ('And Then There Were None',\n",
       "   'United Kingdom',\n",
       "   {'label': 'place of publication'}),\n",
       "  ('The Day of the Triffids',\n",
       "   'United Kingdom',\n",
       "   {'label': 'place of publication'})],\n",
       " 'narrative location': [('The Picture of Dorian Gray',\n",
       "   'United Kingdom',\n",
       "   {'label': 'narrative location'}),\n",
       "  (\"Harry Potter and the Philosopher's Stone\",\n",
       "   'United Kingdom',\n",
       "   {'label': 'narrative location'})],\n",
       " 'author': [('De Alexandri Magni fortuna aut virtute',\n",
       "   'Plutarch',\n",
       "   {'label': 'author'}),\n",
       "  ('De liberis educandis', 'Plutarch', {'label': 'author'}),\n",
       "  ('De facie in orbe Lunae', 'Plutarch', {'label': 'author'}),\n",
       "  ('Isis and Osiris', 'Plutarch', {'label': 'author'}),\n",
       "  ('Quaestiones convivales', 'Plutarch', {'label': 'author'}),\n",
       "  ('Quomodo adolescens poetas audire debeat', 'Plutarch', {'label': 'author'}),\n",
       "  ('De mulierum virtutibus', 'Plutarch', {'label': 'author'}),\n",
       "  ('Parallel Lives', 'Plutarch', {'label': 'author'}),\n",
       "  ('Moralia', 'Plutarch', {'label': 'author'}),\n",
       "  ('Perry Rhodan', 'Walter Ernsting', {'label': 'author'})]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/graph/literary_graph.gpickle', 'rb') as f:\n",
    "    directed_graph = pickle.load(f)\n",
    "\n",
    "relation_set = {}\n",
    "for node in list(directed_graph.nodes())[:10]:\n",
    "    in_edges = directed_graph.in_edges(node, data=True)\n",
    "    print(in_edges)\n",
    "    for edge in in_edges:\n",
    "        if relation_set.get(edge[2]['label']) is None:\n",
    "            relation_set[edge[2]['label']] = [edge]\n",
    "        else:\n",
    "            relation_set[edge[2]['label']].append(edge)\n",
    "print()\n",
    "relation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hops = 2\n",
    "MC_number = 10\n",
    "limit_nodes = 10\n",
    "df_wh_hallu = pd.read_csv(f\"../data/questions/wh_only/hallucination_only/meta_llama_3.1_8b_instruct.csv\")\n",
    "input_nodes = df_wh_hallu.subject.tolist()  # nodes to generate multi-hop questions for\n",
    "input_relations = df_wh_hallu.relation.tolist()  # nodes to generate multi-hop questions for\n",
    "remove_relation = [\"topic's main category\", \"topic's main template\", \"described by source\", \"Commons category\", \"on focus list of Wikimedia project\"]\n",
    "\n",
    "    \n",
    "def MC_question_generation(subject, relation, object):\n",
    "    question_answer_pair = {}\n",
    "    if not question_answer_pair:\n",
    "        return None\n",
    "\n",
    "    question_answer_pair[\"type\"] = \"MC\"\n",
    "    question_answer_pair[\"subject\"] = subject\n",
    "    question_answer_pair[\"relation\"] = relation\n",
    "    question_answer_pair[\"object\"] = object\n",
    "    # question_answer_pair[\"label\"] = object\n",
    "    return question_answer_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_info: {'node path': ['White Tower', 'Tower of London', 'Tower Bridge'], 'edge_labels': ['has part(s)', 'named after']}\n",
      "path_info: {'node path': ['Ictinus', 'Parthenon', 'Acropolis of Athens'], 'edge_labels': ['architect', 'has part(s)']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', 'Epiphany'], 'edge_labels': ['has part(s)', 'closed on']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', \"All Saints' Day\"], 'edge_labels': ['has part(s)', 'closed on']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', 'toilet'], 'edge_labels': ['has part(s)', 'has facility']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', 'Easter Monday'], 'edge_labels': ['has part(s)', 'closed on']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', 'castle'], 'edge_labels': ['has part(s)', 'instance of']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', 'Feast of Corpus Christi'], 'edge_labels': ['has part(s)', 'closed on']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', 'Gothic art'], 'edge_labels': ['has part(s)', 'architectural style']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', 'Easter'], 'edge_labels': ['has part(s)', 'closed on']}\n",
      "path_info: {'node path': ['Museum of the Masovian Nobility in Ciechanów', 'Castle of the Masovian Dukes in Ciechanów', 'January 1'], 'edge_labels': ['has part(s)', 'closed on']}\n",
      "path_info: {'node path': ['Easter Monday', 'Castle of the Masovian Dukes in Ciechanów', 'Museum of the Masovian Nobility in Ciechanów'], 'edge_labels': ['closed on', 'has part(s)']}\n",
      "path_info: {'node path': ['Easter', 'Castle of the Masovian Dukes in Ciechanów', 'Museum of the Masovian Nobility in Ciechanów'], 'edge_labels': ['closed on', 'has part(s)']}\n",
      "path_info: {'node path': [\"St Mark's Campanile\", \"St Mark's Basilica\", 'Piazza San Marco'], 'edge_labels': ['has part(s)', 'named after']}\n"
     ]
    }
   ],
   "source": [
    "with open('../data/graph/landmark_graph.gpickle', 'rb') as f:\n",
    "    directed_graph = pickle.load(f)\n",
    "\n",
    "relation_set = {}\n",
    "# for node in directed_graph.nodes():\n",
    "for node in input_nodes:\n",
    "    in_edges = directed_graph.in_edges(node, data=True)\n",
    "    for edge in in_edges:\n",
    "        if relation_set.get(edge[2]['label']) is None:\n",
    "            relation_set[edge[2]['label']] = [edge]\n",
    "        else:\n",
    "            relation_set[edge[2]['label']].append(edge)\n",
    "\n",
    "MC_question_set = []\n",
    "MC_flag = False\n",
    "for i, source_node in enumerate(input_nodes[:]):\n",
    "    # print(f\"source_node: {source_node}\")\n",
    "    for node, path in nx.single_source_shortest_path(directed_graph, source_node, cutoff=hops).items():\n",
    "        if len(path) == hops+1:\n",
    "            # Retrieve the edge labels based on the path\n",
    "            edge_labels = [directed_graph[path[i]][path[i+1]]['label'] for i in range(len(path)-1)]\n",
    "            if input_relations[i] == edge_labels[0]:\n",
    "                path_info = {'node path': path[:], 'edge_labels': edge_labels}\n",
    "                print(f\"path_info: {path_info}\")\n",
    "    \n",
    "    for node, path in nx.single_source_shortest_path(directed_graph.reverse(), source_node, cutoff=hops).items():\n",
    "        if len(path) == hops+1:\n",
    "            # Retrieve the edge labels based on the path\n",
    "            edge_labels = [directed_graph.reverse()[path[i]][path[i+1]]['label'] for i in range(len(path)-1)]\n",
    "            if input_relations[i] == edge_labels[0]:\n",
    "                path_info = {'node path': path[:], 'edge_labels': edge_labels}\n",
    "                print(f\"path_info: {path_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\")]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'tourist attraction')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'tourist attraction'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Feast of Corpus Christi')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'tourist attraction'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Feast of Corpus Christi'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'architectural style', 'Gothic art')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'tourist attraction'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Feast of Corpus Christi'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'architectural style', 'Gothic art'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'tourist attraction'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Feast of Corpus Christi'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'architectural style', 'Gothic art'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'archaeological museum')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'tourist attraction'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Feast of Corpus Christi'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'architectural style', 'Gothic art'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'archaeological museum'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'January 1')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'tourist attraction'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Feast of Corpus Christi'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'architectural style', 'Gothic art'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'archaeological museum'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'January 1'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Monday')]\n",
      "two_hop_paths: [('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Epiphany'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', \"All Saints' Day\"), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'country', 'Poland'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'activity policy in this place', 'smoking ban'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'has facility', 'toilet'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'payment types accepted', 'cash'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'castle'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'tourist attraction'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Feast of Corpus Christi'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'architectural style', 'Gothic art'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Easter'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'instance of', 'archaeological museum'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'January 1'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'closed on', 'Monday'), ('Museum of the Masovian Nobility in Ciechanów', 'has part(s)', 'Castle of the Masovian Dukes in Ciechanów', 'located in the administrative territorial entity', 'Ciechanów')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Greece': [],\n",
       " 'Panathenaic Stadium': [],\n",
       " 'Deloitte': [],\n",
       " 'Rosersberg Palace': [],\n",
       " 'Mellieħa': [],\n",
       " 'Gustavianum': [],\n",
       " 'Prayerbook Cross': [],\n",
       " 'Haw Par Villa': [],\n",
       " 'Carlos Oswald': [],\n",
       " 'Tsarskoye Selo': [],\n",
       " 'Kungsholmen': [],\n",
       " 'Göta älv': [],\n",
       " 'Sedefkar Mehmed Agha': [],\n",
       " 'Alfred Parland': [],\n",
       " 'Hundertwasserhaus': [],\n",
       " 'Grand Kremlin Palace': [],\n",
       " 'Stourton with Gasper': [],\n",
       " 'Sunset Strip': [],\n",
       " 'İzmir Clock Tower': [],\n",
       " 'Tor Hagfors': [],\n",
       " \"Gustav III's Pavilion\": [],\n",
       " 'Potala Palace': [],\n",
       " 'Saviour Church on Nereditsa': [],\n",
       " 'South Street Seaport': [],\n",
       " 'Louis de Hoÿm de Marien': [],\n",
       " 'Rothenburg ob der Tauber': [],\n",
       " 'Konya Province': [],\n",
       " 'Now You See Me 2': [],\n",
       " 'Abu Dhabi': [],\n",
       " 'Taj Mahal': [],\n",
       " 'Kane County': [],\n",
       " 'Little Rock': [],\n",
       " 'Prague Castle': [],\n",
       " 'Works Progress Administration': [],\n",
       " 'National Garden of Athens': [],\n",
       " 'Yucca gloriosa': [],\n",
       " 'Bayezid I': [],\n",
       " 'Cambridge': [],\n",
       " 'Ushaw College': [],\n",
       " 'Henry VII': [],\n",
       " 'Washingtonia filifera': [],\n",
       " 'Peter Walker': [],\n",
       " 'Yusupov Palace on Moika': [],\n",
       " 'Teton County': [],\n",
       " 'Meteor Crater': [],\n",
       " 'Kirk R. Johnson': [],\n",
       " 'Sentosa': [],\n",
       " 'funeral': [],\n",
       " 'drapery': [],\n",
       " 'George VI': [],\n",
       " 'Disneyland': [],\n",
       " 'Central Park': [],\n",
       " 'Colosseum': [],\n",
       " 'Skanska': [],\n",
       " 'Bartolommeo Berrecci': [],\n",
       " 'Tsaritsyno Park': [],\n",
       " 'The Bank of New York Mellon': [],\n",
       " 'Kamikochi': [],\n",
       " 'Gran Canaria': [],\n",
       " 'Justinian I': [],\n",
       " 'Beijing': [],\n",
       " 'Nördlingen': [],\n",
       " \"St Paul's Cathedral\": [],\n",
       " 'paint': [],\n",
       " 'Bodo Ebhardt': [],\n",
       " 'Aksaray Province': [],\n",
       " 'Visby': [],\n",
       " 'Daniel Burnham': [],\n",
       " 'The Church of Jesus Christ of Latter-day Saints': [],\n",
       " 'war memorial': [],\n",
       " 'Chamaerops humilis': [],\n",
       " 'Schlaich Bergermann Partner': [],\n",
       " 'repoussé': [],\n",
       " 'Independence National Historical Park': [],\n",
       " 'Bangkok': [],\n",
       " 'antiprism': [],\n",
       " 'navel': [],\n",
       " 'Willis Tower': [],\n",
       " 'Jan Styka': [],\n",
       " 'Person of Interest, season 2': [],\n",
       " \"Gustav III's museum of antiquities\": [],\n",
       " 'White Tower': [],\n",
       " 'Moscow': [],\n",
       " 'Abbaye de la Fille-Dieu Romont': [],\n",
       " 'Château de Montsoreau-Museum of Contemporary Art': [],\n",
       " 'Ahmed I': [],\n",
       " 'National Museum of Natural History': [],\n",
       " 'North Sea': [],\n",
       " 'Linköping Cathedral': [],\n",
       " 'Stonehenge': [],\n",
       " 'Karlstad Cathedral': [],\n",
       " 'Jardin du Luxembourg': [],\n",
       " 'Münchner Altstadt': [],\n",
       " 'Naples': [],\n",
       " 'Skokloster Castle': [],\n",
       " 'Government of New South Wales': [],\n",
       " 'George IV of the United Kingdom': [],\n",
       " 'South Dakota': [],\n",
       " 'Ford, Powell & Carson': [],\n",
       " 'Hawaiian Volcano Observatory': [],\n",
       " 'Grampian Mountains': [],\n",
       " 'Orange County': [],\n",
       " 'book': [],\n",
       " 'Pinus canariensis': [],\n",
       " 'Girolamo Cassar': [],\n",
       " 'Leander': [],\n",
       " 'Dresden Zoo': [],\n",
       " 'Jean-Nicolas Jadot de Ville-Issey': [],\n",
       " 'Ankara Province': [],\n",
       " 'Östersund Municipality': [],\n",
       " 'mercury': [],\n",
       " 'Antonio Bernocchi': [],\n",
       " \"Disney's Blizzard Beach\": [],\n",
       " 'Korea Development Bank': [],\n",
       " 'Haga park': [],\n",
       " 'Burj Khalifa': [],\n",
       " 'Royal Mile': [],\n",
       " 'Roman Abramovich': [],\n",
       " 'California Academy of Sciences': [],\n",
       " 'Stein Olav Henrichsen': [],\n",
       " 'Santa Rosa Island, Florida': [],\n",
       " 'Epidavros Municipality': [],\n",
       " 'Detroit': [],\n",
       " 'County Kerry': [],\n",
       " 'Luigi Canonica': [],\n",
       " 'Cloud Gate': [],\n",
       " 'Balboa Park': [],\n",
       " 'Roly Keating': [],\n",
       " \"Fondo per l'Ambiente Italiano\": [],\n",
       " 'Stockholm Municipality': [],\n",
       " 'Ictinus': [],\n",
       " 'Ulriksdal Palace': [],\n",
       " 'Roman Forum': [],\n",
       " 'Dömitz Fortress': [],\n",
       " 'classicism': [],\n",
       " \"St Mark's Basilica\": [],\n",
       " 'Mariefred': [],\n",
       " 'Altare della Patria': [],\n",
       " 'bulletproof glass': [],\n",
       " 'Harburg': [],\n",
       " 'Philippe Méaille': [],\n",
       " 'Charles François de Mondion': [],\n",
       " 'Mount Ararat': [],\n",
       " 'National Portrait Gallery of Sweden': [],\n",
       " 'Bosporus': [],\n",
       " \"Saint-Germain-l'Auxerrois\": [],\n",
       " 'Garage Center for Contemporary Culture': [],\n",
       " 'Bavarian Administration of State-Owned Palaces, Gardens and Lakes': [],\n",
       " 'Daniel Libeskind': [],\n",
       " 'Justus Vingboons': [],\n",
       " 'Greenwich': [],\n",
       " 'Surabaya': [],\n",
       " 'Lambeth': [],\n",
       " 'Commerzbank AG': [],\n",
       " 'Mount Faber': [],\n",
       " 'avenue du Maine': [],\n",
       " 'Charles Follen McKim': [],\n",
       " 'Yıldız Palace': [],\n",
       " 'Drottningholm Palace': [],\n",
       " 'Jakob Nielsen': [],\n",
       " 'Kenneth Murray': [],\n",
       " 'concert hall': [],\n",
       " 'English Heritage': [],\n",
       " 'Strand': [],\n",
       " 'Museum of the Masovian Nobility in Ciechanów': [],\n",
       " 'John McShain': [],\n",
       " 'Royal Observatory': [],\n",
       " '17th arrondissement of Paris': [],\n",
       " 'dome': [],\n",
       " 'Mount Fuji': [],\n",
       " 'Humboldt County': [],\n",
       " 'Leskovac': [],\n",
       " 'Helsinki Parish Union': [],\n",
       " 'University of Central Florida': [],\n",
       " 'Domvs Romana': [],\n",
       " 'neoclassicism': [],\n",
       " 'gneiss': [],\n",
       " 'Tendai': [],\n",
       " 'Qinghai': [],\n",
       " 'Belém Tower': [],\n",
       " 'Helgeandsholmen': [],\n",
       " 'Platz der Republik': [],\n",
       " 'transport': [],\n",
       " 'sand': [],\n",
       " 'Centro de Arte Moderna Gulbenkian': [],\n",
       " 'Easter Monday': [],\n",
       " 'Moscow Planetarium': [],\n",
       " 'Easter': [],\n",
       " 'Frank Lloyd Wright': [],\n",
       " 'Hermitage Museum': [],\n",
       " 'Königsberg Cathedral': [],\n",
       " 'Strängnäs Cathedral': [],\n",
       " 'Gateway Arch': [],\n",
       " 'Falun': [],\n",
       " 'Palace Embankment': [],\n",
       " 'Antwerp Zoo': [],\n",
       " 'demolition': [],\n",
       " 'Linnanmäki': [],\n",
       " 'Petronas Towers': [],\n",
       " 'World War II': [],\n",
       " 'Henry II of England': [],\n",
       " 'Santa Fe': [],\n",
       " 'Hornblower & Marshall': [],\n",
       " 'Fort Canning Hill': [],\n",
       " 'Pierre de Chelles': [],\n",
       " 'contemporary art': [],\n",
       " 'Colen Campbell': [],\n",
       " 'Nemrut': [],\n",
       " 'Maurice de Sully': [],\n",
       " 'Ancient Egypt': [],\n",
       " 'Fernsehturm Berlin': [],\n",
       " 'Dasha Zhukova': [],\n",
       " 'Ferdinand I of the Two Sicilies': [],\n",
       " 'Jurong East': [],\n",
       " 'Carlos J. Gradin': [],\n",
       " 'Montparnasse – Bienvenüe': [],\n",
       " 'John J. Borg': [],\n",
       " 'Southbank Centre': [],\n",
       " 'Mecca': [],\n",
       " 'Paolo Giulierini': [],\n",
       " \"Grandmaster's Palace\": [],\n",
       " 'Theater Confidencen': [],\n",
       " 'MUNCH': [],\n",
       " 'Bridget of Sweden': [],\n",
       " 'Smithsonian Marine Station at Fort Pierce': [],\n",
       " \"St Mark's Campanile\": []}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_2_hop_paths(kg, input_nodes, input_relations):\n",
    "    result = {}\n",
    "    for i, node in enumerate(input_nodes):\n",
    "        two_hop_paths = []\n",
    "        # First hop\n",
    "        for neighbor1 in kg.neighbors(node):\n",
    "            # print(f\"neighbor1: {neighbor1}\")\n",
    "            # Check if the relation (edge) between node and neighbor1 is in input_relations\n",
    "            if kg.has_edge(node, neighbor1):\n",
    "                relation1 = kg.edges[node, neighbor1]['label']\n",
    "                if relation1 == input_relations[i]:\n",
    "                    # print(f\"neighbor1: {neighbor1}, relation1: {relation1}\")\n",
    "                    # Second hop\n",
    "                    for neighbor2 in kg.neighbors(neighbor1):\n",
    "                        if kg.has_edge(neighbor1, neighbor2):\n",
    "                            relation2 = kg.edges[neighbor1, neighbor2]['label']\n",
    "                            two_hop_paths.append((node, relation1, neighbor1, relation2, neighbor2))\n",
    "                            print(f\"two_hop_paths: {two_hop_paths}\")\n",
    "\n",
    "        result[node] = two_hop_paths\n",
    "    return result\n",
    "find_2_hop_paths(directed_graph, input_nodes, input_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ls = ['meta-llama/Meta-Llama-3.1-8B-Instruct', 'mistralai/Mistral-7B-Instruct-v0.3', 'lmsys/vicuna-7b-v1.5', \n",
    "            'google/gemma-2-9b-it', 'chavinlo/alpaca-native', 'meta-llama/Meta-Llama-3.1-8B-Instruct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In places_city.csv, question 36: response: Slavic Tribes (Unknown Leader)\n",
      "In places_city.csv, question 1650: response: Unknown (from the provided information)\n",
      "In places_city.csv, question 1772: response: Unknown (from the provided information)\n",
      "In places_city.csv, question 1871: response: Unknown (Naska is not a recognized country)\n",
      "In places_city.csv, question 2104: response: Unknown (as Virbalis is a fictional country)\n",
      "In places_city.csv, question 2137: response: Unknown (source doesn't specify)\n",
      "In places_city.csv, question 2944: response: Unknown (Oni is a mythological creature, not a country or region\n",
      "In places_city.csv, question 3101: response: Unknown (not a recognized country)\n",
      "In places_city.csv, question 3157: response: Unknown\n",
      "In places_city.csv, question 3617: response: Unknown without specific context. Please provide more details\n",
      "In places_city.csv, question 3951: response: Unknown (provide specific location or individual)\n",
      "In places_city.csv, question 3980: response: Unknown (source doesn't specify)\n",
      "In places_city.csv, question 4207: response: Unknown (La Maná is a region in Colombia, not associated with a\n",
      "In places_city.csv, question 4733: response: Unknown (No official patron saint is known for Argos.)\n",
      "In places_city.csv, question 5418: response: Unknown (No specific information provided)\n",
      "In places_city.csv, question 5634: response: Unknown (from the provided information)\n",
      "In places_city.csv, question 5712: response: Unknown without specific details\n",
      "In places_city.csv, question 5914: response: Unknown. Please provide the correct name of the administrative territorial entity\n",
      "In places_city.csv, question 5989: response: Unknown (provide more context)\n",
      "In places_city.csv, question 6241: response: Unknown (Pre-Inca civilization)\n",
      "In places_city.csv, question 6302: response: Unknown. The name Kam is not associated with a specific country\n",
      "In places_city.csv, question 6449: response: Unknown (provided no specific individual is mentioned in the source)\n",
      "In places_city.csv, question 6628: response: Unknown (Quevedo is a city in Spain, not associated with a\n",
      "In places_city.csv, question 6737: response: Unknown (No widely recognized patron saint for Huaquillas)\n",
      "In places_city.csv, question 6803: response: Unknown\n",
      "In entertainment_music_genre.csv, question 2: response: Origin (unknown)\n",
      "In entertainment_song.csv, question 235: response: Unknown. Please specify the artist's full name or the specific work\n",
      "In entertainment_song.csv, question 543: response: Unknown. Please provide more context or specific details\n",
      "In entertainment_song.csv, question 555: response: Unknown (Information not provided)\n",
      "In entertainment_song.csv, question 557: response: Unknown (Not specified in the provided information)\n",
      "In entertainment_song.csv, question 558: response: Unknown (No specific label information available)\n",
      "In entertainment_song.csv, question 564: response: Unknown (Anonymous)\n",
      "In entertainment_song.csv, question 576: response: Unknown\n",
      "In entertainment_song.csv, question 581: response: Unknown\n",
      "In entertainment_song.csv, question 585: response: Unknown (provide specific artist name)\n",
      "In entertainment_song.csv, question 586: response: Unknown (as of my last update)\n",
      "In entertainment_song.csv, question 590: response: Unknown (as 3e sexe is not a recognized work or title\n",
      "In entertainment_song.csv, question 613: response: Unknown\n",
      "In entertainment_song.csv, question 615: response: Unknown\n",
      "In entertainment_song.csv, question 626: response: Unknown (No widely recognized record label associated with this artist name.)\n",
      "In entertainment_song.csv, question 652: response: Unknown (as of my last update)\n",
      "In entertainment_song.csv, question 728: response: Unknown\n",
      "In entertainment_song.csv, question 747: response: Unknown\n",
      "In entertainment_song.csv, question 749: response: Unknown\n",
      "In entertainment_song.csv, question 772: response: Unknown (provide specific song or artist for accurate label identification)\n",
      "In entertainment_song.csv, question 873: response: Unknown (Composition not specified)\n",
      "In entertainment_song.csv, question 923: response: Unknown\n",
      "In entertainment_song.csv, question 1116: response: Unknown (specific information not provided)\n",
      "In entertainment_song.csv, question 1155: response: Unknown (Independent)\n",
      "In entertainment_song.csv, question 1205: response: Unknown\n",
      "In entertainment_song.csv, question 1228: response: Unknown (as of my last update)\n",
      "In entertainment_song.csv, question 1230: response: Unknown (information not provided)\n",
      "In entertainment_song.csv, question 1236: response: Unknown (provide specific artist and album)\n",
      "In entertainment_song.csv, question 1372: response: Unknown\n",
      "In entertainment_song.csv, question 1456: response: Unknown (song title)\n",
      "In entertainment_song.csv, question 1468: response: Unknown (as of my last update)\n",
      "In entertainment_song.csv, question 1486: response: Unknown\n",
      "In entertainment_song.csv, question 1512: response: Unknown (Independent)\n",
      "In entertainment_song.csv, question 1528: response: Unknown (provide more context)\n",
      "In entertainment_song.csv, question 1529: response: Unknown (No specific record label information available for Allô maman bobo\n",
      "In entertainment_song.csv, question 1546: response: Unknown (No specific performer provided in the question)\n",
      "In entertainment_song.csv, question 1570: response: Unknown (original piece)\n",
      "In entertainment_song.csv, question 1604: response: Unknown\n",
      "In entertainment_song.csv, question 1614: response: Unknown\n",
      "In entertainment_song.csv, question 1625: response: Unknown (as of my last update)\n",
      "In entertainment_song.csv, question 1701: response: Unknown (song title)\n",
      "In entertainment_song.csv, question 1703: response: Unknown\n",
      "In entertainment_song.csv, question 1707: response: Unknown (information not provided)\n",
      "In entertainment_song.csv, question 1729: response: Unknown (without specific reference)\n",
      "In entertainment_song.csv, question 1737: response: Unknown. The song \"Alright\" has been recorded by multiple artists\n",
      "In entertainment_song.csv, question 1744: response: Unknown (provide specific song or artist)\n",
      "In entertainment_song.csv, question 1781: response: Unknown (as of my last update)\n",
      "In entertainment_song.csv, question 1803: response: Unknown\n",
      "In entertainment_song.csv, question 1877: response: Unknown (as of my last update)\n",
      "In entertainment_song.csv, question 1883: response: Unknown. Please provide the name of the band\n",
      "In entertainment_song.csv, question 1898: response: Unknown (as of my last update)\n",
      "In entertainment_song.csv, question 1925: response: Unknown\n",
      "In human_athlete.csv, question 21: response: Unknown (Public record shows no known political party affiliation for Matti Ny\n",
      "In human_athlete.csv, question 71: response: Unknown without specific context or match details\n",
      "In human_athlete.csv, question 72: response: Unknown (No specific information provided)\n",
      "In human_athlete.csv, question 78: response: Unknown (as of current information)\n",
      "In human_athlete.csv, question 83: response: Ivan Hlinka's place of burial is unknown, as he was\n",
      "In human_athlete.csv, question 164: response: Unknown\n",
      "In human_athlete.csv, question 231: response: Unknown (Information not provided)\n",
      "In human_athlete.csv, question 243: response: Unknown\n",
      "In human_athlete.csv, question 324: response: Unknown\n",
      "In human_athlete.csv, question 375: response: Unknown (historical figure)\n",
      "In human_athlete.csv, question 402: response: Unknown (provided name does not indicate gender)\n",
      "In human_athlete.csv, question 419: response: Unknown (Information not provided)\n",
      "In human_athlete.csv, question 420: response: Unknown (Information not provided)\n",
      "In human_athlete.csv, question 428: response: Unknown without specific reference\n",
      "In human_athlete.csv, question 437: response: Unknown without specific context or game details\n",
      "In human_athlete.csv, question 448: response: Not specified (or unknown) in the provided information\n",
      "In human_athlete.csv, question 462: response: Unknown\n",
      "In human_athlete.csv, question 489: response: Unknown\n",
      "In human_athlete.csv, question 490: response: Unknown (No public record)\n",
      "In human_athlete.csv, question 521: response: Unknown (historical figure)\n",
      "In human_athlete.csv, question 524: response: Unknown (not specified)\n",
      "In human_athlete.csv, question 530: response: Unknown (Erling Kagge is a polar explorer, not a\n",
      "In human_athlete.csv, question 542: response: Unknown (Specific information about a league associated with Karel Rada was\n",
      "In human_athlete.csv, question 555: response: Mariano Haro's residence is unknown\n",
      "In human_athlete.csv, question 608: response: Unknown\n",
      "In human_athlete.csv, question 620: response: Not specified (or unknown)\n",
      "In human_athlete.csv, question 676: response: Unknown (provide specific match details)\n",
      "In human_athlete.csv, question 687: response: Unknown (No specific information provided)\n",
      "In human_athlete.csv, question 699: response: Unknown\n",
      "In human_athlete.csv, question 710: response: Unknown (No publicly available information)\n",
      "In human_athlete.csv, question 725: response: Unknown\n",
      "In human_athlete.csv, question 727: response: Unknown (provide specific match details for accurate answer)\n",
      "In human_athlete.csv, question 759: response: Unknown (as of my last update)\n",
      "In human_athlete.csv, question 761: response: Unknown\n",
      "In human_athlete.csv, question 768: response: Unknown\n",
      "In human_athlete.csv, question 788: response: Unknown\n",
      "In human_athlete.csv, question 799: response: Unknown (No specific information provided)\n",
      "In human_athlete.csv, question 823: response: Unknown\n",
      "In human_athlete.csv, question 848: response: Unknown (No publicly available information)\n",
      "In human_athlete.csv, question 854: response: Unknown\n",
      "In human_athlete.csv, question 869: response: Unknown\n",
      "In human_athlete.csv, question 877: response: Unknown (No specific information provided)\n",
      "In human_athlete.csv, question 901: response: Unknown (Information not provided)\n",
      "In human_athlete.csv, question 905: response: Unknown (information not provided)\n",
      "In human_athlete.csv, question 916: response: Unknown (Gunnar Sjögren is a medical researcher,\n",
      "In human_athlete.csv, question 924: response: Unknown\n",
      "In human_athlete.csv, question 947: response: Unknown (Information not provided)\n",
      "In human_athlete.csv, question 953: response: Unknown (No public record available)\n",
      "In human_athlete.csv, question 978: response: Unknown\n",
      "In human_athlete.csv, question 995: response: Unknown\n",
      "In human_athlete.csv, question 1023: response: Not specified (or unknown) in the provided context\n",
      "In human_athlete.csv, question 1049: response: Unknown (historical figure)\n",
      "In human_athlete.csv, question 1060: response: Unknown (No public record)\n",
      "In human_athlete.csv, question 1063: response: Unknown (Gustav Schwarzenegger is not a known political\n",
      "In human_athlete.csv, question 1065: response: Unknown (Gustav Schwarzenegger is not a well-\n",
      "In human_athlete.csv, question 1083: response: Unknown\n",
      "In human_athlete.csv, question 1093: response: Unknown (as per publicly available information)\n",
      "In human_athlete.csv, question 1094: response: Unknown (No specific information provided)\n",
      "In human_athlete.csv, question 1096: response: Unknown (No specific information provided)\n",
      "In human_athlete.csv, question 1102: response: Unknown\n",
      "In human_athlete.csv, question 1115: response: Unknown (provided no information)\n",
      "In human_athlete.csv, question 1129: response: Unknown (No specific employer information provided)\n",
      "In human_athlete.csv, question 1130: response: Unknown (No publicly available information)\n",
      "In human_athlete.csv, question 1140: response: Unknown (Bronisław Gustawicz is a historical figure, and\n",
      "In human_athlete.csv, question 1152: response: Unknown\n",
      "In human_athlete.csv, question 1155: response: Unknown (No specific rank provided)\n",
      "In human_athlete.csv, question 1207: response: Unknown\n",
      "In human_athlete.csv, question 1214: response: Unknown\n",
      "In human_athlete.csv, question 1237: response: Unknown\n",
      "In human_athlete.csv, question 1254: response: Unknown (No specific information provided)\n",
      "In human_athlete.csv, question 1256: response: Unknown\n",
      "In human_athlete.csv, question 1268: response: Employer (of Terry Todd) is unknown (as of my last update\n",
      "In human_athlete.csv, question 1282: response: Unknown (No specific employer provided)\n",
      "In human_athlete.csv, question 1307: response: Unknown (No publicly known information)\n",
      "In entertainment_anime.csv, question 108: response: Unknown (Specific method not provided in the reference)\n",
      "In places_landmark.csv, question 206: response: Unknown (historical records vary)\n",
      "In places_landmark.csv, question 223: response: Unknown\n",
      "In places_landmark.csv, question 429: response: Unknown (Information not provided)\n",
      "In places_landmark.csv, question 562: response: Unknown without specific reference\n",
      "In places_landmark.csv, question 594: response: Unknown (Medieval artist)\n",
      "In places_landmark.csv, question 637: response: Unknown (historical), Khemraj Ramjattan (current German\n",
      "In places_landmark.csv, question 679: response: Unknown (No specific contractor information provided on the official website.)\n",
      "In places_landmark.csv, question 969: response: Unknown (No specific architect is attributed to Karlstad Cathedral in the provided\n",
      "In places_landmark.csv, question 1051: response: Unknown (not specified in historical records)\n",
      "In places_landmark.csv, question 1059: response: Unknown (Traditionally attributed to James the Apostle)\n",
      "In places_landmark.csv, question 1158: response: Unknown (No specific information provided)\n",
      "In places_landmark.csv, question 1331: response: Unknown (specific architect not provided in the question)\n",
      "In places_landmark.csv, question 1372: response: Unknown (provided no specific architect is mentioned)\n",
      "In places_landmark.csv, question 1479: response: Unknown (No specific architect is credited for Bohus Fortress.)\n",
      "In places_landmark.csv, question 1506: response: Unknown\n",
      "In places_landmark.csv, question 1592: response: Unknown (No specific architect is attributed to Glimmingehus)\n",
      "In places_landmark.csv, question 1629: response: Unknown\n",
      "In places_landmark.csv, question 1637: response: Unknown without specific context\n",
      "In places_landmark.csv, question 1639: response: Remission Gate's location is unknown\n",
      "In places_landmark.csv, question 1669: response: Unknown (fictional location)\n",
      "In places_landmark.csv, question 1706: response: Unknown (Specific architect not provided in the question.)\n",
      "In places_landmark.csv, question 1734: response: Unknown (No publicly available information)\n",
      "In places_landmark.csv, question 1759: response: Unknown (No specific architect is associated with Salle Favart.)\n",
      "In places_landmark.csv, question 1826: response: Unknown (No specific architect is credited for Spasskaya Tower.)\n",
      "In business_industry.csv, question 107: response: Unknown (plumbing is an ancient practice)\n",
      "In human_scientist.csv, question 19: response: Unknown (historical records are incomplete)\n",
      "In human_scientist.csv, question 48: response: Unknown (No specific information available)\n",
      "In human_scientist.csv, question 54: response: Unknown (No specific information provided)\n",
      "In human_scientist.csv, question 56: response: Unknown (No specific employer information provided)\n",
      "In human_scientist.csv, question 77: response: Unknown (adopted)\n",
      "In human_scientist.csv, question 82: response: Unknown (as of my last update)\n",
      "In human_scientist.csv, question 83: response: Unknown (No public record)\n",
      "In human_scientist.csv, question 113: response: Unknown (as of the time of this response, specific party information for Vik\n",
      "In human_scientist.csv, question 120: response: Unknown (as of my last update)\n",
      "In human_scientist.csv, question 129: response: Unknown (No specific child is mentioned in historical records)\n",
      "In human_scientist.csv, question 140: response: Unknown (Historical figure)\n",
      "In human_scientist.csv, question 144: response: Unknown, as no specific student is mentioned in relation to Tytus M\n",
      "In human_scientist.csv, question 160: response: Unknown\n",
      "In human_scientist.csv, question 164: response: Unknown (No specific information provided)\n",
      "In human_scientist.csv, question 168: response: Unknown (No publicly known political party affiliation for Vitaly Ginz\n",
      "In human_scientist.csv, question 179: response: Unknown (Information not provided)\n",
      "In human_scientist.csv, question 194: response: Unknown (provided no specific student name was given in the question)\n",
      "In human_scientist.csv, question 196: response: Unknown (provided name only)\n",
      "In human_scientist.csv, question 199: response: Unknown (No public record)\n",
      "In human_scientist.csv, question 216: response: Unknown (Euler did not publicly discuss his religious beliefs extensively)\n",
      "In human_scientist.csv, question 261: response: Nikolay Kamov's place of burial is unknown\n",
      "In human_scientist.csv, question 273: response: Unknown (No specific military or police rank provided for Heinrich Rantzau\n",
      "In human_scientist.csv, question 275: response: Unknown (No specific information available)\n",
      "In human_scientist.csv, question 276: response: Unknown (No publicly known spouse for Heinrich Rantzau)\n",
      "In human_scientist.csv, question 320: response: Unknown\n",
      "In human_scientist.csv, question 342: response: Unknown (heart attack suspected)\n",
      "In human_scientist.csv, question 372: response: Unknown (No specific military or police rank is associated with Semyon Lav\n",
      "In human_scientist.csv, question 375: response: Unknown (No public record)\n",
      "In human_scientist.csv, question 377: response: Unknown (provided no specific employer was mentioned)\n",
      "In human_scientist.csv, question 404: response: Unknown (information not provided)\n",
      "In human_scientist.csv, question 410: response: Unknown (Information not provided)\n",
      "In human_scientist.csv, question 417: response: Unknown (Not publicly disclosed)\n",
      "In human_scientist.csv, question 436: response: Unknown (historical figure)\n",
      "In human_scientist.csv, question 449: response: Unknown (Information not provided)\n",
      "In human_scientist.csv, question 450: response: Unknown information\n",
      "In human_scientist.csv, question 455: response: Unknown (No specific information provided)\n",
      "In human_scientist.csv, question 463: response: Unknown (No information provided about sponsorship.)\n",
      "In human_scientist.csv, question 478: response: Unknown (No specific information provided)\n",
      "In human_scientist.csv, question 485: response: Unknown (Christoph Wilhelm von Koch was a mathematician,\n",
      "In human_scientist.csv, question 505: response: Unknown (Historical figure)\n",
      "In human_scientist.csv, question 507: response: Unknown (No specific information provided)\n",
      "In human_scientist.csv, question 508: response: Unknown (No specific information provided)\n",
      "In human_scientist.csv, question 515: response: Unknown (Max Factor did not have any publicly known children)\n",
      "In human_scientist.csv, question 521: response: Unknown (Specific religious or worldview information not readily available)\n",
      "In human_scientist.csv, question 535: response: Unknown (No specific employer information provided)\n",
      "In human_scientist.csv, question 556: response: Unknown (No specific student mentioned in the provided context.)\n",
      "In human_scientist.csv, question 576: response: Unknown. No specific student is mentioned in relation to Francesco Redi\n",
      "In human_scientist.csv, question 589: response: Unknown (No specific information available)\n",
      "In human_scientist.csv, question 597: response: Unknown\n",
      "In human_scientist.csv, question 632: response: Unknown (No specific information provided)\n",
      "In human_scientist.csv, question 633: response: Unknown\n",
      "In human_scientist.csv, question 639: response: Unknown (No specific information provided)\n",
      "In human_scientist.csv, question 642: response: Unknown\n",
      "In human_scientist.csv, question 650: response: Unknown (specific medical condition)\n",
      "In human_scientist.csv, question 654: response: Unknown (No specific information available)\n",
      "In human_scientist.csv, question 676: response: Unknown\n",
      "In human_scientist.csv, question 704: response: Unknown\n",
      "In human_scientist.csv, question 717: response: Unknown (Information not provided)\n",
      "In human_scientist.csv, question 724: response: Unknown (Information not provided)\n",
      "In human_scientist.csv, question 734: response: Unknown (Information not provided)\n",
      "In human_scientist.csv, question 753: response: Unknown (he was unmarried)\n",
      "In human_scientist.csv, question 765: response: Unknown (No information available about Hans Conrad Escher von der Lin\n",
      "In human_scientist.csv, question 773: response: Unknown (as of 2022)\n",
      "In human_scientist.csv, question 774: response: Unknown (No specific employer information provided)\n",
      "In human_scientist.csv, question 776: response: Unknown (no publicly available information)\n",
      "In human_scientist.csv, question 777: response: Unknown (information not readily available)\n",
      "In human_scientist.csv, question 785: response: Unknown (as of 2021)\n",
      "In human_scientist.csv, question 822: response: Unknown (No specific employer information provided)\n",
      "In human_scientist.csv, question 831: response: Nikolai Nadezhdin's place of burial is unknown\n",
      "In human_scientist.csv, question 844: response: Unknown (Secular/Atheist is often speculated)\n",
      "In human_scientist.csv, question 845: response: Unknown\n",
      "In human_scientist.csv, question 853: response: Employer (of Edward Waring): Unknown (Mathematician\n",
      "In human_scientist.csv, question 879: response: Unknown (No publicly stated religious affiliation)\n",
      "In human_scientist.csv, question 891: response: Unknown (No specific information available)\n",
      "In human_scientist.csv, question 894: response: Unknown (murdered)\n",
      "In human_scientist.csv, question 897: response: Unknown (no publicly available information)\n",
      "In human_scientist.csv, question 967: response: Unknown (Specific information not readily available)\n",
      "In human_scientist.csv, question 980: response: Unknown (Specific information not provided)\n",
      "In human_scientist.csv, question 1011: response: Unknown (No specific employer information provided)\n",
      "In human_scientist.csv, question 1019: response: Unknown\n",
      "In human_scientist.csv, question 1029: response: Unknown (No publicly stated religion or worldview)\n",
      "In human_scientist.csv, question 1033: response: Unknown\n",
      "In art_sculpture.csv, question 6: response: Unknown (Cultural artifact)\n",
      "In art_sculpture.csv, question 41: response: Unknown (Lost to history)\n",
      "In art_sculpture.csv, question 78: response: Unknown (Italian statue with anonymous inscriptions)\n",
      "In art_sculpture.csv, question 79: response: Unknown (Medieval artist)\n",
      "In art_sculpture.csv, question 92: response: Unknown (Anonymous)\n",
      "In art_sculpture.csv, question 126: response: Riace bronzes' creator is unknown\n",
      "In art_sculpture.csv, question 161: response: Unknown (Anonymous)\n",
      "In art_sculpture.csv, question 196: response: Unknown (No specific creator is attributed to the Earl Haig Memorial.)\n",
      "In art_sculpture.csv, question 199: response: Unknown\n",
      "In art_sculpture.csv, question 203: response: Unknown\n",
      "In art_sculpture.csv, question 209: response: Unknown (attributed to Tlalocan Quematzin\n",
      "In art_sculpture.csv, question 211: response: Unknown (Lost original) - Priscilla Susan Bury (Re\n",
      "In art_sculpture.csv, question 245: response: Unknown (Anonymous)\n",
      "In art_sculpture.csv, question 291: response: Unknown (not identified)\n",
      "In art_sculpture.csv, question 313: response: Unknown without specific context\n",
      "In art_sculpture.csv, question 327: response: Unknown without context\n",
      "In art_sculpture.csv, question 334: response: Unknown (Biblical figure)\n",
      "In art_sculpture.csv, question 360: response: Unknown (Information not provided)\n",
      "In art_sculpture.csv, question 372: response: Unknown\n",
      "In health_disease.csv, question 7: response: Unknown (Historical)\n",
      "In health_disease.csv, question 23: response: Unknown (complex and multifactorial)\n",
      "In health_disease.csv, question 57: response: Unknown (often benign, but can be malignant; causes\n",
      "In health_disease.csv, question 117: response: Unknown (heterogeneous)\n",
      "In health_disease.csv, question 191: response: Unknown (Multifactorial)\n",
      "In health_disease.csv, question 312: response: Unknown (complex genetic factors)\n",
      "In health_disease.csv, question 346: response: Unknown (varies with specific conditions)\n",
      "In health_disease.csv, question 362: response: Unknown (varies for individuals)\n",
      "In health_disease.csv, question 392: response: Unknown (varies widely)\n",
      "In geography_volcano.csv, question 187: response: Unknown (Island, not a discovery or invention)\n",
      "In geography_volcano.csv, question 385: response: Unknown information\n",
      "In geography_volcano.csv, question 390: response: Momotombo Volcano Observatory is unknown\n",
      "In business_brand.csv, question 508: response: Unknown (acronym CUFFS doesn't refer to a\n",
      "In business_brand.csv, question 552: response: Unknown\n",
      "In business_brand.csv, question 754: response: Unknown (no location named Oakpont)\n",
      "In business_brand.csv, question 758: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 101: response: Health status unknown\n",
      "In human_writer.csv, question 111: response: Unknown (No information provided about his military or police rank.)\n",
      "In human_writer.csv, question 136: response: Unknown (She is a retired U.S. Supreme Court Justice, not\n",
      "In human_writer.csv, question 188: response: Health status unknown\n",
      "In human_writer.csv, question 209: response: Unknown (No specific public declaration)\n",
      "In human_writer.csv, question 268: response: Unknown (Specific military or police rank not provided)\n",
      "In human_writer.csv, question 271: response: Unknown (Nixon was not publicly known for adhering to a specific\n",
      "In human_writer.csv, question 316: response: Unknown (Public information about her personal beliefs is not readily available.)\n",
      "In human_writer.csv, question 369: response: Unknown (No specific public statement on record)\n",
      "In human_writer.csv, question 405: response: Unknown without specific context\n",
      "In human_writer.csv, question 465: response: Unknown (No specific religious affiliation documented)\n",
      "In human_writer.csv, question 478: response: Health condition unknown (as of my last update)\n",
      "In human_writer.csv, question 495: response: Unknown\n",
      "In human_writer.csv, question 524: response: Unknown (Specific religious affiliation not publicly known)\n",
      "In human_writer.csv, question 549: response: Unknown (Publicly, he identifies as a Catholic but also discusses\n",
      "In human_writer.csv, question 575: response: Unknown (Specific religious affiliation not well-documented)\n",
      "In human_writer.csv, question 587: response: Unknown (Public information about her personal beliefs is not readily available.)\n",
      "In human_writer.csv, question 595: response: Party (of Willoughby Newton) is unknown\n",
      "In human_writer.csv, question 600: response: Willoughby Newton's place of death is unknown\n",
      "In human_writer.csv, question 602: response: Unknown (Information not provided)\n",
      "In human_writer.csv, question 617: response: Unknown (No specific religious affiliation known)\n",
      "In human_writer.csv, question 627: response: Unknown (No specific public statement)\n",
      "In human_writer.csv, question 675: response: Unknown (No specific information available)\n",
      "In human_writer.csv, question 685: response: Unknown (provide specific context for accurate response)\n",
      "In human_writer.csv, question 686: response: Unknown (Specific rank not provided)\n",
      "In human_writer.csv, question 697: response: Unknown (Public record does not provide this information)\n",
      "In human_writer.csv, question 701: response: Unknown (provide specific date and context for accurate answer)\n",
      "In human_writer.csv, question 708: response: Unknown\n",
      "In human_writer.csv, question 718: response: Unknown (No public record)\n",
      "In human_writer.csv, question 726: response: Unknown\n",
      "In human_writer.csv, question 738: response: Unknown (Public records do not provide specific information on his personal worldview or\n",
      "In human_writer.csv, question 797: response: Unknown (Cochran was a pilot and aviation administrator, not\n",
      "In human_writer.csv, question 848: response: Unknown (Information not provided)\n",
      "In human_writer.csv, question 870: response: Unknown (Mystery)\n",
      "In human_writer.csv, question 873: response: Unknown (Public information does not specify)\n",
      "In human_writer.csv, question 878: response: Unknown (provided no personal information is publicly available)\n",
      "In human_writer.csv, question 899: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 906: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 916: response: Male (Albert Einstein) or Unknown (Albert Johnson, a generic\n",
      "In human_writer.csv, question 928: response: Unknown (No publicly available information)\n",
      "In human_writer.csv, question 932: response: Unknown without specific context\n",
      "In human_writer.csv, question 936: response: Unknown (Information not provided)\n",
      "In human_writer.csv, question 947: response: Unknown without specific context\n",
      "In human_writer.csv, question 950: response: Unknown without specific context\n",
      "In human_writer.csv, question 994: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 998: response: Unknown\n",
      "In human_writer.csv, question 1014: response: Unknown (provide specific context)\n",
      "In human_writer.csv, question 1019: response: Unknown (Public information not available)\n",
      "In human_writer.csv, question 1025: response: Not specified (Edward Lucas is a pseudonym, real identity unknown)\n",
      "In human_writer.csv, question 1042: response: Edwards' work location is unknown\n",
      "In human_writer.csv, question 1075: response: Unknown\n",
      "In human_writer.csv, question 1123: response: Unknown (Information not provided)\n",
      "In human_writer.csv, question 1150: response: Unknown\n",
      "In human_writer.csv, question 1166: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 1199: response: Unknown (No publicly available information on his religious or worldview beliefs.)\n",
      "In human_writer.csv, question 1226: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 1235: response: Unknown (public information not available)\n",
      "In human_writer.csv, question 1262: response: Unknown\n",
      "In human_writer.csv, question 1284: response: Unknown\n",
      "In human_writer.csv, question 1291: response: Unknown\n",
      "In human_writer.csv, question 1296: response: Unknown (Specific religious affiliation not provided)\n",
      "In human_writer.csv, question 1305: response: Unknown (Public information does not specify a specific religion or worldview for Mar\n",
      "In human_writer.csv, question 1314: response: Unknown\n",
      "In human_writer.csv, question 1326: response: Unknown without specific context\n",
      "In human_writer.csv, question 1356: response: Unknown\n",
      "In human_writer.csv, question 1377: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 1398: response: Unknown (Specific religious affiliation not publicly disclosed)\n",
      "In human_writer.csv, question 1407: response: Unknown (Publicly stated as \"The School of Hard Knocks\")\n",
      "In human_writer.csv, question 1408: response: Unknown\n",
      "In human_writer.csv, question 1450: response: Unknown (Public information about his religious beliefs is not readily available.)\n",
      "In human_writer.csv, question 1467: response: Unknown without specific context\n",
      "In human_writer.csv, question 1483: response: Unknown (No publicly stated religion or worldview)\n",
      "In human_writer.csv, question 1513: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 1524: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 1529: response: Unknown (as per publicly available information)\n",
      "In human_writer.csv, question 1532: response: Unknown (No public record)\n",
      "In human_writer.csv, question 1546: response: Unknown (No public record available)\n",
      "In human_writer.csv, question 1551: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 1553: response: Hempstead Washburne's place of death is unknown\n",
      "In human_writer.csv, question 1554: response: Unknown (Specify the context or subject of Herbert B. Maw to\n",
      "In human_writer.csv, question 1582: response: Unknown (provided no specific party information)\n",
      "In human_writer.csv, question 1585: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 1587: response: Unknown (provided no specific employer was given in the context)\n",
      "In human_writer.csv, question 1599: response: Unknown (Public information is limited)\n",
      "In human_writer.csv, question 1601: response: Unknown (for business); None (for sport)\n",
      "In human_writer.csv, question 1622: response: Unknown (Specific rank not provided)\n",
      "In human_writer.csv, question 1633: response: Unknown (No specific hand mentioned in the question)\n",
      "In human_writer.csv, question 1639: response: Unknown (No specific party information available)\n",
      "In human_writer.csv, question 1654: response: Unknown (Public information does not specify her religion or worldview.)\n",
      "In human_writer.csv, question 1672: response: Unknown (No publicly available information)\n",
      "In human_writer.csv, question 1674: response: Unknown\n",
      "In human_writer.csv, question 1676: response: Unknown (No publicly available information on political party affiliation for Artur Tay\n",
      "In human_writer.csv, question 1685: response: Unknown (No specific military or police rank is publicly known for Grigory\n",
      "In human_writer.csv, question 1708: response: Unknown\n",
      "In human_writer.csv, question 1713: response: Unknown (No publicly known political party affiliation for Alexander Karelin)\n",
      "In human_writer.csv, question 1715: response: Unknown\n",
      "In human_writer.csv, question 1721: response: Unknown\n",
      "In human_writer.csv, question 1723: response: Unknown\n",
      "In human_writer.csv, question 1725: response: Unknown\n",
      "In human_writer.csv, question 1728: response: Unknown. (No specific military service information is publicly available about Boris G\n",
      "In human_writer.csv, question 1743: response: Unknown (No publicly available information)\n",
      "In human_writer.csv, question 1762: response: Unknown (as of current information)\n",
      "In human_writer.csv, question 1764: response: Unknown (provided no additional information)\n",
      "In human_writer.csv, question 1785: response: Unknown (No specific information available)\n",
      "In human_writer.csv, question 1788: response: Unknown\n",
      "In human_writer.csv, question 1808: response: Unknown (Publicly, he identifies as Russian Orthodox Christian)\n",
      "In human_writer.csv, question 1825: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 1833: response: Unknown (as of current information)\n",
      "In human_writer.csv, question 1841: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 1870: response: Unknown (Political party information not provided in the question)\n",
      "In human_writer.csv, question 1880: response: Health status unknown\n",
      "In human_writer.csv, question 1909: response: Unknown (provided no specific employer was given in the context)\n",
      "In human_writer.csv, question 1910: response: Unknown (as of my last update)\n",
      "In human_writer.csv, question 1917: response: Unknown\n",
      "In human_writer.csv, question 1935: response: Unknown (Specific religious affiliation not publicly disclosed)\n",
      "In human_writer.csv, question 1971: response: Unknown (No specific employer information provided)\n",
      "In human_writer.csv, question 1973: response: Unknown (No specific military branch information provided)\n",
      "In human_writer.csv, question 1998: response: Unknown\n",
      "In human_writer.csv, question 1999: response: Unknown (No specific party information available)\n",
      "In human_writer.csv, question 2004: response: Unknown (publicly)\n",
      "In human_writer.csv, question 2006: response: Unknown\n",
      "In human_writer.csv, question 2007: response: Unknown (provide specific context)\n",
      "In human_writer.csv, question 2017: response: Unknown (as provided in the book \"The Martian Chronicles\" by\n",
      "In human_writer.csv, question 2046: response: Unknown\n",
      "In human_writer.csv, question 2087: response: Unknown (No specific employer information provided)\n",
      "In human_writer.csv, question 2088: response: Unknown (Not publicly disclosed)\n",
      "In human_writer.csv, question 2093: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 2100: response: Unknown (provided no specific party affiliation was given for Vasily Ale\n",
      "In human_writer.csv, question 2103: response: Unknown (She is a Russian athlete, not a military or police officer\n",
      "In human_writer.csv, question 2104: response: Unknown (No publicly available information)\n",
      "In human_writer.csv, question 2118: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 2126: response: Unknown (No information provided about a conflict involving Sergei Shoigu)\n",
      "In human_writer.csv, question 2130: response: Unknown (Not publicly disclosed)\n",
      "In human_writer.csv, question 2145: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 2148: response: Unknown\n",
      "In human_writer.csv, question 2165: response: Unknown (No specific party information provided)\n",
      "In human_writer.csv, question 2193: response: Unknown\n",
      "In human_writer.csv, question 2196: response: Unknown (No specific employer information provided)\n",
      "In human_writer.csv, question 2206: response: Unknown (as of my last update)\n",
      "In human_writer.csv, question 2209: response: Unknown (Secular or Atheist, according to some sources)\n",
      "In human_writer.csv, question 2242: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 2243: response: Unknown (No specific employer provided)\n",
      "In human_writer.csv, question 2248: response: Unknown (No specific information provided)\n",
      "In human_writer.csv, question 2254: response: Unknown\n",
      "In human_writer.csv, question 2268: response: Unknown\n",
      "In human_writer.csv, question 2273: response: Unknown\n",
      "In human_writer.csv, question 2277: response: Unknown\n",
      "In human_writer.csv, question 2291: response: Unknown (No specific employer information provided)\n",
      "In human_writer.csv, question 2302: response: Unknown (provided no specific child is mentioned in context)\n",
      "In human_writer.csv, question 2305: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 2312: response: Unknown (He is a former ice hockey player, not a military or police\n",
      "In human_writer.csv, question 2315: response: Unknown (No specific political party information available for Leonid Leonov)\n",
      "In human_writer.csv, question 2363: response: Unknown (Public information is limited)\n",
      "In human_writer.csv, question 2370: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 2372: response: Unknown information\n",
      "In human_writer.csv, question 2399: response: Unknown (No publicly known political party affiliation)\n",
      "In human_writer.csv, question 2420: response: Unknown (no specific employer provided)\n",
      "In human_writer.csv, question 2427: response: Unknown\n",
      "In human_writer.csv, question 2434: response: Unknown (Information not provided)\n",
      "In human_writer.csv, question 2448: response: Unknown (provided no specific party information)\n",
      "In human_writer.csv, question 2464: response: Unknown (No public record)\n",
      "In human_writer.csv, question 2465: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 2471: response: Unknown\n",
      "In human_writer.csv, question 2478: response: Unknown (No publicly available information suggests he served in the military.)\n",
      "In human_writer.csv, question 2493: response: Unknown\n",
      "In human_writer.csv, question 2500: response: Unknown (as specific party information isn't readily available)\n",
      "In human_writer.csv, question 2501: response: Unknown\n",
      "In human_writer.csv, question 2512: response: Irina Arkhipova's employer is unknown from the provided context\n",
      "In human_writer.csv, question 2524: response: Unknown (Public information not readily available)\n",
      "In human_writer.csv, question 2542: response: Unknown (Public figure, no specific employer provided)\n",
      "In human_writer.csv, question 2567: response: Unknown\n",
      "In human_writer.csv, question 2569: response: Unknown (No publicly available information)\n",
      "In human_writer.csv, question 2593: response: Unknown (as of 2022)\n",
      "In human_writer.csv, question 2597: response: Unknown (provide specific context or title of the work)\n",
      "In human_writer.csv, question 2601: response: Unknown\n",
      "In human_writer.csv, question 2608: response: Unknown (as of my last update)\n",
      "In human_writer.csv, question 2648: response: Unknown (provided no specific party is mentioned in context)\n",
      "In human_writer.csv, question 2681: response: Unknown (provided no specific employer was given in the context)\n",
      "In technology_software.csv, question 94: response: Unknown (Napster's history involves multiple legal disputes and changes\n",
      "In technology_software.csv, question 404: response: Unknown (No specific parent organization mentioned in the provided information about TeamSense\n",
      "In human_entrepreneur.csv, question 22: response: Unknown (as of current information)\n",
      "In human_entrepreneur.csv, question 25: response: Unknown (as of current information)\n",
      "In human_entrepreneur.csv, question 62: response: Unknown (Norwegian far-right terrorist)\n",
      "In human_entrepreneur.csv, question 87: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 88: response: Unknown. Please provide specific details\n",
      "In human_entrepreneur.csv, question 89: response: Unknown\n",
      "In human_entrepreneur.csv, question 91: response: Unknown (as of March 2023)\n",
      "In human_entrepreneur.csv, question 99: response: Unknown (No publicly available information)\n",
      "In human_entrepreneur.csv, question 109: response: Not provided (or unknown)\n",
      "In human_entrepreneur.csv, question 112: response: Unknown. No specific information provided\n",
      "In human_entrepreneur.csv, question 160: response: Unknown (Information not provided)\n",
      "In human_entrepreneur.csv, question 191: response: Unknown (Specific religious affiliation not documented)\n",
      "In human_entrepreneur.csv, question 206: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 233: response: Unknown (Information not provided)\n",
      "In human_entrepreneur.csv, question 234: response: Unknown (as of my last update). Martin Schlaff is a businessman\n",
      "In human_entrepreneur.csv, question 250: response: Unknown (Publicly, he identifies as a Catholic but also discusses\n",
      "In human_entrepreneur.csv, question 275: response: Unknown\n",
      "In human_entrepreneur.csv, question 278: response: Unknown (public information not available)\n",
      "In human_entrepreneur.csv, question 327: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 332: response: Unknown (provided name does not correspond to any known political figure)\n",
      "In human_entrepreneur.csv, question 349: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 374: response: Unknown (Information not provided)\n",
      "In human_entrepreneur.csv, question 381: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 385: response: Unknown (public information not readily available)\n",
      "In human_entrepreneur.csv, question 388: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 394: response: Unknown (Specific religious information not readily available)\n",
      "In human_entrepreneur.csv, question 413: response: Unknown (No publicly available information)\n",
      "In human_entrepreneur.csv, question 418: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 436: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 439: response: Unknown (as of my last update)\n",
      "In human_entrepreneur.csv, question 496: response: Unknown\n",
      "In human_entrepreneur.csv, question 497: response: Unknown (No publicly available information)\n",
      "In human_entrepreneur.csv, question 501: response: Unknown\n",
      "In human_entrepreneur.csv, question 509: response: Unknown (No specific hand provided in the question)\n",
      "In human_entrepreneur.csv, question 510: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 529: response: Unknown\n",
      "In human_entrepreneur.csv, question 559: response: Unknown (Public information about her personal beliefs is not readily available.)\n",
      "In human_entrepreneur.csv, question 575: response: Unknown (Information not provided)\n",
      "In human_entrepreneur.csv, question 577: response: Unknown (No publicly available information)\n",
      "In human_entrepreneur.csv, question 587: response: Unknown (No specific party information provided)\n",
      "In human_entrepreneur.csv, question 612: response: Unknown\n",
      "In human_entrepreneur.csv, question 617: response: Unknown (historical figure)\n",
      "In human_entrepreneur.csv, question 634: response: Unknown (No publicly known spouse for Adolf Magnus d'Hoy\n",
      "In human_entrepreneur.csv, question 639: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 653: response: Unknown (Not publicly disclosed)\n",
      "In human_entrepreneur.csv, question 674: response: Unknown (provided no specific information was given)\n",
      "In human_entrepreneur.csv, question 694: response: Unknown (provided no information was given)\n",
      "In human_entrepreneur.csv, question 728: response: Unknown (Information not provided)\n",
      "In human_entrepreneur.csv, question 738: response: Unknown (No information available about Friedrich Bayer's gender or sex.)\n",
      "In human_entrepreneur.csv, question 757: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 759: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 775: response: Unknown (Information not provided)\n",
      "In human_entrepreneur.csv, question 777: response: Unknown (No specific child is publicly known for Adolph von Hansemann\n",
      "In human_entrepreneur.csv, question 783: response: Unknown\n",
      "In human_entrepreneur.csv, question 790: response: Unknown (adopted)\n",
      "In human_entrepreneur.csv, question 805: response: Unknown (No definitive information available)\n",
      "In human_entrepreneur.csv, question 812: response: Residence (unknown)\n",
      "In human_entrepreneur.csv, question 819: response: Unknown (No publicly stated religion or worldview)\n",
      "In human_entrepreneur.csv, question 832: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 833: response: Unknown information\n",
      "In human_entrepreneur.csv, question 834: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 842: response: Anton Magnis' work location is unknown\n",
      "In human_entrepreneur.csv, question 845: response: Unknown (No specific party information available for Anton Magnis.)\n",
      "In human_entrepreneur.csv, question 850: response: Unknown (No specific work location provided)\n",
      "In human_entrepreneur.csv, question 852: response: Unknown (provided no information)\n",
      "In human_entrepreneur.csv, question 856: response: Unknown (No specific party mentioned)\n",
      "In human_entrepreneur.csv, question 873: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 884: response: Unknown (No publicly available information)\n",
      "In human_entrepreneur.csv, question 897: response: Unknown (Specific religious affiliation not publicly disclosed)\n",
      "In human_entrepreneur.csv, question 913: response: Unknown (as of 2022)\n",
      "In human_entrepreneur.csv, question 921: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 950: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 953: response: Strousberg (financier)'s place of death is unknown\n",
      "In human_entrepreneur.csv, question 959: response: Unknown (historical figure)\n",
      "In human_entrepreneur.csv, question 983: response: Unknown (No specific Kurt von Kleefeld with a known political party is\n",
      "In human_entrepreneur.csv, question 990: response: Unknown (No publicly known political party affiliation for Emich, 5\n",
      "In human_entrepreneur.csv, question 1041: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 1047: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 1057: response: Health status unknown\n",
      "In human_entrepreneur.csv, question 1068: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 1073: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 1074: response: Unknown (No publicly available information)\n",
      "In human_entrepreneur.csv, question 1100: response: Unknown (provided no specific employer was given in the context)\n",
      "In human_entrepreneur.csv, question 1127: response: Unknown (No public record of military or police service for Hans Heinrich XI,\n",
      "In human_entrepreneur.csv, question 1129: response: Unknown (No specific political party information available for Hans Heinrich XI, 2\n",
      "In human_entrepreneur.csv, question 1131: response: Not provided (or unknown)\n",
      "In human_entrepreneur.csv, question 1150: response: Unknown (as of my last update)\n",
      "In human_entrepreneur.csv, question 1160: response: Unknown (Prince, not publicly disclosed)\n",
      "In human_entrepreneur.csv, question 1172: response: Unknown\n",
      "In human_entrepreneur.csv, question 1190: response: Unknown (private information)\n",
      "In human_entrepreneur.csv, question 1205: response: Mayer's residence is unknown\n",
      "In human_entrepreneur.csv, question 1225: response: Unknown (not provided in the question)\n",
      "In human_entrepreneur.csv, question 1233: response: Unknown\n",
      "In human_entrepreneur.csv, question 1234: response: Unknown (No specific information provided)\n",
      "In human_entrepreneur.csv, question 1240: response: Unknown (provide specific party name)\n",
      "In human_entrepreneur.csv, question 1249: response: Unknown\n",
      "In human_entrepreneur.csv, question 1252: response: Unknown (Information not provided)\n",
      "In human_entrepreneur.csv, question 1298: response: Unknown\n",
      "In human_entrepreneur.csv, question 1314: response: Günther Fielmann's place of death is unknown (as of\n",
      "In human_entrepreneur.csv, question 1316: response: Unknown (No specific public statement found)\n",
      "In human_entrepreneur.csv, question 1326: response: Unknown (No publicly stated religion or worldview)\n",
      "In human_entrepreneur.csv, question 1339: response: Unknown\n",
      "In human_entrepreneur.csv, question 1341: response: Unknown\n",
      "In human_entrepreneur.csv, question 1360: response: Unknown (historical figure)\n",
      "In human_entrepreneur.csv, question 1367: response: Unknown (information not provided)\n",
      "In human_entrepreneur.csv, question 1377: response: Unknown (Hevelius' student is not specifically named in historical records)\n",
      "In human_entrepreneur.csv, question 1382: response: Unknown (Information not provided)\n",
      "In human_entrepreneur.csv, question 1391: response: Unknown (No specific party information provided)\n",
      "In human_entrepreneur.csv, question 1436: response: Unknown (No specific public statement)\n",
      "In human_entrepreneur.csv, question 1452: response: Unknown (Gender not specified)\n",
      "In human_entrepreneur.csv, question 1465: response: Unknown (No specific academic major information available for Ferdinand Schichau.)\n",
      "In human_entrepreneur.csv, question 1470: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 1476: response: Unknown (No specific information available)\n",
      "In human_entrepreneur.csv, question 1497: response: Unknown (No specific party information available)\n",
      "In human_entrepreneur.csv, question 1501: response: Unknown (provided no information)\n",
      "In geography_glacier.csv, question 37: response: Glacier (origin unknown)\n",
      "In geography_glacier.csv, question 54: response: Unknown (Discovery is attributed to explorers Malaspina and B\n"
     ]
    }
   ],
   "source": [
    "# Detect if model outputs contain 'unknown', output from mistral_7b_instruct_v0.3\n",
    "for filename in os.listdir(folder_unfiltered):\n",
    "    df = pd.read_csv(os.path.join(folder_unfiltered, filename))\n",
    "    for i in df.index[:]:\n",
    "        output_qa = df.loc[i, f\"output_{model_id_format}\"]\n",
    "        if 'unknown' in output_qa.lower():\n",
    "            print(f'In {filename}, question {i}: response: {output_qa}')\n",
    "            df = df.drop(i)  # remove rows with 'unknown' in output_qa\n",
    "    df.to_csv(os.path.join(folder_unfiltered, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May generate unreleated distractors\n",
    "# Create a question with four answer options. Include the correct answer (the object in the triplet), one provided wrong answer, and two additional distractors.\n",
    "ls_multiple_choices, ls_multiple_choice_labels = [], []\n",
    "for i in df_hallu.index[:]:\n",
    "    subject, relation, label, question = df_hallu.loc[i, 'subject'], df_hallu.loc[i, 'relation'], df_hallu.loc[i, 'object'], df_hallu.loc[i, 'question']\n",
    "    wrong_ans = df_hallu.loc[i, f'output_{model_id_format}']\n",
    "\n",
    "    distractor_ls = df_hallu['object'].tolist()\n",
    "    distractor_ls = [e for e in distractor_ls if label.lower() not in e.lower() and e.lower() not in label.lower()]\n",
    "    distractor_ls = [e for e in distractor_ls if wrong_ans.lower() not in e.lower() and e.lower() not in wrong_ans.lower()]\n",
    "    distractor1, distractor2 = random.sample(distractor_ls, 2)\n",
    "    choice = [label, wrong_ans, distractor1, distractor2]\n",
    "    MC_dict = {\"0\": \"A\", \"1\": \"B\", \"2\": \"C\", \"3\": \"D\"}\n",
    "    random.shuffle(choice)\n",
    "    correct_answer = MC_dict[str(choice.index(label))]\n",
    "    choice_str = \"\"\n",
    "    for i in range(4):\n",
    "        choice_str += (MC_dict[str(i)] + \". \" + choice[i] + \"  \")\n",
    "    # print(choice_str)\n",
    "    ls_multiple_choices.append(choice_str.strip())\n",
    "    ls_multiple_choice_labels.append(correct_answer)\n",
    "    print(f\"choice_str: {choice_str}\")\n",
    "    # if correct_answer != label:\n",
    "    #     print(f\"correct_answer: {correct_answer}, label: {label}\")\n",
    "len(ls_multiple_choices), len(ls_multiple_choice_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where is the Cloud Gate located? Follow-up response: Chicago\n",
      "Question: Are you sure? Follow-up response: Yes\n",
      "Question: The answer is wrong. Please give the correct answer. Follow-up response: Millennium Park\n",
      "Question: The correct answer is Paris, isn’t it? Follow-up response: No\n"
     ]
    }
   ],
   "source": [
    "# multi-turn conversation\n",
    "target = \"Paris\"\n",
    "question = \"Where is the Cloud Gate located?\"\n",
    "messages_qa = [{\"role\": \"system\", \"content\": system_msg_qa}, {\"role\": \"user\", \"content\": question}]\n",
    "current_output = get_response(model_qa, tok_qa, messages_qa, max_new_tokens=16)\n",
    "print(f\"Question: {question} Follow-up response: {current_output}\")\n",
    "\n",
    "# Add more follow-up questions in a loop\n",
    "follow_up_questions = [\n",
    "    \"Are you sure?\",\n",
    "    \"The answer is wrong. Please give the correct answer.\",\n",
    "    f\"The correct answer is {target}, isn’t it?\",\n",
    "]\n",
    "\n",
    "for question in follow_up_questions:\n",
    "    messages_qa.append({\"role\": \"assistant\", \"content\": current_output})\n",
    "    messages_qa.append({\"role\": \"user\", \"content\": question})\n",
    "    current_output = get_response(model_qa, tok_qa, messages_qa, max_new_tokens=32)\n",
    "    print(f\"Question: {question} Follow-up response: {current_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luang Prabang Province.\n",
      "Laos.\n",
      "Islamabad.\n",
      "Istanbul.\n",
      "Canada\n",
      "England.\n"
     ]
    }
   ],
   "source": [
    "# f'Question: {prompt_qa}. Answer:' makes a difference\n",
    "for prompt_qa in ['What is the located in the administrative territorial entity of Luang Prabang?', 'Which city was named after Islam?', 'What is the country of Windsor?']:\n",
    "    messages_qa = [{\"role\": \"system\", \"content\": system_msg_qa}, {\"role\": \"user\", \"content\": f'Question: {prompt_qa}. Answer:'}]\n",
    "    output_qa = get_response(model_qa, tok_qa, messages_qa, max_new_tokens=16)\n",
    "    print(output_qa)\n",
    "\n",
    "    messages_qa = [{\"role\": \"system\", \"content\": system_msg_qa}, {\"role\": \"user\", \"content\": prompt_qa}]\n",
    "    output_qa = get_response(model_qa, tok_qa, messages_qa, max_new_tokens=16)\n",
    "    print(output_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e83176bfa84e0897715022a0626ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d408fd9c5aa44b6684ed0bb716a6d5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Apple'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "device = 'cuda:5'\n",
    "model_id_eval = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id_eval)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id_eval, torch_dtype='auto').to(device)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"you're an assistant\"}, {\"role\": \"user\", \"content\": \"Who controls the Apple company?\"}]\n",
    "msg_tokenized = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = model.generate(msg_tokenized.to(device), max_new_tokens=2, do_sample=False, pad_token_id=tok.eos_token_id)\n",
    "tok.decode(output_ids[0][msg_tokenized.shape[-1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<|begin_of_text|>Who controls the Apple company? The Apple company is a publicly traded company, listed on the NAS',\n",
       " ' The Apple company is a publicly traded company, listed on the NAS')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_tokenized = tok(\"Who controls the Apple company?\", return_tensors='pt')\n",
    "output_ids = model.generate(**msg_tokenized.to(device))\n",
    "# tok.decode(output_ids[0][msg_tokenized.shape[-1]:], skip_special_tokens=True)\n",
    "tok.decode(output_ids[0]), tok.decode(output_ids[0][msg_tokenized['input_ids'].shape[-1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question\\n##ting-edge provides: March 122\\n\\nTopic's: 02 Dec 2023\\n\\nThe are a AI toassistant\\n\\nI is the internet Watch?assistant\\n\\nThe\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# msg_tokenized = tok(\"Who is the US president\", return_tensors='pt').to(device)\n",
    "messages = [{\"role\": \"system\", \"content\": \"you're an assistant\"}, {\"role\": \"user\", \"content\": \"Who controls the Apple company?\"}]\n",
    "msg_tokenized = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt', return_dict=True).to(device)\n",
    "outputs = model(**msg_tokenized)\n",
    "if type(outputs) is torch.Tensor:\n",
    "    logits = outputs\n",
    "else:\n",
    "    logits = outputs.logits\n",
    "answers = torch.argmax(logits, dim=-1)\n",
    "tok.decode(answers[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271,   9514,   2351,\n",
       "            459,  18328, 128009, 128006,    882, 128007,    271,  15546,  11835,\n",
       "            279,   8325,   2883,     30, 128009, 128006,  78191, 128007,    271]],\n",
       "       device='cuda:5'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:5')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"you're an assistant\"}, {\"role\": \"user\", \"content\": \"Who controls the Apple company?\"}]\n",
    "msg_tokenized = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt', return_dict=True).to(device)\n",
    "msg_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question<|start_header_id|>\\n<|start_header_id|>##ting-edge provides: March 122\\n\\nTopic's: 02 Dec 2023\\n\\nThe are a AI to<|start_header_id|>assistant<|start_header_id|>\\n\\nI is the internet Watch?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**msg_tokenized)\n",
    "if type(outputs) is torch.Tensor:\n",
    "    logits = outputs\n",
    "else:\n",
    "    logits = outputs.logits\n",
    "\n",
    "for i in\n",
    "next_token_logits = [:, -1, :]  # Logits for the next token\n",
    "next_token = torch.argmax(next_token_logits, dim=-1)  # Get the token with the highest probability\n",
    "\n",
    "# Decode the token to string\n",
    "tok.decode(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_topics = pd.DataFrame()\n",
    "for filename in os.listdir(\"../data/questions/wh_only/mistral_7b_instruct_v0.3\"):\n",
    "    tmp = pd.read_csv(f\"../data/questions/wh_only/mistral_7b_instruct_v0.3/{filename}\")\n",
    "    tmp['topic'] = domain_topic_name\n",
    "    df_all_topics = pd.concat([df_all_topics, tmp], axis=0)\n",
    "    print(filename, df_all_topics.shape)\n",
    "\n",
    "df_all_topics = df_all_topics['topic', 'type', 'subject', 'relation', 'object', 'question', 'label', f'output_{model_id_format}']\n",
    "df_all_topics.to_csv(f\"../data/questions/wh_only/all_topics_{model_id_format}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f\"../data/questions/{topic_name}_questions.json\", lines=True)\n",
    "df_wh = df[df.type=='wh'].copy()\n",
    "len(df), len(df_wh)\n",
    "\n",
    "ls_output = []\n",
    "for i in tqdm(df_wh.index):\n",
    "    question_type = df_wh.loc[i, 'type']\n",
    "    question, label = df_wh.loc[i, 'question'], df_wh.loc[i, 'label']\n",
    "    if question_type == \"MC\":\n",
    "        current_prompt = MC_content + \"\\nQuestion:\" + question \n",
    "    elif question_type == \"yes_no\":\n",
    "        current_prompt = yes_no_content + \"\\nQuestion:\" + question\n",
    "    else:\n",
    "        current_prompt = Wh_content + \"\\nQuestion:\" + question\n",
    "        \n",
    "    messages = [{\"role\": \"system\", \"content\": system_msg_qa}, {\"role\": \"user\", \"content\": current_prompt}]\n",
    "    msg_tokenized = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt')\n",
    "    output_ids = model.generate(msg_tokenized.to(device), max_new_tokens=32, eos_token_id=terminators, do_sample=False, pad_token_id=tok.eos_token_id)\n",
    "    output_decoded = tok.decode(output_ids[0][msg_tokenized.shape[-1]:], skip_special_tokens=True)\n",
    "    ls_output.append(output_decoded)\n",
    "    # print(f\"Question: {question} Label: {label} | Prediction: {output_decoded}\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "df_wh[f\"output_{model_id_format}\"] = ls_output\n",
    "df_wh.to_csv(f\"../data/questions/wh_only/{topic_name}_{model_id_format}.csv\", index=False)\n",
    "df_wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # if not os.path.exists(f\"../data/questions/wh_only/{model_id_format}\"):\n",
    "        #     os.makedirs(f\"../data/questions/wh_only/{model_id_format}\")\n",
    "        # df_wh.to_csv(f\"../data/questions/wh_only/{model_id_format}/{domain_topic_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007613658905029297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2030e887047845deacc5b1f648ba36af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0027933120727539062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2219155fd4e247bd8b75c7fd02aaf6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "model | LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.embed_tokens | Embedding(128256, 4096)\n",
      "model.layers | ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaSdpaAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      ")\n",
      "model.layers.0 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.0.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.0.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.0.mlp.act_fn | SiLU()\n",
      "model.layers.0.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.0.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.1 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.1.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.1.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.1.mlp.act_fn | SiLU()\n",
      "model.layers.1.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.1.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.2 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.2.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.2.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.2.mlp.act_fn | SiLU()\n",
      "model.layers.2.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.2.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.3 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.3.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.3.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.3.mlp.act_fn | SiLU()\n",
      "model.layers.3.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.3.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.4 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.4.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.4.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.4.mlp.act_fn | SiLU()\n",
      "model.layers.4.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.4.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.5 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.5.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.5.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.5.mlp.act_fn | SiLU()\n",
      "model.layers.5.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.5.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.6 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.6.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.6.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.6.mlp.act_fn | SiLU()\n",
      "model.layers.6.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.6.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.7 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.7.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.7.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.7.mlp.act_fn | SiLU()\n",
      "model.layers.7.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.7.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.8 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.8.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.8.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.8.mlp.act_fn | SiLU()\n",
      "model.layers.8.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.8.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.9 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.9.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.9.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.9.mlp.act_fn | SiLU()\n",
      "model.layers.9.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.9.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.10 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.10.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.10.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.10.mlp.act_fn | SiLU()\n",
      "model.layers.10.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.10.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.11 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.11.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.11.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.11.mlp.act_fn | SiLU()\n",
      "model.layers.11.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.11.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.12 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.12.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.12.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.12.mlp.act_fn | SiLU()\n",
      "model.layers.12.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.12.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.13 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.13.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.13.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.13.mlp.act_fn | SiLU()\n",
      "model.layers.13.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.13.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.14 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.14.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.14.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.14.mlp.act_fn | SiLU()\n",
      "model.layers.14.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.14.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.15 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.15.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.15.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.15.mlp.act_fn | SiLU()\n",
      "model.layers.15.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.15.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.16 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.16.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.16.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.16.mlp.act_fn | SiLU()\n",
      "model.layers.16.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.16.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.17 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.17.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.17.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.17.mlp.act_fn | SiLU()\n",
      "model.layers.17.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.17.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.18 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.18.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.18.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.18.mlp.act_fn | SiLU()\n",
      "model.layers.18.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.18.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.19 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.19.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.19.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.19.mlp.act_fn | SiLU()\n",
      "model.layers.19.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.19.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.20 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.20.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.20.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.20.mlp.act_fn | SiLU()\n",
      "model.layers.20.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.20.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.21 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.21.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.21.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.21.mlp.act_fn | SiLU()\n",
      "model.layers.21.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.21.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.22 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.22.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.22.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.22.mlp.act_fn | SiLU()\n",
      "model.layers.22.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.22.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.23 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.23.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.23.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.23.mlp.act_fn | SiLU()\n",
      "model.layers.23.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.23.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.24 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.24.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.24.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.24.mlp.act_fn | SiLU()\n",
      "model.layers.24.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.24.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.25 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.25.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.25.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.25.mlp.act_fn | SiLU()\n",
      "model.layers.25.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.25.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.26 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.26.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.26.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.26.mlp.act_fn | SiLU()\n",
      "model.layers.26.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.26.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.27 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.27.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.27.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.27.mlp.act_fn | SiLU()\n",
      "model.layers.27.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.27.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.28 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.28.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.28.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.28.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.28.mlp.act_fn | SiLU()\n",
      "model.layers.28.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.28.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.29 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.29.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.29.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.29.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.29.mlp.act_fn | SiLU()\n",
      "model.layers.29.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.29.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.30 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.30.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.30.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.30.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.30.mlp.act_fn | SiLU()\n",
      "model.layers.30.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.30.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.31 | LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.31.self_attn | LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.31.self_attn.q_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.k_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.v_proj | Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.o_proj | Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb | LlamaRotaryEmbedding()\n",
      "model.layers.31.mlp | LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.up_proj | Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.down_proj | Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.31.mlp.act_fn | SiLU()\n",
      "model.layers.31.input_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.31.post_attention_layernorm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.norm | LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.rotary_emb | LlamaRotaryEmbedding()\n",
      "lm_head | Linear(in_features=4096, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "model_tmp = transformers.AutoModelForCausalLM.from_pretrained(model_ls[0]).to('cuda:5')\n",
    "for n, m in model_tmp.named_modules():\n",
    "    print(n, '|', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-41): 42 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "          (rotary_emb): Gemma2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
      ")\n",
      "model | Gemma2Model(\n",
      "  (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-41): 42 x Gemma2DecoderLayer(\n",
      "      (self_attn): Gemma2SdpaAttention(\n",
      "        (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "        (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "        (rotary_emb): Gemma2RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): Gemma2MLP(\n",
      "        (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "        (act_fn): PytorchGELUTanh()\n",
      "      )\n",
      "      (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "      (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "      (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "      (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.embed_tokens | Embedding(256000, 3584, padding_idx=0)\n",
      "model.layers | ModuleList(\n",
      "  (0-41): 42 x Gemma2DecoderLayer(\n",
      "    (self_attn): Gemma2SdpaAttention(\n",
      "      (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "      (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "      (rotary_emb): Gemma2RotaryEmbedding()\n",
      "    )\n",
      "    (mlp): Gemma2MLP(\n",
      "      (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "      (act_fn): PytorchGELUTanh()\n",
      "    )\n",
      "    (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "    (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "    (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "    (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  )\n",
      ")\n",
      "model.layers.0 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.0.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.0.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.0.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.0.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.0.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.0.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.0.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.0.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.0.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.1 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.1.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.1.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.1.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.1.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.1.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.1.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.1.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.1.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.1.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.2 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.2.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.2.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.2.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.2.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.2.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.2.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.2.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.2.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.2.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.3 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.3.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.3.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.3.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.3.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.3.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.3.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.3.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.3.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.3.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.4 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.4.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.4.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.4.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.4.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.4.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.4.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.4.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.4.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.4.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.5 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.5.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.5.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.5.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.5.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.5.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.5.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.5.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.5.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.5.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.6 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.6.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.6.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.6.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.6.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.6.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.6.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.6.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.6.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.6.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.7 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.7.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.7.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.7.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.7.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.7.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.7.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.7.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.7.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.7.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.8 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.8.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.8.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.8.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.8.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.8.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.8.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.8.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.8.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.8.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.9 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.9.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.9.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.9.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.9.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.9.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.9.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.9.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.9.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.9.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.10 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.10.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.10.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.10.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.10.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.10.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.10.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.10.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.10.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.10.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.11 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.11.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.11.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.11.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.11.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.11.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.11.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.11.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.11.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.11.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.12 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.12.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.12.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.12.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.12.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.12.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.12.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.12.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.12.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.12.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.13 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.13.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.13.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.13.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.13.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.13.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.13.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.13.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.13.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.13.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.14 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.14.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.14.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.14.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.14.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.14.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.14.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.14.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.14.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.14.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.15 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.15.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.15.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.15.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.15.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.15.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.15.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.15.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.15.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.15.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.16 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.16.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.16.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.16.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.16.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.16.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.16.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.16.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.16.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.16.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.17 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.17.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.17.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.17.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.17.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.17.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.17.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.17.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.17.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.17.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.18 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.18.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.18.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.18.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.18.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.18.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.18.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.18.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.18.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.18.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.19 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.19.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.19.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.19.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.19.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.19.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.19.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.19.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.19.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.19.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.20 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.20.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.20.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.20.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.20.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.20.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.20.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.20.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.20.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.20.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.21 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.21.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.21.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.21.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.21.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.21.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.21.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.21.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.21.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.21.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.22 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.22.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.22.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.22.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.22.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.22.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.22.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.22.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.22.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.22.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.23 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.23.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.23.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.23.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.23.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.23.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.23.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.23.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.23.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.23.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.24 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.24.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.24.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.24.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.24.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.24.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.24.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.24.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.24.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.24.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.25 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.25.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.25.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.25.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.25.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.25.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.25.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.25.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.25.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.25.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.26 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.26.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.26.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.26.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.26.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.26.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.26.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.26.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.26.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.26.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.27 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.27.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.27.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.27.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.27.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.27.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.27.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.27.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.27.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.27.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.28 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.28.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.28.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.28.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.28.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.28.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.28.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.28.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.28.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.28.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.28.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.29 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.29.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.29.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.29.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.29.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.29.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.29.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.29.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.29.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.29.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.29.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.30 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.30.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.30.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.30.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.30.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.30.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.30.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.30.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.30.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.30.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.30.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.31 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.31.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.31.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.31.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.31.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.31.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.31.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.31.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.31.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.31.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.31.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.32 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.32.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.32.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.32.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.32.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.32.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.32.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.32.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.32.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.32.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.32.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.32.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.32.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.32.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.32.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.32.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.33 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.33.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.33.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.33.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.33.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.33.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.33.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.33.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.33.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.33.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.33.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.33.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.33.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.33.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.33.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.33.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.34 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.34.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.34.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.34.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.34.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.34.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.34.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.34.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.34.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.34.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.34.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.34.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.34.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.34.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.34.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.34.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.35 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.35.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.35.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.35.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.35.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.35.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.35.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.35.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.35.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.35.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.35.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.35.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.35.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.35.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.35.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.35.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.36 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.36.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.36.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.36.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.36.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.36.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.36.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.36.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.36.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.36.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.36.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.36.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.36.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.36.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.36.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.36.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.37 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.37.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.37.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.37.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.37.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.37.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.37.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.37.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.37.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.37.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.37.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.37.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.37.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.37.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.37.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.37.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.38 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.38.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.38.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.38.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.38.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.38.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.38.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.38.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.38.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.38.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.38.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.38.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.38.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.38.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.38.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.38.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.39 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.39.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.39.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.39.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.39.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.39.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.39.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.39.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.39.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.39.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.39.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.39.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.39.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.39.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.39.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.39.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.40 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.40.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.40.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.40.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.40.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.40.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.40.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.40.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.40.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.40.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.40.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.40.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.40.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.40.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.40.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.40.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.41 | Gemma2DecoderLayer(\n",
      "  (self_attn): Gemma2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Gemma2MLP(\n",
      "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "    (act_fn): PytorchGELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      ")\n",
      "model.layers.41.self_attn | Gemma2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
      "  (rotary_emb): Gemma2RotaryEmbedding()\n",
      ")\n",
      "model.layers.41.self_attn.q_proj | Linear(in_features=3584, out_features=4096, bias=False)\n",
      "model.layers.41.self_attn.k_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.41.self_attn.v_proj | Linear(in_features=3584, out_features=2048, bias=False)\n",
      "model.layers.41.self_attn.o_proj | Linear(in_features=4096, out_features=3584, bias=False)\n",
      "model.layers.41.self_attn.rotary_emb | Gemma2RotaryEmbedding()\n",
      "model.layers.41.mlp | Gemma2MLP(\n",
      "  (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "model.layers.41.mlp.gate_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.41.mlp.up_proj | Linear(in_features=3584, out_features=14336, bias=False)\n",
      "model.layers.41.mlp.down_proj | Linear(in_features=14336, out_features=3584, bias=False)\n",
      "model.layers.41.mlp.act_fn | PytorchGELUTanh()\n",
      "model.layers.41.input_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.41.post_attention_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.41.pre_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.layers.41.post_feedforward_layernorm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "model.norm | Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "lm_head | Linear(in_features=3584, out_features=256000, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for n, m in model_qa.named_modules(): # both llama3 and gemma2 have similar structure\n",
    "    print(n, '|', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(hparams, model, tok, messages, max_new_tokens=1, eval_flag=False, device_eval='cuda:0'):\n",
    "    device = device_eval if eval_flag else hparams.device\n",
    "    # device = device_eval if eval_flag else model.device\n",
    "    terminators = [tok.eos_token_id, tok.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    \n",
    "    if eval_flag is False and hparams and hparams.alg_name in ['SERAC', 'MEND', 'LoRA']:  # \n",
    "        msg_tokenized = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt', return_dict=True).to(device)\n",
    "        output_ids = model.generate(**msg_tokenized, max_new_tokens=max_new_tokens, eos_token_id=terminators, do_sample=False, pad_token_id=tok.eos_token_id)\n",
    "        return tok.decode(output_ids[0][msg_tokenized['input_ids'].shape[-1]:], skip_special_tokens=True).replace('\\n', ' ').strip().rstrip('.')\n",
    "        # outputs = model(**msg_tokenized)\n",
    "        # if type(outputs) is torch.Tensor:\n",
    "        #     logits = outputs\n",
    "        # else:\n",
    "        #     logits = outputs.logits\n",
    "        # answers = torch.argmax(logits, dim=-1)\n",
    "        # return tok.decode(answers[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "The wh question double_check_accuracy of the language model is 0.484\n"
     ]
    }
   ],
   "source": [
    "# Double check\n",
    "# df_wh = pd.read_csv(f\"../data/questions/wh_only/hallucination_only/{model_id_format}.csv\")\n",
    "\n",
    "\n",
    "system_msg_double_check = \"\"\"Given a question and a answer, evaluate whether the answer is correct. \\\n",
    "Output '1' if they the answer to the question is correct. Otherwise, output '0'. Do not repeat the question or provide an explanation.\"\"\"\n",
    "wh_count = 0\n",
    "wh_correct = 0\n",
    "for i in df_wh.index[:]:\n",
    "    question, label, output_qa = df_wh.loc[i, 'question'], df_wh.loc[i, 'label'], df_wh.loc[i, f\"output_{model_id_format}\"]\n",
    "    # prompt_eval = f\"\"\"question: {question} \\nlabel: {label} \\nprediction: {output_qa}\\n\"\"\"\n",
    "    eval_res = 0\n",
    "    wh_count += 1 \n",
    "\n",
    "    if output_qa.lower() in label.lower() or label.lower() in output_qa.lower():  # Rule-basd fuzzy match\n",
    "        wh_correct += 1\n",
    "        eval_res = 1\n",
    "    else:\n",
    "        user_msg_eval = f\"\"\"The input texts are given as below: \\nquestion: {question} \\nanswer: {output_qa}\\n\"\"\"\n",
    "        messages_eval = [{\"role\": \"system\", \"content\": system_msg_double_check}, {\"role\": \"user\", \"content\": user_msg_eval}]\n",
    "        response_eval = get_response(model_eval, tok_eval, messages_eval)\n",
    "        print(response_eval)\n",
    "        if response_eval == '1':\n",
    "            wh_correct += 1\n",
    "            eval_res = 1\n",
    "            \n",
    "    df_wh.loc[i, f\"double_check_{model_id_format}\"] = eval_res\n",
    "    \n",
    "print(f\"The wh question double_check_accuracy of the language model is {wh_correct / wh_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "The wh question double_check_accuracy of the language model is 0.698\n"
     ]
    }
   ],
   "source": [
    "# Double check using GPT-4o\n",
    "# df_wh = pd.read_csv(f\"../data/questions/wh_only/hallucination_only/{model_id_format}.csv\")\n",
    "\n",
    "system_msg_double_check = \"\"\"Given a question and a answer, evaluate whether the answer is correct. \\\n",
    "Output '1' if they the answer to the question is correct. Otherwise, output '0'. Do not repeat the question or provide an explanation.\"\"\"\n",
    "wh_count = 0\n",
    "wh_correct = 0\n",
    "for i in df_wh.index[:]:\n",
    "    question, label, output_qa = df_wh.loc[i, 'question'], df_wh.loc[i, 'label'], df_wh.loc[i, f\"output_{model_id_format}\"]\n",
    "    # prompt_eval = f\"\"\"question: {question} \\nlabel: {label} \\nprediction: {output_qa}\\n\"\"\"\n",
    "    eval_res = 0\n",
    "    wh_count += 1 \n",
    "\n",
    "    if output_qa.lower() in label.lower() or label.lower() in output_qa.lower():  # Rule-basd fuzzy match\n",
    "        wh_correct += 1\n",
    "        eval_res = 1\n",
    "    else:\n",
    "        user_msg_eval = f\"\"\"The input texts are given as below: \\nquestion: {question} \\nanswer: {output_qa}\\n\"\"\"\n",
    "        \n",
    "        raw_response = client.chat.completions.create(\n",
    "            model='gpt-4o', \n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg_double_check}, {\"role\": \"user\", \"content\": user_msg_eval}], \n",
    "            temperature=0\n",
    "        )\n",
    "        response_eval = raw_response.choices[0].message.content\n",
    "\n",
    "        print(response_eval)\n",
    "        if response_eval == '1':\n",
    "            wh_correct += 1\n",
    "            eval_res = 1\n",
    "            \n",
    "    df_wh.loc[i, f\"double_check_{model_id_format}\"] = eval_res\n",
    "    \n",
    "print(f\"The wh question double_check_accuracy of the language model is {wh_correct / wh_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/baixiang/env/anaconda3/envs/edit/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from easyeditor import BaseEditor\n",
    "# from hallucination_editor import BaseEditor\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from easyeditor import FTHyperParams, IKEHyperParams, ROMEHyperParams, MEMITHyperParams\n",
    "\n",
    "test_data = json.load(open(os.path.join('../../editing-attack-backup-2024-july-26/data_old/zsre_mend_eval_portability_gpt4.json'), 'r', encoding='utf-8'))\n",
    "test_data = random.sample(test_data, 50)\n",
    "questions = [test_data_['src'] for test_data_ in test_data]\n",
    "rephrase_prompts = [edit_data_['rephrase'] for edit_data_ in test_data]\n",
    "targets = [edit_data_['alt'] for edit_data_ in test_data]\n",
    "subjects = [edit_data_['subject'] for edit_data_ in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 18:02:21,934 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-08 18:02:21,934 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/08/2024 18:02:21 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007400035858154297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf65a107a13b48a3a81e11792b2a27b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0028488636016845703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056fa3ce4219457f98dff438a790cc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.44it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "2024-08-08 18:03:12,653 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:03:12,653 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:03:12 - INFO - easyeditor.editors.editor -   0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "  2%|▏         | 1/50 [00:06<05:03,  6.20s/it]2024-08-08 18:03:18,684 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:03:18,684 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:03:18 - INFO - easyeditor.editors.editor -   1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "  4%|▍         | 2/50 [00:12<04:52,  6.10s/it]2024-08-08 18:03:25,620 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:03:25,620 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:03:25 - INFO - easyeditor.editors.editor -   2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "  6%|▌         | 3/50 [00:19<05:04,  6.48s/it]2024-08-08 18:03:31,514 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:03:31,514 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:03:31 - INFO - easyeditor.editors.editor -   3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "  8%|▊         | 4/50 [00:25<04:47,  6.25s/it]2024-08-08 18:03:37,772 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:03:37,772 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:03:37 - INFO - easyeditor.editors.editor -   4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 10%|█         | 5/50 [00:31<04:41,  6.25s/it]2024-08-08 18:03:43,466 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:03:43,466 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:03:43 - INFO - easyeditor.editors.editor -   5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 12%|█▏        | 6/50 [00:37<04:26,  6.06s/it]2024-08-08 18:03:49,917 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:03:49,917 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:03:49 - INFO - easyeditor.editors.editor -   6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 14%|█▍        | 7/50 [00:43<04:26,  6.19s/it]2024-08-08 18:03:55,965 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:03:55,965 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:03:55 - INFO - easyeditor.editors.editor -   7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 16%|█▌        | 8/50 [00:49<04:18,  6.14s/it]2024-08-08 18:04:02,193 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:02,193 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:02 - INFO - easyeditor.editors.editor -   8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 18%|█▊        | 9/50 [00:55<04:12,  6.17s/it]2024-08-08 18:04:07,920 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:07,920 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:07 - INFO - easyeditor.editors.editor -   9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 20%|██        | 10/50 [01:01<04:01,  6.03s/it]2024-08-08 18:04:14,136 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:14,136 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:14 - INFO - easyeditor.editors.editor -   10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 22%|██▏       | 11/50 [01:07<03:57,  6.09s/it]2024-08-08 18:04:20,380 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:20,380 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:20 - INFO - easyeditor.editors.editor -   11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 24%|██▍       | 12/50 [01:13<03:53,  6.14s/it]2024-08-08 18:04:26,093 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:26,093 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:26 - INFO - easyeditor.editors.editor -   12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 26%|██▌       | 13/50 [01:19<03:42,  6.01s/it]2024-08-08 18:04:31,951 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:31,951 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:31 - INFO - easyeditor.editors.editor -   13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 28%|██▊       | 14/50 [01:25<03:34,  5.96s/it]2024-08-08 18:04:37,822 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:37,822 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:37 - INFO - easyeditor.editors.editor -   14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 30%|███       | 15/50 [01:31<03:27,  5.94s/it]2024-08-08 18:04:44,029 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:44,029 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:44 - INFO - easyeditor.editors.editor -   15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 32%|███▏      | 16/50 [01:37<03:24,  6.02s/it]2024-08-08 18:04:50,052 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:50,052 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:50 - INFO - easyeditor.editors.editor -   16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 34%|███▍      | 17/50 [01:43<03:18,  6.02s/it]2024-08-08 18:04:56,253 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:04:56,253 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:04:56 - INFO - easyeditor.editors.editor -   17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 36%|███▌      | 18/50 [01:49<03:14,  6.07s/it]2024-08-08 18:05:01,776 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:01,776 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:01 - INFO - easyeditor.editors.editor -   18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 38%|███▊      | 19/50 [01:55<03:03,  5.91s/it]2024-08-08 18:05:08,008 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:08,008 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:08 - INFO - easyeditor.editors.editor -   19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 40%|████      | 20/50 [02:01<03:00,  6.01s/it]2024-08-08 18:05:14,215 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:14,215 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:14 - INFO - easyeditor.editors.editor -   20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 42%|████▏     | 21/50 [02:07<02:55,  6.07s/it]2024-08-08 18:05:20,457 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:20,457 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:20 - INFO - easyeditor.editors.editor -   21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 44%|████▍     | 22/50 [02:14<02:51,  6.12s/it]2024-08-08 18:05:26,303 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:26,303 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:26 - INFO - easyeditor.editors.editor -   22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 46%|████▌     | 23/50 [02:19<02:42,  6.04s/it]2024-08-08 18:05:32,550 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:32,550 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:32 - INFO - easyeditor.editors.editor -   23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 48%|████▊     | 24/50 [02:26<02:38,  6.10s/it]2024-08-08 18:05:38,682 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:38,682 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:38 - INFO - easyeditor.editors.editor -   24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 50%|█████     | 25/50 [02:32<02:32,  6.11s/it]2024-08-08 18:05:44,798 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:44,798 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:44 - INFO - easyeditor.editors.editor -   25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 52%|█████▏    | 26/50 [02:38<02:26,  6.11s/it]2024-08-08 18:05:51,017 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:51,017 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:51 - INFO - easyeditor.editors.editor -   26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 54%|█████▍    | 27/50 [02:44<02:21,  6.14s/it]2024-08-08 18:05:57,784 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:05:57,784 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:05:57 - INFO - easyeditor.editors.editor -   27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 56%|█████▌    | 28/50 [02:51<02:19,  6.33s/it]2024-08-08 18:06:03,873 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:03,873 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:03 - INFO - easyeditor.editors.editor -   28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 58%|█████▊    | 29/50 [02:57<02:11,  6.26s/it]2024-08-08 18:06:09,551 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:09,551 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:09 - INFO - easyeditor.editors.editor -   29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 60%|██████    | 30/50 [03:03<02:01,  6.08s/it]2024-08-08 18:06:15,468 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:15,468 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:15 - INFO - easyeditor.editors.editor -   30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 62%|██████▏   | 31/50 [03:09<01:54,  6.03s/it]2024-08-08 18:06:21,620 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:21,620 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:21 - INFO - easyeditor.editors.editor -   31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 64%|██████▍   | 32/50 [03:15<01:49,  6.07s/it]2024-08-08 18:06:27,764 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.8333333333333333, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:27,764 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.8333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:27 - INFO - easyeditor.editors.editor -   32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.8333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 66%|██████▌   | 33/50 [03:21<01:43,  6.09s/it]2024-08-08 18:06:33,869 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:33,869 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:33 - INFO - easyeditor.editors.editor -   33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 68%|██████▊   | 34/50 [03:27<01:37,  6.10s/it]2024-08-08 18:06:39,461 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:39,461 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:39 - INFO - easyeditor.editors.editor -   34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 70%|███████   | 35/50 [03:33<01:29,  5.94s/it]2024-08-08 18:06:45,632 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:45,632 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:45 - INFO - easyeditor.editors.editor -   35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 72%|███████▏  | 36/50 [03:39<01:24,  6.01s/it]2024-08-08 18:06:51,446 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:51,446 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:51 - INFO - easyeditor.editors.editor -   36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 74%|███████▍  | 37/50 [03:44<01:17,  5.95s/it]2024-08-08 18:06:57,183 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:06:57,183 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:06:57 - INFO - easyeditor.editors.editor -   37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 76%|███████▌  | 38/50 [03:50<01:10,  5.89s/it]2024-08-08 18:07:03,252 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:03,252 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:03 - INFO - easyeditor.editors.editor -   38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 78%|███████▊  | 39/50 [03:56<01:05,  5.94s/it]2024-08-08 18:07:08,991 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:08,991 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:08 - INFO - easyeditor.editors.editor -   39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 80%|████████  | 40/50 [04:02<00:58,  5.88s/it]2024-08-08 18:07:15,064 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:15,064 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:15 - INFO - easyeditor.editors.editor -   40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 82%|████████▏ | 41/50 [04:08<00:53,  5.94s/it]2024-08-08 18:07:21,853 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:21,853 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:21 - INFO - easyeditor.editors.editor -   41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 84%|████████▍ | 42/50 [04:15<00:49,  6.19s/it]2024-08-08 18:07:27,598 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:27,598 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:27 - INFO - easyeditor.editors.editor -   42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 86%|████████▌ | 43/50 [04:21<00:42,  6.06s/it]2024-08-08 18:07:33,616 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:33,616 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:33 - INFO - easyeditor.editors.editor -   43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 88%|████████▊ | 44/50 [04:27<00:36,  6.05s/it]2024-08-08 18:07:39,884 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:39,884 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:39 - INFO - easyeditor.editors.editor -   44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 90%|█████████ | 45/50 [04:33<00:30,  6.11s/it]2024-08-08 18:07:45,625 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:45,625 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:45 - INFO - easyeditor.editors.editor -   45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 92%|█████████▏| 46/50 [04:39<00:24,  6.00s/it]2024-08-08 18:07:51,320 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:51,320 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:51 - INFO - easyeditor.editors.editor -   46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 94%|█████████▍| 47/50 [04:44<00:17,  5.91s/it]2024-08-08 18:07:57,565 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:07:57,565 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:07:57 - INFO - easyeditor.editors.editor -   47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 96%|█████████▌| 48/50 [04:51<00:12,  6.01s/it]2024-08-08 18:08:03,645 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:08:03,645 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:08:03 - INFO - easyeditor.editors.editor -   48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      " 98%|█████████▊| 49/50 [04:57<00:06,  6.03s/it]2024-08-08 18:08:09,584 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "2024-08-08 18:08:09,584 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 18:08:09 - INFO - easyeditor.editors.editor -   49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [1.0], 'rewrite_F1': 1.0, 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 50/50 [05:03<00:00,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.013333333333333332}, 'post': {'rewrite_acc': 0.956}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easyeditor import GraceHyperParams\n",
    "hparams = GraceHyperParams.from_hparams('./hparams/GRACE/mistral-7b-v3')  # llama3-8b\n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 1\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "# Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 17:14:42,990 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/08/2024 17:14:42 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010990619659423828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568ba03fa28748119ec67c8bcfe41b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007191658020019531,
       "initial": 3701473280,
       "n": 3701473280,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00001-of-00004.safetensors",
       "rate": null,
       "total": 4976698672,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7fd2bc4d624eb2a87764fde283ece8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:  74%|#######4  | 3.70G/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004301786422729492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00002-of-00004.safetensors",
       "rate": null,
       "total": 4999802720,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fd8da0c71f4ce0961fe4c6f2f785b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008191347122192383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00003-of-00004.safetensors",
       "rate": null,
       "total": 4915916176,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0d123bed6c463aa124459201c1a8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008571147918701172,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00004-of-00004.safetensors",
       "rate": null,
       "total": 1168138808,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b445d335db6f46ce8c38b42818a16bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003348112106323242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3587a43bbc947fcae2e83fa643b95df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035796165466308594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 187,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984b5af4366447a4a7a7927026261820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003972291946411133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 50977,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61653bb6e9b24e689bc8d32520ca9d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0034940242767333984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 9085698,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382b3a2a546e4afbae09354c5d021ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00709843635559082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "special_tokens_map.json",
       "rate": null,
       "total": 73,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe34bf1103d4e4e8c0cf64fe558b0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.80it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "2024-08-08 17:33:37,103 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:33:37 - INFO - easyeditor.editors.editor -   0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "  2%|▏         | 1/50 [00:05<04:35,  5.63s/it]2024-08-08 17:33:42,772 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:33:42 - INFO - easyeditor.editors.editor -   1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "  4%|▍         | 2/50 [00:11<04:31,  5.65s/it]2024-08-08 17:33:48,940 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:33:48 - INFO - easyeditor.editors.editor -   2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "  6%|▌         | 3/50 [00:17<04:36,  5.89s/it]2024-08-08 17:33:54,761 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:33:54 - INFO - easyeditor.editors.editor -   3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "  8%|▊         | 4/50 [00:23<04:29,  5.86s/it]2024-08-08 17:34:00,438 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:00 - INFO - easyeditor.editors.editor -   4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 10%|█         | 5/50 [00:28<04:20,  5.79s/it]2024-08-08 17:34:06,068 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:06 - INFO - easyeditor.editors.editor -   5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 12%|█▏        | 6/50 [00:34<04:12,  5.74s/it]2024-08-08 17:34:12,095 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [0.75], 'rewrite_F1': 0.6, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:12 - INFO - easyeditor.editors.editor -   6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [0.75], 'rewrite_F1': 0.6, 'locality': {}, 'portability': {}}}\n",
      " 14%|█▍        | 7/50 [00:40<04:10,  5.83s/it]2024-08-08 17:34:18,076 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:18 - INFO - easyeditor.editors.editor -   7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 16%|█▌        | 8/50 [00:46<04:06,  5.88s/it]2024-08-08 17:34:23,734 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:23 - INFO - easyeditor.editors.editor -   8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 18%|█▊        | 9/50 [00:52<03:58,  5.81s/it]2024-08-08 17:34:29,587 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:29 - INFO - easyeditor.editors.editor -   9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 20%|██        | 10/50 [00:58<03:52,  5.82s/it]2024-08-08 17:34:35,564 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [0.75], 'rewrite_F1': 0.6, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:35 - INFO - easyeditor.editors.editor -   10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [0.75], 'rewrite_F1': 0.6, 'locality': {}, 'portability': {}}}\n",
      " 22%|██▏       | 11/50 [01:04<03:48,  5.87s/it]2024-08-08 17:34:41,206 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:41 - INFO - easyeditor.editors.editor -   11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 24%|██▍       | 12/50 [01:09<03:40,  5.80s/it]2024-08-08 17:34:47,014 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:47 - INFO - easyeditor.editors.editor -   12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 26%|██▌       | 13/50 [01:15<03:34,  5.80s/it]2024-08-08 17:34:52,987 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:52 - INFO - easyeditor.editors.editor -   13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 28%|██▊       | 14/50 [01:21<03:30,  5.85s/it]2024-08-08 17:34:58,799 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [0.3333333333333333], 'rewrite_F1': 0.16666666666666666, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:34:58 - INFO - easyeditor.editors.editor -   14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [0.3333333333333333], 'rewrite_F1': 0.16666666666666666, 'locality': {}, 'portability': {}}}\n",
      " 30%|███       | 15/50 [01:27<03:24,  5.84s/it]2024-08-08 17:35:04,442 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:04 - INFO - easyeditor.editors.editor -   15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 32%|███▏      | 16/50 [01:32<03:16,  5.78s/it]2024-08-08 17:35:10,395 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [0.25], 'rewrite_F1': 0.13333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:10 - INFO - easyeditor.editors.editor -   16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [0.25], 'rewrite_F1': 0.13333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 34%|███▍      | 17/50 [01:38<03:12,  5.83s/it]2024-08-08 17:35:16,011 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:16 - INFO - easyeditor.editors.editor -   17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 36%|███▌      | 18/50 [01:44<03:04,  5.77s/it]2024-08-08 17:35:21,457 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:21 - INFO - easyeditor.editors.editor -   18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 38%|███▊      | 19/50 [01:49<02:55,  5.67s/it]2024-08-08 17:35:27,636 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:27 - INFO - easyeditor.editors.editor -   19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      " 40%|████      | 20/50 [01:56<02:54,  5.82s/it]2024-08-08 17:35:33,780 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:33 - INFO - easyeditor.editors.editor -   20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      " 42%|████▏     | 21/50 [02:02<02:51,  5.92s/it]2024-08-08 17:35:40,086 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:40 - INFO - easyeditor.editors.editor -   21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 44%|████▍     | 22/50 [02:08<02:48,  6.04s/it]2024-08-08 17:35:45,721 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:45 - INFO - easyeditor.editors.editor -   22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 46%|████▌     | 23/50 [02:14<02:39,  5.92s/it]2024-08-08 17:35:51,368 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:51 - INFO - easyeditor.editors.editor -   23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 48%|████▊     | 24/50 [02:19<02:31,  5.84s/it]2024-08-08 17:35:57,267 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:35:57 - INFO - easyeditor.editors.editor -   24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 50%|█████     | 25/50 [02:25<02:26,  5.85s/it]2024-08-08 17:36:03,294 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:03 - INFO - easyeditor.editors.editor -   25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 52%|█████▏    | 26/50 [02:31<02:21,  5.91s/it]2024-08-08 17:36:09,260 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:09 - INFO - easyeditor.editors.editor -   26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 54%|█████▍    | 27/50 [02:37<02:16,  5.92s/it]2024-08-08 17:36:15,442 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:15 - INFO - easyeditor.editors.editor -   27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 56%|█████▌    | 28/50 [02:43<02:12,  6.00s/it]2024-08-08 17:36:21,442 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.14285714285714285, 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [0.75], 'rewrite_F1': 0.6, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:21 - INFO - easyeditor.editors.editor -   28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.14285714285714285, 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [0.75], 'rewrite_F1': 0.6, 'locality': {}, 'portability': {}}}\n",
      " 58%|█████▊    | 29/50 [02:49<02:06,  6.00s/it]2024-08-08 17:36:27,084 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:27 - INFO - easyeditor.editors.editor -   29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 60%|██████    | 30/50 [02:55<01:57,  5.89s/it]2024-08-08 17:36:32,948 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:32 - INFO - easyeditor.editors.editor -   30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'locality': {}, 'portability': {}}}\n",
      " 62%|██████▏   | 31/50 [03:01<01:51,  5.88s/it]2024-08-08 17:36:39,116 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:39 - INFO - easyeditor.editors.editor -   31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      " 64%|██████▍   | 32/50 [03:07<01:47,  5.97s/it]2024-08-08 17:36:44,766 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:44 - INFO - easyeditor.editors.editor -   32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 66%|██████▌   | 33/50 [03:13<01:39,  5.87s/it]2024-08-08 17:36:50,416 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:50 - INFO - easyeditor.editors.editor -   33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 68%|██████▊   | 34/50 [03:18<01:32,  5.81s/it]2024-08-08 17:36:56,050 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:36:56 - INFO - easyeditor.editors.editor -   34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 70%|███████   | 35/50 [03:24<01:26,  5.75s/it]2024-08-08 17:37:02,286 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:02 - INFO - easyeditor.editors.editor -   35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      " 72%|███████▏  | 36/50 [03:30<01:22,  5.90s/it]2024-08-08 17:37:08,431 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:08 - INFO - easyeditor.editors.editor -   36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 74%|███████▍  | 37/50 [03:36<01:17,  5.97s/it]2024-08-08 17:37:14,088 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:14 - INFO - easyeditor.editors.editor -   37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 76%|███████▌  | 38/50 [03:42<01:10,  5.88s/it]2024-08-08 17:37:19,742 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:19 - INFO - easyeditor.editors.editor -   38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 78%|███████▊  | 39/50 [03:48<01:03,  5.81s/it]2024-08-08 17:37:25,432 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:25 - INFO - easyeditor.editors.editor -   39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 80%|████████  | 40/50 [03:53<00:57,  5.77s/it]2024-08-08 17:37:31,460 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [0.75], 'rewrite_F1': 0.6, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:31 - INFO - easyeditor.editors.editor -   40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [0.75], 'rewrite_F1': 0.6, 'locality': {}, 'portability': {}}}\n",
      " 82%|████████▏ | 41/50 [03:59<00:52,  5.85s/it]2024-08-08 17:37:37,669 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:37 - INFO - easyeditor.editors.editor -   41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 84%|████████▍ | 42/50 [04:06<00:47,  5.96s/it]2024-08-08 17:37:43,360 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:43 - INFO - easyeditor.editors.editor -   42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 86%|████████▌ | 43/50 [04:11<00:41,  5.88s/it]2024-08-08 17:37:49,351 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:49 - INFO - easyeditor.editors.editor -   43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 88%|████████▊ | 44/50 [04:17<00:35,  5.91s/it]2024-08-08 17:37:55,217 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:37:55 - INFO - easyeditor.editors.editor -   44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'locality': {}, 'portability': {}}}\n",
      " 90%|█████████ | 45/50 [04:23<00:29,  5.90s/it]2024-08-08 17:38:00,890 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:38:00 - INFO - easyeditor.editors.editor -   45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 92%|█████████▏| 46/50 [04:29<00:23,  5.83s/it]2024-08-08 17:38:06,694 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:38:06 - INFO - easyeditor.editors.editor -   46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'locality': {}, 'portability': {}}}\n",
      " 94%|█████████▍| 47/50 [04:35<00:17,  5.82s/it]2024-08-08 17:38:12,879 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:38:12 - INFO - easyeditor.editors.editor -   47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [0.8], 'rewrite_F1': 0.6666666666666666, 'locality': {}, 'portability': {}}}\n",
      " 96%|█████████▌| 48/50 [04:41<00:11,  5.93s/it]2024-08-08 17:38:18,889 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:38:18 - INFO - easyeditor.editors.editor -   48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [0.5], 'rewrite_F1': 0.3333333333333333, 'locality': {}, 'portability': {}}}\n",
      " 98%|█████████▊| 49/50 [04:47<00:05,  5.95s/it]2024-08-08 17:38:24,758 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'locality': {}, 'portability': {}}}\n",
      "08/08/2024 17:38:24 - INFO - easyeditor.editors.editor -   49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'rewrite_F1': 0.0, 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [0.6666666666666666], 'rewrite_F1': 0.5, 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 50/50 [04:53<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.01}, 'post': {'rewrite_acc': 0.27166666666666667}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easyeditor import GraceHyperParams\n",
    "hparams = GraceHyperParams.from_hparams('./hparams/GRACE/llama3-8b')  # llama3-8b\n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 2\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "# Metrics Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:49:21,600 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/02/2024 15:49:21 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24469b190064420cb79dccd40025b19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.45it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What was the death date of Thomas Farnaby?] -> [1815]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.693404197692871\n",
      "Total loss 5.693404197692871\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.4923534393310547\n",
      "Total loss 3.4923534393310547\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.8184893131256104\n",
      "Total loss 1.8184893131256104\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8806086182594299\n",
      "Total loss 0.8806086182594299\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.8040114641189575\n",
      "Total loss 0.8040114641189575\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.8052194118499756\n",
      "Total loss 0.8052194118499756\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.7848984003067017\n",
      "Total loss 0.7848984003067017\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.739143431186676\n",
      "Total loss 0.739143431186676\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.6737039685249329\n",
      "Total loss 0.6737039685249329\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.6054604053497314\n",
      "Total loss 0.6054604053497314\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.5398155450820923\n",
      "Total loss 0.5398155450820923\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.47909027338027954\n",
      "Total loss 0.47909027338027954\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.4256579279899597\n",
      "Total loss 0.4256579279899597\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.38149937987327576\n",
      "Total loss 0.38149937987327576\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3430868983268738\n",
      "Total loss 0.3430868983268738\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.3104054927825928\n",
      "Total loss 0.3104054927825928\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.28231316804885864\n",
      "Total loss 0.28231316804885864\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.25966504216194153\n",
      "Total loss 0.25966504216194153\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.2417575865983963\n",
      "Total loss 0.2417575865983963\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.22366635501384735\n",
      "Total loss 0.22366635501384735\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.20674876868724823\n",
      "Total loss 0.20674876868724823\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.1919666975736618\n",
      "Total loss 0.1919666975736618\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.17696824669837952\n",
      "Total loss 0.17696824669837952\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.16204924881458282\n",
      "Total loss 0.16204924881458282\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.1481691598892212\n",
      "Total loss 0.1481691598892212\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.13639138638973236\n",
      "Total loss 0.13639138638973236\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.12626893818378448\n",
      "Total loss 0.12626893818378448\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.11763233691453934\n",
      "Total loss 0.11763233691453934\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.10693653672933578\n",
      "Total loss 0.10693653672933578\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.09686681628227234\n",
      "Total loss 0.09686681628227234\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0899653434753418\n",
      "Total loss 0.0899653434753418\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08431737869977951\n",
      "Total loss 0.08431737869977951\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07698570191860199\n",
      "Total loss 0.07698570191860199\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07214610278606415\n",
      "Total loss 0.07214610278606415\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06844858825206757\n",
      "Total loss 0.06844858825206757\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06415113806724548\n",
      "Total loss 0.06415113806724548\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0618547685444355\n",
      "Total loss 0.0618547685444355\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06023961678147316\n",
      "Total loss 0.06023961678147316\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05760905519127846\n",
      "Total loss 0.05760905519127846\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05596720799803734\n",
      "Total loss 0.05596720799803734\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.053442299365997314\n",
      "Total loss 0.053442299365997314\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05174478143453598\n",
      "Total loss 0.05174478143453598\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04935403913259506\n",
      "Total loss 0.04935403913259506\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04783697426319122\n",
      "Total loss 0.04783697426319122\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0468832403421402\n",
      "Total loss 0.0468832403421402\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04602895677089691\n",
      "Total loss 0.04602895677089691\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04470865800976753\n",
      "Total loss 0.04470865800976753\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04380117356777191\n",
      "Total loss 0.04380117356777191\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04176681116223335\n",
      "Total loss 0.04176681116223335\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.041761159896850586\n",
      "Total loss 0.041761159896850586\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.039620816707611084\n",
      "Total loss 0.039620816707611084\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03986261412501335\n",
      "Total loss 0.03986261412501335\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03820524364709854\n",
      "Total loss 0.03820524364709854\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.038808997720479965\n",
      "Total loss 0.038808997720479965\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03685171902179718\n",
      "Total loss 0.03685171902179718\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.037316083908081055\n",
      "Total loss 0.037316083908081055\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.035839952528476715\n",
      "Total loss 0.035839952528476715\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0356702022254467\n",
      "Total loss 0.0356702022254467\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03506723791360855\n",
      "Total loss 0.03506723791360855\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03509930148720741\n",
      "Total loss 0.03509930148720741\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03532702475786209\n",
      "Total loss 0.03532702475786209\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03407936915755272\n",
      "Total loss 0.03407936915755272\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.033686745911836624\n",
      "Total loss 0.033686745911836624\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03353429585695267\n",
      "Total loss 0.03353429585695267\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03307337313890457\n",
      "Total loss 0.03307337313890457\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03323981538414955\n",
      "Total loss 0.03323981538414955\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.032696451991796494\n",
      "Total loss 0.032696451991796494\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03327619284391403\n",
      "Total loss 0.03327619284391403\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0322396345436573\n",
      "Total loss 0.0322396345436573\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03336526080965996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:50:11,622 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:50:11 - INFO - easyeditor.editors.editor -   0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  2%|▏         | 1/50 [00:24<19:39, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03336526080965996\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Who was the dad of Jane Seymour?] -> [Henry Seymour]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.129804611206055\n",
      "Total loss 8.129804611206055\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.088177442550659\n",
      "Total loss 3.088177442550659\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.6375858783721924\n",
      "Total loss 2.6375858783721924\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.0566645860671997\n",
      "Total loss 1.0566645860671997\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.9833149313926697\n",
      "Total loss 0.9833149313926697\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.0774849653244019\n",
      "Total loss 1.0774849653244019\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.1536004543304443\n",
      "Total loss 1.1536004543304443\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.194530963897705\n",
      "Total loss 1.194530963897705\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.2047420740127563\n",
      "Total loss 1.2047420740127563\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.1894348859786987\n",
      "Total loss 1.1894348859786987\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.1548199653625488\n",
      "Total loss 1.1548199653625488\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.1060928106307983\n",
      "Total loss 1.1060928106307983\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.050087332725525\n",
      "Total loss 1.050087332725525\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.9901809692382812\n",
      "Total loss 0.9901809692382812\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.9260231256484985\n",
      "Total loss 0.9260231256484985\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.8591609597206116\n",
      "Total loss 0.8591609597206116\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.7931697964668274\n",
      "Total loss 0.7931697964668274\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.7284497022628784\n",
      "Total loss 0.7284497022628784\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.668758749961853\n",
      "Total loss 0.668758749961853\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6162510514259338\n",
      "Total loss 0.6162510514259338\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.5683788061141968\n",
      "Total loss 0.5683788061141968\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.5224536061286926\n",
      "Total loss 0.5224536061286926\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.48047056794166565\n",
      "Total loss 0.48047056794166565\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.4424014389514923\n",
      "Total loss 0.4424014389514923\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.4095647633075714\n",
      "Total loss 0.4095647633075714\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.37929511070251465\n",
      "Total loss 0.37929511070251465\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.3520258367061615\n",
      "Total loss 0.3520258367061615\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.32932934165000916\n",
      "Total loss 0.32932934165000916\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.30950459837913513\n",
      "Total loss 0.30950459837913513\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.29192453622817993\n",
      "Total loss 0.29192453622817993\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.2758484482765198\n",
      "Total loss 0.2758484482765198\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.260525107383728\n",
      "Total loss 0.260525107383728\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.24635818600654602\n",
      "Total loss 0.24635818600654602\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.23312905430793762\n",
      "Total loss 0.23312905430793762\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.2209891825914383\n",
      "Total loss 0.2209891825914383\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.20818382501602173\n",
      "Total loss 0.20818382501602173\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.1962948888540268\n",
      "Total loss 0.1962948888540268\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.18594136834144592\n",
      "Total loss 0.18594136834144592\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.1761375218629837\n",
      "Total loss 0.1761375218629837\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.16587160527706146\n",
      "Total loss 0.16587160527706146\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.15710856020450592\n",
      "Total loss 0.15710856020450592\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.14946579933166504\n",
      "Total loss 0.14946579933166504\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.1428188532590866\n",
      "Total loss 0.1428188532590866\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.13734593987464905\n",
      "Total loss 0.13734593987464905\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.13182885944843292\n",
      "Total loss 0.13182885944843292\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.1246485486626625\n",
      "Total loss 0.1246485486626625\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.12004635483026505\n",
      "Total loss 0.12004635483026505\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.11383140087127686\n",
      "Total loss 0.11383140087127686\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.10918916016817093\n",
      "Total loss 0.10918916016817093\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.10429304838180542\n",
      "Total loss 0.10429304838180542\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.09821993112564087\n",
      "Total loss 0.09821993112564087\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.09462463855743408\n",
      "Total loss 0.09462463855743408\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.08849463611841202\n",
      "Total loss 0.08849463611841202\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.08471295237541199\n",
      "Total loss 0.08471295237541199\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.07975122332572937\n",
      "Total loss 0.07975122332572937\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.07653895765542984\n",
      "Total loss 0.07653895765542984\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.07289648056030273\n",
      "Total loss 0.07289648056030273\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.06904444843530655\n",
      "Total loss 0.06904444843530655\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.06591811031103134\n",
      "Total loss 0.06591811031103134\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.061719272285699844\n",
      "Total loss 0.061719272285699844\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.06003755331039429\n",
      "Total loss 0.06003755331039429\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.05605543032288551\n",
      "Total loss 0.05605543032288551\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.05475320667028427\n",
      "Total loss 0.05475320667028427\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.05167018249630928\n",
      "Total loss 0.05167018249630928\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.05044809728860855\n",
      "Total loss 0.05044809728860855\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.04712657630443573\n",
      "Total loss 0.04712657630443573\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.045918047428131104\n",
      "Total loss 0.045918047428131104\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.04432186484336853\n",
      "Total loss 0.04432186484336853\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.043685756623744965\n",
      "Total loss 0.043685756623744965\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.042855404317379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:50:35,996 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:50:35 - INFO - easyeditor.editors.editor -   1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  4%|▍         | 2/50 [00:48<19:24, 24.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.042855404317379\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What is the date of death for Joan Standing?] -> [16 May 2008]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.9315619468688965\n",
      "Total loss 3.9315619468688965\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.8881211280822754\n",
      "Total loss 2.8881211280822754\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.063239812850952\n",
      "Total loss 2.063239812850952\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.3973520994186401\n",
      "Total loss 1.3973520994186401\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.9410701990127563\n",
      "Total loss 1.9410701990127563\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.7867363691329956\n",
      "Total loss 0.7867363691329956\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6145498156547546\n",
      "Total loss 0.6145498156547546\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6502947807312012\n",
      "Total loss 0.6502947807312012\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.709078848361969\n",
      "Total loss 0.709078848361969\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.7357771396636963\n",
      "Total loss 0.7357771396636963\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.7351357936859131\n",
      "Total loss 0.7351357936859131\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.7139874696731567\n",
      "Total loss 0.7139874696731567\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.6774539947509766\n",
      "Total loss 0.6774539947509766\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.632836103439331\n",
      "Total loss 0.632836103439331\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.5884643793106079\n",
      "Total loss 0.5884643793106079\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.5445029139518738\n",
      "Total loss 0.5445029139518738\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.5031304359436035\n",
      "Total loss 0.5031304359436035\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.4677862823009491\n",
      "Total loss 0.4677862823009491\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.4351333975791931\n",
      "Total loss 0.4351333975791931\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.4041615426540375\n",
      "Total loss 0.4041615426540375\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.37443745136260986\n",
      "Total loss 0.37443745136260986\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.3456193506717682\n",
      "Total loss 0.3456193506717682\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3193206489086151\n",
      "Total loss 0.3193206489086151\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.29663965106010437\n",
      "Total loss 0.29663965106010437\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.2756744921207428\n",
      "Total loss 0.2756744921207428\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.25481748580932617\n",
      "Total loss 0.25481748580932617\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.2343572974205017\n",
      "Total loss 0.2343572974205017\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.21581149101257324\n",
      "Total loss 0.21581149101257324\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.2012055367231369\n",
      "Total loss 0.2012055367231369\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.18741700053215027\n",
      "Total loss 0.18741700053215027\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.173106387257576\n",
      "Total loss 0.173106387257576\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.16003422439098358\n",
      "Total loss 0.16003422439098358\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.1480567902326584\n",
      "Total loss 0.1480567902326584\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.13644753396511078\n",
      "Total loss 0.13644753396511078\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.12514981627464294\n",
      "Total loss 0.12514981627464294\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.11609366536140442\n",
      "Total loss 0.11609366536140442\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.10860766470432281\n",
      "Total loss 0.10860766470432281\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.10202750563621521\n",
      "Total loss 0.10202750563621521\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.09483983367681503\n",
      "Total loss 0.09483983367681503\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.08895887434482574\n",
      "Total loss 0.08895887434482574\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.08386276662349701\n",
      "Total loss 0.08386276662349701\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.07959240674972534\n",
      "Total loss 0.07959240674972534\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.07420630753040314\n",
      "Total loss 0.07420630753040314\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.07097411155700684\n",
      "Total loss 0.07097411155700684\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.06829091161489487\n",
      "Total loss 0.06829091161489487\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.06598568707704544\n",
      "Total loss 0.06598568707704544\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.06277216970920563\n",
      "Total loss 0.06277216970920563\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.060514092445373535\n",
      "Total loss 0.060514092445373535\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.05857282131910324\n",
      "Total loss 0.05857282131910324\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.056946467608213425\n",
      "Total loss 0.056946467608213425\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.055022142827510834\n",
      "Total loss 0.055022142827510834\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.053481314331293106\n",
      "Total loss 0.053481314331293106\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.051473673433065414\n",
      "Total loss 0.051473673433065414\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.049955517053604126\n",
      "Total loss 0.049955517053604126\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.048227258026599884\n",
      "Total loss 0.048227258026599884\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.04770710691809654\n",
      "Total loss 0.04770710691809654\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04560806602239609\n",
      "Total loss 0.04560806602239609\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0450252927839756\n",
      "Total loss 0.0450252927839756\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.044058773666620255\n",
      "Total loss 0.044058773666620255\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.04195140302181244\n",
      "Total loss 0.04195140302181244\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04248490184545517\n",
      "Total loss 0.04248490184545517\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03982018679380417\n",
      "Total loss 0.03982018679380417\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.04133739322423935\n",
      "Total loss 0.04133739322423935\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03936707600951195\n",
      "Total loss 0.03936707600951195\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.040200211107730865\n",
      "Total loss 0.040200211107730865\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03869067505002022\n",
      "Total loss 0.03869067505002022\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03965208679437637\n",
      "Total loss 0.03965208679437637\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03770637884736061\n",
      "Total loss 0.03770637884736061\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.037262801080942154\n",
      "Total loss 0.037262801080942154\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03627685457468033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:51:02,811 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:51:02 - INFO - easyeditor.editors.editor -   2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03627685457468033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:15<19:55, 25.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What city did Abel Seyler live when he died?] -> [Tirana]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.208139896392822\n",
      "Total loss 7.208139896392822\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 4.67896842956543\n",
      "Total loss 4.67896842956543\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.030844211578369\n",
      "Total loss 3.030844211578369\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7753530144691467\n",
      "Total loss 0.7753530144691467\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.8732513189315796\n",
      "Total loss 0.8732513189315796\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.9528228640556335\n",
      "Total loss 0.9528228640556335\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.0091294050216675\n",
      "Total loss 1.0091294050216675\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.031429409980774\n",
      "Total loss 1.031429409980774\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.0181657075881958\n",
      "Total loss 1.0181657075881958\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.9801937937736511\n",
      "Total loss 0.9801937937736511\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.9293602705001831\n",
      "Total loss 0.9293602705001831\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.8698524832725525\n",
      "Total loss 0.8698524832725525\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.8061149716377258\n",
      "Total loss 0.8061149716377258\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.741072952747345\n",
      "Total loss 0.741072952747345\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.6780296564102173\n",
      "Total loss 0.6780296564102173\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.6207835078239441\n",
      "Total loss 0.6207835078239441\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.5660815834999084\n",
      "Total loss 0.5660815834999084\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.5144674181938171\n",
      "Total loss 0.5144674181938171\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.4686712622642517\n",
      "Total loss 0.4686712622642517\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.42707163095474243\n",
      "Total loss 0.42707163095474243\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.3898574411869049\n",
      "Total loss 0.3898574411869049\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.3583228290081024\n",
      "Total loss 0.3583228290081024\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3314671814441681\n",
      "Total loss 0.3314671814441681\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.30789703130722046\n",
      "Total loss 0.30789703130722046\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.284236878156662\n",
      "Total loss 0.284236878156662\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.2611101567745209\n",
      "Total loss 0.2611101567745209\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.23966437578201294\n",
      "Total loss 0.23966437578201294\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.22059644758701324\n",
      "Total loss 0.22059644758701324\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.2047555297613144\n",
      "Total loss 0.2047555297613144\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.19082941114902496\n",
      "Total loss 0.19082941114902496\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.17760685086250305\n",
      "Total loss 0.17760685086250305\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.16615745425224304\n",
      "Total loss 0.16615745425224304\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.15584295988082886\n",
      "Total loss 0.15584295988082886\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.1473788619041443\n",
      "Total loss 0.1473788619041443\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.13976840674877167\n",
      "Total loss 0.13976840674877167\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.1307796835899353\n",
      "Total loss 0.1307796835899353\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.1232488602399826\n",
      "Total loss 0.1232488602399826\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.11632595211267471\n",
      "Total loss 0.11632595211267471\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.10949107259511948\n",
      "Total loss 0.10949107259511948\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.102681465446949\n",
      "Total loss 0.102681465446949\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.09565151482820511\n",
      "Total loss 0.09565151482820511\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.08993041515350342\n",
      "Total loss 0.08993041515350342\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.08405690640211105\n",
      "Total loss 0.08405690640211105\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.08083714544773102\n",
      "Total loss 0.08083714544773102\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.07684299349784851\n",
      "Total loss 0.07684299349784851\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.07180943340063095\n",
      "Total loss 0.07180943340063095\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.06802998483181\n",
      "Total loss 0.06802998483181\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.06459568440914154\n",
      "Total loss 0.06459568440914154\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.06117451190948486\n",
      "Total loss 0.06117451190948486\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.05696742981672287\n",
      "Total loss 0.05696742981672287\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.05485856905579567\n",
      "Total loss 0.05485856905579567\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.05221050605177879\n",
      "Total loss 0.05221050605177879\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.05006558075547218\n",
      "Total loss 0.05006558075547218\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04827851429581642\n",
      "Total loss 0.04827851429581642\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.046571727842092514\n",
      "Total loss 0.046571727842092514\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.045231498777866364\n",
      "Total loss 0.045231498777866364\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04412039741873741\n",
      "Total loss 0.04412039741873741\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.042408641427755356\n",
      "Total loss 0.042408641427755356\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.04138284921646118\n",
      "Total loss 0.04138284921646118\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.038975466042757034\n",
      "Total loss 0.038975466042757034\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03941566124558449\n",
      "Total loss 0.03941566124558449\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03801567479968071\n",
      "Total loss 0.03801567479968071\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.037513602524995804\n",
      "Total loss 0.037513602524995804\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03714171424508095\n",
      "Total loss 0.03714171424508095\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03562554717063904\n",
      "Total loss 0.03562554717063904\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03610171377658844\n",
      "Total loss 0.03610171377658844\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03508959710597992\n",
      "Total loss 0.03508959710597992\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.034779902547597885\n",
      "Total loss 0.034779902547597885\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03508787229657173\n",
      "Total loss 0.03508787229657173\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03400120139122009\n",
      "Total loss 0.03400120139122009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:51:29,172 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:51:29 - INFO - easyeditor.editors.editor -   3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  8%|▊         | 4/50 [01:41<19:46, 25.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [In which year was the service entry date for Kh-58?] -> [1980]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.204338312149048\n",
      "Total loss 3.204338312149048\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.7819442749023438\n",
      "Total loss 1.7819442749023438\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8479734659194946\n",
      "Total loss 0.8479734659194946\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6775450110435486\n",
      "Total loss 0.6775450110435486\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6341421008110046\n",
      "Total loss 0.6341421008110046\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6015759110450745\n",
      "Total loss 0.6015759110450745\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.5748606324195862\n",
      "Total loss 0.5748606324195862\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.5256219506263733\n",
      "Total loss 0.5256219506263733\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.4639904499053955\n",
      "Total loss 0.4639904499053955\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.41261938214302063\n",
      "Total loss 0.41261938214302063\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.37352997064590454\n",
      "Total loss 0.37352997064590454\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3366900682449341\n",
      "Total loss 0.3366900682449341\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.30273738503456116\n",
      "Total loss 0.30273738503456116\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.27644968032836914\n",
      "Total loss 0.27644968032836914\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.25767406821250916\n",
      "Total loss 0.25767406821250916\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.24180868268013\n",
      "Total loss 0.24180868268013\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.22595356404781342\n",
      "Total loss 0.22595356404781342\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.21121153235435486\n",
      "Total loss 0.21121153235435486\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.19690321385860443\n",
      "Total loss 0.19690321385860443\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.18223154544830322\n",
      "Total loss 0.18223154544830322\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.1704874336719513\n",
      "Total loss 0.1704874336719513\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.15863293409347534\n",
      "Total loss 0.15863293409347534\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.14639197289943695\n",
      "Total loss 0.14639197289943695\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.13464117050170898\n",
      "Total loss 0.13464117050170898\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.12540900707244873\n",
      "Total loss 0.12540900707244873\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1142662912607193\n",
      "Total loss 0.1142662912607193\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.10660235583782196\n",
      "Total loss 0.10660235583782196\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.10083422809839249\n",
      "Total loss 0.10083422809839249\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0942746251821518\n",
      "Total loss 0.0942746251821518\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.08820609003305435\n",
      "Total loss 0.08820609003305435\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.08155803382396698\n",
      "Total loss 0.08155803382396698\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.07562939077615738\n",
      "Total loss 0.07562939077615738\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0715814083814621\n",
      "Total loss 0.0715814083814621\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.06997856497764587\n",
      "Total loss 0.06997856497764587\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06606098264455795\n",
      "Total loss 0.06606098264455795\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06496573984622955\n",
      "Total loss 0.06496573984622955\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0623127780854702\n",
      "Total loss 0.0623127780854702\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.058033086359500885\n",
      "Total loss 0.058033086359500885\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.058741629123687744\n",
      "Total loss 0.058741629123687744\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05655500292778015\n",
      "Total loss 0.05655500292778015\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.054705724120140076\n",
      "Total loss 0.054705724120140076\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0525987483561039\n",
      "Total loss 0.0525987483561039\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05167820304632187\n",
      "Total loss 0.05167820304632187\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04939277470111847\n",
      "Total loss 0.04939277470111847\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04792465642094612\n",
      "Total loss 0.04792465642094612\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04708807170391083\n",
      "Total loss 0.04708807170391083\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.046148695051670074\n",
      "Total loss 0.046148695051670074\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04519551247358322\n",
      "Total loss 0.04519551247358322\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.044370848685503006\n",
      "Total loss 0.044370848685503006\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0425744354724884\n",
      "Total loss 0.0425744354724884\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.041787147521972656\n",
      "Total loss 0.041787147521972656\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04024364426732063\n",
      "Total loss 0.04024364426732063\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.039958856999874115\n",
      "Total loss 0.039958856999874115\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03790920600295067\n",
      "Total loss 0.03790920600295067\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03839651495218277\n",
      "Total loss 0.03839651495218277\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.037737153470516205\n",
      "Total loss 0.037737153470516205\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03701768070459366\n",
      "Total loss 0.03701768070459366\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03703317046165466\n",
      "Total loss 0.03703317046165466\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.035668838769197464\n",
      "Total loss 0.035668838769197464\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03723179176449776\n",
      "Total loss 0.03723179176449776\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.034678082913160324\n",
      "Total loss 0.034678082913160324\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.036193810403347015\n",
      "Total loss 0.036193810403347015\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03440191224217415\n",
      "Total loss 0.03440191224217415\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03525657579302788\n",
      "Total loss 0.03525657579302788\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03510945662856102\n",
      "Total loss 0.03510945662856102\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.034370943903923035\n",
      "Total loss 0.034370943903923035\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03500949218869209\n",
      "Total loss 0.03500949218869209\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0340232327580452\n",
      "Total loss 0.0340232327580452\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03404445946216583\n",
      "Total loss 0.03404445946216583\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03384554386138916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:51:53,502 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:51:53 - INFO - easyeditor.editors.editor -   4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 10%|█         | 5/50 [02:05<18:56, 25.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03384554386138916\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Which college or university is related with Gar Forman?] -> [Brown University]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.670093536376953\n",
      "Total loss 6.670093536376953\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 4.169139862060547\n",
      "Total loss 4.169139862060547\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.7107017040252686\n",
      "Total loss 2.7107017040252686\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.510727643966675\n",
      "Total loss 2.510727643966675\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.212564468383789\n",
      "Total loss 3.212564468383789\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.919636070728302\n",
      "Total loss 0.919636070728302\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.9827500581741333\n",
      "Total loss 0.9827500581741333\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.0630781650543213\n",
      "Total loss 1.0630781650543213\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.1236461400985718\n",
      "Total loss 1.1236461400985718\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.159558653831482\n",
      "Total loss 1.159558653831482\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.1729066371917725\n",
      "Total loss 1.1729066371917725\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.1656798124313354\n",
      "Total loss 1.1656798124313354\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.1402313709259033\n",
      "Total loss 1.1402313709259033\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.0998165607452393\n",
      "Total loss 1.0998165607452393\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.0502164363861084\n",
      "Total loss 1.0502164363861084\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.9948298335075378\n",
      "Total loss 0.9948298335075378\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.9369854927062988\n",
      "Total loss 0.9369854927062988\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.8779479265213013\n",
      "Total loss 0.8779479265213013\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.8200864195823669\n",
      "Total loss 0.8200864195823669\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.7655850052833557\n",
      "Total loss 0.7655850052833557\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.7135616540908813\n",
      "Total loss 0.7135616540908813\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6640109419822693\n",
      "Total loss 0.6640109419822693\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.6181696057319641\n",
      "Total loss 0.6181696057319641\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.5767613649368286\n",
      "Total loss 0.5767613649368286\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.538289487361908\n",
      "Total loss 0.538289487361908\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.5021460652351379\n",
      "Total loss 0.5021460652351379\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.4689404368400574\n",
      "Total loss 0.4689404368400574\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.43828973174095154\n",
      "Total loss 0.43828973174095154\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.4094296395778656\n",
      "Total loss 0.4094296395778656\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.3834424912929535\n",
      "Total loss 0.3834424912929535\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.3605457842350006\n",
      "Total loss 0.3605457842350006\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.34067395329475403\n",
      "Total loss 0.34067395329475403\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.32182979583740234\n",
      "Total loss 0.32182979583740234\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.30426323413848877\n",
      "Total loss 0.30426323413848877\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.2867470979690552\n",
      "Total loss 0.2867470979690552\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.26990899443626404\n",
      "Total loss 0.26990899443626404\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.25380265712738037\n",
      "Total loss 0.25380265712738037\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.23949621617794037\n",
      "Total loss 0.23949621617794037\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.22549109160900116\n",
      "Total loss 0.22549109160900116\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.21272321045398712\n",
      "Total loss 0.21272321045398712\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.2000180184841156\n",
      "Total loss 0.2000180184841156\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.1879754662513733\n",
      "Total loss 0.1879754662513733\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.17658494412899017\n",
      "Total loss 0.17658494412899017\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.16725993156433105\n",
      "Total loss 0.16725993156433105\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.15790309011936188\n",
      "Total loss 0.15790309011936188\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.1494954377412796\n",
      "Total loss 0.1494954377412796\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.14205418527126312\n",
      "Total loss 0.14205418527126312\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.1346319168806076\n",
      "Total loss 0.1346319168806076\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.12702633440494537\n",
      "Total loss 0.12702633440494537\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.12036346644163132\n",
      "Total loss 0.12036346644163132\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.11365493386983871\n",
      "Total loss 0.11365493386983871\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.10851539671421051\n",
      "Total loss 0.10851539671421051\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.1025780513882637\n",
      "Total loss 0.1025780513882637\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.09692259877920151\n",
      "Total loss 0.09692259877920151\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.09168298542499542\n",
      "Total loss 0.09168298542499542\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.08673544228076935\n",
      "Total loss 0.08673544228076935\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.08247926831245422\n",
      "Total loss 0.08247926831245422\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.07970507442951202\n",
      "Total loss 0.07970507442951202\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.07683783769607544\n",
      "Total loss 0.07683783769607544\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.07339003682136536\n",
      "Total loss 0.07339003682136536\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0714886263012886\n",
      "Total loss 0.0714886263012886\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0681324452161789\n",
      "Total loss 0.0681324452161789\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.06677687168121338\n",
      "Total loss 0.06677687168121338\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.06430438905954361\n",
      "Total loss 0.06430438905954361\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.06294196844100952\n",
      "Total loss 0.06294196844100952\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.060559939593076706\n",
      "Total loss 0.060559939593076706\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.05921993404626846\n",
      "Total loss 0.05921993404626846\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.056480903178453445\n",
      "Total loss 0.056480903178453445\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.053876664489507675\n",
      "Total loss 0.053876664489507675\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.05282467603683472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:52:17,836 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:52:17 - INFO - easyeditor.editors.editor -   5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 12%|█▏        | 6/50 [02:30<18:17, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.05282467603683472\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [The person that is the mother of Bushra al-Assad is who?] -> [Reba al-Assad]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.148066520690918\n",
      "Total loss 6.148066520690918\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 4.447464942932129\n",
      "Total loss 4.447464942932129\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.0020534992218018\n",
      "Total loss 2.0020534992218018\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8040145635604858\n",
      "Total loss 0.8040145635604858\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6654754281044006\n",
      "Total loss 0.6654754281044006\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6709045171737671\n",
      "Total loss 0.6709045171737671\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6707863807678223\n",
      "Total loss 0.6707863807678223\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6400467157363892\n",
      "Total loss 0.6400467157363892\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5803226828575134\n",
      "Total loss 0.5803226828575134\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.5191009640693665\n",
      "Total loss 0.5191009640693665\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.46973463892936707\n",
      "Total loss 0.46973463892936707\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.42487895488739014\n",
      "Total loss 0.42487895488739014\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.3861895799636841\n",
      "Total loss 0.3861895799636841\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.35398706793785095\n",
      "Total loss 0.35398706793785095\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3258725106716156\n",
      "Total loss 0.3258725106716156\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.3005377948284149\n",
      "Total loss 0.3005377948284149\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.2776763439178467\n",
      "Total loss 0.2776763439178467\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.25606387853622437\n",
      "Total loss 0.25606387853622437\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.23886525630950928\n",
      "Total loss 0.23886525630950928\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2236909121274948\n",
      "Total loss 0.2236909121274948\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.20746029913425446\n",
      "Total loss 0.20746029913425446\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.19009940326213837\n",
      "Total loss 0.19009940326213837\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.17320619523525238\n",
      "Total loss 0.17320619523525238\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.15586943924427032\n",
      "Total loss 0.15586943924427032\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.14184577763080597\n",
      "Total loss 0.14184577763080597\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1299077421426773\n",
      "Total loss 0.1299077421426773\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.11690795421600342\n",
      "Total loss 0.11690795421600342\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.10536134243011475\n",
      "Total loss 0.10536134243011475\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.09606266021728516\n",
      "Total loss 0.09606266021728516\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.08999063074588776\n",
      "Total loss 0.08999063074588776\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.08227822929620743\n",
      "Total loss 0.08227822929620743\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.07671806216239929\n",
      "Total loss 0.07671806216239929\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07244767993688583\n",
      "Total loss 0.07244767993688583\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07033633440732956\n",
      "Total loss 0.07033633440732956\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06841669976711273\n",
      "Total loss 0.06841669976711273\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06380745768547058\n",
      "Total loss 0.06380745768547058\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.06184568256139755\n",
      "Total loss 0.06184568256139755\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.05951574817299843\n",
      "Total loss 0.05951574817299843\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.057297512888908386\n",
      "Total loss 0.057297512888908386\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05456283316016197\n",
      "Total loss 0.05456283316016197\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05272984504699707\n",
      "Total loss 0.05272984504699707\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05162016674876213\n",
      "Total loss 0.05162016674876213\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04973594471812248\n",
      "Total loss 0.04973594471812248\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04901585727930069\n",
      "Total loss 0.04901585727930069\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04845812916755676\n",
      "Total loss 0.04845812916755676\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.047028880566358566\n",
      "Total loss 0.047028880566358566\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04567992314696312\n",
      "Total loss 0.04567992314696312\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04393700510263443\n",
      "Total loss 0.04393700510263443\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.042577385902404785\n",
      "Total loss 0.042577385902404785\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04260754585266113\n",
      "Total loss 0.04260754585266113\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04066985100507736\n",
      "Total loss 0.04066985100507736\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.039887696504592896\n",
      "Total loss 0.039887696504592896\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0390351302921772\n",
      "Total loss 0.0390351302921772\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.038005270063877106\n",
      "Total loss 0.038005270063877106\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03752119466662407\n",
      "Total loss 0.03752119466662407\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.037691038101911545\n",
      "Total loss 0.037691038101911545\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03652656450867653\n",
      "Total loss 0.03652656450867653\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03608997166156769\n",
      "Total loss 0.03608997166156769\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.037034548819065094\n",
      "Total loss 0.037034548819065094\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03539547324180603\n",
      "Total loss 0.03539547324180603\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.037027835845947266\n",
      "Total loss 0.037027835845947266\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03610438480973244\n",
      "Total loss 0.03610438480973244\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03660605847835541\n",
      "Total loss 0.03660605847835541\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03606323152780533\n",
      "Total loss 0.03606323152780533\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03503589332103729\n",
      "Total loss 0.03503589332103729\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.036053162068128586\n",
      "Total loss 0.036053162068128586\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.034653931856155396\n",
      "Total loss 0.034653931856155396\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03594112768769264\n",
      "Total loss 0.03594112768769264\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03462962061166763\n",
      "Total loss 0.03462962061166763\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03441553935408592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:52:42,002 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:52:42 - INFO - easyeditor.editors.editor -   6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 14%|█▍        | 7/50 [02:54<17:41, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03441553935408592\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Where did Mohammad Naseem live when he died?] -> [Tajikistan]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.1238603591918945\n",
      "Total loss 5.1238603591918945\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.507610559463501\n",
      "Total loss 3.507610559463501\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.495697498321533\n",
      "Total loss 2.495697498321533\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7199571132659912\n",
      "Total loss 0.7199571132659912\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.7193036079406738\n",
      "Total loss 0.7193036079406738\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.7753328084945679\n",
      "Total loss 0.7753328084945679\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.7908953428268433\n",
      "Total loss 0.7908953428268433\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.7910632491111755\n",
      "Total loss 0.7910632491111755\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.7590133547782898\n",
      "Total loss 0.7590133547782898\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.7081140875816345\n",
      "Total loss 0.7081140875816345\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.6537101864814758\n",
      "Total loss 0.6537101864814758\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.6006808876991272\n",
      "Total loss 0.6006808876991272\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.5504863262176514\n",
      "Total loss 0.5504863262176514\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.5025145411491394\n",
      "Total loss 0.5025145411491394\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.455897718667984\n",
      "Total loss 0.455897718667984\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.4156142473220825\n",
      "Total loss 0.4156142473220825\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.3795890808105469\n",
      "Total loss 0.3795890808105469\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.34625244140625\n",
      "Total loss 0.34625244140625\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.3176634609699249\n",
      "Total loss 0.3176634609699249\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2901975214481354\n",
      "Total loss 0.2901975214481354\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.2651253938674927\n",
      "Total loss 0.2651253938674927\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.24525012075901031\n",
      "Total loss 0.24525012075901031\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.22819176316261292\n",
      "Total loss 0.22819176316261292\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.21306663751602173\n",
      "Total loss 0.21306663751602173\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.19992609322071075\n",
      "Total loss 0.19992609322071075\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.18568558990955353\n",
      "Total loss 0.18568558990955353\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.17236986756324768\n",
      "Total loss 0.17236986756324768\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.16198204457759857\n",
      "Total loss 0.16198204457759857\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.14926935732364655\n",
      "Total loss 0.14926935732364655\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.1385280191898346\n",
      "Total loss 0.1385280191898346\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.12914630770683289\n",
      "Total loss 0.12914630770683289\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.12154814600944519\n",
      "Total loss 0.12154814600944519\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.11280453205108643\n",
      "Total loss 0.11280453205108643\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.1037474200129509\n",
      "Total loss 0.1037474200129509\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.09507396072149277\n",
      "Total loss 0.09507396072149277\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.08900763094425201\n",
      "Total loss 0.08900763094425201\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.08341632783412933\n",
      "Total loss 0.08341632783412933\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.07894334197044373\n",
      "Total loss 0.07894334197044373\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.07244078069925308\n",
      "Total loss 0.07244078069925308\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.06972815841436386\n",
      "Total loss 0.06972815841436386\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.06525424867868423\n",
      "Total loss 0.06525424867868423\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.06305937469005585\n",
      "Total loss 0.06305937469005585\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.06008024141192436\n",
      "Total loss 0.06008024141192436\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.05769823119044304\n",
      "Total loss 0.05769823119044304\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.05539436265826225\n",
      "Total loss 0.05539436265826225\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05401480942964554\n",
      "Total loss 0.05401480942964554\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05039848014712334\n",
      "Total loss 0.05039848014712334\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.049868859350681305\n",
      "Total loss 0.049868859350681305\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.046950049698352814\n",
      "Total loss 0.046950049698352814\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.045002084225416183\n",
      "Total loss 0.045002084225416183\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04292478412389755\n",
      "Total loss 0.04292478412389755\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04201729968190193\n",
      "Total loss 0.04201729968190193\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.041361551731824875\n",
      "Total loss 0.041361551731824875\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04041433706879616\n",
      "Total loss 0.04041433706879616\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.039326053112745285\n",
      "Total loss 0.039326053112745285\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.037859346717596054\n",
      "Total loss 0.037859346717596054\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03778566047549248\n",
      "Total loss 0.03778566047549248\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03679802641272545\n",
      "Total loss 0.03679802641272545\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03677372634410858\n",
      "Total loss 0.03677372634410858\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.037528667598962784\n",
      "Total loss 0.037528667598962784\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03633461892604828\n",
      "Total loss 0.03633461892604828\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03613946959376335\n",
      "Total loss 0.03613946959376335\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.036126188933849335\n",
      "Total loss 0.036126188933849335\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03628954663872719\n",
      "Total loss 0.03628954663872719\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.035493746399879456\n",
      "Total loss 0.035493746399879456\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03527729958295822\n",
      "Total loss 0.03527729958295822\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.034631967544555664\n",
      "Total loss 0.034631967544555664\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03398062288761139\n",
      "Total loss 0.03398062288761139\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03429979458451271\n",
      "Total loss 0.03429979458451271\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03237759694457054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:53:06,318 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:53:06 - INFO - easyeditor.editors.editor -   7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 16%|█▌        | 8/50 [03:18<17:12, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03237759694457054\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What was the year SR N15X class entered service?] -> [1990]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.157668113708496\n",
      "Total loss 4.157668113708496\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.2728991508483887\n",
      "Total loss 2.2728991508483887\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.569277048110962\n",
      "Total loss 1.569277048110962\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.128340721130371\n",
      "Total loss 2.128340721130371\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.0191230773925781\n",
      "Total loss 1.0191230773925781\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.8216730356216431\n",
      "Total loss 0.8216730356216431\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6998717784881592\n",
      "Total loss 0.6998717784881592\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.7013052105903625\n",
      "Total loss 0.7013052105903625\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.6913344264030457\n",
      "Total loss 0.6913344264030457\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.654379665851593\n",
      "Total loss 0.654379665851593\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.6128122806549072\n",
      "Total loss 0.6128122806549072\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.5703831911087036\n",
      "Total loss 0.5703831911087036\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.5239945650100708\n",
      "Total loss 0.5239945650100708\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.4800164997577667\n",
      "Total loss 0.4800164997577667\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.44340527057647705\n",
      "Total loss 0.44340527057647705\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.4122864902019501\n",
      "Total loss 0.4122864902019501\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.384386271238327\n",
      "Total loss 0.384386271238327\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.35814279317855835\n",
      "Total loss 0.35814279317855835\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.3339810073375702\n",
      "Total loss 0.3339810073375702\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.31335797905921936\n",
      "Total loss 0.31335797905921936\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.2926424741744995\n",
      "Total loss 0.2926424741744995\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2728557288646698\n",
      "Total loss 0.2728557288646698\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.2543277442455292\n",
      "Total loss 0.2543277442455292\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.23738588392734528\n",
      "Total loss 0.23738588392734528\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.2209986448287964\n",
      "Total loss 0.2209986448287964\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.20646809041500092\n",
      "Total loss 0.20646809041500092\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.19452302157878876\n",
      "Total loss 0.19452302157878876\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.1838100701570511\n",
      "Total loss 0.1838100701570511\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.17432020604610443\n",
      "Total loss 0.17432020604610443\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.16621144115924835\n",
      "Total loss 0.16621144115924835\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.1577393263578415\n",
      "Total loss 0.1577393263578415\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.15000630915164948\n",
      "Total loss 0.15000630915164948\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.14330226182937622\n",
      "Total loss 0.14330226182937622\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.1363992989063263\n",
      "Total loss 0.1363992989063263\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.12885785102844238\n",
      "Total loss 0.12885785102844238\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.1232299730181694\n",
      "Total loss 0.1232299730181694\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.1168103814125061\n",
      "Total loss 0.1168103814125061\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.10930541902780533\n",
      "Total loss 0.10930541902780533\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.10285728424787521\n",
      "Total loss 0.10285728424787521\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.09812983125448227\n",
      "Total loss 0.09812983125448227\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.09483466297388077\n",
      "Total loss 0.09483466297388077\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0900641530752182\n",
      "Total loss 0.0900641530752182\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.08524174243211746\n",
      "Total loss 0.08524174243211746\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0820503830909729\n",
      "Total loss 0.0820503830909729\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.07861188799142838\n",
      "Total loss 0.07861188799142838\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.07501550763845444\n",
      "Total loss 0.07501550763845444\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.07234951108694077\n",
      "Total loss 0.07234951108694077\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.06960906088352203\n",
      "Total loss 0.06960906088352203\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.06840721517801285\n",
      "Total loss 0.06840721517801285\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.06504256278276443\n",
      "Total loss 0.06504256278276443\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.06491636484861374\n",
      "Total loss 0.06491636484861374\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.061430152505636215\n",
      "Total loss 0.061430152505636215\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.06039201840758324\n",
      "Total loss 0.06039201840758324\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.05834117904305458\n",
      "Total loss 0.05834117904305458\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.05732033774256706\n",
      "Total loss 0.05732033774256706\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.056665267795324326\n",
      "Total loss 0.056665267795324326\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.053790658712387085\n",
      "Total loss 0.053790658712387085\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.05336148664355278\n",
      "Total loss 0.05336148664355278\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.05043889209628105\n",
      "Total loss 0.05043889209628105\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.050136666744947433\n",
      "Total loss 0.050136666744947433\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04874253645539284\n",
      "Total loss 0.04874253645539284\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.047440867871046066\n",
      "Total loss 0.047440867871046066\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.04684244096279144\n",
      "Total loss 0.04684244096279144\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.04507411643862724\n",
      "Total loss 0.04507411643862724\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.045137617737054825\n",
      "Total loss 0.045137617737054825\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.042895954102277756\n",
      "Total loss 0.042895954102277756\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.04230453446507454\n",
      "Total loss 0.04230453446507454\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.041474319994449615\n",
      "Total loss 0.041474319994449615\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.04027208313345909\n",
      "Total loss 0.04027208313345909\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.04054007679224014\n",
      "Total loss 0.04054007679224014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:53:35,055 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:53:35 - INFO - easyeditor.editors.editor -   8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 18%|█▊        | 9/50 [03:47<17:40, 25.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Which college or university is related with Rose Ann Scamardella?] -> [Columbia University]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.5951690673828125\n",
      "Total loss 5.5951690673828125\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.799959659576416\n",
      "Total loss 2.799959659576416\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8358807563781738\n",
      "Total loss 0.8358807563781738\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6745063662528992\n",
      "Total loss 0.6745063662528992\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6464520692825317\n",
      "Total loss 0.6464520692825317\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6022195816040039\n",
      "Total loss 0.6022195816040039\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.5652292370796204\n",
      "Total loss 0.5652292370796204\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.511199951171875\n",
      "Total loss 0.511199951171875\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.44244909286499023\n",
      "Total loss 0.44244909286499023\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.38752293586730957\n",
      "Total loss 0.38752293586730957\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.348628431558609\n",
      "Total loss 0.348628431558609\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3121061623096466\n",
      "Total loss 0.3121061623096466\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.28112688660621643\n",
      "Total loss 0.28112688660621643\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.2568908631801605\n",
      "Total loss 0.2568908631801605\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.23726484179496765\n",
      "Total loss 0.23726484179496765\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2203185111284256\n",
      "Total loss 0.2203185111284256\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.2019912302494049\n",
      "Total loss 0.2019912302494049\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.18457140028476715\n",
      "Total loss 0.18457140028476715\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.17053979635238647\n",
      "Total loss 0.17053979635238647\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.15480734407901764\n",
      "Total loss 0.15480734407901764\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.14158235490322113\n",
      "Total loss 0.14158235490322113\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.13155537843704224\n",
      "Total loss 0.13155537843704224\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.12146884202957153\n",
      "Total loss 0.12146884202957153\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1112692579627037\n",
      "Total loss 0.1112692579627037\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.10264582931995392\n",
      "Total loss 0.10264582931995392\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.09431121498346329\n",
      "Total loss 0.09431121498346329\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.08653384447097778\n",
      "Total loss 0.08653384447097778\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.08126088231801987\n",
      "Total loss 0.08126088231801987\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.07540887594223022\n",
      "Total loss 0.07540887594223022\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.07100818306207657\n",
      "Total loss 0.07100818306207657\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.06959383934736252\n",
      "Total loss 0.06959383934736252\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.06420785188674927\n",
      "Total loss 0.06420785188674927\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.06221509724855423\n",
      "Total loss 0.06221509724855423\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.06122002378106117\n",
      "Total loss 0.06122002378106117\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06024257838726044\n",
      "Total loss 0.06024257838726044\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.05677225440740585\n",
      "Total loss 0.05677225440740585\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.05655287206172943\n",
      "Total loss 0.05655287206172943\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.05528206378221512\n",
      "Total loss 0.05528206378221512\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.052687399089336395\n",
      "Total loss 0.052687399089336395\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.050590526312589645\n",
      "Total loss 0.050590526312589645\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.04866483807563782\n",
      "Total loss 0.04866483807563782\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.04910726100206375\n",
      "Total loss 0.04910726100206375\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04672020301222801\n",
      "Total loss 0.04672020301222801\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.045054227113723755\n",
      "Total loss 0.045054227113723755\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04497326910495758\n",
      "Total loss 0.04497326910495758\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04334002360701561\n",
      "Total loss 0.04334002360701561\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04313362389802933\n",
      "Total loss 0.04313362389802933\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04200832545757294\n",
      "Total loss 0.04200832545757294\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04041488468647003\n",
      "Total loss 0.04041488468647003\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.03991980105638504\n",
      "Total loss 0.03991980105638504\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0388641394674778\n",
      "Total loss 0.0388641394674778\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03889675438404083\n",
      "Total loss 0.03889675438404083\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03899776190519333\n",
      "Total loss 0.03899776190519333\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0381377711892128\n",
      "Total loss 0.0381377711892128\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03823135420680046\n",
      "Total loss 0.03823135420680046\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.037490665912628174\n",
      "Total loss 0.037490665912628174\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.036625370383262634\n",
      "Total loss 0.036625370383262634\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03686349838972092\n",
      "Total loss 0.03686349838972092\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03792316094040871\n",
      "Total loss 0.03792316094040871\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03517536073923111\n",
      "Total loss 0.03517536073923111\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.037698544561862946\n",
      "Total loss 0.037698544561862946\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0355755053460598\n",
      "Total loss 0.0355755053460598\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.036169495433568954\n",
      "Total loss 0.036169495433568954\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03577001020312309\n",
      "Total loss 0.03577001020312309\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03484733775258064\n",
      "Total loss 0.03484733775258064\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03607518970966339\n",
      "Total loss 0.03607518970966339\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.034538596868515015\n",
      "Total loss 0.034538596868515015\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.036300208419561386\n",
      "Total loss 0.036300208419561386\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03468507528305054\n",
      "Total loss 0.03468507528305054\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03512454032897949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:53:59,389 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:53:59 - INFO - easyeditor.editors.editor -   9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 20%|██        | 10/50 [04:11<16:55, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03512454032897949\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What studio produced Kaaki Sattai?] -> [Yash Raj Movies]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.599885940551758\n",
      "Total loss 7.599885940551758\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 4.980886936187744\n",
      "Total loss 4.980886936187744\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.1016898155212402\n",
      "Total loss 2.1016898155212402\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.3510863780975342\n",
      "Total loss 1.3510863780975342\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.9721251726150513\n",
      "Total loss 0.9721251726150513\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.9826090931892395\n",
      "Total loss 0.9826090931892395\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.0079561471939087\n",
      "Total loss 1.0079561471939087\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.9971238970756531\n",
      "Total loss 0.9971238970756531\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.9511167407035828\n",
      "Total loss 0.9511167407035828\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.8842270970344543\n",
      "Total loss 0.8842270970344543\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.8107216954231262\n",
      "Total loss 0.8107216954231262\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.7380452156066895\n",
      "Total loss 0.7380452156066895\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.6655445098876953\n",
      "Total loss 0.6655445098876953\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.5987010598182678\n",
      "Total loss 0.5987010598182678\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.5421797633171082\n",
      "Total loss 0.5421797633171082\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.49386438727378845\n",
      "Total loss 0.49386438727378845\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.45044663548469543\n",
      "Total loss 0.45044663548469543\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.41376760601997375\n",
      "Total loss 0.41376760601997375\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.38177579641342163\n",
      "Total loss 0.38177579641342163\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.353087455034256\n",
      "Total loss 0.353087455034256\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.3277038037776947\n",
      "Total loss 0.3277038037776947\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.30543217062950134\n",
      "Total loss 0.30543217062950134\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.2848889231681824\n",
      "Total loss 0.2848889231681824\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.266409307718277\n",
      "Total loss 0.266409307718277\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.24963726103305817\n",
      "Total loss 0.24963726103305817\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.23133383691310883\n",
      "Total loss 0.23133383691310883\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.21249385178089142\n",
      "Total loss 0.21249385178089142\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.1947706937789917\n",
      "Total loss 0.1947706937789917\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.17989006638526917\n",
      "Total loss 0.17989006638526917\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.1671038568019867\n",
      "Total loss 0.1671038568019867\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.15511181950569153\n",
      "Total loss 0.15511181950569153\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.14478948712348938\n",
      "Total loss 0.14478948712348938\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.13487812876701355\n",
      "Total loss 0.13487812876701355\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.1245146244764328\n",
      "Total loss 0.1245146244764328\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.11603546142578125\n",
      "Total loss 0.11603546142578125\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.1079278439283371\n",
      "Total loss 0.1079278439283371\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.10191977024078369\n",
      "Total loss 0.10191977024078369\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.09486208111047745\n",
      "Total loss 0.09486208111047745\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.08896314352750778\n",
      "Total loss 0.08896314352750778\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.08467569947242737\n",
      "Total loss 0.08467569947242737\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.08154487609863281\n",
      "Total loss 0.08154487609863281\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.07791124284267426\n",
      "Total loss 0.07791124284267426\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0744205117225647\n",
      "Total loss 0.0744205117225647\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.06970013678073883\n",
      "Total loss 0.06970013678073883\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.06640006601810455\n",
      "Total loss 0.06640006601810455\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.06415696442127228\n",
      "Total loss 0.06415696442127228\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.06074090301990509\n",
      "Total loss 0.06074090301990509\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0585278756916523\n",
      "Total loss 0.0585278756916523\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.05594177171587944\n",
      "Total loss 0.05594177171587944\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.05451538413763046\n",
      "Total loss 0.05451538413763046\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.05348992347717285\n",
      "Total loss 0.05348992347717285\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.05202517658472061\n",
      "Total loss 0.05202517658472061\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04846195504069328\n",
      "Total loss 0.04846195504069328\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04844425991177559\n",
      "Total loss 0.04844425991177559\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0466894656419754\n",
      "Total loss 0.0466894656419754\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0451698824763298\n",
      "Total loss 0.0451698824763298\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.043532006442546844\n",
      "Total loss 0.043532006442546844\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.04216291755437851\n",
      "Total loss 0.04216291755437851\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.04126604646444321\n",
      "Total loss 0.04126604646444321\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.04107453301548958\n",
      "Total loss 0.04107453301548958\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03946202993392944\n",
      "Total loss 0.03946202993392944\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03921342268586159\n",
      "Total loss 0.03921342268586159\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.037986189126968384\n",
      "Total loss 0.037986189126968384\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03766443580389023\n",
      "Total loss 0.03766443580389023\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.036696430295705795\n",
      "Total loss 0.036696430295705795\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03686228021979332\n",
      "Total loss 0.03686228021979332\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.036740727722644806\n",
      "Total loss 0.036740727722644806\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03592757508158684\n",
      "Total loss 0.03592757508158684\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03533336520195007\n",
      "Total loss 0.03533336520195007\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03488289937376976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:54:24,018 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:54:24 - INFO - easyeditor.editors.editor -   10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 22%|██▏       | 11/50 [04:36<16:21, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03488289937376976\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [In which year Kaabu ceased to exist?] -> [1994]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.323912620544434\n",
      "Total loss 4.323912620544434\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.462719440460205\n",
      "Total loss 2.462719440460205\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8264875411987305\n",
      "Total loss 0.8264875411987305\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6174976229667664\n",
      "Total loss 0.6174976229667664\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.5834986567497253\n",
      "Total loss 0.5834986567497253\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.5485851764678955\n",
      "Total loss 0.5485851764678955\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.5336009860038757\n",
      "Total loss 0.5336009860038757\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.49318328499794006\n",
      "Total loss 0.49318328499794006\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.4355670213699341\n",
      "Total loss 0.4355670213699341\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4000106155872345\n",
      "Total loss 0.4000106155872345\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.3729811906814575\n",
      "Total loss 0.3729811906814575\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3397418260574341\n",
      "Total loss 0.3397418260574341\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.3109929859638214\n",
      "Total loss 0.3109929859638214\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.28765472769737244\n",
      "Total loss 0.28765472769737244\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.26579371094703674\n",
      "Total loss 0.26579371094703674\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2482011318206787\n",
      "Total loss 0.2482011318206787\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.22980628907680511\n",
      "Total loss 0.22980628907680511\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.2130417674779892\n",
      "Total loss 0.2130417674779892\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.2002141773700714\n",
      "Total loss 0.2002141773700714\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.18550196290016174\n",
      "Total loss 0.18550196290016174\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.17159968614578247\n",
      "Total loss 0.17159968614578247\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.16148576140403748\n",
      "Total loss 0.16148576140403748\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.14991958439350128\n",
      "Total loss 0.14991958439350128\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.13840267062187195\n",
      "Total loss 0.13840267062187195\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.12799325585365295\n",
      "Total loss 0.12799325585365295\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1158495545387268\n",
      "Total loss 0.1158495545387268\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.10649215430021286\n",
      "Total loss 0.10649215430021286\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.09936131536960602\n",
      "Total loss 0.09936131536960602\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.09122111648321152\n",
      "Total loss 0.09122111648321152\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0858580619096756\n",
      "Total loss 0.0858580619096756\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.08086907863616943\n",
      "Total loss 0.08086907863616943\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.07621196657419205\n",
      "Total loss 0.07621196657419205\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.072882741689682\n",
      "Total loss 0.072882741689682\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0700959637761116\n",
      "Total loss 0.0700959637761116\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06775997579097748\n",
      "Total loss 0.06775997579097748\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06335912644863129\n",
      "Total loss 0.06335912644863129\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0601789727807045\n",
      "Total loss 0.0601789727807045\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0591900460422039\n",
      "Total loss 0.0591900460422039\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.057760074734687805\n",
      "Total loss 0.057760074734687805\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05427176505327225\n",
      "Total loss 0.05427176505327225\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05444198101758957\n",
      "Total loss 0.05444198101758957\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0520484633743763\n",
      "Total loss 0.0520484633743763\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.051284823566675186\n",
      "Total loss 0.051284823566675186\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04753894358873367\n",
      "Total loss 0.04753894358873367\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04850068315863609\n",
      "Total loss 0.04850068315863609\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04481879621744156\n",
      "Total loss 0.04481879621744156\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.046300794929265976\n",
      "Total loss 0.046300794929265976\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04465675354003906\n",
      "Total loss 0.04465675354003906\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04321829602122307\n",
      "Total loss 0.04321829602122307\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.043827809393405914\n",
      "Total loss 0.043827809393405914\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04152732342481613\n",
      "Total loss 0.04152732342481613\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04153123125433922\n",
      "Total loss 0.04153123125433922\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.039887890219688416\n",
      "Total loss 0.039887890219688416\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03983448073267937\n",
      "Total loss 0.03983448073267937\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.039595939218997955\n",
      "Total loss 0.039595939218997955\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03901229426264763\n",
      "Total loss 0.03901229426264763\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03867588937282562\n",
      "Total loss 0.03867588937282562\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03842421993613243\n",
      "Total loss 0.03842421993613243\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.037154845893383026\n",
      "Total loss 0.037154845893383026\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.036764439195394516\n",
      "Total loss 0.036764439195394516\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03666194900870323\n",
      "Total loss 0.03666194900870323\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.035845644772052765\n",
      "Total loss 0.035845644772052765\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03632121533155441\n",
      "Total loss 0.03632121533155441\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03510575741529465\n",
      "Total loss 0.03510575741529465\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.036060310900211334\n",
      "Total loss 0.036060310900211334\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03564729541540146\n",
      "Total loss 0.03564729541540146\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.036058735102415085\n",
      "Total loss 0.036058735102415085\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.035376328974962234\n",
      "Total loss 0.035376328974962234\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03691000118851662\n",
      "Total loss 0.03691000118851662\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03506846725940704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:54:47,646 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:54:47 - INFO - easyeditor.editors.editor -   11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 24%|██▍       | 12/50 [05:00<15:38, 24.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03506846725940704\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What was the cause of Mavis Villiers's death?] -> [breast cancer]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.5078125\n",
      "Total loss 8.5078125\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 6.1956562995910645\n",
      "Total loss 6.1956562995910645\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.7968432903289795\n",
      "Total loss 1.7968432903289795\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7506358623504639\n",
      "Total loss 0.7506358623504639\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.832245945930481\n",
      "Total loss 0.832245945930481\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.8973170518875122\n",
      "Total loss 0.8973170518875122\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.9352567195892334\n",
      "Total loss 0.9352567195892334\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.9344568252563477\n",
      "Total loss 0.9344568252563477\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.8963271379470825\n",
      "Total loss 0.8963271379470825\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.8411526679992676\n",
      "Total loss 0.8411526679992676\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.7802501916885376\n",
      "Total loss 0.7802501916885376\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.7164479494094849\n",
      "Total loss 0.7164479494094849\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.6528158783912659\n",
      "Total loss 0.6528158783912659\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.5903376936912537\n",
      "Total loss 0.5903376936912537\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.5320051312446594\n",
      "Total loss 0.5320051312446594\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.479442298412323\n",
      "Total loss 0.479442298412323\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.4333495497703552\n",
      "Total loss 0.4333495497703552\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.393662691116333\n",
      "Total loss 0.393662691116333\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.35631707310676575\n",
      "Total loss 0.35631707310676575\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.32178935408592224\n",
      "Total loss 0.32178935408592224\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.2914247512817383\n",
      "Total loss 0.2914247512817383\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2667997479438782\n",
      "Total loss 0.2667997479438782\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.24503113329410553\n",
      "Total loss 0.24503113329410553\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.2246207892894745\n",
      "Total loss 0.2246207892894745\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.2063334882259369\n",
      "Total loss 0.2063334882259369\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.18876595795154572\n",
      "Total loss 0.18876595795154572\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.17230647802352905\n",
      "Total loss 0.17230647802352905\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.15893395245075226\n",
      "Total loss 0.15893395245075226\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.1468857079744339\n",
      "Total loss 0.1468857079744339\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.13392238318920135\n",
      "Total loss 0.13392238318920135\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.12357451021671295\n",
      "Total loss 0.12357451021671295\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.11488832533359528\n",
      "Total loss 0.11488832533359528\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.10653582960367203\n",
      "Total loss 0.10653582960367203\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0993049368262291\n",
      "Total loss 0.0993049368262291\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0921124517917633\n",
      "Total loss 0.0921124517917633\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.08618932217359543\n",
      "Total loss 0.08618932217359543\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.08103122562170029\n",
      "Total loss 0.08103122562170029\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.07527697086334229\n",
      "Total loss 0.07527697086334229\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.07146909832954407\n",
      "Total loss 0.07146909832954407\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.06715044379234314\n",
      "Total loss 0.06715044379234314\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.06330528855323792\n",
      "Total loss 0.06330528855323792\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.059307459741830826\n",
      "Total loss 0.059307459741830826\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05669819936156273\n",
      "Total loss 0.05669819936156273\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.054737892001867294\n",
      "Total loss 0.054737892001867294\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.05303005129098892\n",
      "Total loss 0.05303005129098892\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05088934302330017\n",
      "Total loss 0.05088934302330017\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04920567199587822\n",
      "Total loss 0.04920567199587822\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.047777265310287476\n",
      "Total loss 0.047777265310287476\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04704202339053154\n",
      "Total loss 0.04704202339053154\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04477576166391373\n",
      "Total loss 0.04477576166391373\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04380840063095093\n",
      "Total loss 0.04380840063095093\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04241654649376869\n",
      "Total loss 0.04241654649376869\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.040194395929574966\n",
      "Total loss 0.040194395929574966\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.039753567427396774\n",
      "Total loss 0.039753567427396774\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03855833783745766\n",
      "Total loss 0.03855833783745766\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03778862953186035\n",
      "Total loss 0.03778862953186035\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03760847821831703\n",
      "Total loss 0.03760847821831703\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.037157345563173294\n",
      "Total loss 0.037157345563173294\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03723509609699249\n",
      "Total loss 0.03723509609699249\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.037371374666690826\n",
      "Total loss 0.037371374666690826\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03535965830087662\n",
      "Total loss 0.03535965830087662\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03554818406701088\n",
      "Total loss 0.03554818406701088\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.034402813762426376\n",
      "Total loss 0.034402813762426376\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03449281305074692\n",
      "Total loss 0.03449281305074692\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03454751521348953\n",
      "Total loss 0.03454751521348953\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03461991623044014\n",
      "Total loss 0.03461991623044014\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03447088226675987\n",
      "Total loss 0.03447088226675987\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03440646454691887\n",
      "Total loss 0.03440646454691887\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03489287942647934\n",
      "Total loss 0.03489287942647934\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03386317938566208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:55:13,255 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:55:13 - INFO - easyeditor.editors.editor -   12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03386317938566208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [05:25<15:24, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What label was responsible for United Abominations?] -> [Arista Records]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.748290538787842\n",
      "Total loss 6.748290538787842\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.4956929683685303\n",
      "Total loss 3.4956929683685303\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.5785629749298096\n",
      "Total loss 1.5785629749298096\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8831586837768555\n",
      "Total loss 0.8831586837768555\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.8375619053840637\n",
      "Total loss 0.8375619053840637\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.881067156791687\n",
      "Total loss 0.881067156791687\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.900192379951477\n",
      "Total loss 0.900192379951477\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.8866929411888123\n",
      "Total loss 0.8866929411888123\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.8452967405319214\n",
      "Total loss 0.8452967405319214\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.7911320924758911\n",
      "Total loss 0.7911320924758911\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.7321938872337341\n",
      "Total loss 0.7321938872337341\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.6699010133743286\n",
      "Total loss 0.6699010133743286\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.6079602241516113\n",
      "Total loss 0.6079602241516113\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.5491223335266113\n",
      "Total loss 0.5491223335266113\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.4948887825012207\n",
      "Total loss 0.4948887825012207\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.4506935477256775\n",
      "Total loss 0.4506935477256775\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.41106683015823364\n",
      "Total loss 0.41106683015823364\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.3749193549156189\n",
      "Total loss 0.3749193549156189\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.34209126234054565\n",
      "Total loss 0.34209126234054565\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.3111928403377533\n",
      "Total loss 0.3111928403377533\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.28186067938804626\n",
      "Total loss 0.28186067938804626\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2561986446380615\n",
      "Total loss 0.2561986446380615\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.23550312221050262\n",
      "Total loss 0.23550312221050262\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.21588721871376038\n",
      "Total loss 0.21588721871376038\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.1974414885044098\n",
      "Total loss 0.1974414885044098\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.18222223222255707\n",
      "Total loss 0.18222223222255707\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.16996212303638458\n",
      "Total loss 0.16996212303638458\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.15804097056388855\n",
      "Total loss 0.15804097056388855\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.1479811668395996\n",
      "Total loss 0.1479811668395996\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.13806048035621643\n",
      "Total loss 0.13806048035621643\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.12799790501594543\n",
      "Total loss 0.12799790501594543\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.11966047435998917\n",
      "Total loss 0.11966047435998917\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.11210600286722183\n",
      "Total loss 0.11210600286722183\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.10491340607404709\n",
      "Total loss 0.10491340607404709\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0993436947464943\n",
      "Total loss 0.0993436947464943\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.09356974810361862\n",
      "Total loss 0.09356974810361862\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.08839324861764908\n",
      "Total loss 0.08839324861764908\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.08378414064645767\n",
      "Total loss 0.08378414064645767\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.07721175253391266\n",
      "Total loss 0.07721175253391266\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.07418902218341827\n",
      "Total loss 0.07418902218341827\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.07082489877939224\n",
      "Total loss 0.07082489877939224\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0680556669831276\n",
      "Total loss 0.0680556669831276\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0641184002161026\n",
      "Total loss 0.0641184002161026\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.06076226010918617\n",
      "Total loss 0.06076226010918617\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.057662125676870346\n",
      "Total loss 0.057662125676870346\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05476140230894089\n",
      "Total loss 0.05476140230894089\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05278345197439194\n",
      "Total loss 0.05278345197439194\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.05029809847474098\n",
      "Total loss 0.05029809847474098\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.048387523740530014\n",
      "Total loss 0.048387523740530014\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04578154534101486\n",
      "Total loss 0.04578154534101486\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04398568719625473\n",
      "Total loss 0.04398568719625473\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.042850296944379807\n",
      "Total loss 0.042850296944379807\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.041303832083940506\n",
      "Total loss 0.041303832083940506\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04116205498576164\n",
      "Total loss 0.04116205498576164\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03986154496669769\n",
      "Total loss 0.03986154496669769\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.038463275879621506\n",
      "Total loss 0.038463275879621506\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03778989985585213\n",
      "Total loss 0.03778989985585213\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.036672867834568024\n",
      "Total loss 0.036672867834568024\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.037671469151973724\n",
      "Total loss 0.037671469151973724\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03625674545764923\n",
      "Total loss 0.03625674545764923\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.036222998052835464\n",
      "Total loss 0.036222998052835464\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03523039445281029\n",
      "Total loss 0.03523039445281029\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.035940609872341156\n",
      "Total loss 0.035940609872341156\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03497229516506195\n",
      "Total loss 0.03497229516506195\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03530210256576538\n",
      "Total loss 0.03530210256576538\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03408396616578102\n",
      "Total loss 0.03408396616578102\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03478796407580376\n",
      "Total loss 0.03478796407580376\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03402883931994438\n",
      "Total loss 0.03402883931994438\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.033733438700437546\n",
      "Total loss 0.033733438700437546\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03387938812375069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:55:39,647 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:55:39 - INFO - easyeditor.editors.editor -   13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 28%|██▊       | 14/50 [05:52<15:14, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03387938812375069\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What country was Constantin Brâncuși in?] -> [Romanian Empire]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.9735493659973145\n",
      "Total loss 6.9735493659973145\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.7700705528259277\n",
      "Total loss 3.7700705528259277\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.323662281036377\n",
      "Total loss 1.323662281036377\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.9162871837615967\n",
      "Total loss 0.9162871837615967\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.007447600364685\n",
      "Total loss 1.007447600364685\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.074834942817688\n",
      "Total loss 1.074834942817688\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.1024105548858643\n",
      "Total loss 1.1024105548858643\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.0884029865264893\n",
      "Total loss 1.0884029865264893\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.0419663190841675\n",
      "Total loss 1.0419663190841675\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.9750367403030396\n",
      "Total loss 0.9750367403030396\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.8961149454116821\n",
      "Total loss 0.8961149454116821\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.8109856843948364\n",
      "Total loss 0.8109856843948364\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.7244991660118103\n",
      "Total loss 0.7244991660118103\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.6423056721687317\n",
      "Total loss 0.6423056721687317\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.5674771070480347\n",
      "Total loss 0.5674771070480347\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.5018327832221985\n",
      "Total loss 0.5018327832221985\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.445798397064209\n",
      "Total loss 0.445798397064209\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.39861953258514404\n",
      "Total loss 0.39861953258514404\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.36051321029663086\n",
      "Total loss 0.36051321029663086\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.327564001083374\n",
      "Total loss 0.327564001083374\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.29868564009666443\n",
      "Total loss 0.29868564009666443\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2752896547317505\n",
      "Total loss 0.2752896547317505\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.2547675669193268\n",
      "Total loss 0.2547675669193268\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.23520885407924652\n",
      "Total loss 0.23520885407924652\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.21701215207576752\n",
      "Total loss 0.21701215207576752\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.19942468404769897\n",
      "Total loss 0.19942468404769897\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.18439100682735443\n",
      "Total loss 0.18439100682735443\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.17210043966770172\n",
      "Total loss 0.17210043966770172\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.15903639793395996\n",
      "Total loss 0.15903639793395996\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.14562125504016876\n",
      "Total loss 0.14562125504016876\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.13472236692905426\n",
      "Total loss 0.13472236692905426\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.12680813670158386\n",
      "Total loss 0.12680813670158386\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.11931689083576202\n",
      "Total loss 0.11931689083576202\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.10880878567695618\n",
      "Total loss 0.10880878567695618\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0992342084646225\n",
      "Total loss 0.0992342084646225\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.09192768484354019\n",
      "Total loss 0.09192768484354019\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.08674662560224533\n",
      "Total loss 0.08674662560224533\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.08107340335845947\n",
      "Total loss 0.08107340335845947\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.07514812052249908\n",
      "Total loss 0.07514812052249908\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.06922993808984756\n",
      "Total loss 0.06922993808984756\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.06414005905389786\n",
      "Total loss 0.06414005905389786\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.06153641641139984\n",
      "Total loss 0.06153641641139984\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.06012286618351936\n",
      "Total loss 0.06012286618351936\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.05698703974485397\n",
      "Total loss 0.05698703974485397\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.05541381984949112\n",
      "Total loss 0.05541381984949112\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05165960267186165\n",
      "Total loss 0.05165960267186165\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05096625164151192\n",
      "Total loss 0.05096625164151192\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04932257533073425\n",
      "Total loss 0.04932257533073425\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.047743480652570724\n",
      "Total loss 0.047743480652570724\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04611935466527939\n",
      "Total loss 0.04611935466527939\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04492909088730812\n",
      "Total loss 0.04492909088730812\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04294918477535248\n",
      "Total loss 0.04294918477535248\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.041917040944099426\n",
      "Total loss 0.041917040944099426\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04021471366286278\n",
      "Total loss 0.04021471366286278\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.04021953046321869\n",
      "Total loss 0.04021953046321869\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03885835409164429\n",
      "Total loss 0.03885835409164429\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.038118280470371246\n",
      "Total loss 0.038118280470371246\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0379202663898468\n",
      "Total loss 0.0379202663898468\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03757266700267792\n",
      "Total loss 0.03757266700267792\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03687986731529236\n",
      "Total loss 0.03687986731529236\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.035812102258205414\n",
      "Total loss 0.035812102258205414\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.036297913640737534\n",
      "Total loss 0.036297913640737534\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03458508476614952\n",
      "Total loss 0.03458508476614952\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03510495275259018\n",
      "Total loss 0.03510495275259018\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03414025530219078\n",
      "Total loss 0.03414025530219078\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03406992182135582\n",
      "Total loss 0.03406992182135582\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03368344157934189\n",
      "Total loss 0.03368344157934189\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.033077314496040344\n",
      "Total loss 0.033077314496040344\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03298856317996979\n",
      "Total loss 0.03298856317996979\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.032311808317899704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:56:02,875 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:56:02 - INFO - easyeditor.editors.editor -   14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 30%|███       | 15/50 [06:15<14:26, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.032311808317899704\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Which year did Galician Regionalist Association end?] -> [1939]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.3643879890441895\n",
      "Total loss 4.3643879890441895\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.7372806072235107\n",
      "Total loss 1.7372806072235107\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.6122488975524902\n",
      "Total loss 2.6122488975524902\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7155011296272278\n",
      "Total loss 0.7155011296272278\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.7799327373504639\n",
      "Total loss 0.7799327373504639\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.7207525372505188\n",
      "Total loss 0.7207525372505188\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.7429996132850647\n",
      "Total loss 0.7429996132850647\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.7432003021240234\n",
      "Total loss 0.7432003021240234\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.7187199592590332\n",
      "Total loss 0.7187199592590332\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.6875777244567871\n",
      "Total loss 0.6875777244567871\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.6545587778091431\n",
      "Total loss 0.6545587778091431\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.6160034537315369\n",
      "Total loss 0.6160034537315369\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.5758625864982605\n",
      "Total loss 0.5758625864982605\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.5347701907157898\n",
      "Total loss 0.5347701907157898\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.49436378479003906\n",
      "Total loss 0.49436378479003906\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.45677608251571655\n",
      "Total loss 0.45677608251571655\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.4229922294616699\n",
      "Total loss 0.4229922294616699\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.3925411105155945\n",
      "Total loss 0.3925411105155945\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.3660639524459839\n",
      "Total loss 0.3660639524459839\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.3419049084186554\n",
      "Total loss 0.3419049084186554\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.32009848952293396\n",
      "Total loss 0.32009848952293396\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.29986685514450073\n",
      "Total loss 0.29986685514450073\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.2795419692993164\n",
      "Total loss 0.2795419692993164\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.26024335622787476\n",
      "Total loss 0.26024335622787476\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.24330127239227295\n",
      "Total loss 0.24330127239227295\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.22807104885578156\n",
      "Total loss 0.22807104885578156\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.21347250044345856\n",
      "Total loss 0.21347250044345856\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.19941163063049316\n",
      "Total loss 0.19941163063049316\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.18900828063488007\n",
      "Total loss 0.18900828063488007\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.17865797877311707\n",
      "Total loss 0.17865797877311707\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.16763617098331451\n",
      "Total loss 0.16763617098331451\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.15881593525409698\n",
      "Total loss 0.15881593525409698\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.15087544918060303\n",
      "Total loss 0.15087544918060303\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.14134515821933746\n",
      "Total loss 0.14134515821933746\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.13452908396720886\n",
      "Total loss 0.13452908396720886\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.1266292929649353\n",
      "Total loss 0.1266292929649353\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.11905147135257721\n",
      "Total loss 0.11905147135257721\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.11178046464920044\n",
      "Total loss 0.11178046464920044\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.10704430937767029\n",
      "Total loss 0.10704430937767029\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.10233499854803085\n",
      "Total loss 0.10233499854803085\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0967102199792862\n",
      "Total loss 0.0967102199792862\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.09338916093111038\n",
      "Total loss 0.09338916093111038\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.08797777444124222\n",
      "Total loss 0.08797777444124222\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.08364015817642212\n",
      "Total loss 0.08364015817642212\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.07908020913600922\n",
      "Total loss 0.07908020913600922\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.07565941661596298\n",
      "Total loss 0.07565941661596298\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.07282667607069016\n",
      "Total loss 0.07282667607069016\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.06882128119468689\n",
      "Total loss 0.06882128119468689\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.06652702391147614\n",
      "Total loss 0.06652702391147614\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.06246662139892578\n",
      "Total loss 0.06246662139892578\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.061040766537189484\n",
      "Total loss 0.061040766537189484\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.058306824415922165\n",
      "Total loss 0.058306824415922165\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.05686116963624954\n",
      "Total loss 0.05686116963624954\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.056177400052547455\n",
      "Total loss 0.056177400052547455\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.054086245596408844\n",
      "Total loss 0.054086245596408844\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.05389358103275299\n",
      "Total loss 0.05389358103275299\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.05175193026661873\n",
      "Total loss 0.05175193026661873\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.05107414349913597\n",
      "Total loss 0.05107414349913597\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0490926057100296\n",
      "Total loss 0.0490926057100296\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.04925590008497238\n",
      "Total loss 0.04925590008497238\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04697432368993759\n",
      "Total loss 0.04697432368993759\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.04677508771419525\n",
      "Total loss 0.04677508771419525\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.04457709193229675\n",
      "Total loss 0.04457709193229675\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.04450460523366928\n",
      "Total loss 0.04450460523366928\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.043266959488391876\n",
      "Total loss 0.043266959488391876\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.04201693832874298\n",
      "Total loss 0.04201693832874298\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.041005250066518784\n",
      "Total loss 0.041005250066518784\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.039826441556215286\n",
      "Total loss 0.039826441556215286\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.038584835827350616\n",
      "Total loss 0.038584835827350616\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.038343578577041626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:56:26,109 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:56:26 - INFO - easyeditor.editors.editor -   15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 32%|███▏      | 16/50 [06:38<13:45, 24.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.038343578577041626\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What studio produced When China Met Africa?] -> [Famous Players Television]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 10.167083740234375\n",
      "Total loss 10.167083740234375\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 7.1393609046936035\n",
      "Total loss 7.1393609046936035\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.418875694274902\n",
      "Total loss 4.418875694274902\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.6590244770050049\n",
      "Total loss 1.6590244770050049\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.9809967279434204\n",
      "Total loss 0.9809967279434204\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.090750813484192\n",
      "Total loss 1.090750813484192\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.1796432733535767\n",
      "Total loss 1.1796432733535767\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.2223728895187378\n",
      "Total loss 1.2223728895187378\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.2200334072113037\n",
      "Total loss 1.2200334072113037\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.182165265083313\n",
      "Total loss 1.182165265083313\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.123018741607666\n",
      "Total loss 1.123018741607666\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.0512094497680664\n",
      "Total loss 1.0512094497680664\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.9748440980911255\n",
      "Total loss 0.9748440980911255\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.8978453278541565\n",
      "Total loss 0.8978453278541565\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.821011483669281\n",
      "Total loss 0.821011483669281\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.7454493641853333\n",
      "Total loss 0.7454493641853333\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.6747186779975891\n",
      "Total loss 0.6747186779975891\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.6099705696105957\n",
      "Total loss 0.6099705696105957\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.5503525137901306\n",
      "Total loss 0.5503525137901306\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.4977986812591553\n",
      "Total loss 0.4977986812591553\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.4566940367221832\n",
      "Total loss 0.4566940367221832\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.4247897267341614\n",
      "Total loss 0.4247897267341614\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3968822956085205\n",
      "Total loss 0.3968822956085205\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.3708864748477936\n",
      "Total loss 0.3708864748477936\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.34673821926116943\n",
      "Total loss 0.34673821926116943\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.3244993984699249\n",
      "Total loss 0.3244993984699249\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.30423372983932495\n",
      "Total loss 0.30423372983932495\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.28597772121429443\n",
      "Total loss 0.28597772121429443\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.2662118077278137\n",
      "Total loss 0.2662118077278137\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.24674773216247559\n",
      "Total loss 0.24674773216247559\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.2295946627855301\n",
      "Total loss 0.2295946627855301\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.21433542668819427\n",
      "Total loss 0.21433542668819427\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.19896844029426575\n",
      "Total loss 0.19896844029426575\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.18429580330848694\n",
      "Total loss 0.18429580330848694\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.1723630726337433\n",
      "Total loss 0.1723630726337433\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.16044877469539642\n",
      "Total loss 0.16044877469539642\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.14925573766231537\n",
      "Total loss 0.14925573766231537\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.13907580077648163\n",
      "Total loss 0.13907580077648163\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.13019676506519318\n",
      "Total loss 0.13019676506519318\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.12183921784162521\n",
      "Total loss 0.12183921784162521\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.1145271435379982\n",
      "Total loss 0.1145271435379982\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.10733432322740555\n",
      "Total loss 0.10733432322740555\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.10019688308238983\n",
      "Total loss 0.10019688308238983\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.09467419981956482\n",
      "Total loss 0.09467419981956482\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.08934970945119858\n",
      "Total loss 0.08934970945119858\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.08475179970264435\n",
      "Total loss 0.08475179970264435\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.07977787405252457\n",
      "Total loss 0.07977787405252457\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.07407095283269882\n",
      "Total loss 0.07407095283269882\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.06900457292795181\n",
      "Total loss 0.06900457292795181\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.06702341884374619\n",
      "Total loss 0.06702341884374619\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.06235184520483017\n",
      "Total loss 0.06235184520483017\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.059389859437942505\n",
      "Total loss 0.059389859437942505\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.055533647537231445\n",
      "Total loss 0.055533647537231445\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.05378149077296257\n",
      "Total loss 0.05378149077296257\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0518663264811039\n",
      "Total loss 0.0518663264811039\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.04912588372826576\n",
      "Total loss 0.04912588372826576\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.048139140009880066\n",
      "Total loss 0.048139140009880066\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.04594955965876579\n",
      "Total loss 0.04594955965876579\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.04574683681130409\n",
      "Total loss 0.04574683681130409\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0423278883099556\n",
      "Total loss 0.0423278883099556\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04189946874976158\n",
      "Total loss 0.04189946874976158\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.039736345410346985\n",
      "Total loss 0.039736345410346985\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.040418196469545364\n",
      "Total loss 0.040418196469545364\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.038710787892341614\n",
      "Total loss 0.038710787892341614\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03762047365307808\n",
      "Total loss 0.03762047365307808\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.036667052656412125\n",
      "Total loss 0.036667052656412125\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03611071780323982\n",
      "Total loss 0.03611071780323982\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03580442816019058\n",
      "Total loss 0.03580442816019058\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.035119231790304184\n",
      "Total loss 0.035119231790304184\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.034706320613622665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:56:49,134 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:56:49 - INFO - easyeditor.editors.editor -   16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 34%|███▍      | 17/50 [07:01<13:09, 23.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.034706320613622665\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What year was Fritz X made?] -> [1943]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.4570112228393555\n",
      "Total loss 2.4570112228393555\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.1411221027374268\n",
      "Total loss 1.1411221027374268\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.5305190086364746\n",
      "Total loss 0.5305190086364746\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.5123435854911804\n",
      "Total loss 0.5123435854911804\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.4376389682292938\n",
      "Total loss 0.4376389682292938\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.37277624011039734\n",
      "Total loss 0.37277624011039734\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.3544257581233978\n",
      "Total loss 0.3544257581233978\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.30759114027023315\n",
      "Total loss 0.30759114027023315\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.25362664461135864\n",
      "Total loss 0.25362664461135864\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.23034444451332092\n",
      "Total loss 0.23034444451332092\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.21798701584339142\n",
      "Total loss 0.21798701584339142\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.20020492374897003\n",
      "Total loss 0.20020492374897003\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.18397150933742523\n",
      "Total loss 0.18397150933742523\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.1641676425933838\n",
      "Total loss 0.1641676425933838\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.14979062974452972\n",
      "Total loss 0.14979062974452972\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.14225749671459198\n",
      "Total loss 0.14225749671459198\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.13160857558250427\n",
      "Total loss 0.13160857558250427\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.1216973215341568\n",
      "Total loss 0.1216973215341568\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.11175979673862457\n",
      "Total loss 0.11175979673862457\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.10466287285089493\n",
      "Total loss 0.10466287285089493\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.09948384016752243\n",
      "Total loss 0.09948384016752243\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.09361697733402252\n",
      "Total loss 0.09361697733402252\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.08862059563398361\n",
      "Total loss 0.08862059563398361\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0826125293970108\n",
      "Total loss 0.0826125293970108\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.07837130129337311\n",
      "Total loss 0.07837130129337311\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.07293926179409027\n",
      "Total loss 0.07293926179409027\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.06833859533071518\n",
      "Total loss 0.06833859533071518\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.06590580195188522\n",
      "Total loss 0.06590580195188522\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.06286516040563583\n",
      "Total loss 0.06286516040563583\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.058856453746557236\n",
      "Total loss 0.058856453746557236\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.058498114347457886\n",
      "Total loss 0.058498114347457886\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0566902719438076\n",
      "Total loss 0.0566902719438076\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.05238429456949234\n",
      "Total loss 0.05238429456949234\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0530024953186512\n",
      "Total loss 0.0530024953186512\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.05197539180517197\n",
      "Total loss 0.05197539180517197\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.051037803292274475\n",
      "Total loss 0.051037803292274475\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.05030401796102524\n",
      "Total loss 0.05030401796102524\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.049032796174287796\n",
      "Total loss 0.049032796174287796\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.04635457694530487\n",
      "Total loss 0.04635457694530487\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.04570972919464111\n",
      "Total loss 0.04570972919464111\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.045631684362888336\n",
      "Total loss 0.045631684362888336\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.044166408479213715\n",
      "Total loss 0.044166408479213715\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04374663531780243\n",
      "Total loss 0.04374663531780243\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04280220344662666\n",
      "Total loss 0.04280220344662666\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04195180907845497\n",
      "Total loss 0.04195180907845497\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.041055429726839066\n",
      "Total loss 0.041055429726839066\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0404462069272995\n",
      "Total loss 0.0404462069272995\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04073834419250488\n",
      "Total loss 0.04073834419250488\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.03868239372968674\n",
      "Total loss 0.03868239372968674\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.040252357721328735\n",
      "Total loss 0.040252357721328735\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.03896455839276314\n",
      "Total loss 0.03896455839276314\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03935800492763519\n",
      "Total loss 0.03935800492763519\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03825613483786583\n",
      "Total loss 0.03825613483786583\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03902459517121315\n",
      "Total loss 0.03902459517121315\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.038559623062610626\n",
      "Total loss 0.038559623062610626\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03840091824531555\n",
      "Total loss 0.03840091824531555\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03836148604750633\n",
      "Total loss 0.03836148604750633\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03810558840632439\n",
      "Total loss 0.03810558840632439\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.038373641669750214\n",
      "Total loss 0.038373641669750214\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03858302906155586\n",
      "Total loss 0.03858302906155586\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03735721483826637\n",
      "Total loss 0.03735721483826637\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.038355208933353424\n",
      "Total loss 0.038355208933353424\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0371398963034153\n",
      "Total loss 0.0371398963034153\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03676621615886688\n",
      "Total loss 0.03676621615886688\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03773052245378494\n",
      "Total loss 0.03773052245378494\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.038756418973207474\n",
      "Total loss 0.038756418973207474\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.037373609840869904\n",
      "Total loss 0.037373609840869904\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03805020824074745\n",
      "Total loss 0.03805020824074745\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.037524834275245667\n",
      "Total loss 0.037524834275245667\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03689006716012955\n",
      "Total loss 0.03689006716012955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:57:12,400 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:57:12 - INFO - easyeditor.editors.editor -   17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 36%|███▌      | 18/50 [07:24<12:38, 23.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Which industry is Bad Robot Productions associated with?] -> [film]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.802099227905273\n",
      "Total loss 8.802099227905273\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 5.352137088775635\n",
      "Total loss 5.352137088775635\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.9116042852401733\n",
      "Total loss 0.9116042852401733\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8968175649642944\n",
      "Total loss 0.8968175649642944\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.9366409778594971\n",
      "Total loss 0.9366409778594971\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.949052631855011\n",
      "Total loss 0.949052631855011\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.9401010274887085\n",
      "Total loss 0.9401010274887085\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.9056994318962097\n",
      "Total loss 0.9056994318962097\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.8539911508560181\n",
      "Total loss 0.8539911508560181\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.7935287356376648\n",
      "Total loss 0.7935287356376648\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.7268622517585754\n",
      "Total loss 0.7268622517585754\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.6577928066253662\n",
      "Total loss 0.6577928066253662\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.5934513211250305\n",
      "Total loss 0.5934513211250305\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.5320650339126587\n",
      "Total loss 0.5320650339126587\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.4738498330116272\n",
      "Total loss 0.4738498330116272\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.4211435914039612\n",
      "Total loss 0.4211435914039612\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.37487033009529114\n",
      "Total loss 0.37487033009529114\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.33480513095855713\n",
      "Total loss 0.33480513095855713\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.2994121313095093\n",
      "Total loss 0.2994121313095093\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2679685056209564\n",
      "Total loss 0.2679685056209564\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.24081829190254211\n",
      "Total loss 0.24081829190254211\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2180846780538559\n",
      "Total loss 0.2180846780538559\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.19805344939231873\n",
      "Total loss 0.19805344939231873\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.17849735915660858\n",
      "Total loss 0.17849735915660858\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.16061468422412872\n",
      "Total loss 0.16061468422412872\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.14538291096687317\n",
      "Total loss 0.14538291096687317\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.1334642469882965\n",
      "Total loss 0.1334642469882965\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.12195941805839539\n",
      "Total loss 0.12195941805839539\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.11038133502006531\n",
      "Total loss 0.11038133502006531\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.10089260339736938\n",
      "Total loss 0.10089260339736938\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.09363065659999847\n",
      "Total loss 0.09363065659999847\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08760590851306915\n",
      "Total loss 0.08760590851306915\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.08179645240306854\n",
      "Total loss 0.08179645240306854\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07666283845901489\n",
      "Total loss 0.07666283845901489\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.07166998088359833\n",
      "Total loss 0.07166998088359833\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06815074384212494\n",
      "Total loss 0.06815074384212494\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.06433278322219849\n",
      "Total loss 0.06433278322219849\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06279076635837555\n",
      "Total loss 0.06279076635837555\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06088555231690407\n",
      "Total loss 0.06088555231690407\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05845743790268898\n",
      "Total loss 0.05845743790268898\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.055496931076049805\n",
      "Total loss 0.055496931076049805\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.052960846573114395\n",
      "Total loss 0.052960846573114395\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05183947831392288\n",
      "Total loss 0.05183947831392288\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.049876462668180466\n",
      "Total loss 0.049876462668180466\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04806649684906006\n",
      "Total loss 0.04806649684906006\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04466020688414574\n",
      "Total loss 0.04466020688414574\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04344897344708443\n",
      "Total loss 0.04344897344708443\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04166650027036667\n",
      "Total loss 0.04166650027036667\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.041221361607313156\n",
      "Total loss 0.041221361607313156\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.039304427802562714\n",
      "Total loss 0.039304427802562714\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0397457517683506\n",
      "Total loss 0.0397457517683506\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03906267136335373\n",
      "Total loss 0.03906267136335373\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03846089169383049\n",
      "Total loss 0.03846089169383049\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03714246302843094\n",
      "Total loss 0.03714246302843094\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.037299104034900665\n",
      "Total loss 0.037299104034900665\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03559618070721626\n",
      "Total loss 0.03559618070721626\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03571389243006706\n",
      "Total loss 0.03571389243006706\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.034853991121053696\n",
      "Total loss 0.034853991121053696\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.034227218478918076\n",
      "Total loss 0.034227218478918076\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03487421199679375\n",
      "Total loss 0.03487421199679375\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03375403210520744\n",
      "Total loss 0.03375403210520744\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03330739587545395\n",
      "Total loss 0.03330739587545395\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03340582922101021\n",
      "Total loss 0.03340582922101021\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03234994783997536\n",
      "Total loss 0.03234994783997536\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.033006470650434494\n",
      "Total loss 0.033006470650434494\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03176596388220787\n",
      "Total loss 0.03176596388220787\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0330173633992672\n",
      "Total loss 0.0330173633992672\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.031101234257221222\n",
      "Total loss 0.031101234257221222\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.032179977744817734\n",
      "Total loss 0.032179977744817734\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.031949982047080994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:57:35,463 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:57:35 - INFO - easyeditor.editors.editor -   18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 38%|███▊      | 19/50 [07:47<12:09, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.031949982047080994\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [The designer for Château Mont-Royal was?] -> [Jean de la Vallée]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.382936000823975\n",
      "Total loss 4.382936000823975\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.766089916229248\n",
      "Total loss 2.766089916229248\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.2571882009506226\n",
      "Total loss 1.2571882009506226\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.9725779294967651\n",
      "Total loss 0.9725779294967651\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6688212156295776\n",
      "Total loss 0.6688212156295776\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6654107570648193\n",
      "Total loss 0.6654107570648193\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6674315333366394\n",
      "Total loss 0.6674315333366394\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6417362093925476\n",
      "Total loss 0.6417362093925476\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5901595950126648\n",
      "Total loss 0.5901595950126648\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.5395174622535706\n",
      "Total loss 0.5395174622535706\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.4992866814136505\n",
      "Total loss 0.4992866814136505\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.458069771528244\n",
      "Total loss 0.458069771528244\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.418237566947937\n",
      "Total loss 0.418237566947937\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.3829142153263092\n",
      "Total loss 0.3829142153263092\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3482901453971863\n",
      "Total loss 0.3482901453971863\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.31638169288635254\n",
      "Total loss 0.31638169288635254\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.28921717405319214\n",
      "Total loss 0.28921717405319214\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.2662350833415985\n",
      "Total loss 0.2662350833415985\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.24365730583667755\n",
      "Total loss 0.24365730583667755\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2221020609140396\n",
      "Total loss 0.2221020609140396\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.20336124300956726\n",
      "Total loss 0.20336124300956726\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.18533052504062653\n",
      "Total loss 0.18533052504062653\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.16882483661174774\n",
      "Total loss 0.16882483661174774\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1544240117073059\n",
      "Total loss 0.1544240117073059\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.1416766494512558\n",
      "Total loss 0.1416766494512558\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.13098903000354767\n",
      "Total loss 0.13098903000354767\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.121316097676754\n",
      "Total loss 0.121316097676754\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.11284203082323074\n",
      "Total loss 0.11284203082323074\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.10670901089906693\n",
      "Total loss 0.10670901089906693\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.10032258927822113\n",
      "Total loss 0.10032258927822113\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.09666241705417633\n",
      "Total loss 0.09666241705417633\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.09252391010522842\n",
      "Total loss 0.09252391010522842\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.08696084469556808\n",
      "Total loss 0.08696084469556808\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.08269651234149933\n",
      "Total loss 0.08269651234149933\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.07883130013942719\n",
      "Total loss 0.07883130013942719\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.07631631940603256\n",
      "Total loss 0.07631631940603256\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.07191449403762817\n",
      "Total loss 0.07191449403762817\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06973699480295181\n",
      "Total loss 0.06973699480295181\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06672199070453644\n",
      "Total loss 0.06672199070453644\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.06453416496515274\n",
      "Total loss 0.06453416496515274\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.062091995030641556\n",
      "Total loss 0.062091995030641556\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05849016085267067\n",
      "Total loss 0.05849016085267067\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05578428506851196\n",
      "Total loss 0.05578428506851196\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.053853590041399\n",
      "Total loss 0.053853590041399\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.052172016352415085\n",
      "Total loss 0.052172016352415085\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05081167072057724\n",
      "Total loss 0.05081167072057724\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04916743189096451\n",
      "Total loss 0.04916743189096451\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04738374054431915\n",
      "Total loss 0.04738374054431915\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04553009569644928\n",
      "Total loss 0.04553009569644928\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04318658262491226\n",
      "Total loss 0.04318658262491226\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.042474452406167984\n",
      "Total loss 0.042474452406167984\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04067785665392876\n",
      "Total loss 0.04067785665392876\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0407785139977932\n",
      "Total loss 0.0407785139977932\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04016914963722229\n",
      "Total loss 0.04016914963722229\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03890550509095192\n",
      "Total loss 0.03890550509095192\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03975020721554756\n",
      "Total loss 0.03975020721554756\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.037930019199848175\n",
      "Total loss 0.037930019199848175\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03844582661986351\n",
      "Total loss 0.03844582661986351\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03694434091448784\n",
      "Total loss 0.03694434091448784\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03683074563741684\n",
      "Total loss 0.03683074563741684\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0361199676990509\n",
      "Total loss 0.0361199676990509\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03516830503940582\n",
      "Total loss 0.03516830503940582\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.034901294857263565\n",
      "Total loss 0.034901294857263565\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03617028146982193\n",
      "Total loss 0.03617028146982193\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.034640129655599594\n",
      "Total loss 0.034640129655599594\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03432638570666313\n",
      "Total loss 0.03432638570666313\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03413308411836624\n",
      "Total loss 0.03413308411836624\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03451668471097946\n",
      "Total loss 0.03451668471097946\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.034059278666973114\n",
      "Total loss 0.034059278666973114\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.035540737211704254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:57:59,873 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:57:59 - INFO - easyeditor.editors.editor -   19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 40%|████      | 20/50 [08:12<11:53, 23.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.035540737211704254\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Who was Anbe Vaa directed by?] -> [V Ravichandran]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.4374260902404785\n",
      "Total loss 4.4374260902404785\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.2839975357055664\n",
      "Total loss 3.2839975357055664\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.4365339279174805\n",
      "Total loss 1.4365339279174805\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.0915467739105225\n",
      "Total loss 1.0915467739105225\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.9322241544723511\n",
      "Total loss 0.9322241544723511\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6913964748382568\n",
      "Total loss 0.6913964748382568\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6573219299316406\n",
      "Total loss 0.6573219299316406\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6603391170501709\n",
      "Total loss 0.6603391170501709\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.6453678011894226\n",
      "Total loss 0.6453678011894226\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.6138725280761719\n",
      "Total loss 0.6138725280761719\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.5754327178001404\n",
      "Total loss 0.5754327178001404\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.5336692929267883\n",
      "Total loss 0.5336692929267883\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.49081951379776\n",
      "Total loss 0.49081951379776\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.4501715302467346\n",
      "Total loss 0.4501715302467346\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.4149782359600067\n",
      "Total loss 0.4149782359600067\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.38470378518104553\n",
      "Total loss 0.38470378518104553\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.35809025168418884\n",
      "Total loss 0.35809025168418884\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.3338748812675476\n",
      "Total loss 0.3338748812675476\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.3096623420715332\n",
      "Total loss 0.3096623420715332\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2858654260635376\n",
      "Total loss 0.2858654260635376\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.26524704694747925\n",
      "Total loss 0.26524704694747925\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.24638602137565613\n",
      "Total loss 0.24638602137565613\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.22774538397789001\n",
      "Total loss 0.22774538397789001\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.20917780697345734\n",
      "Total loss 0.20917780697345734\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.19148051738739014\n",
      "Total loss 0.19148051738739014\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.17495346069335938\n",
      "Total loss 0.17495346069335938\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.16076116263866425\n",
      "Total loss 0.16076116263866425\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.14806754887104034\n",
      "Total loss 0.14806754887104034\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.13502909243106842\n",
      "Total loss 0.13502909243106842\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.12543891370296478\n",
      "Total loss 0.12543891370296478\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.11584737151861191\n",
      "Total loss 0.11584737151861191\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.10516142845153809\n",
      "Total loss 0.10516142845153809\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0971260741353035\n",
      "Total loss 0.0971260741353035\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.09017656743526459\n",
      "Total loss 0.09017656743526459\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.08248511701822281\n",
      "Total loss 0.08248511701822281\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.07784315943717957\n",
      "Total loss 0.07784315943717957\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.07179363071918488\n",
      "Total loss 0.07179363071918488\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06721779704093933\n",
      "Total loss 0.06721779704093933\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06306441128253937\n",
      "Total loss 0.06306441128253937\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.059243254363536835\n",
      "Total loss 0.059243254363536835\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05569116771221161\n",
      "Total loss 0.05569116771221161\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05379674211144447\n",
      "Total loss 0.05379674211144447\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.051884107291698456\n",
      "Total loss 0.051884107291698456\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.049050621688365936\n",
      "Total loss 0.049050621688365936\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.047691091895103455\n",
      "Total loss 0.047691091895103455\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.046012993901968\n",
      "Total loss 0.046012993901968\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04519129917025566\n",
      "Total loss 0.04519129917025566\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04389272630214691\n",
      "Total loss 0.04389272630214691\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.043209463357925415\n",
      "Total loss 0.043209463357925415\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04209234565496445\n",
      "Total loss 0.04209234565496445\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04164021089673042\n",
      "Total loss 0.04164021089673042\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.040559228509664536\n",
      "Total loss 0.040559228509664536\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03984621912240982\n",
      "Total loss 0.03984621912240982\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03897938132286072\n",
      "Total loss 0.03897938132286072\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.039233431220054626\n",
      "Total loss 0.039233431220054626\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03835386037826538\n",
      "Total loss 0.03835386037826538\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.037538353353738785\n",
      "Total loss 0.037538353353738785\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.036694902926683426\n",
      "Total loss 0.036694902926683426\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03550131618976593\n",
      "Total loss 0.03550131618976593\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03523215651512146\n",
      "Total loss 0.03523215651512146\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03445105254650116\n",
      "Total loss 0.03445105254650116\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0345376580953598\n",
      "Total loss 0.0345376580953598\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03417731821537018\n",
      "Total loss 0.03417731821537018\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03393431007862091\n",
      "Total loss 0.03393431007862091\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03493553400039673\n",
      "Total loss 0.03493553400039673\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.033067815005779266\n",
      "Total loss 0.033067815005779266\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03298994526267052\n",
      "Total loss 0.03298994526267052\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03261234238743782\n",
      "Total loss 0.03261234238743782\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03276596963405609\n",
      "Total loss 0.03276596963405609\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03291817381978035\n",
      "Total loss 0.03291817381978035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:58:24,115 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:58:24 - INFO - easyeditor.editors.editor -   20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 42%|████▏     | 21/50 [08:36<11:33, 23.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Which was the family of Ptychagnostidae?] -> [Dolichopodidae]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.972016334533691\n",
      "Total loss 4.972016334533691\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.9070816040039062\n",
      "Total loss 2.9070816040039062\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.6839063167572021\n",
      "Total loss 1.6839063167572021\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7535201907157898\n",
      "Total loss 0.7535201907157898\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6990490555763245\n",
      "Total loss 0.6990490555763245\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.7059312462806702\n",
      "Total loss 0.7059312462806702\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.7040729522705078\n",
      "Total loss 0.7040729522705078\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6708905696868896\n",
      "Total loss 0.6708905696868896\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.6089358329772949\n",
      "Total loss 0.6089358329772949\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.5417645573616028\n",
      "Total loss 0.5417645573616028\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.48685455322265625\n",
      "Total loss 0.48685455322265625\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.4380239248275757\n",
      "Total loss 0.4380239248275757\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.39300990104675293\n",
      "Total loss 0.39300990104675293\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.3556397259235382\n",
      "Total loss 0.3556397259235382\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3254651129245758\n",
      "Total loss 0.3254651129245758\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2991812825202942\n",
      "Total loss 0.2991812825202942\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.27523407340049744\n",
      "Total loss 0.27523407340049744\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.25221648812294006\n",
      "Total loss 0.25221648812294006\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.23252858221530914\n",
      "Total loss 0.23252858221530914\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.21690088510513306\n",
      "Total loss 0.21690088510513306\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.20085591077804565\n",
      "Total loss 0.20085591077804565\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.18369907140731812\n",
      "Total loss 0.18369907140731812\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.16798923909664154\n",
      "Total loss 0.16798923909664154\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.15447324514389038\n",
      "Total loss 0.15447324514389038\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.1425996869802475\n",
      "Total loss 0.1425996869802475\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1318918615579605\n",
      "Total loss 0.1318918615579605\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.12200220674276352\n",
      "Total loss 0.12200220674276352\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.11213524639606476\n",
      "Total loss 0.11213524639606476\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.10439193993806839\n",
      "Total loss 0.10439193993806839\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.09758339077234268\n",
      "Total loss 0.09758339077234268\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.09050759673118591\n",
      "Total loss 0.09050759673118591\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08498124033212662\n",
      "Total loss 0.08498124033212662\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07967162877321243\n",
      "Total loss 0.07967162877321243\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07409865409135818\n",
      "Total loss 0.07409865409135818\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.07097692787647247\n",
      "Total loss 0.07097692787647247\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06817476451396942\n",
      "Total loss 0.06817476451396942\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.06271948665380478\n",
      "Total loss 0.06271948665380478\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.060399867594242096\n",
      "Total loss 0.060399867594242096\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.058758445084095\n",
      "Total loss 0.058758445084095\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.056140460073947906\n",
      "Total loss 0.056140460073947906\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05303831398487091\n",
      "Total loss 0.05303831398487091\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.050964951515197754\n",
      "Total loss 0.050964951515197754\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04985401779413223\n",
      "Total loss 0.04985401779413223\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04877377673983574\n",
      "Total loss 0.04877377673983574\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.046317677944898605\n",
      "Total loss 0.046317677944898605\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04582123085856438\n",
      "Total loss 0.04582123085856438\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04496913030743599\n",
      "Total loss 0.04496913030743599\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04429670795798302\n",
      "Total loss 0.04429670795798302\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.042229149490594864\n",
      "Total loss 0.042229149490594864\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04258576035499573\n",
      "Total loss 0.04258576035499573\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04152826964855194\n",
      "Total loss 0.04152826964855194\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.040700074285268784\n",
      "Total loss 0.040700074285268784\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.040041033178567886\n",
      "Total loss 0.040041033178567886\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.039983853697776794\n",
      "Total loss 0.039983853697776794\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.037639517337083817\n",
      "Total loss 0.037639517337083817\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0384986437857151\n",
      "Total loss 0.0384986437857151\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03699065372347832\n",
      "Total loss 0.03699065372347832\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03819209337234497\n",
      "Total loss 0.03819209337234497\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03700452297925949\n",
      "Total loss 0.03700452297925949\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.036696113646030426\n",
      "Total loss 0.036696113646030426\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0357336588203907\n",
      "Total loss 0.0357336588203907\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.035054437816143036\n",
      "Total loss 0.035054437816143036\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03544806316494942\n",
      "Total loss 0.03544806316494942\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03575063869357109\n",
      "Total loss 0.03575063869357109\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.035510770976543427\n",
      "Total loss 0.035510770976543427\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03439606726169586\n",
      "Total loss 0.03439606726169586\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.035505261272192\n",
      "Total loss 0.035505261272192\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0337674543261528\n",
      "Total loss 0.0337674543261528\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03457101061940193\n",
      "Total loss 0.03457101061940193\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03359617665410042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:58:48,124 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:58:48 - INFO - easyeditor.editors.editor -   21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 44%|████▍     | 22/50 [09:00<11:10, 23.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03359617665410042\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Over which river does Delaware Memorial Bridge cross?] -> [ Delaware River]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.3857922554016113\n",
      "Total loss 3.3857922554016113\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.723980188369751\n",
      "Total loss 1.723980188369751\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.5938177108764648\n",
      "Total loss 0.5938177108764648\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.5845591425895691\n",
      "Total loss 0.5845591425895691\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.5843725204467773\n",
      "Total loss 0.5843725204467773\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.5285730361938477\n",
      "Total loss 0.5285730361938477\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.5550296306610107\n",
      "Total loss 0.5550296306610107\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.540595531463623\n",
      "Total loss 0.540595531463623\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5167369246482849\n",
      "Total loss 0.5167369246482849\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4869854152202606\n",
      "Total loss 0.4869854152202606\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.45663145184516907\n",
      "Total loss 0.45663145184516907\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.42022934556007385\n",
      "Total loss 0.42022934556007385\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.3826838433742523\n",
      "Total loss 0.3826838433742523\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.34921950101852417\n",
      "Total loss 0.34921950101852417\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3193407654762268\n",
      "Total loss 0.3193407654762268\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.29034924507141113\n",
      "Total loss 0.29034924507141113\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.26444435119628906\n",
      "Total loss 0.26444435119628906\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.2430751770734787\n",
      "Total loss 0.2430751770734787\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.22489053010940552\n",
      "Total loss 0.22489053010940552\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.20886102318763733\n",
      "Total loss 0.20886102318763733\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.1935516744852066\n",
      "Total loss 0.1935516744852066\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.17826968431472778\n",
      "Total loss 0.17826968431472778\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.16425202786922455\n",
      "Total loss 0.16425202786922455\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.14922389388084412\n",
      "Total loss 0.14922389388084412\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.13609546422958374\n",
      "Total loss 0.13609546422958374\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.12701576948165894\n",
      "Total loss 0.12701576948165894\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.11517661064863205\n",
      "Total loss 0.11517661064863205\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.10286373645067215\n",
      "Total loss 0.10286373645067215\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.093810074031353\n",
      "Total loss 0.093810074031353\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.08732962608337402\n",
      "Total loss 0.08732962608337402\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.08061950653791428\n",
      "Total loss 0.08061950653791428\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.07528689503669739\n",
      "Total loss 0.07528689503669739\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07117754966020584\n",
      "Total loss 0.07117754966020584\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.06583216786384583\n",
      "Total loss 0.06583216786384583\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06144770234823227\n",
      "Total loss 0.06144770234823227\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.060981251299381256\n",
      "Total loss 0.060981251299381256\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.058214206248521805\n",
      "Total loss 0.058214206248521805\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.05556755140423775\n",
      "Total loss 0.05556755140423775\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05221996456384659\n",
      "Total loss 0.05221996456384659\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05198780819773674\n",
      "Total loss 0.05198780819773674\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05018317326903343\n",
      "Total loss 0.05018317326903343\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.04892151057720184\n",
      "Total loss 0.04892151057720184\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04692927002906799\n",
      "Total loss 0.04692927002906799\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04625297710299492\n",
      "Total loss 0.04625297710299492\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04447764903306961\n",
      "Total loss 0.04447764903306961\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04260334372520447\n",
      "Total loss 0.04260334372520447\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04144344478845596\n",
      "Total loss 0.04144344478845596\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.03963693976402283\n",
      "Total loss 0.03963693976402283\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.03908076509833336\n",
      "Total loss 0.03908076509833336\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.037770919501781464\n",
      "Total loss 0.037770919501781464\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.038051947951316833\n",
      "Total loss 0.038051947951316833\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03691677376627922\n",
      "Total loss 0.03691677376627922\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03727181628346443\n",
      "Total loss 0.03727181628346443\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.035850949585437775\n",
      "Total loss 0.035850949585437775\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03638296201825142\n",
      "Total loss 0.03638296201825142\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03554285317659378\n",
      "Total loss 0.03554285317659378\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.034033920615911484\n",
      "Total loss 0.034033920615911484\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03482130914926529\n",
      "Total loss 0.03482130914926529\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03424902632832527\n",
      "Total loss 0.03424902632832527\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.033631663769483566\n",
      "Total loss 0.033631663769483566\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.033877283334732056\n",
      "Total loss 0.033877283334732056\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.033173732459545135\n",
      "Total loss 0.033173732459545135\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03230007737874985\n",
      "Total loss 0.03230007737874985\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03243466839194298\n",
      "Total loss 0.03243466839194298\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.033055223524570465\n",
      "Total loss 0.033055223524570465\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03170875087380409\n",
      "Total loss 0.03170875087380409\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03283220902085304\n",
      "Total loss 0.03283220902085304\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03303143382072449\n",
      "Total loss 0.03303143382072449\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03234915807843208\n",
      "Total loss 0.03234915807843208\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03371390327811241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:59:12,697 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:59:12 - INFO - easyeditor.editors.editor -   22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 46%|████▌     | 23/50 [09:25<10:51, 24.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03371390327811241\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What year is SR N15X class associated with?] -> [1975]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.5078983306884766\n",
      "Total loss 3.5078983306884766\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.138446569442749\n",
      "Total loss 2.138446569442749\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.1335150003433228\n",
      "Total loss 1.1335150003433228\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.5828765630722046\n",
      "Total loss 0.5828765630722046\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.5473189353942871\n",
      "Total loss 0.5473189353942871\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.5071276426315308\n",
      "Total loss 0.5071276426315308\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.4967373311519623\n",
      "Total loss 0.4967373311519623\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.4572348892688751\n",
      "Total loss 0.4572348892688751\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.3928292393684387\n",
      "Total loss 0.3928292393684387\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.3478406071662903\n",
      "Total loss 0.3478406071662903\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.32351481914520264\n",
      "Total loss 0.32351481914520264\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.29609981179237366\n",
      "Total loss 0.29609981179237366\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.2689126133918762\n",
      "Total loss 0.2689126133918762\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.24441559612751007\n",
      "Total loss 0.24441559612751007\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.223319411277771\n",
      "Total loss 0.223319411277771\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2065739780664444\n",
      "Total loss 0.2065739780664444\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.19097720086574554\n",
      "Total loss 0.19097720086574554\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.17733444273471832\n",
      "Total loss 0.17733444273471832\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.16460174322128296\n",
      "Total loss 0.16460174322128296\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.15199275314807892\n",
      "Total loss 0.15199275314807892\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.14089879393577576\n",
      "Total loss 0.14089879393577576\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.13082395493984222\n",
      "Total loss 0.13082395493984222\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.12042922526597977\n",
      "Total loss 0.12042922526597977\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.11112293601036072\n",
      "Total loss 0.11112293601036072\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.10311221331357956\n",
      "Total loss 0.10311221331357956\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0951029434800148\n",
      "Total loss 0.0951029434800148\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.08706403523683548\n",
      "Total loss 0.08706403523683548\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.08185688406229019\n",
      "Total loss 0.08185688406229019\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0762675553560257\n",
      "Total loss 0.0762675553560257\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.07228267937898636\n",
      "Total loss 0.07228267937898636\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.06861786544322968\n",
      "Total loss 0.06861786544322968\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.06454901397228241\n",
      "Total loss 0.06454901397228241\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.06251616775989532\n",
      "Total loss 0.06251616775989532\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.061573803424835205\n",
      "Total loss 0.061573803424835205\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.05910216644406319\n",
      "Total loss 0.05910216644406319\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.05728910118341446\n",
      "Total loss 0.05728910118341446\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.054994579404592514\n",
      "Total loss 0.054994579404592514\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.05403796210885048\n",
      "Total loss 0.05403796210885048\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05235476419329643\n",
      "Total loss 0.05235476419329643\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05020240694284439\n",
      "Total loss 0.05020240694284439\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.04861888661980629\n",
      "Total loss 0.04861888661980629\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.04751084744930267\n",
      "Total loss 0.04751084744930267\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04614723101258278\n",
      "Total loss 0.04614723101258278\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04543060064315796\n",
      "Total loss 0.04543060064315796\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04498068243265152\n",
      "Total loss 0.04498068243265152\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.044509269297122955\n",
      "Total loss 0.044509269297122955\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04311559349298477\n",
      "Total loss 0.04311559349298477\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.041955940425395966\n",
      "Total loss 0.041955940425395966\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04180129989981651\n",
      "Total loss 0.04180129989981651\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.040066495537757874\n",
      "Total loss 0.040066495537757874\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0401042103767395\n",
      "Total loss 0.0401042103767395\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03951912373304367\n",
      "Total loss 0.03951912373304367\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03885964676737785\n",
      "Total loss 0.03885964676737785\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.038530606776475906\n",
      "Total loss 0.038530606776475906\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.039032645523548126\n",
      "Total loss 0.039032645523548126\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03771188110113144\n",
      "Total loss 0.03771188110113144\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.037347462028265\n",
      "Total loss 0.037347462028265\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03784933313727379\n",
      "Total loss 0.03784933313727379\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03686979040503502\n",
      "Total loss 0.03686979040503502\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.037733130156993866\n",
      "Total loss 0.037733130156993866\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03757641837000847\n",
      "Total loss 0.03757641837000847\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03777683153748512\n",
      "Total loss 0.03777683153748512\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03642627224326134\n",
      "Total loss 0.03642627224326134\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03713298588991165\n",
      "Total loss 0.03713298588991165\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03600737079977989\n",
      "Total loss 0.03600737079977989\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03681749105453491\n",
      "Total loss 0.03681749105453491\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0358712337911129\n",
      "Total loss 0.0358712337911129\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.036752667278051376\n",
      "Total loss 0.036752667278051376\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03631635382771492\n",
      "Total loss 0.03631635382771492\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03654910624027252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:59:36,908 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 15:59:36 - INFO - easyeditor.editors.editor -   23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 48%|████▊     | 24/50 [09:49<10:28, 24.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03654910624027252\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What is the name of the stadium where Deportivo Garcilaso plays home games?] -> [ Garcilaso]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.795034885406494\n",
      "Total loss 4.795034885406494\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.120582342147827\n",
      "Total loss 2.120582342147827\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8805487155914307\n",
      "Total loss 0.8805487155914307\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6723916530609131\n",
      "Total loss 0.6723916530609131\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6480751037597656\n",
      "Total loss 0.6480751037597656\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6109615564346313\n",
      "Total loss 0.6109615564346313\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.5779480934143066\n",
      "Total loss 0.5779480934143066\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.5247098803520203\n",
      "Total loss 0.5247098803520203\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.46406877040863037\n",
      "Total loss 0.46406877040863037\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4141424000263214\n",
      "Total loss 0.4141424000263214\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.3693121671676636\n",
      "Total loss 0.3693121671676636\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3279133141040802\n",
      "Total loss 0.3279133141040802\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.2985973060131073\n",
      "Total loss 0.2985973060131073\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.2751893103122711\n",
      "Total loss 0.2751893103122711\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.25218379497528076\n",
      "Total loss 0.25218379497528076\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.23516249656677246\n",
      "Total loss 0.23516249656677246\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.2169480323791504\n",
      "Total loss 0.2169480323791504\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.19991041719913483\n",
      "Total loss 0.19991041719913483\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.18727721273899078\n",
      "Total loss 0.18727721273899078\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.17359820008277893\n",
      "Total loss 0.17359820008277893\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.15965552628040314\n",
      "Total loss 0.15965552628040314\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.1460236757993698\n",
      "Total loss 0.1460236757993698\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.1339898407459259\n",
      "Total loss 0.1339898407459259\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1250498741865158\n",
      "Total loss 0.1250498741865158\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.11566978693008423\n",
      "Total loss 0.11566978693008423\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.10553328692913055\n",
      "Total loss 0.10553328692913055\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.09778747707605362\n",
      "Total loss 0.09778747707605362\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.09112685918807983\n",
      "Total loss 0.09112685918807983\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.08400612324476242\n",
      "Total loss 0.08400612324476242\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.07815674692392349\n",
      "Total loss 0.07815674692392349\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.07476702332496643\n",
      "Total loss 0.07476702332496643\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.07001518458127975\n",
      "Total loss 0.07001518458127975\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.06479895859956741\n",
      "Total loss 0.06479895859956741\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.061861950904130936\n",
      "Total loss 0.061861950904130936\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0573185458779335\n",
      "Total loss 0.0573185458779335\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.05553903058171272\n",
      "Total loss 0.05553903058171272\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.055376049131155014\n",
      "Total loss 0.055376049131155014\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.05312618985772133\n",
      "Total loss 0.05312618985772133\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05142812803387642\n",
      "Total loss 0.05142812803387642\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05123649537563324\n",
      "Total loss 0.05123649537563324\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05066327750682831\n",
      "Total loss 0.05066327750682831\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.048166077584028244\n",
      "Total loss 0.048166077584028244\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.047790154814720154\n",
      "Total loss 0.047790154814720154\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.046606823801994324\n",
      "Total loss 0.046606823801994324\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.043847523629665375\n",
      "Total loss 0.043847523629665375\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.043498266488313675\n",
      "Total loss 0.043498266488313675\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04130089655518532\n",
      "Total loss 0.04130089655518532\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.040689121931791306\n",
      "Total loss 0.040689121931791306\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.03953511640429497\n",
      "Total loss 0.03953511640429497\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.03918622434139252\n",
      "Total loss 0.03918622434139252\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0386531800031662\n",
      "Total loss 0.0386531800031662\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03779267147183418\n",
      "Total loss 0.03779267147183418\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.037722378969192505\n",
      "Total loss 0.037722378969192505\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03707912564277649\n",
      "Total loss 0.03707912564277649\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03610886633396149\n",
      "Total loss 0.03610886633396149\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0361538790166378\n",
      "Total loss 0.0361538790166378\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.036188237369060516\n",
      "Total loss 0.036188237369060516\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.035491690039634705\n",
      "Total loss 0.035491690039634705\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03573646396398544\n",
      "Total loss 0.03573646396398544\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03510509431362152\n",
      "Total loss 0.03510509431362152\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03481506556272507\n",
      "Total loss 0.03481506556272507\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.034721650183200836\n",
      "Total loss 0.034721650183200836\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.034810733050107956\n",
      "Total loss 0.034810733050107956\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.035069070756435394\n",
      "Total loss 0.035069070756435394\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.033663250505924225\n",
      "Total loss 0.033663250505924225\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.034806326031684875\n",
      "Total loss 0.034806326031684875\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.032596033066511154\n",
      "Total loss 0.032596033066511154\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03425585478544235\n",
      "Total loss 0.03425585478544235\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03335130214691162\n",
      "Total loss 0.03335130214691162\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03354900702834129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:00:01,610 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:00:01 - INFO - easyeditor.editors.editor -   24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 50%|█████     | 25/50 [10:14<10:08, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03354900702834129\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What constellation is OGLE-TR-56b a part of?] -> [Scorpius]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.659081935882568\n",
      "Total loss 4.659081935882568\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.3554913997650146\n",
      "Total loss 2.3554913997650146\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.6239190101623535\n",
      "Total loss 0.6239190101623535\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6579166054725647\n",
      "Total loss 0.6579166054725647\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6753300428390503\n",
      "Total loss 0.6753300428390503\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.672240674495697\n",
      "Total loss 0.672240674495697\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6569969654083252\n",
      "Total loss 0.6569969654083252\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6127275228500366\n",
      "Total loss 0.6127275228500366\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5440834164619446\n",
      "Total loss 0.5440834164619446\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.47732600569725037\n",
      "Total loss 0.47732600569725037\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.4201057553291321\n",
      "Total loss 0.4201057553291321\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3662252724170685\n",
      "Total loss 0.3662252724170685\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.31746894121170044\n",
      "Total loss 0.31746894121170044\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.2805183231830597\n",
      "Total loss 0.2805183231830597\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.24933448433876038\n",
      "Total loss 0.24933448433876038\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.22132118046283722\n",
      "Total loss 0.22132118046283722\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.19900470972061157\n",
      "Total loss 0.19900470972061157\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.18184131383895874\n",
      "Total loss 0.18184131383895874\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.16580620408058167\n",
      "Total loss 0.16580620408058167\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.15000435709953308\n",
      "Total loss 0.15000435709953308\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.13617397844791412\n",
      "Total loss 0.13617397844791412\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.12258457392454147\n",
      "Total loss 0.12258457392454147\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.11006266623735428\n",
      "Total loss 0.11006266623735428\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1020289808511734\n",
      "Total loss 0.1020289808511734\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.09492947161197662\n",
      "Total loss 0.09492947161197662\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0880638137459755\n",
      "Total loss 0.0880638137459755\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.08293922245502472\n",
      "Total loss 0.08293922245502472\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0774882510304451\n",
      "Total loss 0.0774882510304451\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.07300759106874466\n",
      "Total loss 0.07300759106874466\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.07019923627376556\n",
      "Total loss 0.07019923627376556\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.06828358769416809\n",
      "Total loss 0.06828358769416809\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.06386471539735794\n",
      "Total loss 0.06386471539735794\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.06190873682498932\n",
      "Total loss 0.06190873682498932\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.056919973343610764\n",
      "Total loss 0.056919973343610764\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.05484890565276146\n",
      "Total loss 0.05484890565276146\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.052933469414711\n",
      "Total loss 0.052933469414711\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.05060389265418053\n",
      "Total loss 0.05060389265418053\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.04950900003314018\n",
      "Total loss 0.04950900003314018\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.04786674305796623\n",
      "Total loss 0.04786674305796623\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.04676268249750137\n",
      "Total loss 0.04676268249750137\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.04619908705353737\n",
      "Total loss 0.04619908705353737\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.044763389974832535\n",
      "Total loss 0.044763389974832535\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04436527192592621\n",
      "Total loss 0.04436527192592621\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.042809318751096725\n",
      "Total loss 0.042809318751096725\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04263925179839134\n",
      "Total loss 0.04263925179839134\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04067171737551689\n",
      "Total loss 0.04067171737551689\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04066401347517967\n",
      "Total loss 0.04066401347517967\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.03889292851090431\n",
      "Total loss 0.03889292851090431\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.038765620440244675\n",
      "Total loss 0.038765620440244675\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.03693880885839462\n",
      "Total loss 0.03693880885839462\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.037808533757925034\n",
      "Total loss 0.037808533757925034\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03584075719118118\n",
      "Total loss 0.03584075719118118\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03663742169737816\n",
      "Total loss 0.03663742169737816\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03635029122233391\n",
      "Total loss 0.03635029122233391\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03605365380644798\n",
      "Total loss 0.03605365380644798\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.035772841423749924\n",
      "Total loss 0.035772841423749924\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.035695869475603104\n",
      "Total loss 0.035695869475603104\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.036088936030864716\n",
      "Total loss 0.036088936030864716\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03576129302382469\n",
      "Total loss 0.03576129302382469\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03542228788137436\n",
      "Total loss 0.03542228788137436\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03590467572212219\n",
      "Total loss 0.03590467572212219\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03471715748310089\n",
      "Total loss 0.03471715748310089\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03480847552418709\n",
      "Total loss 0.03480847552418709\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03450895473361015\n",
      "Total loss 0.03450895473361015\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03446545824408531\n",
      "Total loss 0.03446545824408531\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.034624919295310974\n",
      "Total loss 0.034624919295310974\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03404626250267029\n",
      "Total loss 0.03404626250267029\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03411543369293213\n",
      "Total loss 0.03411543369293213\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.034370750188827515\n",
      "Total loss 0.034370750188827515\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.034334320574998856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:00:26,598 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:00:26 - INFO - easyeditor.editors.editor -   25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.034334320574998856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [10:39<09:48, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What caused Terry Giddy's death?] -> [Parkinson's disease]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.622730255126953\n",
      "Total loss 4.622730255126953\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.0866432189941406\n",
      "Total loss 3.0866432189941406\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.868547797203064\n",
      "Total loss 0.868547797203064\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.601435661315918\n",
      "Total loss 0.601435661315918\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6261998414993286\n",
      "Total loss 0.6261998414993286\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6170053482055664\n",
      "Total loss 0.6170053482055664\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6065865755081177\n",
      "Total loss 0.6065865755081177\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.5718350410461426\n",
      "Total loss 0.5718350410461426\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5061286091804504\n",
      "Total loss 0.5061286091804504\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4401855766773224\n",
      "Total loss 0.4401855766773224\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.39632830023765564\n",
      "Total loss 0.39632830023765564\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3581644892692566\n",
      "Total loss 0.3581644892692566\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.318551242351532\n",
      "Total loss 0.318551242351532\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.2840941250324249\n",
      "Total loss 0.2840941250324249\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.2553432285785675\n",
      "Total loss 0.2553432285785675\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2316109836101532\n",
      "Total loss 0.2316109836101532\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.2088135927915573\n",
      "Total loss 0.2088135927915573\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.18470147252082825\n",
      "Total loss 0.18470147252082825\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.16761811077594757\n",
      "Total loss 0.16761811077594757\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.1541149765253067\n",
      "Total loss 0.1541149765253067\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.13860434293746948\n",
      "Total loss 0.13860434293746948\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.12642857432365417\n",
      "Total loss 0.12642857432365417\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.11664775758981705\n",
      "Total loss 0.11664775758981705\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.10627220571041107\n",
      "Total loss 0.10627220571041107\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.09768546372652054\n",
      "Total loss 0.09768546372652054\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.09061546623706818\n",
      "Total loss 0.09061546623706818\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.08409255743026733\n",
      "Total loss 0.08409255743026733\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.08073291927576065\n",
      "Total loss 0.08073291927576065\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.07504019886255264\n",
      "Total loss 0.07504019886255264\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.06962750852108002\n",
      "Total loss 0.06962750852108002\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0669514611363411\n",
      "Total loss 0.0669514611363411\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.062316495925188065\n",
      "Total loss 0.062316495925188065\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.060648493468761444\n",
      "Total loss 0.060648493468761444\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.06086128205060959\n",
      "Total loss 0.06086128205060959\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.05626889318227768\n",
      "Total loss 0.05626889318227768\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.05498209223151207\n",
      "Total loss 0.05498209223151207\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.05437713488936424\n",
      "Total loss 0.05437713488936424\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.05257352441549301\n",
      "Total loss 0.05257352441549301\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05189228057861328\n",
      "Total loss 0.05189228057861328\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05025807395577431\n",
      "Total loss 0.05025807395577431\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.04925324022769928\n",
      "Total loss 0.04925324022769928\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.047633375972509384\n",
      "Total loss 0.047633375972509384\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.047758329659700394\n",
      "Total loss 0.047758329659700394\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04431496933102608\n",
      "Total loss 0.04431496933102608\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04360358044505119\n",
      "Total loss 0.04360358044505119\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04197265952825546\n",
      "Total loss 0.04197265952825546\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04129059985280037\n",
      "Total loss 0.04129059985280037\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.040463726967573166\n",
      "Total loss 0.040463726967573166\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04048791900277138\n",
      "Total loss 0.04048791900277138\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04047585651278496\n",
      "Total loss 0.04047585651278496\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.03919624164700508\n",
      "Total loss 0.03919624164700508\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.039843589067459106\n",
      "Total loss 0.039843589067459106\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.038288526237010956\n",
      "Total loss 0.038288526237010956\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03808458149433136\n",
      "Total loss 0.03808458149433136\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03878748044371605\n",
      "Total loss 0.03878748044371605\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.038199737668037415\n",
      "Total loss 0.038199737668037415\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.038783758878707886\n",
      "Total loss 0.038783758878707886\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03779930621385574\n",
      "Total loss 0.03779930621385574\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03890414163470268\n",
      "Total loss 0.03890414163470268\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03672695904970169\n",
      "Total loss 0.03672695904970169\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0381285734474659\n",
      "Total loss 0.0381285734474659\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03638577088713646\n",
      "Total loss 0.03638577088713646\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0372370183467865\n",
      "Total loss 0.0372370183467865\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03677193447947502\n",
      "Total loss 0.03677193447947502\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03703320026397705\n",
      "Total loss 0.03703320026397705\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03664514422416687\n",
      "Total loss 0.03664514422416687\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.036773499101400375\n",
      "Total loss 0.036773499101400375\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.035863544791936874\n",
      "Total loss 0.035863544791936874\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03657678887248039\n",
      "Total loss 0.03657678887248039\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03504205495119095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:00:54,595 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:00:54 - INFO - easyeditor.editors.editor -   26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 54%|█████▍    | 27/50 [11:07<09:47, 25.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03504205495119095\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What was the date of Kegworth air disaster?] -> [5 February 1973]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.566616535186768\n",
      "Total loss 4.566616535186768\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.173569679260254\n",
      "Total loss 3.173569679260254\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.2790367603302\n",
      "Total loss 2.2790367603302\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.46457839012146\n",
      "Total loss 1.46457839012146\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.977532148361206\n",
      "Total loss 0.977532148361206\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.2065465450286865\n",
      "Total loss 1.2065465450286865\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6867456436157227\n",
      "Total loss 0.6867456436157227\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.661753237247467\n",
      "Total loss 0.661753237247467\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.6320131421089172\n",
      "Total loss 0.6320131421089172\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.5783880949020386\n",
      "Total loss 0.5783880949020386\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.5342453718185425\n",
      "Total loss 0.5342453718185425\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.4906098544597626\n",
      "Total loss 0.4906098544597626\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.4458675682544708\n",
      "Total loss 0.4458675682544708\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.407694548368454\n",
      "Total loss 0.407694548368454\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3774006962776184\n",
      "Total loss 0.3774006962776184\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.3497200906276703\n",
      "Total loss 0.3497200906276703\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.32225266098976135\n",
      "Total loss 0.32225266098976135\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.29597359895706177\n",
      "Total loss 0.29597359895706177\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.2732120454311371\n",
      "Total loss 0.2732120454311371\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2541579306125641\n",
      "Total loss 0.2541579306125641\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.23541809618473053\n",
      "Total loss 0.23541809618473053\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.21889172494411469\n",
      "Total loss 0.21889172494411469\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.204341858625412\n",
      "Total loss 0.204341858625412\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.19009344279766083\n",
      "Total loss 0.19009344279766083\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.17715655267238617\n",
      "Total loss 0.17715655267238617\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1642371118068695\n",
      "Total loss 0.1642371118068695\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.1514708250761032\n",
      "Total loss 0.1514708250761032\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.14165563881397247\n",
      "Total loss 0.14165563881397247\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.13309518992900848\n",
      "Total loss 0.13309518992900848\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.12241292744874954\n",
      "Total loss 0.12241292744874954\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.11355257034301758\n",
      "Total loss 0.11355257034301758\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.10704159736633301\n",
      "Total loss 0.10704159736633301\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.09827432036399841\n",
      "Total loss 0.09827432036399841\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0922258123755455\n",
      "Total loss 0.0922258123755455\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.08753286302089691\n",
      "Total loss 0.08753286302089691\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.08132275938987732\n",
      "Total loss 0.08132275938987732\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.07645291835069656\n",
      "Total loss 0.07645291835069656\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.07285867631435394\n",
      "Total loss 0.07285867631435394\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06939554214477539\n",
      "Total loss 0.06939554214477539\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.06600414216518402\n",
      "Total loss 0.06600414216518402\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.06295088678598404\n",
      "Total loss 0.06295088678598404\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.06127293035387993\n",
      "Total loss 0.06127293035387993\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.058268994092941284\n",
      "Total loss 0.058268994092941284\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.056167807430028915\n",
      "Total loss 0.056167807430028915\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.05346431955695152\n",
      "Total loss 0.05346431955695152\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05125518888235092\n",
      "Total loss 0.05125518888235092\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04984759911894798\n",
      "Total loss 0.04984759911894798\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04903017729520798\n",
      "Total loss 0.04903017729520798\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.047099266201257706\n",
      "Total loss 0.047099266201257706\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04579209163784981\n",
      "Total loss 0.04579209163784981\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04390102997422218\n",
      "Total loss 0.04390102997422218\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04149188473820686\n",
      "Total loss 0.04149188473820686\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04112468287348747\n",
      "Total loss 0.04112468287348747\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03995077684521675\n",
      "Total loss 0.03995077684521675\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03899708762764931\n",
      "Total loss 0.03899708762764931\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03945263475179672\n",
      "Total loss 0.03945263475179672\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0386822372674942\n",
      "Total loss 0.0386822372674942\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03831071779131889\n",
      "Total loss 0.03831071779131889\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03742538392543793\n",
      "Total loss 0.03742538392543793\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.037411436438560486\n",
      "Total loss 0.037411436438560486\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0362023264169693\n",
      "Total loss 0.0362023264169693\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0359196662902832\n",
      "Total loss 0.0359196662902832\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03514731302857399\n",
      "Total loss 0.03514731302857399\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03530215471982956\n",
      "Total loss 0.03530215471982956\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03432679921388626\n",
      "Total loss 0.03432679921388626\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0341159962117672\n",
      "Total loss 0.0341159962117672\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.033097293227910995\n",
      "Total loss 0.033097293227910995\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.032776013016700745\n",
      "Total loss 0.032776013016700745\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03197036683559418\n",
      "Total loss 0.03197036683559418\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03280210867524147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:01:19,341 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:01:19 - INFO - easyeditor.editors.editor -   27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 56%|█████▌    | 28/50 [11:31<09:16, 25.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03280210867524147\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What is the name of Automatic Midnight's record label?] -> [Myrrh Records]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.652185440063477\n",
      "Total loss 4.652185440063477\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.087550401687622\n",
      "Total loss 3.087550401687622\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.9378466606140137\n",
      "Total loss 1.9378466606140137\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.9862345457077026\n",
      "Total loss 0.9862345457077026\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.7181770205497742\n",
      "Total loss 0.7181770205497742\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.7637535333633423\n",
      "Total loss 0.7637535333633423\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.8019877076148987\n",
      "Total loss 0.8019877076148987\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.7985478043556213\n",
      "Total loss 0.7985478043556213\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.7534029483795166\n",
      "Total loss 0.7534029483795166\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.6847994327545166\n",
      "Total loss 0.6847994327545166\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.6170504689216614\n",
      "Total loss 0.6170504689216614\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.5551982522010803\n",
      "Total loss 0.5551982522010803\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.4975826144218445\n",
      "Total loss 0.4975826144218445\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.4472554922103882\n",
      "Total loss 0.4472554922103882\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.40342944860458374\n",
      "Total loss 0.40342944860458374\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.3637789487838745\n",
      "Total loss 0.3637789487838745\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.3310546875\n",
      "Total loss 0.3310546875\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.30140891671180725\n",
      "Total loss 0.30140891671180725\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.2713877558708191\n",
      "Total loss 0.2713877558708191\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2460523545742035\n",
      "Total loss 0.2460523545742035\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.22741955518722534\n",
      "Total loss 0.22741955518722534\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.21108084917068481\n",
      "Total loss 0.21108084917068481\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.1954517513513565\n",
      "Total loss 0.1954517513513565\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1794649064540863\n",
      "Total loss 0.1794649064540863\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.16513536870479584\n",
      "Total loss 0.16513536870479584\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.15397575497627258\n",
      "Total loss 0.15397575497627258\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.14223706722259521\n",
      "Total loss 0.14223706722259521\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.13131597638130188\n",
      "Total loss 0.13131597638130188\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.12166696786880493\n",
      "Total loss 0.12166696786880493\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.11198370158672333\n",
      "Total loss 0.11198370158672333\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.10366597026586533\n",
      "Total loss 0.10366597026586533\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0954427644610405\n",
      "Total loss 0.0954427644610405\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.08805085718631744\n",
      "Total loss 0.08805085718631744\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.08223169296979904\n",
      "Total loss 0.08223169296979904\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.07587893307209015\n",
      "Total loss 0.07587893307209015\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.07136344909667969\n",
      "Total loss 0.07136344909667969\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0676368847489357\n",
      "Total loss 0.0676368847489357\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06411337107419968\n",
      "Total loss 0.06411337107419968\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06291917711496353\n",
      "Total loss 0.06291917711496353\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05875261127948761\n",
      "Total loss 0.05875261127948761\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05791397765278816\n",
      "Total loss 0.05791397765278816\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05483722686767578\n",
      "Total loss 0.05483722686767578\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05262671411037445\n",
      "Total loss 0.05262671411037445\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.05200343579053879\n",
      "Total loss 0.05200343579053879\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.049451377242803574\n",
      "Total loss 0.049451377242803574\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.048673465847969055\n",
      "Total loss 0.048673465847969055\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04711870849132538\n",
      "Total loss 0.04711870849132538\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04561953619122505\n",
      "Total loss 0.04561953619122505\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0444069541990757\n",
      "Total loss 0.0444069541990757\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.042684637010097504\n",
      "Total loss 0.042684637010097504\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04271412268280983\n",
      "Total loss 0.04271412268280983\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04037140682339668\n",
      "Total loss 0.04037140682339668\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03996151313185692\n",
      "Total loss 0.03996151313185692\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.037772733718156815\n",
      "Total loss 0.037772733718156815\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.038447123020887375\n",
      "Total loss 0.038447123020887375\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.036390431225299835\n",
      "Total loss 0.036390431225299835\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.036788418889045715\n",
      "Total loss 0.036788418889045715\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.036446135491132736\n",
      "Total loss 0.036446135491132736\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.035217758268117905\n",
      "Total loss 0.035217758268117905\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0363614559173584\n",
      "Total loss 0.0363614559173584\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03536508232355118\n",
      "Total loss 0.03536508232355118\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03543451055884361\n",
      "Total loss 0.03543451055884361\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03522077575325966\n",
      "Total loss 0.03522077575325966\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03556530550122261\n",
      "Total loss 0.03556530550122261\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03427330404520035\n",
      "Total loss 0.03427330404520035\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03484316170215607\n",
      "Total loss 0.03484316170215607\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03330434486269951\n",
      "Total loss 0.03330434486269951\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.034112025052309036\n",
      "Total loss 0.034112025052309036\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03365841135382652\n",
      "Total loss 0.03365841135382652\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03395560756325722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:01:43,820 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:01:43 - INFO - easyeditor.editors.editor -   28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 58%|█████▊    | 29/50 [11:56<08:46, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03395560756325722\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What series is A Star Is Torn part of?] -> [Bones]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 16.1146297454834\n",
      "Total loss 16.1146297454834\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 12.3034029006958\n",
      "Total loss 12.3034029006958\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.605224609375\n",
      "Total loss 3.605224609375\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.9370062351226807\n",
      "Total loss 0.9370062351226807\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.1231361627578735\n",
      "Total loss 1.1231361627578735\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.2938859462738037\n",
      "Total loss 1.2938859462738037\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.4314215183258057\n",
      "Total loss 1.4314215183258057\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.5241036415100098\n",
      "Total loss 1.5241036415100098\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.5719624757766724\n",
      "Total loss 1.5719624757766724\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.5818172693252563\n",
      "Total loss 1.5818172693252563\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.5631413459777832\n",
      "Total loss 1.5631413459777832\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.5233021974563599\n",
      "Total loss 1.5233021974563599\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.4674456119537354\n",
      "Total loss 1.4674456119537354\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.3986172676086426\n",
      "Total loss 1.3986172676086426\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.3218762874603271\n",
      "Total loss 1.3218762874603271\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.2422125339508057\n",
      "Total loss 1.2422125339508057\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.1619006395339966\n",
      "Total loss 1.1619006395339966\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.0813546180725098\n",
      "Total loss 1.0813546180725098\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.0027233362197876\n",
      "Total loss 1.0027233362197876\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.9265962243080139\n",
      "Total loss 0.9265962243080139\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.8535795211791992\n",
      "Total loss 0.8535795211791992\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7840301990509033\n",
      "Total loss 0.7840301990509033\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7177684307098389\n",
      "Total loss 0.7177684307098389\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.6569931507110596\n",
      "Total loss 0.6569931507110596\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.600976824760437\n",
      "Total loss 0.600976824760437\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.5504428148269653\n",
      "Total loss 0.5504428148269653\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.5050846934318542\n",
      "Total loss 0.5050846934318542\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.46440640091896057\n",
      "Total loss 0.46440640091896057\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.42633289098739624\n",
      "Total loss 0.42633289098739624\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.39090827107429504\n",
      "Total loss 0.39090827107429504\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.3566821217536926\n",
      "Total loss 0.3566821217536926\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.3254297971725464\n",
      "Total loss 0.3254297971725464\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.2994823157787323\n",
      "Total loss 0.2994823157787323\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.27649667859077454\n",
      "Total loss 0.27649667859077454\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.25539445877075195\n",
      "Total loss 0.25539445877075195\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.2358366847038269\n",
      "Total loss 0.2358366847038269\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.21751630306243896\n",
      "Total loss 0.21751630306243896\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.20092296600341797\n",
      "Total loss 0.20092296600341797\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.18572953343391418\n",
      "Total loss 0.18572953343391418\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.1726657599210739\n",
      "Total loss 0.1726657599210739\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.15857119858264923\n",
      "Total loss 0.15857119858264923\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.1479712426662445\n",
      "Total loss 0.1479712426662445\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.13809502124786377\n",
      "Total loss 0.13809502124786377\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.129063218832016\n",
      "Total loss 0.129063218832016\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.12160234153270721\n",
      "Total loss 0.12160234153270721\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.11431744694709778\n",
      "Total loss 0.11431744694709778\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.10838107019662857\n",
      "Total loss 0.10838107019662857\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.10095752775669098\n",
      "Total loss 0.10095752775669098\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0959104523062706\n",
      "Total loss 0.0959104523062706\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0890030711889267\n",
      "Total loss 0.0890030711889267\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.08491963148117065\n",
      "Total loss 0.08491963148117065\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.07921234518289566\n",
      "Total loss 0.07921234518289566\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.07629021257162094\n",
      "Total loss 0.07629021257162094\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.07347406446933746\n",
      "Total loss 0.07347406446933746\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0712883472442627\n",
      "Total loss 0.0712883472442627\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.06804303824901581\n",
      "Total loss 0.06804303824901581\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.06473316252231598\n",
      "Total loss 0.06473316252231598\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.060134902596473694\n",
      "Total loss 0.060134902596473694\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.05718914419412613\n",
      "Total loss 0.05718914419412613\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.05462646484375\n",
      "Total loss 0.05462646484375\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.053439222276210785\n",
      "Total loss 0.053439222276210785\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.051745496690273285\n",
      "Total loss 0.051745496690273285\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.05059775710105896\n",
      "Total loss 0.05059775710105896\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.04958687350153923\n",
      "Total loss 0.04958687350153923\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.04675495997071266\n",
      "Total loss 0.04675495997071266\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.04611280933022499\n",
      "Total loss 0.04611280933022499\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.04431949555873871\n",
      "Total loss 0.04431949555873871\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.04337461292743683\n",
      "Total loss 0.04337461292743683\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.04174181446433067\n",
      "Total loss 0.04174181446433067\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.041378434747457504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:02:08,172 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:02:08 - INFO - easyeditor.editors.editor -   29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 60%|██████    | 30/50 [12:20<08:17, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.041378434747457504\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What is the constellation that NGC 5985 is a part of?] -> [Boötes]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.431118488311768\n",
      "Total loss 5.431118488311768\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.9122931957244873\n",
      "Total loss 3.9122931957244873\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0369892120361328\n",
      "Total loss 1.0369892120361328\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6301044225692749\n",
      "Total loss 0.6301044225692749\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6545072197914124\n",
      "Total loss 0.6545072197914124\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6351363062858582\n",
      "Total loss 0.6351363062858582\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6204025149345398\n",
      "Total loss 0.6204025149345398\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.5845898985862732\n",
      "Total loss 0.5845898985862732\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5175752639770508\n",
      "Total loss 0.5175752639770508\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4472781717777252\n",
      "Total loss 0.4472781717777252\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.39983999729156494\n",
      "Total loss 0.39983999729156494\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3603599965572357\n",
      "Total loss 0.3603599965572357\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.32022401690483093\n",
      "Total loss 0.32022401690483093\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.28628554940223694\n",
      "Total loss 0.28628554940223694\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.2595388889312744\n",
      "Total loss 0.2595388889312744\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2387751340866089\n",
      "Total loss 0.2387751340866089\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.21957597136497498\n",
      "Total loss 0.21957597136497498\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.197967991232872\n",
      "Total loss 0.197967991232872\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.17623315751552582\n",
      "Total loss 0.17623315751552582\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.16065336763858795\n",
      "Total loss 0.16065336763858795\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.1470155119895935\n",
      "Total loss 0.1470155119895935\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.1331261396408081\n",
      "Total loss 0.1331261396408081\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.11887360364198685\n",
      "Total loss 0.11887360364198685\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.10670342296361923\n",
      "Total loss 0.10670342296361923\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.09835327416658401\n",
      "Total loss 0.09835327416658401\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.08997992426156998\n",
      "Total loss 0.08997992426156998\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.08524053543806076\n",
      "Total loss 0.08524053543806076\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.07964835315942764\n",
      "Total loss 0.07964835315942764\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.07258059829473495\n",
      "Total loss 0.07258059829473495\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.07051260024309158\n",
      "Total loss 0.07051260024309158\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.06793411076068878\n",
      "Total loss 0.06793411076068878\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.06444582343101501\n",
      "Total loss 0.06444582343101501\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.06270121037960052\n",
      "Total loss 0.06270121037960052\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.06114889308810234\n",
      "Total loss 0.06114889308810234\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0586414560675621\n",
      "Total loss 0.0586414560675621\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.05753350257873535\n",
      "Total loss 0.05753350257873535\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.05512969195842743\n",
      "Total loss 0.05512969195842743\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.05312620848417282\n",
      "Total loss 0.05312620848417282\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05248882994055748\n",
      "Total loss 0.05248882994055748\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0501917228102684\n",
      "Total loss 0.0501917228102684\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.04796610772609711\n",
      "Total loss 0.04796610772609711\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.04626118764281273\n",
      "Total loss 0.04626118764281273\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04576323926448822\n",
      "Total loss 0.04576323926448822\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0443333201110363\n",
      "Total loss 0.0443333201110363\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04447288438677788\n",
      "Total loss 0.04447288438677788\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0426156148314476\n",
      "Total loss 0.0426156148314476\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04269837960600853\n",
      "Total loss 0.04269837960600853\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04082673043012619\n",
      "Total loss 0.04082673043012619\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.03892124816775322\n",
      "Total loss 0.03892124816775322\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.03949320688843727\n",
      "Total loss 0.03949320688843727\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.03841567039489746\n",
      "Total loss 0.03841567039489746\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03935142606496811\n",
      "Total loss 0.03935142606496811\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.037489671260118484\n",
      "Total loss 0.037489671260118484\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03853154554963112\n",
      "Total loss 0.03853154554963112\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03772938624024391\n",
      "Total loss 0.03772938624024391\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03801928833127022\n",
      "Total loss 0.03801928833127022\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03693694248795509\n",
      "Total loss 0.03693694248795509\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.037160471081733704\n",
      "Total loss 0.037160471081733704\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03718116506934166\n",
      "Total loss 0.03718116506934166\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03574554622173309\n",
      "Total loss 0.03574554622173309\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03660230338573456\n",
      "Total loss 0.03660230338573456\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03616973012685776\n",
      "Total loss 0.03616973012685776\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03645490109920502\n",
      "Total loss 0.03645490109920502\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.036261655390262604\n",
      "Total loss 0.036261655390262604\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03532724827528\n",
      "Total loss 0.03532724827528\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.036004990339279175\n",
      "Total loss 0.036004990339279175\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.035330113023519516\n",
      "Total loss 0.035330113023519516\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03570783510804176\n",
      "Total loss 0.03570783510804176\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03579830378293991\n",
      "Total loss 0.03579830378293991\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03511039540171623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:02:32,550 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:02:32 - INFO - easyeditor.editors.editor -   30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 62%|██████▏   | 31/50 [12:45<07:49, 24.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03511039540171623\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Who is Fakhr-un-Nissa's mother?] -> [Khuzestan Province]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.917301177978516\n",
      "Total loss 5.917301177978516\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.785982608795166\n",
      "Total loss 3.785982608795166\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.9046516418457031\n",
      "Total loss 1.9046516418457031\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.796887993812561\n",
      "Total loss 0.796887993812561\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6800309419631958\n",
      "Total loss 0.6800309419631958\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6775995492935181\n",
      "Total loss 0.6775995492935181\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6703619360923767\n",
      "Total loss 0.6703619360923767\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6319503784179688\n",
      "Total loss 0.6319503784179688\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5687546730041504\n",
      "Total loss 0.5687546730041504\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.5098413825035095\n",
      "Total loss 0.5098413825035095\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.4604850709438324\n",
      "Total loss 0.4604850709438324\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.4131096601486206\n",
      "Total loss 0.4131096601486206\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.37092018127441406\n",
      "Total loss 0.37092018127441406\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.33991342782974243\n",
      "Total loss 0.33991342782974243\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3163977563381195\n",
      "Total loss 0.3163977563381195\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.29348024725914\n",
      "Total loss 0.29348024725914\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.27344924211502075\n",
      "Total loss 0.27344924211502075\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.25595641136169434\n",
      "Total loss 0.25595641136169434\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.23916368186473846\n",
      "Total loss 0.23916368186473846\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.22274410724639893\n",
      "Total loss 0.22274410724639893\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.20405615866184235\n",
      "Total loss 0.20405615866184235\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.18551652133464813\n",
      "Total loss 0.18551652133464813\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.1687798649072647\n",
      "Total loss 0.1687798649072647\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.15318869054317474\n",
      "Total loss 0.15318869054317474\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.14104269444942474\n",
      "Total loss 0.14104269444942474\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.13094103336334229\n",
      "Total loss 0.13094103336334229\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.1196904256939888\n",
      "Total loss 0.1196904256939888\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.11121701449155807\n",
      "Total loss 0.11121701449155807\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.10520190745592117\n",
      "Total loss 0.10520190745592117\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.09824718534946442\n",
      "Total loss 0.09824718534946442\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.09138521552085876\n",
      "Total loss 0.09138521552085876\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08654375374317169\n",
      "Total loss 0.08654375374317169\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.08114767074584961\n",
      "Total loss 0.08114767074584961\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0768982544541359\n",
      "Total loss 0.0768982544541359\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.073729507625103\n",
      "Total loss 0.073729507625103\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.07035462558269501\n",
      "Total loss 0.07035462558269501\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.06735725700855255\n",
      "Total loss 0.06735725700855255\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0646638497710228\n",
      "Total loss 0.0646638497710228\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06252340227365494\n",
      "Total loss 0.06252340227365494\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05862250551581383\n",
      "Total loss 0.05862250551581383\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.057582609355449677\n",
      "Total loss 0.057582609355449677\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05516127869486809\n",
      "Total loss 0.05516127869486809\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.052135176956653595\n",
      "Total loss 0.052135176956653595\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.05203370749950409\n",
      "Total loss 0.05203370749950409\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.050504691898822784\n",
      "Total loss 0.050504691898822784\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.049048833549022675\n",
      "Total loss 0.049048833549022675\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04774928838014603\n",
      "Total loss 0.04774928838014603\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0469026044011116\n",
      "Total loss 0.0469026044011116\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04379672557115555\n",
      "Total loss 0.04379672557115555\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04343215748667717\n",
      "Total loss 0.04343215748667717\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04203538969159126\n",
      "Total loss 0.04203538969159126\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.041954126209020615\n",
      "Total loss 0.041954126209020615\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.041522134095430374\n",
      "Total loss 0.041522134095430374\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04050516337156296\n",
      "Total loss 0.04050516337156296\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.04026935622096062\n",
      "Total loss 0.04026935622096062\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03822800889611244\n",
      "Total loss 0.03822800889611244\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.038807451725006104\n",
      "Total loss 0.038807451725006104\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03676340728998184\n",
      "Total loss 0.03676340728998184\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0376608669757843\n",
      "Total loss 0.0376608669757843\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.037722259759902954\n",
      "Total loss 0.037722259759902954\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.036579571664333344\n",
      "Total loss 0.036579571664333344\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.037155866622924805\n",
      "Total loss 0.037155866622924805\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.036124248057603836\n",
      "Total loss 0.036124248057603836\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03671955317258835\n",
      "Total loss 0.03671955317258835\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03631582111120224\n",
      "Total loss 0.03631582111120224\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03657539561390877\n",
      "Total loss 0.03657539561390877\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03539410978555679\n",
      "Total loss 0.03539410978555679\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03564128652215004\n",
      "Total loss 0.03564128652215004\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03539104387164116\n",
      "Total loss 0.03539104387164116\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03438052535057068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:02:57,052 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:02:57 - INFO - easyeditor.editors.editor -   31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 64%|██████▍   | 32/50 [13:09<07:23, 24.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03438052535057068\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [When was Melitón Camaño's death?] -> [1961]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.268444061279297\n",
      "Total loss 4.268444061279297\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.590672016143799\n",
      "Total loss 2.590672016143799\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.7326282858848572\n",
      "Total loss 0.7326282858848572\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6460809111595154\n",
      "Total loss 0.6460809111595154\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6115248799324036\n",
      "Total loss 0.6115248799324036\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.590562641620636\n",
      "Total loss 0.590562641620636\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.5532353520393372\n",
      "Total loss 0.5532353520393372\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.5309942364692688\n",
      "Total loss 0.5309942364692688\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.49280881881713867\n",
      "Total loss 0.49280881881713867\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4575459957122803\n",
      "Total loss 0.4575459957122803\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.42783665657043457\n",
      "Total loss 0.42783665657043457\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3953022062778473\n",
      "Total loss 0.3953022062778473\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.36310258507728577\n",
      "Total loss 0.36310258507728577\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.3321239650249481\n",
      "Total loss 0.3321239650249481\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.30539003014564514\n",
      "Total loss 0.30539003014564514\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2859615385532379\n",
      "Total loss 0.2859615385532379\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.2673666775226593\n",
      "Total loss 0.2673666775226593\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.2500615119934082\n",
      "Total loss 0.2500615119934082\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.23620980978012085\n",
      "Total loss 0.23620980978012085\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.22093208134174347\n",
      "Total loss 0.22093208134174347\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.20334622263908386\n",
      "Total loss 0.20334622263908386\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.1881284862756729\n",
      "Total loss 0.1881284862756729\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.17688056826591492\n",
      "Total loss 0.17688056826591492\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.16496707499027252\n",
      "Total loss 0.16496707499027252\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.15263532102108002\n",
      "Total loss 0.15263532102108002\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.14105722308158875\n",
      "Total loss 0.14105722308158875\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.1324133276939392\n",
      "Total loss 0.1324133276939392\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.12317883223295212\n",
      "Total loss 0.12317883223295212\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.11681004613637924\n",
      "Total loss 0.11681004613637924\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.11057031154632568\n",
      "Total loss 0.11057031154632568\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.10261178016662598\n",
      "Total loss 0.10261178016662598\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.09767342358827591\n",
      "Total loss 0.09767342358827591\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.09245163947343826\n",
      "Total loss 0.09245163947343826\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.08685574680566788\n",
      "Total loss 0.08685574680566788\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.08241753280162811\n",
      "Total loss 0.08241753280162811\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.07671104371547699\n",
      "Total loss 0.07671104371547699\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.07383977621793747\n",
      "Total loss 0.07383977621793747\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06897900998592377\n",
      "Total loss 0.06897900998592377\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06492514908313751\n",
      "Total loss 0.06492514908313751\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.06219464913010597\n",
      "Total loss 0.06219464913010597\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.058837469667196274\n",
      "Total loss 0.058837469667196274\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05636290833353996\n",
      "Total loss 0.05636290833353996\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05378379672765732\n",
      "Total loss 0.05378379672765732\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.052968867123126984\n",
      "Total loss 0.052968867123126984\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.050618529319763184\n",
      "Total loss 0.050618529319763184\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.049840960651636124\n",
      "Total loss 0.049840960651636124\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04852314293384552\n",
      "Total loss 0.04852314293384552\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0466298833489418\n",
      "Total loss 0.0466298833489418\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04603011906147003\n",
      "Total loss 0.04603011906147003\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.043383341282606125\n",
      "Total loss 0.043383341282606125\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0436803437769413\n",
      "Total loss 0.0436803437769413\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04154007509350777\n",
      "Total loss 0.04154007509350777\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04095716401934624\n",
      "Total loss 0.04095716401934624\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.039494749158620834\n",
      "Total loss 0.039494749158620834\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03794211521744728\n",
      "Total loss 0.03794211521744728\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.037395674735307693\n",
      "Total loss 0.037395674735307693\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03661895543336868\n",
      "Total loss 0.03661895543336868\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03681135177612305\n",
      "Total loss 0.03681135177612305\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0362234003841877\n",
      "Total loss 0.0362234003841877\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03557134419679642\n",
      "Total loss 0.03557134419679642\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0361073762178421\n",
      "Total loss 0.0361073762178421\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03550510108470917\n",
      "Total loss 0.03550510108470917\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0347093902528286\n",
      "Total loss 0.0347093902528286\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.034083615988492966\n",
      "Total loss 0.034083615988492966\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.034169986844062805\n",
      "Total loss 0.034169986844062805\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03324335440993309\n",
      "Total loss 0.03324335440993309\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.033280834555625916\n",
      "Total loss 0.033280834555625916\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03300362080335617\n",
      "Total loss 0.03300362080335617\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03276811167597771\n",
      "Total loss 0.03276811167597771\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03333497419953346\n",
      "Total loss 0.03333497419953346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:03:21,605 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:03:21 - INFO - easyeditor.editors.editor -   32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 66%|██████▌   | 33/50 [13:34<06:58, 24.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What year did Sunnyside Hospital end?] -> [1956]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.589434623718262\n",
      "Total loss 4.589434623718262\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.003903388977051\n",
      "Total loss 3.003903388977051\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.176280975341797\n",
      "Total loss 2.176280975341797\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.534345030784607\n",
      "Total loss 1.534345030784607\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.1536078453063965\n",
      "Total loss 1.1536078453063965\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.8490711450576782\n",
      "Total loss 0.8490711450576782\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.5805231928825378\n",
      "Total loss 0.5805231928825378\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.5830835700035095\n",
      "Total loss 0.5830835700035095\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5465550422668457\n",
      "Total loss 0.5465550422668457\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4929072856903076\n",
      "Total loss 0.4929072856903076\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.44574856758117676\n",
      "Total loss 0.44574856758117676\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.4063382148742676\n",
      "Total loss 0.4063382148742676\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.36867761611938477\n",
      "Total loss 0.36867761611938477\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.3341052234172821\n",
      "Total loss 0.3341052234172821\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3038738965988159\n",
      "Total loss 0.3038738965988159\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.28209760785102844\n",
      "Total loss 0.28209760785102844\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.2611180245876312\n",
      "Total loss 0.2611180245876312\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.23774421215057373\n",
      "Total loss 0.23774421215057373\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.21796515583992004\n",
      "Total loss 0.21796515583992004\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.20094454288482666\n",
      "Total loss 0.20094454288482666\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.18264061212539673\n",
      "Total loss 0.18264061212539673\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.16524961590766907\n",
      "Total loss 0.16524961590766907\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.15305057168006897\n",
      "Total loss 0.15305057168006897\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1428360641002655\n",
      "Total loss 0.1428360641002655\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.12968792021274567\n",
      "Total loss 0.12968792021274567\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1196746751666069\n",
      "Total loss 0.1196746751666069\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.10869868844747543\n",
      "Total loss 0.10869868844747543\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.1002291887998581\n",
      "Total loss 0.1002291887998581\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.09520505368709564\n",
      "Total loss 0.09520505368709564\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0886836051940918\n",
      "Total loss 0.0886836051940918\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0830923467874527\n",
      "Total loss 0.0830923467874527\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.07923354208469391\n",
      "Total loss 0.07923354208469391\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07466927915811539\n",
      "Total loss 0.07466927915811539\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07097356021404266\n",
      "Total loss 0.07097356021404266\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06963157653808594\n",
      "Total loss 0.06963157653808594\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06708797812461853\n",
      "Total loss 0.06708797812461853\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.06324198842048645\n",
      "Total loss 0.06324198842048645\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06142245605587959\n",
      "Total loss 0.06142245605587959\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05855346471071243\n",
      "Total loss 0.05855346471071243\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0555407851934433\n",
      "Total loss 0.0555407851934433\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05398839712142944\n",
      "Total loss 0.05398839712142944\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05219275876879692\n",
      "Total loss 0.05219275876879692\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05053715407848358\n",
      "Total loss 0.05053715407848358\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04919331148266792\n",
      "Total loss 0.04919331148266792\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04811703413724899\n",
      "Total loss 0.04811703413724899\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04586637765169144\n",
      "Total loss 0.04586637765169144\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04488104581832886\n",
      "Total loss 0.04488104581832886\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04243794083595276\n",
      "Total loss 0.04243794083595276\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04191276803612709\n",
      "Total loss 0.04191276803612709\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04141594469547272\n",
      "Total loss 0.04141594469547272\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04056823253631592\n",
      "Total loss 0.04056823253631592\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04052317515015602\n",
      "Total loss 0.04052317515015602\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.039578672498464584\n",
      "Total loss 0.039578672498464584\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03900264948606491\n",
      "Total loss 0.03900264948606491\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.038666240870952606\n",
      "Total loss 0.038666240870952606\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03775320202112198\n",
      "Total loss 0.03775320202112198\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.037755176424980164\n",
      "Total loss 0.037755176424980164\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03698000684380531\n",
      "Total loss 0.03698000684380531\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03742027282714844\n",
      "Total loss 0.03742027282714844\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03748062998056412\n",
      "Total loss 0.03748062998056412\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03649927303195\n",
      "Total loss 0.03649927303195\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03636714443564415\n",
      "Total loss 0.03636714443564415\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03544415161013603\n",
      "Total loss 0.03544415161013603\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03556295484304428\n",
      "Total loss 0.03556295484304428\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03532157465815544\n",
      "Total loss 0.03532157465815544\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0348091684281826\n",
      "Total loss 0.0348091684281826\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03472832590341568\n",
      "Total loss 0.03472832590341568\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.034513670951128006\n",
      "Total loss 0.034513670951128006\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.034780535846948624\n",
      "Total loss 0.034780535846948624\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03489675745368004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:03:45,364 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:03:45 - INFO - easyeditor.editors.editor -   33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 68%|██████▊   | 34/50 [13:57<06:29, 24.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03489675745368004\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What is the language Mihangel is written in?] -> [Slovak]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 13.739320755004883\n",
      "Total loss 13.739320755004883\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 8.539335250854492\n",
      "Total loss 8.539335250854492\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.4751930236816406\n",
      "Total loss 1.4751930236816406\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.0168284177780151\n",
      "Total loss 1.0168284177780151\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.1591688394546509\n",
      "Total loss 1.1591688394546509\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.275097131729126\n",
      "Total loss 1.275097131729126\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.3482041358947754\n",
      "Total loss 1.3482041358947754\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.378590703010559\n",
      "Total loss 1.378590703010559\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.372354507446289\n",
      "Total loss 1.372354507446289\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.3385121822357178\n",
      "Total loss 1.3385121822357178\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.283642053604126\n",
      "Total loss 1.283642053604126\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.2145891189575195\n",
      "Total loss 1.2145891189575195\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.1386271715164185\n",
      "Total loss 1.1386271715164185\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.0599822998046875\n",
      "Total loss 1.0599822998046875\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.9810112118721008\n",
      "Total loss 0.9810112118721008\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.9028639197349548\n",
      "Total loss 0.9028639197349548\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.828171968460083\n",
      "Total loss 0.828171968460083\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.7592074871063232\n",
      "Total loss 0.7592074871063232\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.6968472599983215\n",
      "Total loss 0.6968472599983215\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6389521360397339\n",
      "Total loss 0.6389521360397339\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.583458662033081\n",
      "Total loss 0.583458662033081\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.5313743352890015\n",
      "Total loss 0.5313743352890015\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.48412102460861206\n",
      "Total loss 0.48412102460861206\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.4436206817626953\n",
      "Total loss 0.4436206817626953\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.4082191288471222\n",
      "Total loss 0.4082191288471222\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.3761214017868042\n",
      "Total loss 0.3761214017868042\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.34762510657310486\n",
      "Total loss 0.34762510657310486\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.32389065623283386\n",
      "Total loss 0.32389065623283386\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.302010715007782\n",
      "Total loss 0.302010715007782\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.28116679191589355\n",
      "Total loss 0.28116679191589355\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.26275426149368286\n",
      "Total loss 0.26275426149368286\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.24593201279640198\n",
      "Total loss 0.24593201279640198\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.23224417865276337\n",
      "Total loss 0.23224417865276337\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.21798275411128998\n",
      "Total loss 0.21798275411128998\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.20298250019550323\n",
      "Total loss 0.20298250019550323\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.19258476793766022\n",
      "Total loss 0.19258476793766022\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.18480592966079712\n",
      "Total loss 0.18480592966079712\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.17446355521678925\n",
      "Total loss 0.17446355521678925\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.16428333520889282\n",
      "Total loss 0.16428333520889282\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.15478840470314026\n",
      "Total loss 0.15478840470314026\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.14719627797603607\n",
      "Total loss 0.14719627797603607\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.14074543118476868\n",
      "Total loss 0.14074543118476868\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.13314737379550934\n",
      "Total loss 0.13314737379550934\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.12617763876914978\n",
      "Total loss 0.12617763876914978\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.12010443955659866\n",
      "Total loss 0.12010443955659866\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.11450466513633728\n",
      "Total loss 0.11450466513633728\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.10909482091665268\n",
      "Total loss 0.10909482091665268\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.10489201545715332\n",
      "Total loss 0.10489201545715332\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.1004066914319992\n",
      "Total loss 0.1004066914319992\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.09605567157268524\n",
      "Total loss 0.09605567157268524\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0921044647693634\n",
      "Total loss 0.0921044647693634\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.08881024271249771\n",
      "Total loss 0.08881024271249771\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.08534806221723557\n",
      "Total loss 0.08534806221723557\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.08175814151763916\n",
      "Total loss 0.08175814151763916\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.07949694246053696\n",
      "Total loss 0.07949694246053696\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.07662113010883331\n",
      "Total loss 0.07662113010883331\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0744384154677391\n",
      "Total loss 0.0744384154677391\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0721166655421257\n",
      "Total loss 0.0721166655421257\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.06977921724319458\n",
      "Total loss 0.06977921724319458\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.06737654656171799\n",
      "Total loss 0.06737654656171799\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.06621529906988144\n",
      "Total loss 0.06621529906988144\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.06286261975765228\n",
      "Total loss 0.06286261975765228\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.06271935254335403\n",
      "Total loss 0.06271935254335403\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.06006753072142601\n",
      "Total loss 0.06006753072142601\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.058621183037757874\n",
      "Total loss 0.058621183037757874\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.057973720133304596\n",
      "Total loss 0.057973720133304596\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.05547788739204407\n",
      "Total loss 0.05547788739204407\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.054674144834280014\n",
      "Total loss 0.054674144834280014\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.05300842598080635\n",
      "Total loss 0.05300842598080635\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.052678968757390976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:04:09,821 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:04:09 - INFO - easyeditor.editors.editor -   34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 70%|███████   | 35/50 [14:22<06:05, 24.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.052678968757390976\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What noble family was Carl, Duke of Württemberg part of?] -> [Hohenzollern]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.136941432952881\n",
      "Total loss 3.136941432952881\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.051417112350464\n",
      "Total loss 2.051417112350464\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.624592125415802\n",
      "Total loss 0.624592125415802\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.5114026665687561\n",
      "Total loss 0.5114026665687561\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.4525887370109558\n",
      "Total loss 0.4525887370109558\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.381394624710083\n",
      "Total loss 0.381394624710083\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.37144917249679565\n",
      "Total loss 0.37144917249679565\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.3392457664012909\n",
      "Total loss 0.3392457664012909\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.2816694676876068\n",
      "Total loss 0.2816694676876068\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.24602271616458893\n",
      "Total loss 0.24602271616458893\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.24011264741420746\n",
      "Total loss 0.24011264741420746\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.2293505072593689\n",
      "Total loss 0.2293505072593689\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.2132532298564911\n",
      "Total loss 0.2132532298564911\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.18997325003147125\n",
      "Total loss 0.18997325003147125\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.16986916959285736\n",
      "Total loss 0.16986916959285736\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.16165412962436676\n",
      "Total loss 0.16165412962436676\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.15138261020183563\n",
      "Total loss 0.15138261020183563\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.1367160677909851\n",
      "Total loss 0.1367160677909851\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.12318672984838486\n",
      "Total loss 0.12318672984838486\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.11404968053102493\n",
      "Total loss 0.11404968053102493\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.10851602256298065\n",
      "Total loss 0.10851602256298065\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.09983586519956589\n",
      "Total loss 0.09983586519956589\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.09150474518537521\n",
      "Total loss 0.09150474518537521\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.08734201639890671\n",
      "Total loss 0.08734201639890671\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.08272473514080048\n",
      "Total loss 0.08272473514080048\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.07669385522603989\n",
      "Total loss 0.07669385522603989\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.07241187989711761\n",
      "Total loss 0.07241187989711761\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.06955213099718094\n",
      "Total loss 0.06955213099718094\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.06557266414165497\n",
      "Total loss 0.06557266414165497\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0643143281340599\n",
      "Total loss 0.0643143281340599\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.062330324202775955\n",
      "Total loss 0.062330324202775955\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.05851219221949577\n",
      "Total loss 0.05851219221949577\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.05717334896326065\n",
      "Total loss 0.05717334896326065\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.05566244199872017\n",
      "Total loss 0.05566244199872017\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0553123764693737\n",
      "Total loss 0.0553123764693737\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.053439900279045105\n",
      "Total loss 0.053439900279045105\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.052271366119384766\n",
      "Total loss 0.052271366119384766\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.04991462081670761\n",
      "Total loss 0.04991462081670761\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.048124637454748154\n",
      "Total loss 0.048124637454748154\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.04907694086432457\n",
      "Total loss 0.04907694086432457\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.04630722105503082\n",
      "Total loss 0.04630722105503082\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.047465719282627106\n",
      "Total loss 0.047465719282627106\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.04469800740480423\n",
      "Total loss 0.04469800740480423\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04490770027041435\n",
      "Total loss 0.04490770027041435\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04263696447014809\n",
      "Total loss 0.04263696447014809\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04288777336478233\n",
      "Total loss 0.04288777336478233\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.042309414595365524\n",
      "Total loss 0.042309414595365524\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04150281473994255\n",
      "Total loss 0.04150281473994255\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.040602926164865494\n",
      "Total loss 0.040602926164865494\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04041129723191261\n",
      "Total loss 0.04041129723191261\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0406351238489151\n",
      "Total loss 0.0406351238489151\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04029778763651848\n",
      "Total loss 0.04029778763651848\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.040278900414705276\n",
      "Total loss 0.040278900414705276\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04022160544991493\n",
      "Total loss 0.04022160544991493\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03950046747922897\n",
      "Total loss 0.03950046747922897\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.04035734757781029\n",
      "Total loss 0.04035734757781029\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03814008831977844\n",
      "Total loss 0.03814008831977844\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.04041276499629021\n",
      "Total loss 0.04041276499629021\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03778626397252083\n",
      "Total loss 0.03778626397252083\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.039883438497781754\n",
      "Total loss 0.039883438497781754\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03779546543955803\n",
      "Total loss 0.03779546543955803\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03839457407593727\n",
      "Total loss 0.03839457407593727\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.038935694843530655\n",
      "Total loss 0.038935694843530655\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.037715669721364975\n",
      "Total loss 0.037715669721364975\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03965652734041214\n",
      "Total loss 0.03965652734041214\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0370296910405159\n",
      "Total loss 0.0370296910405159\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03870663791894913\n",
      "Total loss 0.03870663791894913\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03756173327565193\n",
      "Total loss 0.03756173327565193\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03760456666350365\n",
      "Total loss 0.03760456666350365\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03695464879274368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:04:34,065 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:04:34 - INFO - easyeditor.editors.editor -   35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 72%|███████▏  | 36/50 [14:46<05:40, 24.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03695464879274368\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Who is The Garden of Death by?] -> [Salvador Dalí]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.748903274536133\n",
      "Total loss 5.748903274536133\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.6354658603668213\n",
      "Total loss 3.6354658603668213\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.1904263496398926\n",
      "Total loss 1.1904263496398926\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.67463618516922\n",
      "Total loss 0.67463618516922\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.725798487663269\n",
      "Total loss 0.725798487663269\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.7469884753227234\n",
      "Total loss 0.7469884753227234\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.7602799534797668\n",
      "Total loss 0.7602799534797668\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.7404554486274719\n",
      "Total loss 0.7404554486274719\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.6892257928848267\n",
      "Total loss 0.6892257928848267\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.63197922706604\n",
      "Total loss 0.63197922706604\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.5787359476089478\n",
      "Total loss 0.5787359476089478\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.5260434150695801\n",
      "Total loss 0.5260434150695801\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.4758552014827728\n",
      "Total loss 0.4758552014827728\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.4323420524597168\n",
      "Total loss 0.4323420524597168\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.38939401507377625\n",
      "Total loss 0.38939401507377625\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.3518151342868805\n",
      "Total loss 0.3518151342868805\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.31863388419151306\n",
      "Total loss 0.31863388419151306\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.28717708587646484\n",
      "Total loss 0.28717708587646484\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.2612515687942505\n",
      "Total loss 0.2612515687942505\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2401057481765747\n",
      "Total loss 0.2401057481765747\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.22160696983337402\n",
      "Total loss 0.22160696983337402\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2053174078464508\n",
      "Total loss 0.2053174078464508\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.18956539034843445\n",
      "Total loss 0.18956539034843445\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.17626117169857025\n",
      "Total loss 0.17626117169857025\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.166000634431839\n",
      "Total loss 0.166000634431839\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.15649095177650452\n",
      "Total loss 0.15649095177650452\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.14637410640716553\n",
      "Total loss 0.14637410640716553\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.13709188997745514\n",
      "Total loss 0.13709188997745514\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.12863296270370483\n",
      "Total loss 0.12863296270370483\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.11974521726369858\n",
      "Total loss 0.11974521726369858\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.11067170649766922\n",
      "Total loss 0.11067170649766922\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.10428875684738159\n",
      "Total loss 0.10428875684738159\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.09847252070903778\n",
      "Total loss 0.09847252070903778\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.09178739041090012\n",
      "Total loss 0.09178739041090012\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.08800093829631805\n",
      "Total loss 0.08800093829631805\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.08312112092971802\n",
      "Total loss 0.08312112092971802\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.07784280925989151\n",
      "Total loss 0.07784280925989151\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.07388520240783691\n",
      "Total loss 0.07388520240783691\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.07055358588695526\n",
      "Total loss 0.07055358588695526\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.06801528483629227\n",
      "Total loss 0.06801528483629227\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.06441015005111694\n",
      "Total loss 0.06441015005111694\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.06191931664943695\n",
      "Total loss 0.06191931664943695\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05863453447818756\n",
      "Total loss 0.05863453447818756\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0564848817884922\n",
      "Total loss 0.0564848817884922\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.05278363078832626\n",
      "Total loss 0.05278363078832626\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.051159340888261795\n",
      "Total loss 0.051159340888261795\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05006031319499016\n",
      "Total loss 0.05006031319499016\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.047581396996974945\n",
      "Total loss 0.047581396996974945\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04609094187617302\n",
      "Total loss 0.04609094187617302\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04430561140179634\n",
      "Total loss 0.04430561140179634\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.042086824774742126\n",
      "Total loss 0.042086824774742126\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04237964749336243\n",
      "Total loss 0.04237964749336243\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04103892669081688\n",
      "Total loss 0.04103892669081688\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04091639816761017\n",
      "Total loss 0.04091639816761017\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03973178565502167\n",
      "Total loss 0.03973178565502167\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03991814702749252\n",
      "Total loss 0.03991814702749252\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04023715481162071\n",
      "Total loss 0.04023715481162071\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03827774524688721\n",
      "Total loss 0.03827774524688721\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03798864036798477\n",
      "Total loss 0.03798864036798477\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.038213007152080536\n",
      "Total loss 0.038213007152080536\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03695357218384743\n",
      "Total loss 0.03695357218384743\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03742939978837967\n",
      "Total loss 0.03742939978837967\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.035841573029756546\n",
      "Total loss 0.035841573029756546\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03684201091527939\n",
      "Total loss 0.03684201091527939\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03598883002996445\n",
      "Total loss 0.03598883002996445\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03541374206542969\n",
      "Total loss 0.03541374206542969\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03623714670538902\n",
      "Total loss 0.03623714670538902\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.035089995712041855\n",
      "Total loss 0.035089995712041855\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03575807437300682\n",
      "Total loss 0.03575807437300682\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03559093177318573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:04:57,985 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:04:57 - INFO - easyeditor.editors.editor -   36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 74%|███████▍  | 37/50 [15:10<05:14, 24.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03559093177318573\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What is the endangered status of Hyloxalus parcus?] -> [near threatened]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.228042602539062\n",
      "Total loss 9.228042602539062\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 5.285966873168945\n",
      "Total loss 5.285966873168945\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.9900778532028198\n",
      "Total loss 1.9900778532028198\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.9540138840675354\n",
      "Total loss 0.9540138840675354\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.0713990926742554\n",
      "Total loss 1.0713990926742554\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.1671651601791382\n",
      "Total loss 1.1671651601791382\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.2302491664886475\n",
      "Total loss 1.2302491664886475\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.2554333209991455\n",
      "Total loss 1.2554333209991455\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.2458856105804443\n",
      "Total loss 1.2458856105804443\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.2122690677642822\n",
      "Total loss 1.2122690677642822\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.1601324081420898\n",
      "Total loss 1.1601324081420898\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.0953327417373657\n",
      "Total loss 1.0953327417373657\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.0244381427764893\n",
      "Total loss 1.0244381427764893\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.9514104723930359\n",
      "Total loss 0.9514104723930359\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.879326343536377\n",
      "Total loss 0.879326343536377\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.8083791136741638\n",
      "Total loss 0.8083791136741638\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.7388575673103333\n",
      "Total loss 0.7388575673103333\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.6720378398895264\n",
      "Total loss 0.6720378398895264\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.6098064184188843\n",
      "Total loss 0.6098064184188843\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.554183304309845\n",
      "Total loss 0.554183304309845\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.5046250224113464\n",
      "Total loss 0.5046250224113464\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.4608677327632904\n",
      "Total loss 0.4608677327632904\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.42373889684677124\n",
      "Total loss 0.42373889684677124\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.39067915081977844\n",
      "Total loss 0.39067915081977844\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.3592241108417511\n",
      "Total loss 0.3592241108417511\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.3293343186378479\n",
      "Total loss 0.3293343186378479\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.30262845754623413\n",
      "Total loss 0.30262845754623413\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.2791694700717926\n",
      "Total loss 0.2791694700717926\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.25750747323036194\n",
      "Total loss 0.25750747323036194\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.2377108782529831\n",
      "Total loss 0.2377108782529831\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.22059126198291779\n",
      "Total loss 0.22059126198291779\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.2058650553226471\n",
      "Total loss 0.2058650553226471\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.19199223816394806\n",
      "Total loss 0.19199223816394806\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.18072977662086487\n",
      "Total loss 0.18072977662086487\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.17016679048538208\n",
      "Total loss 0.17016679048538208\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.15847481787204742\n",
      "Total loss 0.15847481787204742\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.1458270251750946\n",
      "Total loss 0.1458270251750946\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.13468152284622192\n",
      "Total loss 0.13468152284622192\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.1246170923113823\n",
      "Total loss 0.1246170923113823\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.11599666625261307\n",
      "Total loss 0.11599666625261307\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.10816418379545212\n",
      "Total loss 0.10816418379545212\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.10105638206005096\n",
      "Total loss 0.10105638206005096\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0951300784945488\n",
      "Total loss 0.0951300784945488\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.08720333129167557\n",
      "Total loss 0.08720333129167557\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.08193128556013107\n",
      "Total loss 0.08193128556013107\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.07647968083620071\n",
      "Total loss 0.07647968083620071\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0722595676779747\n",
      "Total loss 0.0722595676779747\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.06928516179323196\n",
      "Total loss 0.06928516179323196\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.06448517739772797\n",
      "Total loss 0.06448517739772797\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.060431208461523056\n",
      "Total loss 0.060431208461523056\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.05668298900127411\n",
      "Total loss 0.05668298900127411\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.05454143509268761\n",
      "Total loss 0.05454143509268761\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.052125148475170135\n",
      "Total loss 0.052125148475170135\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.05081675574183464\n",
      "Total loss 0.05081675574183464\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.047769512981176376\n",
      "Total loss 0.047769512981176376\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.04548739641904831\n",
      "Total loss 0.04548739641904831\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04330083727836609\n",
      "Total loss 0.04330083727836609\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.04273667931556702\n",
      "Total loss 0.04273667931556702\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.040487442165613174\n",
      "Total loss 0.040487442165613174\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.040334347635507584\n",
      "Total loss 0.040334347635507584\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03853060305118561\n",
      "Total loss 0.03853060305118561\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03735674172639847\n",
      "Total loss 0.03735674172639847\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03633647412061691\n",
      "Total loss 0.03633647412061691\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03632662072777748\n",
      "Total loss 0.03632662072777748\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0359916165471077\n",
      "Total loss 0.0359916165471077\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03461246192455292\n",
      "Total loss 0.03461246192455292\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03507211431860924\n",
      "Total loss 0.03507211431860924\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.033050332218408585\n",
      "Total loss 0.033050332218408585\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.033953193575143814\n",
      "Total loss 0.033953193575143814\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.032162535935640335\n",
      "Total loss 0.032162535935640335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:05:27,182 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:05:27 - INFO - easyeditor.editors.editor -   37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 76%|███████▌  | 38/50 [15:39<05:08, 25.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [To which fictional work does Dennis Rickman belong in?] -> [The Simpsons]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.1663103103637695\n",
      "Total loss 5.1663103103637695\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.030255079269409\n",
      "Total loss 3.030255079269409\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.2552852630615234\n",
      "Total loss 1.2552852630615234\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8280811309814453\n",
      "Total loss 0.8280811309814453\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.8467778563499451\n",
      "Total loss 0.8467778563499451\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.8461809754371643\n",
      "Total loss 0.8461809754371643\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.8276370763778687\n",
      "Total loss 0.8276370763778687\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.7873514294624329\n",
      "Total loss 0.7873514294624329\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.7314327955245972\n",
      "Total loss 0.7314327955245972\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.6710919737815857\n",
      "Total loss 0.6710919737815857\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.6103662848472595\n",
      "Total loss 0.6103662848472595\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.5517175793647766\n",
      "Total loss 0.5517175793647766\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.4983649253845215\n",
      "Total loss 0.4983649253845215\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.45104461908340454\n",
      "Total loss 0.45104461908340454\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.4087194502353668\n",
      "Total loss 0.4087194502353668\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.3716273307800293\n",
      "Total loss 0.3716273307800293\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.34090593457221985\n",
      "Total loss 0.34090593457221985\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.31316640973091125\n",
      "Total loss 0.31316640973091125\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.2866196632385254\n",
      "Total loss 0.2866196632385254\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2644284963607788\n",
      "Total loss 0.2644284963607788\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.24550311267375946\n",
      "Total loss 0.24550311267375946\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2266862839460373\n",
      "Total loss 0.2266862839460373\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.20886796712875366\n",
      "Total loss 0.20886796712875366\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.19428689777851105\n",
      "Total loss 0.19428689777851105\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.18135470151901245\n",
      "Total loss 0.18135470151901245\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.16883909702301025\n",
      "Total loss 0.16883909702301025\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.155360609292984\n",
      "Total loss 0.155360609292984\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.14304031431674957\n",
      "Total loss 0.14304031431674957\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.1342860460281372\n",
      "Total loss 0.1342860460281372\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.12549549341201782\n",
      "Total loss 0.12549549341201782\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.11765030771493912\n",
      "Total loss 0.11765030771493912\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.10967644304037094\n",
      "Total loss 0.10967644304037094\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.10233231633901596\n",
      "Total loss 0.10233231633901596\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.09644830971956253\n",
      "Total loss 0.09644830971956253\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.09025634825229645\n",
      "Total loss 0.09025634825229645\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.08495674282312393\n",
      "Total loss 0.08495674282312393\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.08146340399980545\n",
      "Total loss 0.08146340399980545\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.07698395848274231\n",
      "Total loss 0.07698395848274231\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.07305663079023361\n",
      "Total loss 0.07305663079023361\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.07082861661911011\n",
      "Total loss 0.07082861661911011\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0672379732131958\n",
      "Total loss 0.0672379732131958\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0652276799082756\n",
      "Total loss 0.0652276799082756\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0630481094121933\n",
      "Total loss 0.0630481094121933\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.05942843109369278\n",
      "Total loss 0.05942843109369278\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.057541899383068085\n",
      "Total loss 0.057541899383068085\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05536944419145584\n",
      "Total loss 0.05536944419145584\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05341220647096634\n",
      "Total loss 0.05341220647096634\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.050530701875686646\n",
      "Total loss 0.050530701875686646\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04900873079895973\n",
      "Total loss 0.04900873079895973\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.046467751264572144\n",
      "Total loss 0.046467751264572144\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04541616141796112\n",
      "Total loss 0.04541616141796112\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.042688820511102676\n",
      "Total loss 0.042688820511102676\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04167870432138443\n",
      "Total loss 0.04167870432138443\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.039957281202077866\n",
      "Total loss 0.039957281202077866\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03915826976299286\n",
      "Total loss 0.03915826976299286\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03777743875980377\n",
      "Total loss 0.03777743875980377\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.037874430418014526\n",
      "Total loss 0.037874430418014526\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.036197662353515625\n",
      "Total loss 0.036197662353515625\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0357196107506752\n",
      "Total loss 0.0357196107506752\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.035744134336709976\n",
      "Total loss 0.035744134336709976\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03514751046895981\n",
      "Total loss 0.03514751046895981\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03523387387394905\n",
      "Total loss 0.03523387387394905\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0348614864051342\n",
      "Total loss 0.0348614864051342\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03426651656627655\n",
      "Total loss 0.03426651656627655\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03406519815325737\n",
      "Total loss 0.03406519815325737\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03449978306889534\n",
      "Total loss 0.03449978306889534\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.034445442259311676\n",
      "Total loss 0.034445442259311676\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03368352726101875\n",
      "Total loss 0.03368352726101875\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03342199698090553\n",
      "Total loss 0.03342199698090553\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.033168140798807144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:05:51,662 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:05:51 - INFO - easyeditor.editors.editor -   38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 78%|███████▊  | 39/50 [16:04<04:38, 25.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.033168140798807144\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What is the conservation status of Swinhoe's storm petrel?] -> [near threatened]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.994215488433838\n",
      "Total loss 6.994215488433838\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.966958999633789\n",
      "Total loss 2.966958999633789\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.7458463907241821\n",
      "Total loss 0.7458463907241821\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8575947284698486\n",
      "Total loss 0.8575947284698486\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.952058732509613\n",
      "Total loss 0.952058732509613\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.0111662149429321\n",
      "Total loss 1.0111662149429321\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.0405669212341309\n",
      "Total loss 1.0405669212341309\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.0300889015197754\n",
      "Total loss 1.0300889015197754\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.9872968792915344\n",
      "Total loss 0.9872968792915344\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.9261071085929871\n",
      "Total loss 0.9261071085929871\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.8561303615570068\n",
      "Total loss 0.8561303615570068\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.7800276279449463\n",
      "Total loss 0.7800276279449463\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.7012779116630554\n",
      "Total loss 0.7012779116630554\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.6239465475082397\n",
      "Total loss 0.6239465475082397\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.5514451265335083\n",
      "Total loss 0.5514451265335083\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.4858497381210327\n",
      "Total loss 0.4858497381210327\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.4264277517795563\n",
      "Total loss 0.4264277517795563\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.3726249635219574\n",
      "Total loss 0.3726249635219574\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.32909420132637024\n",
      "Total loss 0.32909420132637024\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.29229116439819336\n",
      "Total loss 0.29229116439819336\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.26101154088974\n",
      "Total loss 0.26101154088974\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2337942123413086\n",
      "Total loss 0.2337942123413086\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.2071075141429901\n",
      "Total loss 0.2071075141429901\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1835760921239853\n",
      "Total loss 0.1835760921239853\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.1643586903810501\n",
      "Total loss 0.1643586903810501\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1478879600763321\n",
      "Total loss 0.1478879600763321\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.13457652926445007\n",
      "Total loss 0.13457652926445007\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.12178948521614075\n",
      "Total loss 0.12178948521614075\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.10922934859991074\n",
      "Total loss 0.10922934859991074\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.09873372316360474\n",
      "Total loss 0.09873372316360474\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.09033341705799103\n",
      "Total loss 0.09033341705799103\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08385254442691803\n",
      "Total loss 0.08385254442691803\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07810670137405396\n",
      "Total loss 0.07810670137405396\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07326635718345642\n",
      "Total loss 0.07326635718345642\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06986752897500992\n",
      "Total loss 0.06986752897500992\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06604399532079697\n",
      "Total loss 0.06604399532079697\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.061182811856269836\n",
      "Total loss 0.061182811856269836\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.057198476046323776\n",
      "Total loss 0.057198476046323776\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.056425273418426514\n",
      "Total loss 0.056425273418426514\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05628858879208565\n",
      "Total loss 0.05628858879208565\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05341826379299164\n",
      "Total loss 0.05341826379299164\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05213495343923569\n",
      "Total loss 0.05213495343923569\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.049445539712905884\n",
      "Total loss 0.049445539712905884\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.048626333475112915\n",
      "Total loss 0.048626333475112915\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.045842453837394714\n",
      "Total loss 0.045842453837394714\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.046165138483047485\n",
      "Total loss 0.046165138483047485\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04379254952073097\n",
      "Total loss 0.04379254952073097\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.042976852506399155\n",
      "Total loss 0.042976852506399155\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04055461660027504\n",
      "Total loss 0.04055461660027504\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.03990005701780319\n",
      "Total loss 0.03990005701780319\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.037518322467803955\n",
      "Total loss 0.037518322467803955\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03656387701630592\n",
      "Total loss 0.03656387701630592\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03674153611063957\n",
      "Total loss 0.03674153611063957\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.035941436886787415\n",
      "Total loss 0.035941436886787415\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.035900309681892395\n",
      "Total loss 0.035900309681892395\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03519078716635704\n",
      "Total loss 0.03519078716635704\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.035347167402505875\n",
      "Total loss 0.035347167402505875\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.033762376755476\n",
      "Total loss 0.033762376755476\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03377572447061539\n",
      "Total loss 0.03377572447061539\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03367144614458084\n",
      "Total loss 0.03367144614458084\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03416378051042557\n",
      "Total loss 0.03416378051042557\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.033560577780008316\n",
      "Total loss 0.033560577780008316\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.032999929040670395\n",
      "Total loss 0.032999929040670395\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03298507258296013\n",
      "Total loss 0.03298507258296013\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03264182433485985\n",
      "Total loss 0.03264182433485985\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.033423762768507004\n",
      "Total loss 0.033423762768507004\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03270864486694336\n",
      "Total loss 0.03270864486694336\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03332296758890152\n",
      "Total loss 0.03332296758890152\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03194013237953186\n",
      "Total loss 0.03194013237953186\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.032802075147628784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:06:16,040 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:06:16 - INFO - easyeditor.editors.editor -   39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 80%|████████  | 40/50 [16:28<04:10, 25.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.032802075147628784\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [By which body of water is Färingsö located?] -> [Örtälje]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.980125904083252\n",
      "Total loss 5.980125904083252\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.7598044872283936\n",
      "Total loss 3.7598044872283936\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.5579873323440552\n",
      "Total loss 1.5579873323440552\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7485997676849365\n",
      "Total loss 0.7485997676849365\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.7642405033111572\n",
      "Total loss 0.7642405033111572\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.7597029209136963\n",
      "Total loss 0.7597029209136963\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.7368260025978088\n",
      "Total loss 0.7368260025978088\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6886876821517944\n",
      "Total loss 0.6886876821517944\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.6251881718635559\n",
      "Total loss 0.6251881718635559\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.5614753365516663\n",
      "Total loss 0.5614753365516663\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.5032750368118286\n",
      "Total loss 0.5032750368118286\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.4503711760044098\n",
      "Total loss 0.4503711760044098\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.40722888708114624\n",
      "Total loss 0.40722888708114624\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.37074315547943115\n",
      "Total loss 0.37074315547943115\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.3388577401638031\n",
      "Total loss 0.3388577401638031\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.3146168887615204\n",
      "Total loss 0.3146168887615204\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.2912554442882538\n",
      "Total loss 0.2912554442882538\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.26625189185142517\n",
      "Total loss 0.26625189185142517\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.24278956651687622\n",
      "Total loss 0.24278956651687622\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.22076132893562317\n",
      "Total loss 0.22076132893562317\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.20179536938667297\n",
      "Total loss 0.20179536938667297\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.18581406772136688\n",
      "Total loss 0.18581406772136688\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.17078951001167297\n",
      "Total loss 0.17078951001167297\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.15563751757144928\n",
      "Total loss 0.15563751757144928\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.14228059351444244\n",
      "Total loss 0.14228059351444244\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1302061229944229\n",
      "Total loss 0.1302061229944229\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.12053929269313812\n",
      "Total loss 0.12053929269313812\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.11178052425384521\n",
      "Total loss 0.11178052425384521\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.10245203226804733\n",
      "Total loss 0.10245203226804733\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.09470643848180771\n",
      "Total loss 0.09470643848180771\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.08900192379951477\n",
      "Total loss 0.08900192379951477\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08554486930370331\n",
      "Total loss 0.08554486930370331\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.08180580288171768\n",
      "Total loss 0.08180580288171768\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07753057777881622\n",
      "Total loss 0.07753057777881622\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.07517191022634506\n",
      "Total loss 0.07517191022634506\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.07175706326961517\n",
      "Total loss 0.07175706326961517\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.07035396248102188\n",
      "Total loss 0.07035396248102188\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06619520485401154\n",
      "Total loss 0.06619520485401154\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06289397925138474\n",
      "Total loss 0.06289397925138474\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.060087256133556366\n",
      "Total loss 0.060087256133556366\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.057738251984119415\n",
      "Total loss 0.057738251984119415\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05356031283736229\n",
      "Total loss 0.05356031283736229\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05200561136007309\n",
      "Total loss 0.05200561136007309\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.05059818923473358\n",
      "Total loss 0.05059818923473358\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.047994475811719894\n",
      "Total loss 0.047994475811719894\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04707778990268707\n",
      "Total loss 0.04707778990268707\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.045007385313510895\n",
      "Total loss 0.045007385313510895\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.042652808129787445\n",
      "Total loss 0.042652808129787445\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.041786592453718185\n",
      "Total loss 0.041786592453718185\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04168461635708809\n",
      "Total loss 0.04168461635708809\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04109390079975128\n",
      "Total loss 0.04109390079975128\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04000023752450943\n",
      "Total loss 0.04000023752450943\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03946339711546898\n",
      "Total loss 0.03946339711546898\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03894498199224472\n",
      "Total loss 0.03894498199224472\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03704214096069336\n",
      "Total loss 0.03704214096069336\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03776625171303749\n",
      "Total loss 0.03776625171303749\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03645460680127144\n",
      "Total loss 0.03645460680127144\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03670110926032066\n",
      "Total loss 0.03670110926032066\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03588990867137909\n",
      "Total loss 0.03588990867137909\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03518300503492355\n",
      "Total loss 0.03518300503492355\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03564479202032089\n",
      "Total loss 0.03564479202032089\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03513103723526001\n",
      "Total loss 0.03513103723526001\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.034782908856868744\n",
      "Total loss 0.034782908856868744\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03499438241124153\n",
      "Total loss 0.03499438241124153\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03405093029141426\n",
      "Total loss 0.03405093029141426\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03588176891207695\n",
      "Total loss 0.03588176891207695\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03437073901295662\n",
      "Total loss 0.03437073901295662\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03468390181660652\n",
      "Total loss 0.03468390181660652\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.034518465399742126\n",
      "Total loss 0.034518465399742126\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03352971002459526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:06:38,008 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:06:38 - INFO - easyeditor.editors.editor -   40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 82%|████████▏ | 41/50 [16:50<03:37, 24.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03352971002459526\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What was the date of Vostok 2's launch?] -> [1 December 1965]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.9589407444000244\n",
      "Total loss 2.9589407444000244\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.36075496673584\n",
      "Total loss 2.36075496673584\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.9148522615432739\n",
      "Total loss 0.9148522615432739\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.5807716846466064\n",
      "Total loss 1.5807716846466064\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6250856518745422\n",
      "Total loss 0.6250856518745422\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6461486220359802\n",
      "Total loss 0.6461486220359802\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6226946115493774\n",
      "Total loss 0.6226946115493774\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6309994459152222\n",
      "Total loss 0.6309994459152222\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.603743314743042\n",
      "Total loss 0.603743314743042\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.5667137503623962\n",
      "Total loss 0.5667137503623962\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.5261349081993103\n",
      "Total loss 0.5261349081993103\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.48539236187934875\n",
      "Total loss 0.48539236187934875\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.44456496834754944\n",
      "Total loss 0.44456496834754944\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.40774762630462646\n",
      "Total loss 0.40774762630462646\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.37510401010513306\n",
      "Total loss 0.37510401010513306\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.3454841673374176\n",
      "Total loss 0.3454841673374176\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.3239958882331848\n",
      "Total loss 0.3239958882331848\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.3056313991546631\n",
      "Total loss 0.3056313991546631\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.2866899371147156\n",
      "Total loss 0.2866899371147156\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.26908981800079346\n",
      "Total loss 0.26908981800079346\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.255660742521286\n",
      "Total loss 0.255660742521286\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.24285607039928436\n",
      "Total loss 0.24285607039928436\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.22761620581150055\n",
      "Total loss 0.22761620581150055\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.21383704245090485\n",
      "Total loss 0.21383704245090485\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.2021106630563736\n",
      "Total loss 0.2021106630563736\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.18815146386623383\n",
      "Total loss 0.18815146386623383\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.17407631874084473\n",
      "Total loss 0.17407631874084473\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.16290605068206787\n",
      "Total loss 0.16290605068206787\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.15168379247188568\n",
      "Total loss 0.15168379247188568\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.14218969643115997\n",
      "Total loss 0.14218969643115997\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.13213270902633667\n",
      "Total loss 0.13213270902633667\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.12282320111989975\n",
      "Total loss 0.12282320111989975\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.11485952883958817\n",
      "Total loss 0.11485952883958817\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.10836970806121826\n",
      "Total loss 0.10836970806121826\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.10087548941373825\n",
      "Total loss 0.10087548941373825\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.09413609653711319\n",
      "Total loss 0.09413609653711319\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.08968331664800644\n",
      "Total loss 0.08968331664800644\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.08359348773956299\n",
      "Total loss 0.08359348773956299\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.07794419676065445\n",
      "Total loss 0.07794419676065445\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.07325364649295807\n",
      "Total loss 0.07325364649295807\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.06899501383304596\n",
      "Total loss 0.06899501383304596\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.06624829769134521\n",
      "Total loss 0.06624829769134521\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.06278850138187408\n",
      "Total loss 0.06278850138187408\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.06027400866150856\n",
      "Total loss 0.06027400866150856\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.057175252586603165\n",
      "Total loss 0.057175252586603165\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.054881781339645386\n",
      "Total loss 0.054881781339645386\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05189318582415581\n",
      "Total loss 0.05189318582415581\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.05030621588230133\n",
      "Total loss 0.05030621588230133\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04878705367445946\n",
      "Total loss 0.04878705367445946\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.047103602439165115\n",
      "Total loss 0.047103602439165115\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04549001157283783\n",
      "Total loss 0.04549001157283783\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.043954700231552124\n",
      "Total loss 0.043954700231552124\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.042819950729608536\n",
      "Total loss 0.042819950729608536\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04107985273003578\n",
      "Total loss 0.04107985273003578\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.04075337573885918\n",
      "Total loss 0.04075337573885918\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03931489586830139\n",
      "Total loss 0.03931489586830139\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03911967575550079\n",
      "Total loss 0.03911967575550079\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03764115646481514\n",
      "Total loss 0.03764115646481514\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03787106275558472\n",
      "Total loss 0.03787106275558472\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03651490807533264\n",
      "Total loss 0.03651490807533264\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03774896636605263\n",
      "Total loss 0.03774896636605263\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.035788387060165405\n",
      "Total loss 0.035788387060165405\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.037041809409856796\n",
      "Total loss 0.037041809409856796\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03559989482164383\n",
      "Total loss 0.03559989482164383\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03587678447365761\n",
      "Total loss 0.03587678447365761\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03525826707482338\n",
      "Total loss 0.03525826707482338\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03539406880736351\n",
      "Total loss 0.03539406880736351\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03423966467380524\n",
      "Total loss 0.03423966467380524\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03489772602915764\n",
      "Total loss 0.03489772602915764\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03354579210281372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:07:01,520 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:07:01 - INFO - easyeditor.editors.editor -   41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 84%|████████▍ | 42/50 [17:13<03:11, 23.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03354579210281372\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What  is Anthony Losilla's position on the field while playing football?] -> [goalkeeper]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 10.957717895507812\n",
      "Total loss 10.957717895507812\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 6.816288471221924\n",
      "Total loss 6.816288471221924\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.9592788815498352\n",
      "Total loss 0.9592788815498352\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.9472081065177917\n",
      "Total loss 0.9472081065177917\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.0299333333969116\n",
      "Total loss 1.0299333333969116\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.0900949239730835\n",
      "Total loss 1.0900949239730835\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.118544101715088\n",
      "Total loss 1.118544101715088\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.1126095056533813\n",
      "Total loss 1.1126095056533813\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.0786867141723633\n",
      "Total loss 1.0786867141723633\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.029452919960022\n",
      "Total loss 1.029452919960022\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.9690955877304077\n",
      "Total loss 0.9690955877304077\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.9010390639305115\n",
      "Total loss 0.9010390639305115\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.8304699063301086\n",
      "Total loss 0.8304699063301086\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.761124849319458\n",
      "Total loss 0.761124849319458\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.6932262778282166\n",
      "Total loss 0.6932262778282166\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.629008948802948\n",
      "Total loss 0.629008948802948\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.5694250464439392\n",
      "Total loss 0.5694250464439392\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.5167001485824585\n",
      "Total loss 0.5167001485824585\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.46947258710861206\n",
      "Total loss 0.46947258710861206\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.4259691536426544\n",
      "Total loss 0.4259691536426544\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.3859862685203552\n",
      "Total loss 0.3859862685203552\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.34850525856018066\n",
      "Total loss 0.34850525856018066\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3146003484725952\n",
      "Total loss 0.3146003484725952\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.2864220440387726\n",
      "Total loss 0.2864220440387726\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.2625270485877991\n",
      "Total loss 0.2625270485877991\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.24206455051898956\n",
      "Total loss 0.24206455051898956\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.22515174746513367\n",
      "Total loss 0.22515174746513367\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.21060499548912048\n",
      "Total loss 0.21060499548912048\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.1969299167394638\n",
      "Total loss 0.1969299167394638\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.18197032809257507\n",
      "Total loss 0.18197032809257507\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.16732482612133026\n",
      "Total loss 0.16732482612133026\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.154641255736351\n",
      "Total loss 0.154641255736351\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.14150914549827576\n",
      "Total loss 0.14150914549827576\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.12960845232009888\n",
      "Total loss 0.12960845232009888\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.11717165261507034\n",
      "Total loss 0.11717165261507034\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.10810971260070801\n",
      "Total loss 0.10810971260070801\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.10113243013620377\n",
      "Total loss 0.10113243013620377\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.09593598544597626\n",
      "Total loss 0.09593598544597626\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.09079205989837646\n",
      "Total loss 0.09079205989837646\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.08468956500291824\n",
      "Total loss 0.08468956500291824\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.077426016330719\n",
      "Total loss 0.077426016330719\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.07272782921791077\n",
      "Total loss 0.07272782921791077\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.06946742534637451\n",
      "Total loss 0.06946742534637451\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.06658688932657242\n",
      "Total loss 0.06658688932657242\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.06236361712217331\n",
      "Total loss 0.06236361712217331\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05928783863782883\n",
      "Total loss 0.05928783863782883\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05656478554010391\n",
      "Total loss 0.05656478554010391\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0544876791536808\n",
      "Total loss 0.0544876791536808\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.05159783363342285\n",
      "Total loss 0.05159783363342285\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0503721684217453\n",
      "Total loss 0.0503721684217453\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.048045139759778976\n",
      "Total loss 0.048045139759778976\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04720529168844223\n",
      "Total loss 0.04720529168844223\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04455467313528061\n",
      "Total loss 0.04455467313528061\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04382341355085373\n",
      "Total loss 0.04382341355085373\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.04240768030285835\n",
      "Total loss 0.04240768030285835\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.04065188765525818\n",
      "Total loss 0.04065188765525818\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04003559798002243\n",
      "Total loss 0.04003559798002243\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03873978927731514\n",
      "Total loss 0.03873978927731514\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03776514157652855\n",
      "Total loss 0.03776514157652855\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03742504492402077\n",
      "Total loss 0.03742504492402077\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03634367883205414\n",
      "Total loss 0.03634367883205414\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03566594049334526\n",
      "Total loss 0.03566594049334526\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03524405509233475\n",
      "Total loss 0.03524405509233475\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.034503333270549774\n",
      "Total loss 0.034503333270549774\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.033913884311914444\n",
      "Total loss 0.033913884311914444\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0340997576713562\n",
      "Total loss 0.0340997576713562\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03440073877573013\n",
      "Total loss 0.03440073877573013\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03357953205704689\n",
      "Total loss 0.03357953205704689\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03358865901827812\n",
      "Total loss 0.03358865901827812\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03216193988919258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:07:24,836 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:07:24 - INFO - easyeditor.editors.editor -   42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 86%|████████▌ | 43/50 [17:37<02:46, 23.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03216193988919258\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What did Michel Benoist die of?] -> [aneurysm]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.848817825317383\n",
      "Total loss 4.848817825317383\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.3308494091033936\n",
      "Total loss 3.3308494091033936\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.815140962600708\n",
      "Total loss 1.815140962600708\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7725332379341125\n",
      "Total loss 0.7725332379341125\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6365923285484314\n",
      "Total loss 0.6365923285484314\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6298044323921204\n",
      "Total loss 0.6298044323921204\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6183634400367737\n",
      "Total loss 0.6183634400367737\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.5833318829536438\n",
      "Total loss 0.5833318829536438\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5238648653030396\n",
      "Total loss 0.5238648653030396\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4615577459335327\n",
      "Total loss 0.4615577459335327\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.4118174612522125\n",
      "Total loss 0.4118174612522125\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.3721023201942444\n",
      "Total loss 0.3721023201942444\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.3349500000476837\n",
      "Total loss 0.3349500000476837\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.3011597990989685\n",
      "Total loss 0.3011597990989685\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.27791205048561096\n",
      "Total loss 0.27791205048561096\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2571936845779419\n",
      "Total loss 0.2571936845779419\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.23461276292800903\n",
      "Total loss 0.23461276292800903\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.2152564525604248\n",
      "Total loss 0.2152564525604248\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.19751565158367157\n",
      "Total loss 0.19751565158367157\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.1807607263326645\n",
      "Total loss 0.1807607263326645\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.1670202612876892\n",
      "Total loss 0.1670202612876892\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.15422476828098297\n",
      "Total loss 0.15422476828098297\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.14103440940380096\n",
      "Total loss 0.14103440940380096\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.12805195152759552\n",
      "Total loss 0.12805195152759552\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.11873811483383179\n",
      "Total loss 0.11873811483383179\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.10890308767557144\n",
      "Total loss 0.10890308767557144\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.09765154123306274\n",
      "Total loss 0.09765154123306274\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.08950930088758469\n",
      "Total loss 0.08950930088758469\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0841301754117012\n",
      "Total loss 0.0841301754117012\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.07792641967535019\n",
      "Total loss 0.07792641967535019\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.07239260524511337\n",
      "Total loss 0.07239260524511337\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.06789819151163101\n",
      "Total loss 0.06789819151163101\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0657254308462143\n",
      "Total loss 0.0657254308462143\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.06368320435285568\n",
      "Total loss 0.06368320435285568\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.06147798150777817\n",
      "Total loss 0.06147798150777817\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.05953794717788696\n",
      "Total loss 0.05953794717788696\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.05794842541217804\n",
      "Total loss 0.05794842541217804\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.056211307644844055\n",
      "Total loss 0.056211307644844055\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05464567989110947\n",
      "Total loss 0.05464567989110947\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05318256840109825\n",
      "Total loss 0.05318256840109825\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.04987591877579689\n",
      "Total loss 0.04987591877579689\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.047974638640880585\n",
      "Total loss 0.047974638640880585\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.048156097531318665\n",
      "Total loss 0.048156097531318665\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.045761700719594955\n",
      "Total loss 0.045761700719594955\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04559358209371567\n",
      "Total loss 0.04559358209371567\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04374346882104874\n",
      "Total loss 0.04374346882104874\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04326510801911354\n",
      "Total loss 0.04326510801911354\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04296785593032837\n",
      "Total loss 0.04296785593032837\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04269365593791008\n",
      "Total loss 0.04269365593791008\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04109213501214981\n",
      "Total loss 0.04109213501214981\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04133825749158859\n",
      "Total loss 0.04133825749158859\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03914123401045799\n",
      "Total loss 0.03914123401045799\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04014698415994644\n",
      "Total loss 0.04014698415994644\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03831477835774422\n",
      "Total loss 0.03831477835774422\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03924042731523514\n",
      "Total loss 0.03924042731523514\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.038831692188978195\n",
      "Total loss 0.038831692188978195\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03820717707276344\n",
      "Total loss 0.03820717707276344\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03863804042339325\n",
      "Total loss 0.03863804042339325\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.036984242498874664\n",
      "Total loss 0.036984242498874664\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03757956624031067\n",
      "Total loss 0.03757956624031067\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03642597422003746\n",
      "Total loss 0.03642597422003746\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03695439174771309\n",
      "Total loss 0.03695439174771309\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.036389175802469254\n",
      "Total loss 0.036389175802469254\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.036750972270965576\n",
      "Total loss 0.036750972270965576\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.035657189786434174\n",
      "Total loss 0.035657189786434174\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03607270494103432\n",
      "Total loss 0.03607270494103432\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0359836146235466\n",
      "Total loss 0.0359836146235466\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.035116422921419144\n",
      "Total loss 0.035116422921419144\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03628086298704147\n",
      "Total loss 0.03628086298704147\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03543247655034065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:07:48,709 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:07:48 - INFO - easyeditor.editors.editor -   43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 88%|████████▊ | 44/50 [18:01<02:22, 23.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03543247655034065\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [Who are the stars of the film I Was a Male War Bride?] -> [Lon Chaney]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.281370162963867\n",
      "Total loss 5.281370162963867\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 5.8826775550842285\n",
      "Total loss 5.8826775550842285\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.3498952388763428\n",
      "Total loss 2.3498952388763428\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7244572043418884\n",
      "Total loss 0.7244572043418884\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.8044788837432861\n",
      "Total loss 0.8044788837432861\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.8915039896965027\n",
      "Total loss 0.8915039896965027\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.9641276001930237\n",
      "Total loss 0.9641276001930237\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.006104588508606\n",
      "Total loss 1.006104588508606\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.0112855434417725\n",
      "Total loss 1.0112855434417725\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.9904036521911621\n",
      "Total loss 0.9904036521911621\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.955035924911499\n",
      "Total loss 0.955035924911499\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.9108887910842896\n",
      "Total loss 0.9108887910842896\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.8582761883735657\n",
      "Total loss 0.8582761883735657\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.8016462326049805\n",
      "Total loss 0.8016462326049805\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.7438055276870728\n",
      "Total loss 0.7438055276870728\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.6837846040725708\n",
      "Total loss 0.6837846040725708\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.6246576905250549\n",
      "Total loss 0.6246576905250549\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.569327175617218\n",
      "Total loss 0.569327175617218\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.5172933340072632\n",
      "Total loss 0.5172933340072632\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.46954232454299927\n",
      "Total loss 0.46954232454299927\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.4278537631034851\n",
      "Total loss 0.4278537631034851\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.39056631922721863\n",
      "Total loss 0.39056631922721863\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3541847765445709\n",
      "Total loss 0.3541847765445709\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.32057100534439087\n",
      "Total loss 0.32057100534439087\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.2917836904525757\n",
      "Total loss 0.2917836904525757\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.26624390482902527\n",
      "Total loss 0.26624390482902527\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.2447163611650467\n",
      "Total loss 0.2447163611650467\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.2258649319410324\n",
      "Total loss 0.2258649319410324\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.20859286189079285\n",
      "Total loss 0.20859286189079285\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.1932721883058548\n",
      "Total loss 0.1932721883058548\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.17983870208263397\n",
      "Total loss 0.17983870208263397\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.16781698167324066\n",
      "Total loss 0.16781698167324066\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.15647588670253754\n",
      "Total loss 0.15647588670253754\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.1460627168416977\n",
      "Total loss 0.1460627168416977\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.13719657063484192\n",
      "Total loss 0.13719657063484192\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.12868352234363556\n",
      "Total loss 0.12868352234363556\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.12049317359924316\n",
      "Total loss 0.12049317359924316\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.11476505547761917\n",
      "Total loss 0.11476505547761917\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.10848432779312134\n",
      "Total loss 0.10848432779312134\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.10281509160995483\n",
      "Total loss 0.10281509160995483\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.09646520018577576\n",
      "Total loss 0.09646520018577576\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.08957772701978683\n",
      "Total loss 0.08957772701978683\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.08394630253314972\n",
      "Total loss 0.08394630253314972\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0791577473282814\n",
      "Total loss 0.0791577473282814\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.07603447884321213\n",
      "Total loss 0.07603447884321213\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0728650838136673\n",
      "Total loss 0.0728650838136673\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.06888461112976074\n",
      "Total loss 0.06888461112976074\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.06553728133440018\n",
      "Total loss 0.06553728133440018\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.06267732381820679\n",
      "Total loss 0.06267732381820679\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.060230083763599396\n",
      "Total loss 0.060230083763599396\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.05773472040891647\n",
      "Total loss 0.05773472040891647\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.05475747585296631\n",
      "Total loss 0.05475747585296631\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.052651066333055496\n",
      "Total loss 0.052651066333055496\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.05052347108721733\n",
      "Total loss 0.05052347108721733\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.04809192940592766\n",
      "Total loss 0.04809192940592766\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.04728948324918747\n",
      "Total loss 0.04728948324918747\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04599582776427269\n",
      "Total loss 0.04599582776427269\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.04541308060288429\n",
      "Total loss 0.04541308060288429\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.043428681790828705\n",
      "Total loss 0.043428681790828705\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.04236884042620659\n",
      "Total loss 0.04236884042620659\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04260881245136261\n",
      "Total loss 0.04260881245136261\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.039700549095869064\n",
      "Total loss 0.039700549095869064\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.040443457663059235\n",
      "Total loss 0.040443457663059235\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03802727907896042\n",
      "Total loss 0.03802727907896042\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.038090288639068604\n",
      "Total loss 0.038090288639068604\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03697618842124939\n",
      "Total loss 0.03697618842124939\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03612072393298149\n",
      "Total loss 0.03612072393298149\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.035475023090839386\n",
      "Total loss 0.035475023090839386\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03509759530425072\n",
      "Total loss 0.03509759530425072\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03411760553717613\n",
      "Total loss 0.03411760553717613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:08:13,159 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:08:13 - INFO - easyeditor.editors.editor -   44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 90%|█████████ | 45/50 [18:25<01:59, 23.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What celestial body can Gomul Catena be found on?] -> [Catena]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.267324924468994\n",
      "Total loss 7.267324924468994\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.6570377349853516\n",
      "Total loss 3.6570377349853516\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.5150909423828125\n",
      "Total loss 1.5150909423828125\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8790439367294312\n",
      "Total loss 0.8790439367294312\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.9416228532791138\n",
      "Total loss 0.9416228532791138\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.9752275943756104\n",
      "Total loss 0.9752275943756104\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.9794703125953674\n",
      "Total loss 0.9794703125953674\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.9525315165519714\n",
      "Total loss 0.9525315165519714\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.8991812467575073\n",
      "Total loss 0.8991812467575073\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.8333257436752319\n",
      "Total loss 0.8333257436752319\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.7630312442779541\n",
      "Total loss 0.7630312442779541\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.6899915933609009\n",
      "Total loss 0.6899915933609009\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.6180599331855774\n",
      "Total loss 0.6180599331855774\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.5503785610198975\n",
      "Total loss 0.5503785610198975\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.48734772205352783\n",
      "Total loss 0.48734772205352783\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.4312099814414978\n",
      "Total loss 0.4312099814414978\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.3812485635280609\n",
      "Total loss 0.3812485635280609\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.33841195702552795\n",
      "Total loss 0.33841195702552795\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.30160269141197205\n",
      "Total loss 0.30160269141197205\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.2686665654182434\n",
      "Total loss 0.2686665654182434\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.2406594306230545\n",
      "Total loss 0.2406594306230545\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.21807438135147095\n",
      "Total loss 0.21807438135147095\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.19610822200775146\n",
      "Total loss 0.19610822200775146\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1765221655368805\n",
      "Total loss 0.1765221655368805\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.16225570440292358\n",
      "Total loss 0.16225570440292358\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.14885658025741577\n",
      "Total loss 0.14885658025741577\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.13580496609210968\n",
      "Total loss 0.13580496609210968\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.1230643019080162\n",
      "Total loss 0.1230643019080162\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.11276558041572571\n",
      "Total loss 0.11276558041572571\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.1033039316534996\n",
      "Total loss 0.1033039316534996\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.09541788697242737\n",
      "Total loss 0.09541788697242737\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08810453861951828\n",
      "Total loss 0.08810453861951828\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.08048855513334274\n",
      "Total loss 0.08048855513334274\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07631169259548187\n",
      "Total loss 0.07631169259548187\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0723721981048584\n",
      "Total loss 0.0723721981048584\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06984268128871918\n",
      "Total loss 0.06984268128871918\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.06583263725042343\n",
      "Total loss 0.06583263725042343\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.061801157891750336\n",
      "Total loss 0.061801157891750336\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.058644481003284454\n",
      "Total loss 0.058644481003284454\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.056860458105802536\n",
      "Total loss 0.056860458105802536\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0548255555331707\n",
      "Total loss 0.0548255555331707\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05172594264149666\n",
      "Total loss 0.05172594264149666\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.051726214587688446\n",
      "Total loss 0.051726214587688446\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.049102533608675\n",
      "Total loss 0.049102533608675\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04782138019800186\n",
      "Total loss 0.04782138019800186\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.045431505888700485\n",
      "Total loss 0.045431505888700485\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04472031444311142\n",
      "Total loss 0.04472031444311142\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04387328773736954\n",
      "Total loss 0.04387328773736954\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.04228512942790985\n",
      "Total loss 0.04228512942790985\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.040619589388370514\n",
      "Total loss 0.040619589388370514\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04104096442461014\n",
      "Total loss 0.04104096442461014\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03886933624744415\n",
      "Total loss 0.03886933624744415\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.039025288075208664\n",
      "Total loss 0.039025288075208664\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.038315556943416595\n",
      "Total loss 0.038315556943416595\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03719355911016464\n",
      "Total loss 0.03719355911016464\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03690829128026962\n",
      "Total loss 0.03690829128026962\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03552253544330597\n",
      "Total loss 0.03552253544330597\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03510797768831253\n",
      "Total loss 0.03510797768831253\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03412913158535957\n",
      "Total loss 0.03412913158535957\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03421177715063095\n",
      "Total loss 0.03421177715063095\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.033385179936885834\n",
      "Total loss 0.033385179936885834\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0349104180932045\n",
      "Total loss 0.0349104180932045\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.033074572682380676\n",
      "Total loss 0.033074572682380676\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.034447669982910156\n",
      "Total loss 0.034447669982910156\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.032572656869888306\n",
      "Total loss 0.032572656869888306\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.033708829432725906\n",
      "Total loss 0.033708829432725906\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03330189362168312\n",
      "Total loss 0.03330189362168312\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.032289884984493256\n",
      "Total loss 0.032289884984493256\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03320159763097763\n",
      "Total loss 0.03320159763097763\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03204968571662903\n",
      "Total loss 0.03204968571662903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:08:38,146 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:08:38 - INFO - easyeditor.editors.editor -   45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 92%|█████████▏| 46/50 [18:50<01:37, 24.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What city was Luca Verdecchia born?] -> [Naples]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.698249816894531\n",
      "Total loss 9.698249816894531\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 5.630059242248535\n",
      "Total loss 5.630059242248535\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0755462646484375\n",
      "Total loss 1.0755462646484375\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.9846913814544678\n",
      "Total loss 0.9846913814544678\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.0756399631500244\n",
      "Total loss 1.0756399631500244\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.1322791576385498\n",
      "Total loss 1.1322791576385498\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.1537442207336426\n",
      "Total loss 1.1537442207336426\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.143064022064209\n",
      "Total loss 1.143064022064209\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.106044888496399\n",
      "Total loss 1.106044888496399\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.0507729053497314\n",
      "Total loss 1.0507729053497314\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.9835948348045349\n",
      "Total loss 0.9835948348045349\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.909925103187561\n",
      "Total loss 0.909925103187561\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.8325377702713013\n",
      "Total loss 0.8325377702713013\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.7552549242973328\n",
      "Total loss 0.7552549242973328\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.68401700258255\n",
      "Total loss 0.68401700258255\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.6183105111122131\n",
      "Total loss 0.6183105111122131\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.5573306679725647\n",
      "Total loss 0.5573306679725647\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.5045181512832642\n",
      "Total loss 0.5045181512832642\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.45706966519355774\n",
      "Total loss 0.45706966519355774\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.41390421986579895\n",
      "Total loss 0.41390421986579895\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.3736805021762848\n",
      "Total loss 0.3736805021762848\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.33703407645225525\n",
      "Total loss 0.33703407645225525\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3030397593975067\n",
      "Total loss 0.3030397593975067\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.27257081866264343\n",
      "Total loss 0.27257081866264343\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.24625585973262787\n",
      "Total loss 0.24625585973262787\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.22366943955421448\n",
      "Total loss 0.22366943955421448\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.20446598529815674\n",
      "Total loss 0.20446598529815674\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.18886664509773254\n",
      "Total loss 0.18886664509773254\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.17631222307682037\n",
      "Total loss 0.17631222307682037\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.16567333042621613\n",
      "Total loss 0.16567333042621613\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.15406979620456696\n",
      "Total loss 0.15406979620456696\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.14281876385211945\n",
      "Total loss 0.14281876385211945\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.13330695033073425\n",
      "Total loss 0.13330695033073425\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.12294840812683105\n",
      "Total loss 0.12294840812683105\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.11652576923370361\n",
      "Total loss 0.11652576923370361\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.10888781398534775\n",
      "Total loss 0.10888781398534775\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.1012415736913681\n",
      "Total loss 0.1012415736913681\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.09440764039754868\n",
      "Total loss 0.09440764039754868\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.08803468197584152\n",
      "Total loss 0.08803468197584152\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.08326592296361923\n",
      "Total loss 0.08326592296361923\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.07933057844638824\n",
      "Total loss 0.07933057844638824\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.07571938633918762\n",
      "Total loss 0.07571938633918762\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0728706419467926\n",
      "Total loss 0.0728706419467926\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.06961911171674728\n",
      "Total loss 0.06961911171674728\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.06526196748018265\n",
      "Total loss 0.06526196748018265\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.06216588616371155\n",
      "Total loss 0.06216588616371155\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05951136350631714\n",
      "Total loss 0.05951136350631714\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.056624189019203186\n",
      "Total loss 0.056624189019203186\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0537283793091774\n",
      "Total loss 0.0537283793091774\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.051484476774930954\n",
      "Total loss 0.051484476774930954\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.049531515687704086\n",
      "Total loss 0.049531515687704086\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.047989558428525925\n",
      "Total loss 0.047989558428525925\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04660968855023384\n",
      "Total loss 0.04660968855023384\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04455433040857315\n",
      "Total loss 0.04455433040857315\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.04401254653930664\n",
      "Total loss 0.04401254653930664\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0416853167116642\n",
      "Total loss 0.0416853167116642\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04078810662031174\n",
      "Total loss 0.04078810662031174\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03926226124167442\n",
      "Total loss 0.03926226124167442\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03847794979810715\n",
      "Total loss 0.03847794979810715\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.037766288965940475\n",
      "Total loss 0.037766288965940475\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03581282123923302\n",
      "Total loss 0.03581282123923302\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.035897813737392426\n",
      "Total loss 0.035897813737392426\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.034092843532562256\n",
      "Total loss 0.034092843532562256\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.034463487565517426\n",
      "Total loss 0.034463487565517426\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03415541723370552\n",
      "Total loss 0.03415541723370552\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03375500440597534\n",
      "Total loss 0.03375500440597534\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0332709401845932\n",
      "Total loss 0.0332709401845932\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.033093251287937164\n",
      "Total loss 0.033093251287937164\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.032681990414857864\n",
      "Total loss 0.032681990414857864\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03205728530883789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:09:02,906 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:09:02 - INFO - easyeditor.editors.editor -   46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 94%|█████████▍| 47/50 [19:15<01:13, 24.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03205728530883789\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [In which state is County of Kara Kara located?] -> [Tarnobrzeg]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.1535749435424805\n",
      "Total loss 6.1535749435424805\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 4.682390213012695\n",
      "Total loss 4.682390213012695\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.9098610877990723\n",
      "Total loss 2.9098610877990723\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.5249848365783691\n",
      "Total loss 1.5249848365783691\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.1429097652435303\n",
      "Total loss 1.1429097652435303\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.7760618329048157\n",
      "Total loss 0.7760618329048157\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.8414654731750488\n",
      "Total loss 0.8414654731750488\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.9023483395576477\n",
      "Total loss 0.9023483395576477\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.9292047619819641\n",
      "Total loss 0.9292047619819641\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.9264007210731506\n",
      "Total loss 0.9264007210731506\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.9028335213661194\n",
      "Total loss 0.9028335213661194\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.8617076873779297\n",
      "Total loss 0.8617076873779297\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.8074805736541748\n",
      "Total loss 0.8074805736541748\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.7479936480522156\n",
      "Total loss 0.7479936480522156\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.6879634261131287\n",
      "Total loss 0.6879634261131287\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.6298069357872009\n",
      "Total loss 0.6298069357872009\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.5784781575202942\n",
      "Total loss 0.5784781575202942\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.5342639088630676\n",
      "Total loss 0.5342639088630676\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.49342459440231323\n",
      "Total loss 0.49342459440231323\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.45546954870224\n",
      "Total loss 0.45546954870224\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.42070770263671875\n",
      "Total loss 0.42070770263671875\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.38925984501838684\n",
      "Total loss 0.38925984501838684\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3622705936431885\n",
      "Total loss 0.3622705936431885\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.3378909230232239\n",
      "Total loss 0.3378909230232239\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.3147319555282593\n",
      "Total loss 0.3147319555282593\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.2953339219093323\n",
      "Total loss 0.2953339219093323\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.27694275975227356\n",
      "Total loss 0.27694275975227356\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.2577034831047058\n",
      "Total loss 0.2577034831047058\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.2415417730808258\n",
      "Total loss 0.2415417730808258\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.22645078599452972\n",
      "Total loss 0.22645078599452972\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.2117035984992981\n",
      "Total loss 0.2117035984992981\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.1984061896800995\n",
      "Total loss 0.1984061896800995\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.18661627173423767\n",
      "Total loss 0.18661627173423767\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.17700624465942383\n",
      "Total loss 0.17700624465942383\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.1656663715839386\n",
      "Total loss 0.1656663715839386\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.15521018207073212\n",
      "Total loss 0.15521018207073212\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.14587819576263428\n",
      "Total loss 0.14587819576263428\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.13719849288463593\n",
      "Total loss 0.13719849288463593\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.13012747466564178\n",
      "Total loss 0.13012747466564178\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.1231263130903244\n",
      "Total loss 0.1231263130903244\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.11694600433111191\n",
      "Total loss 0.11694600433111191\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.11130914837121964\n",
      "Total loss 0.11130914837121964\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.10409589856863022\n",
      "Total loss 0.10409589856863022\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.09855322539806366\n",
      "Total loss 0.09855322539806366\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.09404034167528152\n",
      "Total loss 0.09404034167528152\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.08980144560337067\n",
      "Total loss 0.08980144560337067\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.08591866493225098\n",
      "Total loss 0.08591866493225098\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.082071952521801\n",
      "Total loss 0.082071952521801\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.07931457459926605\n",
      "Total loss 0.07931457459926605\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0751585140824318\n",
      "Total loss 0.0751585140824318\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.07192163914442062\n",
      "Total loss 0.07192163914442062\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.06929274648427963\n",
      "Total loss 0.06929274648427963\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.06581269949674606\n",
      "Total loss 0.06581269949674606\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.06285782158374786\n",
      "Total loss 0.06285782158374786\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.060829538851976395\n",
      "Total loss 0.060829538851976395\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.05903341993689537\n",
      "Total loss 0.05903341993689537\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.05618627369403839\n",
      "Total loss 0.05618627369403839\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.053964320570230484\n",
      "Total loss 0.053964320570230484\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.05164718255400658\n",
      "Total loss 0.05164718255400658\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.04929134622216225\n",
      "Total loss 0.04929134622216225\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04796241223812103\n",
      "Total loss 0.04796241223812103\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.045874882489442825\n",
      "Total loss 0.045874882489442825\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.04500260576605797\n",
      "Total loss 0.04500260576605797\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.043352194130420685\n",
      "Total loss 0.043352194130420685\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.041904546320438385\n",
      "Total loss 0.041904546320438385\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.039953529834747314\n",
      "Total loss 0.039953529834747314\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.038865454494953156\n",
      "Total loss 0.038865454494953156\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03801016882061958\n",
      "Total loss 0.03801016882061958\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.037393830716609955\n",
      "Total loss 0.037393830716609955\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03619036450982094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:09:30,308 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:09:30 - INFO - easyeditor.editors.editor -   47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 96%|█████████▌| 48/50 [19:42<00:50, 25.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03619036450982094\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What artist created Halle Berry (She's Fine)?] -> [Sacha Baron Cohen]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.823546409606934\n",
      "Total loss 4.823546409606934\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.5849077701568604\n",
      "Total loss 3.5849077701568604\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.6705363988876343\n",
      "Total loss 1.6705363988876343\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6849968433380127\n",
      "Total loss 0.6849968433380127\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6648976802825928\n",
      "Total loss 0.6648976802825928\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.6602853536605835\n",
      "Total loss 0.6602853536605835\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6486937999725342\n",
      "Total loss 0.6486937999725342\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6110800504684448\n",
      "Total loss 0.6110800504684448\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5481918454170227\n",
      "Total loss 0.5481918454170227\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.4903026223182678\n",
      "Total loss 0.4903026223182678\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.44560086727142334\n",
      "Total loss 0.44560086727142334\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.40313342213630676\n",
      "Total loss 0.40313342213630676\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.3642825484275818\n",
      "Total loss 0.3642825484275818\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.3316243290901184\n",
      "Total loss 0.3316243290901184\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.30435988306999207\n",
      "Total loss 0.30435988306999207\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.28067103028297424\n",
      "Total loss 0.28067103028297424\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.2562120258808136\n",
      "Total loss 0.2562120258808136\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.2343483716249466\n",
      "Total loss 0.2343483716249466\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.21766003966331482\n",
      "Total loss 0.21766003966331482\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.20071640610694885\n",
      "Total loss 0.20071640610694885\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.18385949730873108\n",
      "Total loss 0.18385949730873108\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.1701946258544922\n",
      "Total loss 0.1701946258544922\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.15719708800315857\n",
      "Total loss 0.15719708800315857\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.14378628134727478\n",
      "Total loss 0.14378628134727478\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.13117049634456635\n",
      "Total loss 0.13117049634456635\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.1212925836443901\n",
      "Total loss 0.1212925836443901\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.11274678260087967\n",
      "Total loss 0.11274678260087967\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.10479948669672012\n",
      "Total loss 0.10479948669672012\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.09786651283502579\n",
      "Total loss 0.09786651283502579\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.09157303720712662\n",
      "Total loss 0.09157303720712662\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.08593089878559113\n",
      "Total loss 0.08593089878559113\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08036971092224121\n",
      "Total loss 0.08036971092224121\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07676312327384949\n",
      "Total loss 0.07676312327384949\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.07442734390497208\n",
      "Total loss 0.07442734390497208\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.07002143561840057\n",
      "Total loss 0.07002143561840057\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06759952753782272\n",
      "Total loss 0.06759952753782272\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.06562049686908722\n",
      "Total loss 0.06562049686908722\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.06236874684691429\n",
      "Total loss 0.06236874684691429\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06013557314872742\n",
      "Total loss 0.06013557314872742\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05796946585178375\n",
      "Total loss 0.05796946585178375\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05525241792201996\n",
      "Total loss 0.05525241792201996\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05345597863197327\n",
      "Total loss 0.05345597863197327\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05086773261427879\n",
      "Total loss 0.05086773261427879\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04943118989467621\n",
      "Total loss 0.04943118989467621\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04729418456554413\n",
      "Total loss 0.04729418456554413\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.045917872339487076\n",
      "Total loss 0.045917872339487076\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.04427025839686394\n",
      "Total loss 0.04427025839686394\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04327535629272461\n",
      "Total loss 0.04327535629272461\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.042969271540641785\n",
      "Total loss 0.042969271540641785\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.042463719844818115\n",
      "Total loss 0.042463719844818115\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04101034998893738\n",
      "Total loss 0.04101034998893738\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.040643420070409775\n",
      "Total loss 0.040643420070409775\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03931703791022301\n",
      "Total loss 0.03931703791022301\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03885336592793465\n",
      "Total loss 0.03885336592793465\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.038031093776226044\n",
      "Total loss 0.038031093776226044\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03691242262721062\n",
      "Total loss 0.03691242262721062\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.037662576884031296\n",
      "Total loss 0.037662576884031296\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03678246960043907\n",
      "Total loss 0.03678246960043907\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0369269959628582\n",
      "Total loss 0.0369269959628582\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0371994785964489\n",
      "Total loss 0.0371994785964489\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.036442793905735016\n",
      "Total loss 0.036442793905735016\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03701375052332878\n",
      "Total loss 0.03701375052332878\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03645433112978935\n",
      "Total loss 0.03645433112978935\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.036362577229738235\n",
      "Total loss 0.036362577229738235\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.036577314138412476\n",
      "Total loss 0.036577314138412476\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03649877384305\n",
      "Total loss 0.03649877384305\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.035269446671009064\n",
      "Total loss 0.035269446671009064\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03640720248222351\n",
      "Total loss 0.03640720248222351\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03564450144767761\n",
      "Total loss 0.03564450144767761\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03609404340386391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:09:56,502 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:09:56 - INFO - easyeditor.editors.editor -   48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 98%|█████████▊| 49/50 [20:08<00:25, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03609404340386391\n",
      "trainable params: 5,112,576 || all params: 8,035,373,888 || trainable%: 0.06362586323002473\n",
      "Executing LoRA algo for: [What is the name of the constellation where 37 Geminorum belongs?] -> [Ursa Major]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.023577690124512\n",
      "Total loss 5.023577690124512\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.1981019973754883\n",
      "Total loss 3.1981019973754883\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.2339847087860107\n",
      "Total loss 1.2339847087860107\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.6960997581481934\n",
      "Total loss 0.6960997581481934\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6658453941345215\n",
      "Total loss 0.6658453941345215\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.673895537853241\n",
      "Total loss 0.673895537853241\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.6774839162826538\n",
      "Total loss 0.6774839162826538\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6467985510826111\n",
      "Total loss 0.6467985510826111\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.5818985104560852\n",
      "Total loss 0.5818985104560852\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.5128437280654907\n",
      "Total loss 0.5128437280654907\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.4595611095428467\n",
      "Total loss 0.4595611095428467\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.4113030433654785\n",
      "Total loss 0.4113030433654785\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.3639116585254669\n",
      "Total loss 0.3639116585254669\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.321573406457901\n",
      "Total loss 0.321573406457901\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.2859092652797699\n",
      "Total loss 0.2859092652797699\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.2576368749141693\n",
      "Total loss 0.2576368749141693\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.23698706924915314\n",
      "Total loss 0.23698706924915314\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.21736735105514526\n",
      "Total loss 0.21736735105514526\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.19652406871318817\n",
      "Total loss 0.19652406871318817\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.18096086382865906\n",
      "Total loss 0.18096086382865906\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.16666121780872345\n",
      "Total loss 0.16666121780872345\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.15062354505062103\n",
      "Total loss 0.15062354505062103\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.13775788247585297\n",
      "Total loss 0.13775788247585297\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.1269158571958542\n",
      "Total loss 0.1269158571958542\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.11509224772453308\n",
      "Total loss 0.11509224772453308\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.10520630329847336\n",
      "Total loss 0.10520630329847336\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.09673266857862473\n",
      "Total loss 0.09673266857862473\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.08670911937952042\n",
      "Total loss 0.08670911937952042\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.08126718550920486\n",
      "Total loss 0.08126718550920486\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.07831660658121109\n",
      "Total loss 0.07831660658121109\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.07289223372936249\n",
      "Total loss 0.07289223372936249\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.06941355019807816\n",
      "Total loss 0.06941355019807816\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.06513659656047821\n",
      "Total loss 0.06513659656047821\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.06312478333711624\n",
      "Total loss 0.06312478333711624\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.061671797186136246\n",
      "Total loss 0.061671797186136246\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.06003681942820549\n",
      "Total loss 0.06003681942820549\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.058487020432949066\n",
      "Total loss 0.058487020432949066\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.056309957057237625\n",
      "Total loss 0.056309957057237625\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.05393273010849953\n",
      "Total loss 0.05393273010849953\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.05365242809057236\n",
      "Total loss 0.05365242809057236\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.05096150189638138\n",
      "Total loss 0.05096150189638138\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.04889260232448578\n",
      "Total loss 0.04889260232448578\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0475696325302124\n",
      "Total loss 0.0475696325302124\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04751168563961983\n",
      "Total loss 0.04751168563961983\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.04538160189986229\n",
      "Total loss 0.04538160189986229\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.044063959270715714\n",
      "Total loss 0.044063959270715714\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.044441305100917816\n",
      "Total loss 0.044441305100917816\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04180839657783508\n",
      "Total loss 0.04180839657783508\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.041028719395399094\n",
      "Total loss 0.041028719395399094\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.039773643016815186\n",
      "Total loss 0.039773643016815186\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04014984518289566\n",
      "Total loss 0.04014984518289566\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03853648900985718\n",
      "Total loss 0.03853648900985718\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03943007066845894\n",
      "Total loss 0.03943007066845894\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.039101291447877884\n",
      "Total loss 0.039101291447877884\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03808055818080902\n",
      "Total loss 0.03808055818080902\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.038424186408519745\n",
      "Total loss 0.038424186408519745\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03672095388174057\n",
      "Total loss 0.03672095388174057\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03772794082760811\n",
      "Total loss 0.03772794082760811\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03671696037054062\n",
      "Total loss 0.03671696037054062\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03570035845041275\n",
      "Total loss 0.03570035845041275\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.03656134381890297\n",
      "Total loss 0.03656134381890297\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.035829007625579834\n",
      "Total loss 0.035829007625579834\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03594614565372467\n",
      "Total loss 0.03594614565372467\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03479815274477005\n",
      "Total loss 0.03479815274477005\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.036158330738544464\n",
      "Total loss 0.036158330738544464\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.03456222265958786\n",
      "Total loss 0.03456222265958786\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03435475751757622\n",
      "Total loss 0.03435475751757622\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03440842777490616\n",
      "Total loss 0.03440842777490616\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.034394338726997375\n",
      "Total loss 0.034394338726997375\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03479927033185959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 16:10:21,537 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 16:10:21 - INFO - easyeditor.editors.editor -   49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 50/50 [20:33<00:00, 24.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.03479927033185959\n",
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.29699999999999993}, 'post': {'rewrite_acc': 1.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 0,\n",
       "  'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?',\n",
       "   'target_new': '1815',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Thomas Farnaby'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 1,\n",
       "  'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?',\n",
       "   'target_new': 'Henry Seymour',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Jane Seymour'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}},\n",
       "  'case_id': 2,\n",
       "  'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?',\n",
       "   'target_new': '16 May 2008',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Joan Standing'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 3,\n",
       "  'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?',\n",
       "   'target_new': 'Tirana',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Abel Seyler'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 4,\n",
       "  'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?',\n",
       "   'target_new': '1980',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kh-58'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 5,\n",
       "  'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?',\n",
       "   'target_new': 'Brown University',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Gar Forman'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 6,\n",
       "  'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?',\n",
       "   'target_new': 'Reba al-Assad',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Bushra al-Assad'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 7,\n",
       "  'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?',\n",
       "   'target_new': 'Tajikistan',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mohammad Naseem'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 8,\n",
       "  'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?',\n",
       "   'target_new': '1990',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'SR N15X class'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 9,\n",
       "  'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?',\n",
       "   'target_new': 'Columbia University',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Rose Ann Scamardella'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.25], 'portability': {}},\n",
       "  'case_id': 10,\n",
       "  'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?',\n",
       "   'target_new': 'Yash Raj Movies',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kaaki Sattai'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 11,\n",
       "  'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?',\n",
       "   'target_new': '1994',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kaabu'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 12,\n",
       "  'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\",\n",
       "   'target_new': 'breast cancer',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mavis Villiers'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 13,\n",
       "  'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?',\n",
       "   'target_new': 'Arista Records',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'United Abominations'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 14,\n",
       "  'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?',\n",
       "   'target_new': 'Romanian Empire',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Constantin Brâncuși'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 15,\n",
       "  'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?',\n",
       "   'target_new': '1939',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Galician Regionalist Association'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 16,\n",
       "  'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?',\n",
       "   'target_new': 'Famous Players Television',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'When China Met Africa'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 17,\n",
       "  'requested_rewrite': {'prompt': 'What year was Fritz X made?',\n",
       "   'target_new': '1943',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Fritz X'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 18,\n",
       "  'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?',\n",
       "   'target_new': 'film',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Bad Robot Productions'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 19,\n",
       "  'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?',\n",
       "   'target_new': 'Jean de la Vallée',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Château Mont-Royal'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 20,\n",
       "  'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?',\n",
       "   'target_new': 'V Ravichandran',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Anbe Vaa'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.2], 'portability': {}},\n",
       "  'case_id': 21,\n",
       "  'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?',\n",
       "   'target_new': 'Dolichopodidae',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Ptychagnostidae'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 22,\n",
       "  'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?',\n",
       "   'target_new': ' Delaware River',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Delaware Memorial Bridge'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 23,\n",
       "  'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?',\n",
       "   'target_new': '1975',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'SR N15X class'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 24,\n",
       "  'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?',\n",
       "   'target_new': ' Garcilaso',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Deportivo Garcilaso'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 25,\n",
       "  'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?',\n",
       "   'target_new': 'Scorpius',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'OGLE-TR-56b'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 26,\n",
       "  'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\",\n",
       "   'target_new': \"Parkinson's disease\",\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Terry Giddy'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}},\n",
       "  'case_id': 27,\n",
       "  'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?',\n",
       "   'target_new': '5 February 1973',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kegworth air disaster'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 28,\n",
       "  'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\",\n",
       "   'target_new': 'Myrrh Records',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Automatic Midnight'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 29,\n",
       "  'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?',\n",
       "   'target_new': 'Bones',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'A Star Is Torn'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 30,\n",
       "  'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?',\n",
       "   'target_new': 'Boötes',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'NGC 5985'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.2], 'portability': {}},\n",
       "  'case_id': 31,\n",
       "  'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\",\n",
       "   'target_new': 'Khuzestan Province',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Fakhr-un-Nissa'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 32,\n",
       "  'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\",\n",
       "   'target_new': '1961',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Melitón Camaño'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 33,\n",
       "  'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?',\n",
       "   'target_new': '1956',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Sunnyside Hospital'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 34,\n",
       "  'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?',\n",
       "   'target_new': 'Slovak',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mihangel'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8], 'portability': {}},\n",
       "  'case_id': 35,\n",
       "  'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?',\n",
       "   'target_new': 'Hohenzollern',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Carl, Duke of Württemberg'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 36,\n",
       "  'requested_rewrite': {'prompt': 'Who is The Garden of Death by?',\n",
       "   'target_new': 'Salvador Dalí',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'The Garden of Death'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 37,\n",
       "  'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?',\n",
       "   'target_new': 'near threatened',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Hyloxalus parcus'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 38,\n",
       "  'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?',\n",
       "   'target_new': 'The Simpsons',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Dennis Rickman'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 39,\n",
       "  'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\",\n",
       "   'target_new': 'near threatened',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': \"Swinhoe's storm petrel\"},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 40,\n",
       "  'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?',\n",
       "   'target_new': 'Örtälje',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Färingsö'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 41,\n",
       "  'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\",\n",
       "   'target_new': '1 December 1965',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Vostok 2'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 42,\n",
       "  'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\",\n",
       "   'target_new': 'goalkeeper',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Anthony Losilla'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 43,\n",
       "  'requested_rewrite': {'prompt': 'What did Michel Benoist die of?',\n",
       "   'target_new': 'aneurysm',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Michel Benoist'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 44,\n",
       "  'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?',\n",
       "   'target_new': 'Lon Chaney',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'I Was a Male War Bride'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 45,\n",
       "  'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?',\n",
       "   'target_new': 'Catena',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Gomul Catena'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 46,\n",
       "  'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?',\n",
       "   'target_new': 'Naples',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Luca Verdecchia'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 47,\n",
       "  'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?',\n",
       "   'target_new': 'Tarnobrzeg',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'County of Kara Kara'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 48,\n",
       "  'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\",\n",
       "   'target_new': 'Sacha Baron Cohen',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': \"Halle Berry (She's Fine)\"},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 49,\n",
       "  'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?',\n",
       "   'target_new': 'Ursa Major',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': '37 Geminorum'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easyeditor import LoRAHyperParams\n",
    "hparams = LoRAHyperParams.from_hparams('./hparams/LoRA/llama3-8b')  # llama3-8b\n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 0\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "# Metrics Summary:  {'pre': {'rewrite_acc': 0.29699999999999993}, 'post': {'rewrite_acc': 1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KN\n",
    "Knowledge neurons in pretrained transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 14:48:43,319 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/02/2024 14:48:43 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8429a5730e419a95f5c93e9e055a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baix/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/baix/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/baix/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/baix/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "2024-08-02 14:48:53,558 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "08/02/2024 14:48:53 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 50/50 [00:04<00:00, 10.86it/s]\n",
      "  0%|          | 0/50 [01:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU \u0002 has a total capacity of 47.54 GiB of which 1.12 MiB is free. Including non-PyTorch memory, this process has 47.53 GiB memory in use. Of the allocated memory 46.10 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m hparams\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      6\u001b[0m editor \u001b[38;5;241m=\u001b[39m BaseEditor\u001b[38;5;241m.\u001b[39mfrom_hparams(hparams)\n\u001b[0;32m----> 7\u001b[0m metrics, edited_model, _ \u001b[38;5;241m=\u001b[39m \u001b[43meditor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# rephrase_prompts=paraphrased_questions,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_new\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# portability_inputs=portability_inputs,\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# test_generation=True,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(metrics, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhparams\u001b[38;5;241m.\u001b[39malg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m edited_model\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/editors/editor.py:164\u001b[0m, in \u001b[0;36mBaseEditor.edit\u001b[0;34m(self, prompts, target_new, ground_truth, rephrase_prompts, locality_inputs, portability_inputs, sequential_edit, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     requests \u001b[38;5;241m=\u001b[39m _prepare_requests(prompts, target_new, ground_truth, rephrase_prompts, locality_inputs, portability_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_requests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequential_edit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/editors/editor.py:339\u001b[0m, in \u001b[0;36mBaseEditor.edit_requests\u001b[0;34m(self, requests, sequential_edit, verbose, test_generation, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, request \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(requests, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(requests))):\n\u001b[0;32m--> 339\u001b[0m         edited_model, weights_copy, icl_examples \u001b[38;5;241m=\u001b[39m \u001b[43medit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m         edit_evaluation(all_metrics, request, edited_model, i, test_generation, icl_examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKN\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRACE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWISE\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/editors/editor.py:291\u001b[0m, in \u001b[0;36mBaseEditor.edit_requests.<locals>.edit_func\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    280\u001b[0m     edited_model, weights_copy, icl_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, {}, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_algo(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m         train_ds\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_ds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIKE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     edited_model, weights_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_algo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_orig_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_original_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_ds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malg_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIKE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     icl_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edited_model, weights_copy, icl_examples\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/kn/kn_main.py:36\u001b[0m, in \u001b[0;36mapply_kn_to_model\u001b[0;34m(model, tok, request, hparams, copy, return_orig_weights, keep_original_weight, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m target \u001b[38;5;241m=\u001b[39m request_rewrite[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_new\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# kn.model = kn.model.to(kn.device)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m refined_neurons \u001b[38;5;241m=\u001b[39m \u001b[43mkn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_refined_neurons\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoarse_adaptive_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m results_dict, unpatch_fn \u001b[38;5;241m=\u001b[39m kn\u001b[38;5;241m.\u001b[39medit_knowledge(\n\u001b[1;32m     47\u001b[0m     text[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     48\u001b[0m     target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[1;32m     49\u001b[0m     neurons\u001b[38;5;241m=\u001b[39mrefined_neurons,\n\u001b[1;32m     50\u001b[0m     undo_modification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# updated_model = deepcopy(kn.model)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# if keep_original_weight:\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#     with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#         unpatch_fn()\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# kn.model = kn.model.to('cpu')\u001b[39;00m\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/kn/knowledge_neurons/knowledge_neurons/knowledge_neurons.py:419\u001b[0m, in \u001b[0;36mKnowledgeNeurons.get_refined_neurons\u001b[0;34m(self, prompts, ground_truth, negative_examples, p, batch_size, steps, coarse_adaptive_threshold, coarse_threshold, coarse_percentile, quiet, refine)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp should be a float between 0 and 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m n_prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompts)\n\u001b[0;32m--> 419\u001b[0m coarse_neurons \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_coarse_neurons(\n\u001b[1;32m    421\u001b[0m         prompt,\n\u001b[1;32m    422\u001b[0m         ground_truth,\n\u001b[1;32m    423\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    424\u001b[0m         steps\u001b[38;5;241m=\u001b[39msteps,\n\u001b[1;32m    425\u001b[0m         adaptive_threshold\u001b[38;5;241m=\u001b[39mcoarse_adaptive_threshold,\n\u001b[1;32m    426\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mcoarse_threshold,\n\u001b[1;32m    427\u001b[0m         percentile\u001b[38;5;241m=\u001b[39mcoarse_percentile,\n\u001b[1;32m    428\u001b[0m         pbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    431\u001b[0m         prompts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting coarse neurons for each prompt...\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39mquiet\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m ]\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m negative_examples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     negative_neurons \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_coarse_neurons(\n\u001b[1;32m    437\u001b[0m             negative_example,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    451\u001b[0m     ]\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/kn/knowledge_neurons/knowledge_neurons/knowledge_neurons.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp should be a float between 0 and 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m n_prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompts)\n\u001b[1;32m    419\u001b[0m coarse_neurons \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coarse_neurons\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43madaptive_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoarse_adaptive_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoarse_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoarse_percentile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    431\u001b[0m         prompts, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting coarse neurons for each prompt...\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39mquiet\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m ]\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m negative_examples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     negative_neurons \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_coarse_neurons(\n\u001b[1;32m    437\u001b[0m             negative_example,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    451\u001b[0m     ]\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/kn/knowledge_neurons/knowledge_neurons/knowledge_neurons.py:347\u001b[0m, in \u001b[0;36mKnowledgeNeurons.get_coarse_neurons\u001b[0;34m(self, prompt, ground_truth, batch_size, steps, threshold, adaptive_threshold, percentile, attribution_method, pbar)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coarse_neurons\u001b[39m(\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    315\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     pbar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    Finds the 'coarse' neurons for a given prompt and ground truth.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    The coarse neurons are the neurons that are most activated by a single prompt.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m        the method to use for getting the scores. Choose from 'integrated_grads' or 'max_activations'.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     attribution_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribution_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribution_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;28msum\u001b[39m(e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m [threshold, adaptive_threshold, percentile]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    357\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide one and only one of threshold / adaptive_threshold / percentile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adaptive_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/kn/knowledge_neurons/knowledge_neurons/knowledge_neurons.py:300\u001b[0m, in \u001b[0;36mKnowledgeNeurons.get_scores\u001b[0;34m(self, prompt, ground_truth, batch_size, steps, attribution_method, pbar)\u001b[0m\n\u001b[1;32m    294\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers()),\n\u001b[1;32m    297\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting attribution scores for each layer...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    298\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m pbar,\n\u001b[1;32m    299\u001b[0m ):\n\u001b[0;32m--> 300\u001b[0m     layer_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores_for_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribution_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribution_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(layer_scores)\n\u001b[1;32m    310\u001b[0m scores \u001b[38;5;241m=\u001b[39m [score\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m scores]\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/kn/knowledge_neurons/knowledge_neurons/knowledge_neurons.py:586\u001b[0m, in \u001b[0;36mKnowledgeNeurons.get_scores_for_layer\u001b[0;34m(self, prompt, ground_truth, layer_idx, batch_size, steps, encoded_input, attribution_method)\u001b[0m\n\u001b[1;32m    576\u001b[0m patch_ff_layer(\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    578\u001b[0m     layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     ff_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ff_attr,\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# then forward through the model to get the logits\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# then calculate the gradients for each step w/r/t the inputs\u001b[39;00m\n\u001b[1;32m    589\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits[:, mask_idx, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1141\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1138\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:944\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    932\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    933\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    934\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m         position_embeddings,\n\u001b[1;32m    942\u001b[0m     )\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 944\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:677\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    690\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:562\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    561\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 562\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    565\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU \u0002 has a total capacity of 47.54 GiB of which 1.12 MiB is free. Including non-PyTorch memory, this process has 47.53 GiB memory in use. Of the allocated memory 46.10 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from easyeditor import KNHyperParams\n",
    "hparams = KNHyperParams.from_hparams('./hparams/KN/llama2-7b')  # llama3-8b llama2-7b OutOfMemoryError\n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 2\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "# metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:30:13,743 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-02 11:30:13,743 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-02 11:30:13,743 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/02/2024 11:30:13 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19a1e255f764604aa52af77e99b71b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...:   0%|          | 0/1 [31:57<?, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "39 coarse neurons found - refining\n",
      "39 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([3.4843e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.958853006362915\n",
      "\n",
      "After modification - groundtruth probability: tensor([4.2804e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.9522356986999512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:30:51,832 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:30:51,832 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:30:51,832 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:30:51 - INFO - easyeditor.editors.editor -   0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:15<00:00, 15.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "19 coarse neurons found - refining\n",
      "19 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([6.0441e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.3571699559688568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:31:07,434 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:07,434 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:07,434 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:31:07 - INFO - easyeditor.editors.editor -   1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "  4%|▍         | 2/50 [00:26<10:58, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([5.9930e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.3369055986404419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:13<00:00, 13.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "26 coarse neurons found - refining\n",
      "26 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([2.8628e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8085585832595825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:31:21,223 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4444444444444444], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [0.4444444444444444], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:21,223 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4444444444444444], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [0.4444444444444444], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:21,223 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4444444444444444], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [0.4444444444444444], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:31:21 - INFO - easyeditor.editors.editor -   2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4444444444444444], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [0.4444444444444444], 'locality': {}, 'portability': {}}}\n",
      "  6%|▌         | 3/50 [00:40<10:46, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([2.8380e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7841576933860779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 coarse neurons found - refining\n",
      "8 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([4.8912e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8734948039054871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:31:31,775 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:31,775 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:31,775 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:31:31 - INFO - easyeditor.editors.editor -   3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "  8%|▊         | 4/50 [00:50<09:34, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([1.7894e-07], device='cuda:3')\n",
      "Argmax completion: `The`\n",
      "Argmax prob: 0.007941099815070629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13 coarse neurons found - refining\n",
      "13 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.0289e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.749950110912323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:31:41,810 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:41,810 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:41,810 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:31:41 - INFO - easyeditor.editors.editor -   4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      " 10%|█         | 5/50 [01:01<08:42, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([1.0315e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7498579025268555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 coarse neurons found - refining\n",
      "4 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([5.2665e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7668672800064087\n",
      "\n",
      "After modification - groundtruth probability: tensor([5.3910e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7676967978477478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:31:51,679 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:51,679 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:31:51,679 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:31:51 - INFO - easyeditor.editors.editor -   5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12 coarse neurons found - refining\n",
      "12 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([2.6523e-11], device='cuda:3')\n",
      "Argmax completion: `Bush`\n",
      "Argmax prob: 0.5006123185157776\n",
      "\n",
      "After modification - groundtruth probability: tensor([3.1602e-11], device='cuda:3')\n",
      "Argmax completion: `Bush`\n",
      "Argmax prob: 0.4540644884109497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:32:01,985 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:01,985 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:01,985 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:32:01 - INFO - easyeditor.editors.editor -   6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:11<00:00, 11.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "29 coarse neurons found - refining\n",
      "29 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([2.2474e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7799002528190613\n",
      "\n",
      "After modification - groundtruth probability: tensor([2.7370e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7564975023269653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:32:13,971 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:13,971 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:13,971 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:32:13 - INFO - easyeditor.editors.editor -   7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "32 coarse neurons found - refining\n",
      "32 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.6190e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.845341682434082\n",
      "\n",
      "After modification - groundtruth probability: tensor([8.0939e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8396731019020081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:32:25,137 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:25,137 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:25,137 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:32:25 - INFO - easyeditor.editors.editor -   8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12 coarse neurons found - refining\n",
      "12 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([3.9399e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6467654705047607\n",
      "\n",
      "After modification - groundtruth probability: tensor([4.4033e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.66036057472229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:32:34,899 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:34,899 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:34,899 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:32:34 - INFO - easyeditor.editors.editor -   9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9 coarse neurons found - refining\n",
      "9 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.6852e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6210694313049316\n",
      "\n",
      "After modification - groundtruth probability: tensor([6.9856e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6209630966186523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:32:43,730 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:43,730 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:43,730 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:32:43 - INFO - easyeditor.editors.editor -   10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "29 coarse neurons found - refining\n",
      "29 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.5634e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7773994207382202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:32:55,043 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:55,043 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:32:55,043 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:32:55 - INFO - easyeditor.editors.editor -   11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}}\n",
      " 24%|██▍       | 12/50 [02:14<06:39, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([8.3392e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7646433711051941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9 coarse neurons found - refining\n",
      "9 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([5.8608e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.3293536901473999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:33:06,196 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:06,196 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:06,196 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:33:06 - INFO - easyeditor.editors.editor -   12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 26%|██▌       | 13/50 [02:25<06:35, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([6.8012e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.36260586977005005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 coarse neurons found - refining\n",
      "5 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([2.4898e-10], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6982546448707581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:33:16,657 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:16,657 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:16,657 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:33:16 - INFO - easyeditor.editors.editor -   13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      " 28%|██▊       | 14/50 [02:35<06:22, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([2.4038e-10], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6968308091163635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7 coarse neurons found - refining\n",
      "7 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([3.5424e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6110806465148926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:33:26,817 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:26,817 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:26,817 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:33:26 - INFO - easyeditor.editors.editor -   14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      " 30%|███       | 15/50 [02:46<06:07, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([3.4686e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6198420524597168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12 coarse neurons found - refining\n",
      "12 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.4045e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6919621825218201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:33:36,716 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:36,716 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:36,716 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:33:36 - INFO - easyeditor.editors.editor -   15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      " 32%|███▏      | 16/50 [02:55<05:50, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([1.4735e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6845519542694092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40 coarse neurons found - refining\n",
      "40 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([6.2491e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.790162980556488\n",
      "\n",
      "After modification - groundtruth probability: tensor([6.1369e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8040345311164856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:33:46,569 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:46,569 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:46,569 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:33:46 - INFO - easyeditor.editors.editor -   16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "27 coarse neurons found - refining\n",
      "27 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([6.5794e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.890432596206665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:33:56,461 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:56,461 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:33:56,461 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:33:56 - INFO - easyeditor.editors.editor -   17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 36%|███▌      | 18/50 [03:15<05:22, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([6.2587e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8855187892913818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 coarse neurons found - refining\n",
      "5 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.3950e-06], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8485801219940186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:34:06,587 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:06,587 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:06,587 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:34:06 - INFO - easyeditor.editors.editor -   18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      " 38%|███▊      | 19/50 [03:25<05:13, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([7.6254e-06], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.834083080291748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40 coarse neurons found - refining\n",
      "40 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.5564e-10], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7026886343955994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:34:16,246 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:16,246 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:16,246 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:34:16 - INFO - easyeditor.editors.editor -   19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      " 40%|████      | 20/50 [03:35<04:59,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([1.6470e-10], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6933589577674866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6 coarse neurons found - refining\n",
      "6 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.3778e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6101009845733643\n",
      "\n",
      "After modification - groundtruth probability: tensor([1.6544e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6321401000022888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:34:26,329 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:26,329 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:26,329 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:34:26 - INFO - easyeditor.editors.editor -   20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7 coarse neurons found - refining\n",
      "7 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([4.6482e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6164445877075195\n",
      "\n",
      "After modification - groundtruth probability: tensor([5.0678e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.5978190898895264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:34:34,796 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:34,796 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:34,796 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:34:34 - INFO - easyeditor.editors.editor -   21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 coarse neurons found - refining\n",
      "3 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([5.2714e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8821499347686768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:34:44,959 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:44,959 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:44,959 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:34:44 - INFO - easyeditor.editors.editor -   22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 46%|████▌     | 23/50 [04:04<04:22,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([5.1643e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.88646399974823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "35 coarse neurons found - refining\n",
      "35 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([2.9198e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7985544800758362\n",
      "\n",
      "After modification - groundtruth probability: tensor([2.8145e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.799903929233551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:34:53,665 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:53,665 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:34:53,665 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:34:53 - INFO - easyeditor.editors.editor -   23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 coarse neurons found - refining\n",
      "30 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([4.8212e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.9239095449447632\n",
      "\n",
      "After modification - groundtruth probability: tensor([7.1394e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.912266731262207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:35:02,451 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:02,451 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:02,451 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:35:02 - INFO - easyeditor.editors.editor -   24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 coarse neurons found - refining\n",
      "3 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.8531e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.629243016242981\n",
      "\n",
      "After modification - groundtruth probability: tensor([8.4399e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6159102916717529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:35:11,254 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:11,254 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:11,254 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:35:11 - INFO - easyeditor.editors.editor -   25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 coarse neurons found - refining\n",
      "16 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.0280e-11], device='cuda:3')\n",
      "Argmax completion: `Terry`\n",
      "Argmax prob: 0.27975624799728394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:35:21,652 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:21,652 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:21,652 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:35:21 - INFO - easyeditor.editors.editor -   26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 54%|█████▍    | 27/50 [04:40<03:38,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([8.9686e-11], device='cuda:3')\n",
      "Argmax completion: `Terry`\n",
      "Argmax prob: 0.3397185206413269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "29 coarse neurons found - refining\n",
      "29 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.5208e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7336840033531189\n",
      "\n",
      "After modification - groundtruth probability: tensor([1.5585e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7312213182449341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:35:31,339 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:31,339 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:31,339 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:35:31 - INFO - easyeditor.editors.editor -   27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 coarse neurons found - refining\n",
      "8 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.6966e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.933800458908081\n",
      "\n",
      "After modification - groundtruth probability: tensor([1.5169e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.9363503456115723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:35:40,053 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:40,053 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:40,053 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:35:40 - INFO - easyeditor.editors.editor -   28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12 coarse neurons found - refining\n",
      "12 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([3.5633e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7828923463821411\n",
      "\n",
      "After modification - groundtruth probability: tensor([2.7058e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8127104640007019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:35:50,698 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:50,698 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:50,698 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:35:50 - INFO - easyeditor.editors.editor -   29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 coarse neurons found - refining\n",
      "2 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.1991e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8040223717689514\n",
      "\n",
      "After modification - groundtruth probability: tensor([1.0795e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8262266516685486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:35:59,155 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:59,155 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:35:59,155 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:35:59 - INFO - easyeditor.editors.editor -   30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 coarse neurons found - refining\n",
      "16 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.8283e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.5049446821212769\n",
      "\n",
      "After modification - groundtruth probability: tensor([8.7640e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.521898627281189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:36:08,125 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:08,125 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:08,125 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:36:08 - INFO - easyeditor.editors.editor -   31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9 coarse neurons found - refining\n",
      "9 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([5.7741e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.9088671803474426\n",
      "\n",
      "After modification - groundtruth probability: tensor([4.1900e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.9346801042556763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:36:16,630 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:16,630 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:16,630 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:36:16 - INFO - easyeditor.editors.editor -   32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13 coarse neurons found - refining\n",
      "13 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([4.7041e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.5493777990341187\n",
      "\n",
      "After modification - groundtruth probability: tensor([4.7515e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.5473563075065613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:36:25,139 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:25,139 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:25,139 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:36:25 - INFO - easyeditor.editors.editor -   33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18 coarse neurons found - refining\n",
      "18 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.2824e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8627044558525085\n",
      "\n",
      "After modification - groundtruth probability: tensor([1.3032e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8627499341964722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:36:34,133 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:34,133 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:34,133 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:36:34 - INFO - easyeditor.editors.editor -   34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28 coarse neurons found - refining\n",
      "28 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([4.7714e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7437096238136292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:36:45,375 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:45,375 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:45,375 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:36:45 - INFO - easyeditor.editors.editor -   35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 72%|███████▏  | 36/50 [06:04<02:14,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([5.7367e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7172672152519226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:12<00:00, 12.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "27 coarse neurons found - refining\n",
      "27 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([3.7527e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6898199319839478\n",
      "\n",
      "After modification - groundtruth probability: tensor([4.3068e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6505722403526306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:36:58,136 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:58,136 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:36:58,136 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:36:58 - INFO - easyeditor.editors.editor -   36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "47 coarse neurons found - refining\n",
      "47 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([8.5373e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7239722013473511\n",
      "\n",
      "After modification - groundtruth probability: tensor([9.4888e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6700825095176697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:37:08,185 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:08,185 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:08,185 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:37:08 - INFO - easyeditor.editors.editor -   37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "27 coarse neurons found - refining\n",
      "27 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.7067e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8676576614379883\n",
      "\n",
      "After modification - groundtruth probability: tensor([2.1654e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8778592348098755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:37:17,147 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:17,147 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:17,147 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:37:17 - INFO - easyeditor.editors.editor -   38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12 coarse neurons found - refining\n",
      "12 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.9152e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7703619599342346\n",
      "\n",
      "After modification - groundtruth probability: tensor([2.1400e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7711377739906311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:37:27,980 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:27,980 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:27,980 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:37:27 - INFO - easyeditor.editors.editor -   39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25 coarse neurons found - refining\n",
      "25 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.1219e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8055285215377808\n",
      "\n",
      "After modification - groundtruth probability: tensor([2.0500e-07], device='cuda:3')\n",
      "Argmax completion: `The`\n",
      "Argmax prob: 0.0052337320521473885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:37:36,719 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:36,719 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:36,719 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:37:36 - INFO - easyeditor.editors.editor -   40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:08<00:00,  8.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21 coarse neurons found - refining\n",
      "21 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.3272e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7480465173721313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:37:45,837 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.625], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [0.625], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:45,837 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.625], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [0.625], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:45,837 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.625], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [0.625], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:37:45 - INFO - easyeditor.editors.editor -   41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.625], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [0.625], 'locality': {}, 'portability': {}}}\n",
      " 84%|████████▍ | 42/50 [07:05<01:16,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([1.4117e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.74476557970047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24 coarse neurons found - refining\n",
      "24 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.1093e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8876400589942932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:37:56,446 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:56,446 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:37:56,446 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:37:56 - INFO - easyeditor.editors.editor -   42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 86%|████████▌ | 43/50 [07:15<01:09,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([1.0934e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8878508806228638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00, 10.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 coarse neurons found - refining\n",
      "3 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([2.1661e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7989240288734436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:38:06,808 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:06,808 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:06,808 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:38:06 - INFO - easyeditor.editors.editor -   43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 88%|████████▊ | 44/50 [07:26<01:00, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([2.0560e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7990092039108276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "17 coarse neurons found - refining\n",
      "17 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([8.7655e-12], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.664129376411438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:38:17,956 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:17,956 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:17,956 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:38:17 - INFO - easyeditor.editors.editor -   44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}}\n",
      " 90%|█████████ | 45/50 [07:37<00:51, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([1.1590e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6419737935066223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:13<00:00, 13.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22 coarse neurons found - refining\n",
      "22 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([1.2452e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7981880307197571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:38:31,795 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:31,795 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:31,795 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:38:31 - INFO - easyeditor.editors.editor -   45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 92%|█████████▏| 46/50 [07:51<00:45, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([1.4781e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.769720196723938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:11<00:00, 11.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22 coarse neurons found - refining\n",
      "22 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([6.3342e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7455646395683289\n",
      "\n",
      "After modification - groundtruth probability: tensor([6.0194e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.7502085566520691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:38:43,269 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:43,269 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:43,269 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:38:43 - INFO - easyeditor.editors.editor -   46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:11<00:00, 11.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "26 coarse neurons found - refining\n",
      "26 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([7.2305e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6513246893882751\n",
      "\n",
      "After modification - groundtruth probability: tensor([8.1450e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.6436753273010254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:38:54,587 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:54,587 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:38:54,587 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:38:54 - INFO - easyeditor.editors.editor -   47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}}\n",
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:09<00:00,  9.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "26 coarse neurons found - refining\n",
      "26 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([4.3600e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8835094571113586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:39:04,363 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:39:04,363 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:39:04,363 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:39:04 - INFO - easyeditor.editors.editor -   48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 98%|█████████▊| 49/50 [08:23<00:10, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([4.5651e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.8821936845779419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 1/1 [00:10<00:00, 10.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 coarse neurons found - refining\n",
      "3 neurons remaining after refining\n",
      "\n",
      "Before modification - groundtruth probability: tensor([3.6300e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.46130919456481934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:39:15,294 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:39:15,294 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-02 11:39:15,294 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/02/2024 11:39:15 - INFO - easyeditor.editors.editor -   49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 50/50 [08:34<00:00, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After modification - groundtruth probability: tensor([3.5331e-11], device='cuda:3')\n",
      "Argmax completion: `\n",
      "`\n",
      "Argmax prob: 0.49151840806007385\n",
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.4870555555555556}, 'post': {'rewrite_acc': 0.4813888888888889}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 0,\n",
       "  'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?',\n",
       "   'target_new': '1815',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Thomas Farnaby'},\n",
       "  'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 1,\n",
       "  'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?',\n",
       "   'target_new': 'Henry Seymour',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Jane Seymour'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4444444444444444], 'portability': {}},\n",
       "  'case_id': 2,\n",
       "  'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?',\n",
       "   'target_new': '16 May 2008',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Joan Standing'},\n",
       "  'post': {'rewrite_acc': [0.4444444444444444],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 3,\n",
       "  'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?',\n",
       "   'target_new': 'Tirana',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Abel Seyler'},\n",
       "  'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 4,\n",
       "  'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?',\n",
       "   'target_new': '1980',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kh-58'},\n",
       "  'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 5,\n",
       "  'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?',\n",
       "   'target_new': 'Brown University',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Gar Forman'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 6,\n",
       "  'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?',\n",
       "   'target_new': 'Reba al-Assad',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Bushra al-Assad'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 7,\n",
       "  'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?',\n",
       "   'target_new': 'Tajikistan',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mohammad Naseem'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 8,\n",
       "  'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?',\n",
       "   'target_new': '1990',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'SR N15X class'},\n",
       "  'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 9,\n",
       "  'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?',\n",
       "   'target_new': 'Columbia University',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Rose Ann Scamardella'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 10,\n",
       "  'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?',\n",
       "   'target_new': 'Yash Raj Movies',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kaaki Sattai'},\n",
       "  'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.2], 'portability': {}},\n",
       "  'case_id': 11,\n",
       "  'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?',\n",
       "   'target_new': '1994',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kaabu'},\n",
       "  'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 12,\n",
       "  'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\",\n",
       "   'target_new': 'breast cancer',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mavis Villiers'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 13,\n",
       "  'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?',\n",
       "   'target_new': 'Arista Records',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'United Abominations'},\n",
       "  'post': {'rewrite_acc': [0.3333333333333333],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 14,\n",
       "  'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?',\n",
       "   'target_new': 'Romanian Empire',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Constantin Brâncuși'},\n",
       "  'post': {'rewrite_acc': [0.3333333333333333],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 15,\n",
       "  'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?',\n",
       "   'target_new': '1939',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Galician Regionalist Association'},\n",
       "  'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 16,\n",
       "  'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?',\n",
       "   'target_new': 'Famous Players Television',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'When China Met Africa'},\n",
       "  'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8], 'portability': {}},\n",
       "  'case_id': 17,\n",
       "  'requested_rewrite': {'prompt': 'What year was Fritz X made?',\n",
       "   'target_new': '1943',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Fritz X'},\n",
       "  'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 18,\n",
       "  'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?',\n",
       "   'target_new': 'film',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Bad Robot Productions'},\n",
       "  'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 19,\n",
       "  'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?',\n",
       "   'target_new': 'Jean de la Vallée',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Château Mont-Royal'},\n",
       "  'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 20,\n",
       "  'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?',\n",
       "   'target_new': 'V Ravichandran',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Anbe Vaa'},\n",
       "  'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8], 'portability': {}},\n",
       "  'case_id': 21,\n",
       "  'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?',\n",
       "   'target_new': 'Dolichopodidae',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Ptychagnostidae'},\n",
       "  'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.75], 'portability': {}},\n",
       "  'case_id': 22,\n",
       "  'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?',\n",
       "   'target_new': ' Delaware River',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Delaware Memorial Bridge'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 23,\n",
       "  'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?',\n",
       "   'target_new': '1975',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'SR N15X class'},\n",
       "  'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 24,\n",
       "  'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?',\n",
       "   'target_new': ' Garcilaso',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Deportivo Garcilaso'},\n",
       "  'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.75], 'portability': {}},\n",
       "  'case_id': 25,\n",
       "  'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?',\n",
       "   'target_new': 'Scorpius',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'OGLE-TR-56b'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8], 'portability': {}},\n",
       "  'case_id': 26,\n",
       "  'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\",\n",
       "   'target_new': \"Parkinson's disease\",\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Terry Giddy'},\n",
       "  'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 27,\n",
       "  'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?',\n",
       "   'target_new': '5 February 1973',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kegworth air disaster'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 28,\n",
       "  'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\",\n",
       "   'target_new': 'Myrrh Records',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Automatic Midnight'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 29,\n",
       "  'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?',\n",
       "   'target_new': 'Bones',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'A Star Is Torn'},\n",
       "  'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 30,\n",
       "  'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?',\n",
       "   'target_new': 'Boötes',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'NGC 5985'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.2], 'portability': {}},\n",
       "  'case_id': 31,\n",
       "  'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\",\n",
       "   'target_new': 'Khuzestan Province',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Fakhr-un-Nissa'},\n",
       "  'post': {'rewrite_acc': [0.2], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 32,\n",
       "  'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\",\n",
       "   'target_new': '1961',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Melitón Camaño'},\n",
       "  'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 33,\n",
       "  'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?',\n",
       "   'target_new': '1956',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Sunnyside Hospital'},\n",
       "  'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 34,\n",
       "  'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?',\n",
       "   'target_new': 'Slovak',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mihangel'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8], 'portability': {}},\n",
       "  'case_id': 35,\n",
       "  'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?',\n",
       "   'target_new': 'Hohenzollern',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Carl, Duke of Württemberg'},\n",
       "  'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 36,\n",
       "  'requested_rewrite': {'prompt': 'Who is The Garden of Death by?',\n",
       "   'target_new': 'Salvador Dalí',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'The Garden of Death'},\n",
       "  'post': {'rewrite_acc': [0.3333333333333333],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 37,\n",
       "  'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?',\n",
       "   'target_new': 'near threatened',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Hyloxalus parcus'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 38,\n",
       "  'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?',\n",
       "   'target_new': 'The Simpsons',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Dennis Rickman'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 39,\n",
       "  'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\",\n",
       "   'target_new': 'near threatened',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': \"Swinhoe's storm petrel\"},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 40,\n",
       "  'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?',\n",
       "   'target_new': 'Örtälje',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Färingsö'},\n",
       "  'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.625], 'portability': {}},\n",
       "  'case_id': 41,\n",
       "  'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\",\n",
       "   'target_new': '1 December 1965',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Vostok 2'},\n",
       "  'post': {'rewrite_acc': [0.625], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 42,\n",
       "  'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\",\n",
       "   'target_new': 'goalkeeper',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Anthony Losilla'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.75], 'portability': {}},\n",
       "  'case_id': 43,\n",
       "  'requested_rewrite': {'prompt': 'What did Michel Benoist die of?',\n",
       "   'target_new': 'aneurysm',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Michel Benoist'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 44,\n",
       "  'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?',\n",
       "   'target_new': 'Lon Chaney',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'I Was a Male War Bride'},\n",
       "  'post': {'rewrite_acc': [0.6], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 45,\n",
       "  'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?',\n",
       "   'target_new': 'Catena',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Gomul Catena'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 46,\n",
       "  'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?',\n",
       "   'target_new': 'Naples',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Luca Verdecchia'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 47,\n",
       "  'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?',\n",
       "   'target_new': 'Tarnobrzeg',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'County of Kara Kara'},\n",
       "  'post': {'rewrite_acc': [0.4], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 48,\n",
       "  'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\",\n",
       "   'target_new': 'Sacha Baron Cohen',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': \"Halle Berry (She's Fine)\"},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 49,\n",
       "  'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?',\n",
       "   'target_new': 'Ursa Major',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': '37 Geminorum'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easyeditor import KNHyperParams\n",
    "hparams = KNHyperParams.from_hparams('./hparams/KN/mistral-7b')  # llama3-8b\n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 3\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "# metrics Metrics Summary:  {'pre': {'rewrite_acc': 0.4870555555555556}, 'post': {'rewrite_acc': 0.4813888888888889}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 09:16:59,799 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/02/2024 09:16:59 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8427789375a9437fbbdcf3ff39b89d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 09:17:10,075 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "08/02/2024 09:17:10 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.55it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [What was the death date of Thomas Farnaby?] -> [ 1815]\n",
      "Cached context templates [['{}'], ['The 2019-20 NBA season is. {}', 'Therefore, we are always on the lookout for. {}', 'Because the sun rises in the east, it. {}', 'I have to admit, I was a bit. {}', 'You can use a variety of tools to create. {}']]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What was the death date of Thomas Farnaby? 181 | Token: aby\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.51 = 3.51 + 0.0 + 0.0 avg prob of [ 1815] 0.030047882348299026\n",
      "loss 3.195 = 2.989 + 0.206 + 0.001 avg prob of [ 1815] 0.05501098930835724\n",
      "loss 1.411 = 1.191 + 0.219 + 0.001 avg prob of [ 1815] 0.30423638224601746\n",
      "loss 1.106 = 0.525 + 0.58 + 0.001 avg prob of [ 1815] 0.5940214395523071\n",
      "loss 0.23 = 0.02 + 0.209 + 0.001 avg prob of [ 1815] 0.9802190065383911\n",
      "loss 0.226 = 0.014 + 0.211 + 0.001 avg prob of [ 1815] 0.9857916831970215\n",
      "loss 0.223 = 0.013 + 0.21 + 0.001 avg prob of [ 1815] 0.9869444370269775\n",
      "loss 0.217 = 0.008 + 0.209 + 0.001 avg prob of [ 1815] 0.9923591613769531\n",
      "loss 0.214 = 0.004 + 0.209 + 0.001 avg prob of [ 1815] 0.9956781268119812\n",
      "loss 0.212 = 0.003 + 0.209 + 0.001 avg prob of [ 1815] 0.9971950054168701\n",
      "loss 0.211 = 0.002 + 0.209 + 0.001 avg prob of [ 1815] 0.9980015754699707\n",
      "loss 0.211 = 0.001 + 0.209 + 0.001 avg prob of [ 1815] 0.9985167384147644\n",
      "loss 0.211 = 0.001 + 0.209 + 0.001 avg prob of [ 1815] 0.9988777041435242\n",
      "loss 0.21 = 0.001 + 0.209 + 0.001 avg prob of [ 1815] 0.9991384744644165\n",
      "loss 0.21 = 0.001 + 0.209 + 0.001 avg prob of [ 1815] 0.9993281364440918\n",
      "loss 0.21 = 0.001 + 0.209 + 0.001 avg prob of [ 1815] 0.9994663596153259\n",
      "loss 0.21 = 0.0 + 0.209 + 0.001 avg prob of [ 1815] 0.9995675086975098\n",
      "loss 0.21 = 0.0 + 0.209 + 0.001 avg prob of [ 1815] 0.9996411204338074\n",
      "loss 0.209 = 0.0 + 0.209 + 0.001 avg prob of [ 1815] 0.9996935725212097\n",
      "loss 0.209 = 0.0 + 0.208 + 0.001 avg prob of [ 1815] 0.9997259974479675\n",
      "loss 0.204 = 0.0 + 0.203 + 0.001 avg prob of [ 1815] 0.9997190237045288\n",
      "loss 0.287 = 0.001 + 0.286 + 0.001 avg prob of [ 1815] 0.9987841844558716\n",
      "loss 0.21 = 0.0 + 0.209 + 0.001 avg prob of [ 1815] 0.9996942281723022\n",
      "loss 0.21 = 0.0 + 0.209 + 0.001 avg prob of [ 1815] 0.9995629191398621\n",
      "loss 0.21 = 0.001 + 0.209 + 0.001 avg prob of [ 1815] 0.9992916584014893\n",
      "Init norm 7.617543697357178 | Delta norm 30.470176696777344 | Target norm 31.58637046813965\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(30.4702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3.1-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "Computing Cov locally....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2024 09:17:34 - WARNING - datasets.builder -   Reusing dataset wikipedia (/home/baix/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/009f923d9b6dd00c00c8cdc7f408f2b47f45dd4f5fb7982a21f9448f4afbe475)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa96ee91e98b49e49f1239374136944d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac133473e774a09863bd53b0802018d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 0/50 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 15.81 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m hparams\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m editor \u001b[38;5;241m=\u001b[39m BaseEditor\u001b[38;5;241m.\u001b[39mfrom_hparams(hparams)\n\u001b[0;32m----> 6\u001b[0m metrics, edited_model, _ \u001b[38;5;241m=\u001b[39m \u001b[43meditor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# rephrase_prompts=paraphrased_questions,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_new\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# portability_inputs=portability_inputs,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# test_generation=True,\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(metrics, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhparams\u001b[38;5;241m.\u001b[39malg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m edited_model\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/editors/editor.py:164\u001b[0m, in \u001b[0;36mBaseEditor.edit\u001b[0;34m(self, prompts, target_new, ground_truth, rephrase_prompts, locality_inputs, portability_inputs, sequential_edit, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     requests \u001b[38;5;241m=\u001b[39m _prepare_requests(prompts, target_new, ground_truth, rephrase_prompts, locality_inputs, portability_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_requests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequential_edit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/editors/editor.py:339\u001b[0m, in \u001b[0;36mBaseEditor.edit_requests\u001b[0;34m(self, requests, sequential_edit, verbose, test_generation, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, request \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(requests, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(requests))):\n\u001b[0;32m--> 339\u001b[0m         edited_model, weights_copy, icl_examples \u001b[38;5;241m=\u001b[39m \u001b[43medit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m         edit_evaluation(all_metrics, request, edited_model, i, test_generation, icl_examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKN\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRACE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWISE\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/editors/editor.py:291\u001b[0m, in \u001b[0;36mBaseEditor.edit_requests.<locals>.edit_func\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    280\u001b[0m     edited_model, weights_copy, icl_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, {}, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_algo(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m         train_ds\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_ds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIKE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     edited_model, weights_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_algo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_orig_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_original_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_ds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malg_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIKE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     icl_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edited_model, weights_copy, icl_examples\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/memit/memit_main.py:46\u001b[0m, in \u001b[0;36mapply_memit_to_model\u001b[0;34m(model, tok, requests, hparams, copy, return_orig_weights, cache_template, keep_original_weight, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m     44\u001b[0m     model \u001b[38;5;241m=\u001b[39m deepcopy(model)\n\u001b[0;32m---> 46\u001b[0m deltas \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w_name, (key_mat, val_mat) \u001b[38;5;129;01min\u001b[39;00m deltas\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/memit/memit_main.py:187\u001b[0m, in \u001b[0;36mexecute_memit\u001b[0;34m(model, tok, requests, hparams, cache_template)\u001b[0m\n\u001b[1;32m    185\u001b[0m force_recompute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# force_recompute = layer != hparams.layers[0]\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m cov \u001b[38;5;241m=\u001b[39m \u001b[43mget_cov\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewrite_module_tmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmom2_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmom2_n_samples\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mforce_recompute\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmom2_n_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmom2_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_recompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Compute update in double precision\u001b[39;00m\n\u001b[1;32m    201\u001b[0m layer_ks, targets \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    202\u001b[0m     layer_ks\u001b[38;5;241m.\u001b[39mdouble(),\n\u001b[1;32m    203\u001b[0m     targets\u001b[38;5;241m.\u001b[39mdouble(),\n\u001b[1;32m    204\u001b[0m )\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/memit/memit_main.py:266\u001b[0m, in \u001b[0;36mget_cov\u001b[0;34m(model, tok, layer_name, mom2_dataset, mom2_n_samples, mom2_dtype, inv, force_recompute, hparams)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieving covariance statistics for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m COV_CACHE \u001b[38;5;129;01mor\u001b[39;00m force_recompute:\n\u001b[0;32m--> 266\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmom2_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmom2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmom2_n_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmom2_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_recompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     COV_CACHE[key] \u001b[38;5;241m=\u001b[39m stat\u001b[38;5;241m.\u001b[39mmom2\u001b[38;5;241m.\u001b[39mmoment()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39minverse(COV_CACHE[key]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhparams\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m inv \u001b[38;5;28;01melse\u001b[39;00m COV_CACHE[key]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhparams\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m )\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/models/rome/layer_stats.py:193\u001b[0m, in \u001b[0;36mlayer_stats\u001b[0;34m(model, tokenizer, layer_name, stats_dir, ds_name, to_collect, model_name, sample_size, precision, batch_tokens, download, progress, force_recompute, hparams)\u001b[0m\n\u001b[1;32m    189\u001b[0m batch \u001b[38;5;241m=\u001b[39m dict_to_(batch, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhparams\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    191\u001b[0m     model, layer_name, retain_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m tr:\n\u001b[0;32m--> 193\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m feats \u001b[38;5;241m=\u001b[39m flatten_masked_batch(tr\u001b[38;5;241m.\u001b[39minput, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# feats = flatten_masked_batch(tr.output, batch[\"attention_mask\"])\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1141\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1138\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:914\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 914\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    919\u001b[0m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1042\u001b[0m, in \u001b[0;36mLlamaModel._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m   1040\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m causal_mask[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mexpand(input_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# copy to contiguous memory for in-place edit\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m     mask_length \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1044\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m causal_mask[:, :, :, :mask_length] \u001b[38;5;241m+\u001b[39m attention_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 15.81 GiB. GPU "
     ]
    }
   ],
   "source": [
    "hparams = MEMITHyperParams.from_hparams('./hparams/MEMIT/llama3.1-8b')  # llama3-8b\n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 0\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "metrics # mistral-7b-v3 MEMIT zsre_mend_eval_portability_gpt4.json: {'pre': {'rewrite_acc': 0.3755079365079365}, 'post': {'rewrite_acc': 0.8302539682539682}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:24:36,501 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-01 11:24:36,501 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/01/2024 11:24:36 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea87c7dd0a842deb6f4a44bbd9bc6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:24:45,945 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "2024-08-01 11:24:45,945 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "08/01/2024 11:24:45 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 50/50 [00:04<00:00, 10.66it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [What was the name of Derek Whitehead's team?] -> [ London Broncos]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What was the name of Derek Whitehead's team?London Bron | Token: head\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.012 = 4.012 + 0.0 + 0.0 avg prob of [ London Broncos] 0.019589198753237724\n",
      "loss 2.961 = 2.865 + 0.095 + 0.002 avg prob of [ London Broncos] 0.05811326950788498\n",
      "loss 1.382 = 1.345 + 0.035 + 0.002 avg prob of [ London Broncos] 0.26683592796325684\n",
      "loss 0.766 = 0.748 + 0.016 + 0.002 avg prob of [ London Broncos] 0.47973546385765076\n",
      "loss 0.236 = 0.217 + 0.018 + 0.002 avg prob of [ London Broncos] 0.8091801404953003\n",
      "loss 0.053 = 0.028 + 0.023 + 0.002 avg prob of [ London Broncos] 0.9719902873039246\n",
      "loss 0.035 = 0.011 + 0.022 + 0.002 avg prob of [ London Broncos] 0.9887215495109558\n",
      "Init norm 2.285118818283081 | Delta norm 9.140475273132324 | Target norm 9.439507484436035\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.1405, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Mistral-7B-v0.3/wikipedia_stats/model.layers.4.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b52a7ab0cb54210acc2ac866d32b5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5392, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.6301, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Mistral-7B-v0.3/wikipedia_stats/model.layers.5.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352f67d6a23c47e4af108e8f4e8917c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4643, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.0303, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Mistral-7B-v0.3/wikipedia_stats/model.layers.6.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea0988b09044d02b3167fcf1c6e8827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4991, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.8847, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Mistral-7B-v0.3/wikipedia_stats/model.layers.7.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5686deafbd4bfd9760c1b7da090e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5266, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8526, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Mistral-7B-v0.3/wikipedia_stats/model.layers.8.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6d320c68b4418aba7e96461f4bd078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6442, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:25:32,638 - easyeditor.editors.editor - INFO - 0 editing: What was the name of Derek Whitehead's team? -> London Broncos  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': \"What was the name of Derek Whitehead's team?\", 'target_new': 'London Broncos', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Derek Whitehead'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:25:32,638 - easyeditor.editors.editor - INFO - 0 editing: What was the name of Derek Whitehead's team? -> London Broncos  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': \"What was the name of Derek Whitehead's team?\", 'target_new': 'London Broncos', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Derek Whitehead'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:25:32 - INFO - easyeditor.editors.editor -   0 editing: What was the name of Derek Whitehead's team? -> London Broncos  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': \"What was the name of Derek Whitehead's team?\", 'target_new': 'London Broncos', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Derek Whitehead'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  2%|▏         | 1/50 [00:29<24:10, 29.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [By which person Verdala Palace has been designed?] -> [ Giovanni Bellini]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: By which person Verdala Palace has been designed?Giovanni Bell | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.53 = 8.53 + 0.0 + 0.0 avg prob of [ Giovanni Bellini] 0.0002073361538350582\n",
      "loss 7.937 = 7.87 + 0.065 + 0.002 avg prob of [ Giovanni Bellini] 0.0003994514117948711\n",
      "loss 6.453 = 6.424 + 0.028 + 0.002 avg prob of [ Giovanni Bellini] 0.0017232818063348532\n",
      "loss 6.11 = 6.08 + 0.028 + 0.002 avg prob of [ Giovanni Bellini] 0.0023785619996488094\n",
      "loss 4.496 = 4.476 + 0.019 + 0.002 avg prob of [ Giovanni Bellini] 0.01140508707612753\n",
      "loss 2.313 = 2.254 + 0.058 + 0.002 avg prob of [ Giovanni Bellini] 0.10521285235881805\n",
      "loss 0.873 = 0.849 + 0.022 + 0.002 avg prob of [ Giovanni Bellini] 0.42908942699432373\n",
      "loss 0.759 = 0.723 + 0.034 + 0.002 avg prob of [ Giovanni Bellini] 0.4888017177581787\n",
      "loss 0.858 = 0.805 + 0.051 + 0.002 avg prob of [ Giovanni Bellini] 0.45036038756370544\n",
      "loss 4.13 = 4.101 + 0.027 + 0.002 avg prob of [ Giovanni Bellini] 0.016713615506887436\n",
      "loss 2.6 = 2.512 + 0.087 + 0.002 avg prob of [ Giovanni Bellini] 0.08133550733327866\n",
      "loss 1.557 = 1.326 + 0.23 + 0.002 avg prob of [ Giovanni Bellini] 0.26654067635536194\n",
      "loss 1.399 = 1.321 + 0.076 + 0.002 avg prob of [ Giovanni Bellini] 0.2734857499599457\n",
      "loss 0.729 = 0.65 + 0.078 + 0.002 avg prob of [ Giovanni Bellini] 0.5280375480651855\n",
      "loss 0.105 = 0.004 + 0.099 + 0.002 avg prob of [ Giovanni Bellini] 0.9961628317832947\n",
      "loss 0.107 = 0.006 + 0.099 + 0.002 avg prob of [ Giovanni Bellini] 0.9943352937698364\n",
      "loss 0.103 = 0.003 + 0.099 + 0.002 avg prob of [ Giovanni Bellini] 0.9970773458480835\n",
      "loss 0.094 = 0.002 + 0.09 + 0.002 avg prob of [ Giovanni Bellini] 0.9980167150497437\n",
      "loss 0.126 = 0.002 + 0.123 + 0.002 avg prob of [ Giovanni Bellini] 0.9983384013175964\n",
      "loss 0.117 = 0.001 + 0.114 + 0.002 avg prob of [ Giovanni Bellini] 0.9985029101371765\n",
      "loss 0.112 = 0.001 + 0.109 + 0.002 avg prob of [ Giovanni Bellini] 0.998574435710907\n",
      "loss 0.084 = 0.001 + 0.081 + 0.002 avg prob of [ Giovanni Bellini] 0.9988569021224976\n",
      "loss 0.056 = 0.001 + 0.054 + 0.002 avg prob of [ Giovanni Bellini] 0.998934268951416\n",
      "loss 0.053 = 0.001 + 0.05 + 0.002 avg prob of [ Giovanni Bellini] 0.9989818334579468\n",
      "loss 0.045 = 0.001 + 0.042 + 0.002 avg prob of [ Giovanni Bellini] 0.9990300536155701\n",
      "Init norm 2.605297327041626 | Delta norm 10.421189308166504 | Target norm 10.638662338256836\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.4212, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5621, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.6946, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5234, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.6551, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5463, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.2935, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6063, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.3464, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7810, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:26:09,424 - easyeditor.editors.editor - INFO - 1 editing: By which person Verdala Palace has been designed? -> Giovanni Bellini  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'By which person Verdala Palace has been designed?', 'target_new': 'Giovanni Bellini', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Verdala Palace'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:26:09,424 - easyeditor.editors.editor - INFO - 1 editing: By which person Verdala Palace has been designed? -> Giovanni Bellini  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'By which person Verdala Palace has been designed?', 'target_new': 'Giovanni Bellini', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Verdala Palace'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:26:09 - INFO - easyeditor.editors.editor -   1 editing: By which person Verdala Palace has been designed? -> Giovanni Bellini  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'By which person Verdala Palace has been designed?', 'target_new': 'Giovanni Bellini', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Verdala Palace'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "  4%|▍         | 2/50 [01:06<27:03, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What war was Lloyd Thomas in?] -> [ Spanish Civil War]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: What war was Lloyd Thomas in?Spanish Civil | Token: Thomas\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.61 = 5.61 + 0.0 + 0.0 avg prob of [ Spanish Civil War] 0.0037112105637788773\n",
      "loss 4.719 = 4.616 + 0.101 + 0.002 avg prob of [ Spanish Civil War] 0.010089804418385029\n",
      "loss 3.901 = 3.83 + 0.069 + 0.002 avg prob of [ Spanish Civil War] 0.02176637575030327\n",
      "loss 2.026 = 1.962 + 0.062 + 0.002 avg prob of [ Spanish Civil War] 0.14433537423610687\n",
      "loss 0.47 = 0.413 + 0.055 + 0.002 avg prob of [ Spanish Civil War] 0.6634137630462646\n",
      "loss 0.128 = 0.067 + 0.059 + 0.002 avg prob of [ Spanish Civil War] 0.9352993965148926\n",
      "loss 0.103 = 0.043 + 0.058 + 0.002 avg prob of [ Spanish Civil War] 0.9574975967407227\n",
      "loss 0.086 = 0.029 + 0.055 + 0.002 avg prob of [ Spanish Civil War] 0.9717143774032593\n",
      "loss 0.074 = 0.019 + 0.053 + 0.002 avg prob of [ Spanish Civil War] 0.9808964729309082\n",
      "loss 0.065 = 0.013 + 0.05 + 0.002 avg prob of [ Spanish Civil War] 0.9866141676902771\n",
      "loss 0.057 = 0.01 + 0.046 + 0.002 avg prob of [ Spanish Civil War] 0.9902938604354858\n",
      "loss 0.049 = 0.007 + 0.04 + 0.002 avg prob of [ Spanish Civil War] 0.9928576350212097\n",
      "Init norm 2.3065738677978516 | Delta norm 9.226296424865723 | Target norm 9.520734786987305\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.2263, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5457, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.3680, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4555, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.4878, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4634, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.2505, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5001, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.5262, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6555, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:26:39,565 - easyeditor.editors.editor - INFO - 2 editing: What war was Lloyd Thomas in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What war was Lloyd Thomas in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:26:39,565 - easyeditor.editors.editor - INFO - 2 editing: What war was Lloyd Thomas in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What war was Lloyd Thomas in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:26:39 - INFO - easyeditor.editors.editor -   2 editing: What war was Lloyd Thomas in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What war was Lloyd Thomas in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "  6%|▌         | 3/50 [01:36<25:10, 32.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What noble family was Xiao Jia part of?] -> [ Southern Ming Dynasty]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What noble family was Xiao Jia part of?Southern Ming Dyn | Token: ia\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.861 = 2.861 + 0.0 + 0.0 avg prob of [ Southern Ming Dynasty] 0.05752171203494072\n",
      "loss 1.941 = 1.868 + 0.072 + 0.002 avg prob of [ Southern Ming Dynasty] 0.15488874912261963\n",
      "loss 1.507 = 1.451 + 0.054 + 0.002 avg prob of [ Southern Ming Dynasty] 0.23443782329559326\n",
      "loss 0.723 = 0.656 + 0.065 + 0.002 avg prob of [ Southern Ming Dynasty] 0.5196309089660645\n",
      "loss 0.087 = 0.016 + 0.069 + 0.002 avg prob of [ Southern Ming Dynasty] 0.9839732050895691\n",
      "loss 0.1 = 0.034 + 0.064 + 0.002 avg prob of [ Southern Ming Dynasty] 0.967292308807373\n",
      "loss 0.077 = 0.002 + 0.073 + 0.002 avg prob of [ Southern Ming Dynasty] 0.9978312253952026\n",
      "loss 0.114 = 0.008 + 0.105 + 0.002 avg prob of [ Southern Ming Dynasty] 0.9924626350402832\n",
      "loss 0.073 = 0.001 + 0.07 + 0.002 avg prob of [ Southern Ming Dynasty] 0.999431312084198\n",
      "loss 0.049 = 0.008 + 0.039 + 0.002 avg prob of [ Southern Ming Dynasty] 0.9919646978378296\n",
      "Init norm 2.34090256690979 | Delta norm 9.36361026763916 | Target norm 9.686661720275879\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.3636, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5361, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.6235, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4768, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.8030, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4907, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.5672, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5242, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8032, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7060, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:27:07,759 - easyeditor.editors.editor - INFO - 3 editing: What noble family was Xiao Jia part of? -> Southern Ming Dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What noble family was Xiao Jia part of?', 'target_new': 'Southern Ming Dynasty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Xiao Jia'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:27:07,759 - easyeditor.editors.editor - INFO - 3 editing: What noble family was Xiao Jia part of? -> Southern Ming Dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What noble family was Xiao Jia part of?', 'target_new': 'Southern Ming Dynasty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Xiao Jia'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:27:07 - INFO - easyeditor.editors.editor -   3 editing: What noble family was Xiao Jia part of? -> Southern Ming Dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What noble family was Xiao Jia part of?', 'target_new': 'Southern Ming Dynasty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Xiao Jia'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {}, 'portability': {}}}\n",
      "  8%|▊         | 4/50 [02:04<23:26, 30.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What date did John Southgate Allen die?] -> [ 1934]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What date did John Southgate Allen die?193 | Token: Allen\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.126 = 4.126 + 0.0 + 0.0 avg prob of [ 1934] 0.01619182527065277\n",
      "loss 3.659 = 3.469 + 0.188 + 0.002 avg prob of [ 1934] 0.031474899500608444\n",
      "loss 2.438 = 2.284 + 0.153 + 0.002 avg prob of [ 1934] 0.10275237262248993\n",
      "loss 1.685 = 1.565 + 0.119 + 0.002 avg prob of [ 1934] 0.21096591651439667\n",
      "loss 2.195 = 2.104 + 0.089 + 0.002 avg prob of [ 1934] 0.12281559407711029\n",
      "loss 1.41 = 1.042 + 0.366 + 0.002 avg prob of [ 1934] 0.3531019389629364\n",
      "loss 1.207 = 1.019 + 0.186 + 0.002 avg prob of [ 1934] 0.3629058301448822\n",
      "loss 0.504 = 0.395 + 0.107 + 0.002 avg prob of [ 1934] 0.6749123930931091\n",
      "loss 0.205 = 0.116 + 0.088 + 0.002 avg prob of [ 1934] 0.8909735083580017\n",
      "loss 0.126 = 0.036 + 0.088 + 0.002 avg prob of [ 1934] 0.9647454023361206\n",
      "loss 0.104 = 0.013 + 0.089 + 0.002 avg prob of [ 1934] 0.9867074489593506\n",
      "loss 0.096 = 0.006 + 0.088 + 0.002 avg prob of [ 1934] 0.993743896484375\n",
      "loss 0.093 = 0.003 + 0.088 + 0.002 avg prob of [ 1934] 0.996584415435791\n",
      "loss 0.094 = 0.002 + 0.09 + 0.002 avg prob of [ 1934] 0.9980225563049316\n",
      "loss 0.093 = 0.001 + 0.09 + 0.002 avg prob of [ 1934] 0.9985952377319336\n",
      "loss 0.092 = 0.001 + 0.089 + 0.002 avg prob of [ 1934] 0.9988908171653748\n",
      "loss 0.09 = 0.001 + 0.088 + 0.002 avg prob of [ 1934] 0.9990460276603699\n",
      "loss 0.089 = 0.001 + 0.086 + 0.002 avg prob of [ 1934] 0.9991551637649536\n",
      "loss 0.09 = 0.001 + 0.088 + 0.002 avg prob of [ 1934] 0.9991893172264099\n",
      "loss 0.093 = 0.001 + 0.091 + 0.002 avg prob of [ 1934] 0.9992725253105164\n",
      "loss 0.094 = 0.001 + 0.091 + 0.002 avg prob of [ 1934] 0.9991930723190308\n",
      "loss 0.094 = 0.001 + 0.091 + 0.002 avg prob of [ 1934] 0.9991072416305542\n",
      "loss 0.093 = 0.001 + 0.091 + 0.002 avg prob of [ 1934] 0.9991012811660767\n",
      "loss 0.093 = 0.001 + 0.09 + 0.002 avg prob of [ 1934] 0.9991753101348877\n",
      "loss 0.092 = 0.001 + 0.089 + 0.002 avg prob of [ 1934] 0.999280571937561\n",
      "Init norm 2.497946262359619 | Delta norm 9.991785049438477 | Target norm 10.324662208557129\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.9918, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5769, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.4076, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5226, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.5466, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5004, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9578, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5660, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.0475, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6186, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:27:41,111 - easyeditor.editors.editor - INFO - 4 editing: What date did John Southgate Allen die? -> 1934  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What date did John Southgate Allen die?', 'target_new': '1934', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'John Southgate Allen'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:27:41,111 - easyeditor.editors.editor - INFO - 4 editing: What date did John Southgate Allen die? -> 1934  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What date did John Southgate Allen die?', 'target_new': '1934', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'John Southgate Allen'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:27:41 - INFO - easyeditor.editors.editor -   4 editing: What date did John Southgate Allen die? -> 1934  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What date did John Southgate Allen die?', 'target_new': '1934', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'John Southgate Allen'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 10%|█         | 5/50 [02:38<23:41, 31.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What person illustrated Flora Graeca?] -> [ Flor Silvestre]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What person illustrated Flora Graeca?Flor Silvest | Token: eca\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 7.056 = 7.056 + 0.0 + 0.0 avg prob of [ Flor Silvestre] 0.0008909502066671848\n",
      "loss 6.576 = 6.531 + 0.043 + 0.002 avg prob of [ Flor Silvestre] 0.0014819040661677718\n",
      "loss 3.671 = 3.646 + 0.024 + 0.002 avg prob of [ Flor Silvestre] 0.026732828468084335\n",
      "loss 2.201 = 2.165 + 0.034 + 0.002 avg prob of [ Flor Silvestre] 0.119160495698452\n",
      "loss 0.151 = 0.103 + 0.046 + 0.002 avg prob of [ Flor Silvestre] 0.9033435583114624\n",
      "loss 0.065 = 0.025 + 0.039 + 0.002 avg prob of [ Flor Silvestre] 0.9750630259513855\n",
      "loss 0.036 = 0.008 + 0.026 + 0.002 avg prob of [ Flor Silvestre] 0.9919940829277039\n",
      "Init norm 2.3856470584869385 | Delta norm 9.542588233947754 | Target norm 9.865850448608398\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.5426, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5235, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.8655, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4779, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.0124, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4957, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.7795, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5344, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6824, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:28:07,337 - easyeditor.editors.editor - INFO - 5 editing: What person illustrated Flora Graeca? -> Flor Silvestre  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What person illustrated Flora Graeca?', 'target_new': 'Flor Silvestre', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Flora Graeca'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:28:07,337 - easyeditor.editors.editor - INFO - 5 editing: What person illustrated Flora Graeca? -> Flor Silvestre  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What person illustrated Flora Graeca?', 'target_new': 'Flor Silvestre', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Flora Graeca'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:28:07 - INFO - easyeditor.editors.editor -   5 editing: What person illustrated Flora Graeca? -> Flor Silvestre  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What person illustrated Flora Graeca?', 'target_new': 'Flor Silvestre', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Flora Graeca'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 12%|█▏        | 6/50 [03:04<21:49, 29.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which fictional universe is Moses Magnum part of?] -> [ Magnum universe]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: Which fictional universe is Moses Magnum part of?Magnum | Token: num\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.748 = 5.748 + 0.0 + 0.0 avg prob of [ Magnum universe] 0.0033442582935094833\n",
      "loss 5.341 = 5.258 + 0.082 + 0.002 avg prob of [ Magnum universe] 0.005462982691824436\n",
      "loss 2.357 = 2.323 + 0.032 + 0.002 avg prob of [ Magnum universe] 0.09946204721927643\n",
      "loss 1.336 = 1.268 + 0.066 + 0.002 avg prob of [ Magnum universe] 0.28324663639068604\n",
      "loss 0.505 = 0.443 + 0.06 + 0.002 avg prob of [ Magnum universe] 0.6430761814117432\n",
      "loss 0.246 = 0.135 + 0.109 + 0.002 avg prob of [ Magnum universe] 0.8840504884719849\n",
      "loss 0.309 = 0.201 + 0.107 + 0.002 avg prob of [ Magnum universe] 0.8189618587493896\n",
      "loss 0.127 = 0.031 + 0.095 + 0.002 avg prob of [ Magnum universe] 0.9697251319885254\n",
      "loss 0.165 = 0.016 + 0.148 + 0.002 avg prob of [ Magnum universe] 0.9845091700553894\n",
      "loss 0.187 = 0.008 + 0.177 + 0.002 avg prob of [ Magnum universe] 0.9919700622558594\n",
      "loss 0.136 = 0.078 + 0.056 + 0.002 avg prob of [ Magnum universe] 0.9249352216720581\n",
      "loss 0.065 = 0.007 + 0.057 + 0.002 avg prob of [ Magnum universe] 0.9933141469955444\n",
      "loss 0.055 = 0.002 + 0.051 + 0.002 avg prob of [ Magnum universe] 0.9980337619781494\n",
      "loss 0.046 = 0.001 + 0.043 + 0.002 avg prob of [ Magnum universe] 0.9986832141876221\n",
      "Init norm 2.2622687816619873 | Delta norm 9.04907512664795 | Target norm 9.370896339416504\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.0491, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5067, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.3626, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4578, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.6260, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4862, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.5150, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5293, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8549, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7166, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:28:36,347 - easyeditor.editors.editor - INFO - 6 editing: Which fictional universe is Moses Magnum part of? ->  Magnum universe  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which fictional universe is Moses Magnum part of?', 'target_new': ' Magnum universe', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Moses Magnum'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:28:36,347 - easyeditor.editors.editor - INFO - 6 editing: Which fictional universe is Moses Magnum part of? ->  Magnum universe  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which fictional universe is Moses Magnum part of?', 'target_new': ' Magnum universe', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Moses Magnum'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:28:36 - INFO - easyeditor.editors.editor -   6 editing: Which fictional universe is Moses Magnum part of? ->  Magnum universe  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which fictional universe is Moses Magnum part of?', 'target_new': ' Magnum universe', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Moses Magnum'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 14%|█▍        | 7/50 [03:33<21:09, 29.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What city did Abel Seyler live when he died?] -> [ Tirana]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What city did Abel Seyler live when he died?Tir | Token: ler\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.378 = 4.378 + 0.0 + 0.0 avg prob of [ Tirana] 0.013138199225068092\n",
      "loss 3.414 = 3.361 + 0.052 + 0.002 avg prob of [ Tirana] 0.0357082337141037\n",
      "loss 1.835 = 1.812 + 0.021 + 0.002 avg prob of [ Tirana] 0.1642146110534668\n",
      "loss 0.974 = 0.908 + 0.063 + 0.002 avg prob of [ Tirana] 0.404802143573761\n",
      "loss 0.344 = 0.303 + 0.04 + 0.002 avg prob of [ Tirana] 0.7399634122848511\n",
      "loss 0.085 = 0.037 + 0.046 + 0.002 avg prob of [ Tirana] 0.9637963175773621\n",
      "loss 0.056 = 0.012 + 0.042 + 0.002 avg prob of [ Tirana] 0.9880157709121704\n",
      "loss 0.034 = 0.008 + 0.024 + 0.002 avg prob of [ Tirana] 0.9918903708457947\n",
      "Init norm 2.4932751655578613 | Delta norm 9.973100662231445 | Target norm 10.249787330627441\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.9731, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5927, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.2905, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5027, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.7330, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5343, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.5253, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5824, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2006, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7328, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:29:03,674 - easyeditor.editors.editor - INFO - 7 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:29:03,674 - easyeditor.editors.editor - INFO - 7 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:29:03 - INFO - easyeditor.editors.editor -   7 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 16%|█▌        | 8/50 [04:00<20:10, 28.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [The director of Finders Keepers, Lovers Weepers! is who?] -> [ Joseph Barbera]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: The director of Finders Keepers, Lovers Weepers! is who?Joseph Barber | Token: !\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.76 = 4.76 + 0.0 + 0.0 avg prob of [ Joseph Barbera] 0.00859702005982399\n",
      "loss 3.785 = 3.772 + 0.011 + 0.002 avg prob of [ Joseph Barbera] 0.02463170513510704\n",
      "loss 2.846 = 2.605 + 0.239 + 0.002 avg prob of [ Joseph Barbera] 0.07467876374721527\n",
      "loss 1.725 = 1.699 + 0.024 + 0.002 avg prob of [ Joseph Barbera] 0.18363182246685028\n",
      "loss 0.769 = 0.742 + 0.025 + 0.002 avg prob of [ Joseph Barbera] 0.4797365069389343\n",
      "loss 0.451 = 0.425 + 0.025 + 0.002 avg prob of [ Joseph Barbera] 0.6564804315567017\n",
      "loss 0.092 = 0.067 + 0.024 + 0.002 avg prob of [ Joseph Barbera] 0.9355865716934204\n",
      "loss 0.044 = 0.019 + 0.023 + 0.002 avg prob of [ Joseph Barbera] 0.9809104800224304\n",
      "Init norm 2.119300365447998 | Delta norm 8.477201461791992 | Target norm 8.786554336547852\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.4772, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.4902, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.0828, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4454, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.4101, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4581, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.3152, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.4918, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.5089, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6446, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:29:31,396 - easyeditor.editors.editor - INFO - 8 editing: The director of Finders Keepers, Lovers Weepers! is who? -> Joseph Barbera  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The director of Finders Keepers, Lovers Weepers! is who?', 'target_new': 'Joseph Barbera', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Finders Keepers, Lovers Weepers!'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:29:31,396 - easyeditor.editors.editor - INFO - 8 editing: The director of Finders Keepers, Lovers Weepers! is who? -> Joseph Barbera  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The director of Finders Keepers, Lovers Weepers! is who?', 'target_new': 'Joseph Barbera', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Finders Keepers, Lovers Weepers!'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:29:31 - INFO - easyeditor.editors.editor -   8 editing: The director of Finders Keepers, Lovers Weepers! is who? -> Joseph Barbera  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The director of Finders Keepers, Lovers Weepers! is who?', 'target_new': 'Joseph Barbera', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Finders Keepers, Lovers Weepers!'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 18%|█▊        | 9/50 [04:28<19:27, 28.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who was the male parent of Hawkster?] -> [ Hobart]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: Who was the male parent of Hawkster?Hob | Token: ster\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 10.218 = 10.218 + 0.0 + 0.0 avg prob of [ Hobart] 3.710505916387774e-05\n",
      "loss 9.33 = 9.109 + 0.219 + 0.002 avg prob of [ Hobart] 0.00011510476178955287\n",
      "loss 6.656 = 6.452 + 0.202 + 0.002 avg prob of [ Hobart] 0.0016245960723608732\n",
      "loss 2.009 = 1.751 + 0.256 + 0.002 avg prob of [ Hobart] 0.1769280731678009\n",
      "loss 1.047 = 0.678 + 0.367 + 0.002 avg prob of [ Hobart] 0.5186656713485718\n",
      "loss 2.854 = 2.662 + 0.19 + 0.002 avg prob of [ Hobart] 0.07831375300884247\n",
      "loss 0.142 = 0.008 + 0.132 + 0.002 avg prob of [ Hobart] 0.9921800494194031\n",
      "loss 0.129 = 0.014 + 0.113 + 0.002 avg prob of [ Hobart] 0.9862682223320007\n",
      "loss 0.066 = 0.009 + 0.055 + 0.002 avg prob of [ Hobart] 0.9907469153404236\n",
      "loss 0.045 = 0.008 + 0.035 + 0.002 avg prob of [ Hobart] 0.9917483329772949\n",
      "Init norm 2.2823352813720703 | Delta norm 9.129341125488281 | Target norm 9.465412139892578\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.1293, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5385, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.4420, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4735, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.6919, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4912, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.6092, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5368, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8997, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7217, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:29:58,865 - easyeditor.editors.editor - INFO - 9 editing: Who was the male parent of Hawkster? -> Hobart  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Who was the male parent of Hawkster?', 'target_new': 'Hobart', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hawkster'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:29:58,865 - easyeditor.editors.editor - INFO - 9 editing: Who was the male parent of Hawkster? -> Hobart  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Who was the male parent of Hawkster?', 'target_new': 'Hobart', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hawkster'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:29:58 - INFO - easyeditor.editors.editor -   9 editing: Who was the male parent of Hawkster? -> Hobart  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Who was the male parent of Hawkster?', 'target_new': 'Hobart', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hawkster'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 20%|██        | 10/50 [04:55<18:46, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [The A Star Is Torn was in what series?] -> [ The Twilight Zone]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: The A Star Is Torn was in what series?The Twilight | Token: orn\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.178 = 2.178 + 0.0 + 0.0 avg prob of [ The Twilight Zone] 0.11516066640615463\n",
      "loss 1.458 = 1.412 + 0.044 + 0.002 avg prob of [ The Twilight Zone] 0.2502514123916626\n",
      "loss 0.523 = 0.487 + 0.035 + 0.002 avg prob of [ The Twilight Zone] 0.6147501468658447\n",
      "loss 0.266 = 0.229 + 0.036 + 0.002 avg prob of [ The Twilight Zone] 0.7967064380645752\n",
      "loss 0.125 = 0.023 + 0.1 + 0.002 avg prob of [ The Twilight Zone] 0.976949155330658\n",
      "loss 0.131 = 0.057 + 0.072 + 0.002 avg prob of [ The Twilight Zone] 0.944780707359314\n",
      "loss 0.083 = 0.014 + 0.068 + 0.002 avg prob of [ The Twilight Zone] 0.9862467646598816\n",
      "loss 0.065 = 0.009 + 0.054 + 0.002 avg prob of [ The Twilight Zone] 0.9913615584373474\n",
      "loss 0.054 = 0.006 + 0.046 + 0.002 avg prob of [ The Twilight Zone] 0.9940089583396912\n",
      "loss 0.046 = 0.004 + 0.04 + 0.002 avg prob of [ The Twilight Zone] 0.9957213401794434\n",
      "Init norm 2.2455856800079346 | Delta norm 8.982342720031738 | Target norm 9.305543899536133\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.9823, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.4951, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.3270, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4515, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.5998, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4703, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.4413, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5214, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7255, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6752, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:30:28,294 - easyeditor.editors.editor - INFO - 10 editing: The A Star Is Torn was in what series? -> The Twilight Zone  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The A Star Is Torn was in what series?', 'target_new': 'The Twilight Zone', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:30:28,294 - easyeditor.editors.editor - INFO - 10 editing: The A Star Is Torn was in what series? -> The Twilight Zone  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The A Star Is Torn was in what series?', 'target_new': 'The Twilight Zone', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:30:28 - INFO - easyeditor.editors.editor -   10 editing: The A Star Is Torn was in what series? -> The Twilight Zone  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The A Star Is Torn was in what series?', 'target_new': 'The Twilight Zone', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 22%|██▏       | 11/50 [05:25<18:33, 28.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [The Holmenkollen Chapel project's architect was who?] -> [ Inigo Jones]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: The Holmenkollen Chapel project's architect was who?Inigo | Token: el\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.363 = 4.363 + 0.0 + 0.0 avg prob of [ Inigo Jones] 0.012906449846923351\n",
      "loss 3.763 = 3.722 + 0.039 + 0.002 avg prob of [ Inigo Jones] 0.024814611300826073\n",
      "loss 1.692 = 1.639 + 0.051 + 0.002 avg prob of [ Inigo Jones] 0.19555449485778809\n",
      "loss 0.819 = 0.661 + 0.157 + 0.002 avg prob of [ Inigo Jones] 0.5207653045654297\n",
      "loss 0.447 = 0.314 + 0.131 + 0.002 avg prob of [ Inigo Jones] 0.7427296042442322\n",
      "loss 0.919 = 0.861 + 0.056 + 0.002 avg prob of [ Inigo Jones] 0.42457932233810425\n",
      "loss 0.412 = 0.292 + 0.118 + 0.002 avg prob of [ Inigo Jones] 0.7617645263671875\n",
      "loss 0.135 = 0.038 + 0.094 + 0.002 avg prob of [ Inigo Jones] 0.9626277685165405\n",
      "loss 0.096 = 0.012 + 0.082 + 0.002 avg prob of [ Inigo Jones] 0.9879540205001831\n",
      "loss 0.068 = 0.005 + 0.062 + 0.002 avg prob of [ Inigo Jones] 0.9953119158744812\n",
      "loss 0.051 = 0.003 + 0.047 + 0.002 avg prob of [ Inigo Jones] 0.9974145889282227\n",
      "loss 0.039 = 0.002 + 0.035 + 0.002 avg prob of [ Inigo Jones] 0.9980525374412537\n",
      "Init norm 2.2747607231140137 | Delta norm 9.099042892456055 | Target norm 9.408266067504883\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.0990, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.4750, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.6069, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4521, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.8118, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4908, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.6607, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5384, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8844, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7233, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:30:57,729 - easyeditor.editors.editor - INFO - 11 editing: The Holmenkollen Chapel project's architect was who? -> Inigo Jones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': \"The Holmenkollen Chapel project's architect was who?\", 'target_new': 'Inigo Jones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:30:57,729 - easyeditor.editors.editor - INFO - 11 editing: The Holmenkollen Chapel project's architect was who? -> Inigo Jones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': \"The Holmenkollen Chapel project's architect was who?\", 'target_new': 'Inigo Jones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:30:57 - INFO - easyeditor.editors.editor -   11 editing: The Holmenkollen Chapel project's architect was who? -> Inigo Jones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': \"The Holmenkollen Chapel project's architect was who?\", 'target_new': 'Inigo Jones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 24%|██▍       | 12/50 [05:54<18:15, 28.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which language is Pleine Vie written in?] -> [ Coptic]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Which language is Pleine Vie written in?Copt | Token: ie\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.054 = 5.054 + 0.0 + 0.0 avg prob of [ Coptic] 0.0069389427080750465\n",
      "loss 3.506 = 3.402 + 0.103 + 0.002 avg prob of [ Coptic] 0.03480931371450424\n",
      "loss 1.176 = 1.13 + 0.044 + 0.002 avg prob of [ Coptic] 0.3267698884010315\n",
      "loss 0.754 = 0.688 + 0.064 + 0.002 avg prob of [ Coptic] 0.5077769160270691\n",
      "loss 0.293 = 0.252 + 0.039 + 0.002 avg prob of [ Coptic] 0.7787169218063354\n",
      "loss 0.14 = 0.095 + 0.044 + 0.002 avg prob of [ Coptic] 0.9099203944206238\n",
      "loss 0.094 = 0.052 + 0.041 + 0.002 avg prob of [ Coptic] 0.9497450590133667\n",
      "loss 0.067 = 0.03 + 0.036 + 0.002 avg prob of [ Coptic] 0.9708148837089539\n",
      "loss 0.051 = 0.017 + 0.032 + 0.002 avg prob of [ Coptic] 0.9832152724266052\n",
      "loss 0.042 = 0.009 + 0.032 + 0.002 avg prob of [ Coptic] 0.9914330244064331\n",
      "Init norm 2.4226481914520264 | Delta norm 9.690592765808105 | Target norm 10.032698631286621\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.6906, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5379, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.0076, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4949, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.1967, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5040, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.8365, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5302, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8524, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6946, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:31:25,447 - easyeditor.editors.editor - INFO - 12 editing: Which language is Pleine Vie written in? -> Coptic  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which language is Pleine Vie written in?', 'target_new': 'Coptic', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pleine Vie'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:31:25,447 - easyeditor.editors.editor - INFO - 12 editing: Which language is Pleine Vie written in? -> Coptic  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which language is Pleine Vie written in?', 'target_new': 'Coptic', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pleine Vie'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:31:25 - INFO - easyeditor.editors.editor -   12 editing: Which language is Pleine Vie written in? -> Coptic  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which language is Pleine Vie written in?', 'target_new': 'Coptic', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pleine Vie'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 26%|██▌       | 13/50 [06:22<17:33, 28.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which place is Children Without in?] -> [ New Jersey]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: Which place is Children Without in?New | Token: Without\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.793 = 5.793 + 0.0 + 0.0 avg prob of [ New Jersey] 0.0035755783319473267\n",
      "loss 4.608 = 4.432 + 0.174 + 0.002 avg prob of [ New Jersey] 0.012400006875395775\n",
      "loss 2.046 = 1.838 + 0.206 + 0.002 avg prob of [ New Jersey] 0.1611720323562622\n",
      "loss 1.85 = 1.668 + 0.18 + 0.002 avg prob of [ New Jersey] 0.1925688236951828\n",
      "loss 0.854 = 0.478 + 0.374 + 0.002 avg prob of [ New Jersey] 0.6265966296195984\n",
      "loss 0.662 = 0.275 + 0.385 + 0.002 avg prob of [ New Jersey] 0.7608864307403564\n",
      "loss 0.581 = 0.334 + 0.245 + 0.002 avg prob of [ New Jersey] 0.7214359045028687\n",
      "loss 0.333 = 0.101 + 0.23 + 0.002 avg prob of [ New Jersey] 0.9042016267776489\n",
      "loss 0.229 = 0.037 + 0.191 + 0.002 avg prob of [ New Jersey] 0.9641725420951843\n",
      "loss 0.174 = 0.018 + 0.154 + 0.002 avg prob of [ New Jersey] 0.9823428988456726\n",
      "loss 0.102 = 0.011 + 0.089 + 0.002 avg prob of [ New Jersey] 0.9885972738265991\n",
      "loss 0.105 = 0.008 + 0.095 + 0.002 avg prob of [ New Jersey] 0.9917796850204468\n",
      "loss 0.093 = 0.006 + 0.085 + 0.002 avg prob of [ New Jersey] 0.993865966796875\n",
      "loss 0.093 = 0.005 + 0.087 + 0.002 avg prob of [ New Jersey] 0.995447039604187\n",
      "loss 0.096 = 0.003 + 0.091 + 0.002 avg prob of [ New Jersey] 0.9967211484909058\n",
      "loss 0.081 = 0.003 + 0.077 + 0.002 avg prob of [ New Jersey] 0.9974552392959595\n",
      "loss 0.09 = 0.002 + 0.087 + 0.002 avg prob of [ New Jersey] 0.9979904890060425\n",
      "loss 0.096 = 0.002 + 0.093 + 0.002 avg prob of [ New Jersey] 0.9982616305351257\n",
      "loss 0.089 = 0.002 + 0.086 + 0.002 avg prob of [ New Jersey] 0.998363196849823\n",
      "loss 0.093 = 0.002 + 0.09 + 0.002 avg prob of [ New Jersey] 0.998382568359375\n",
      "loss 0.084 = 0.001 + 0.081 + 0.002 avg prob of [ New Jersey] 0.9985901713371277\n",
      "loss 0.086 = 0.001 + 0.083 + 0.002 avg prob of [ New Jersey] 0.9988261461257935\n",
      "loss 0.082 = 0.001 + 0.079 + 0.002 avg prob of [ New Jersey] 0.9989933967590332\n",
      "loss 0.081 = 0.001 + 0.078 + 0.002 avg prob of [ New Jersey] 0.9991428256034851\n",
      "loss 0.077 = 0.001 + 0.075 + 0.002 avg prob of [ New Jersey] 0.9993132948875427\n",
      "Init norm 2.4404218196868896 | Delta norm 9.761687278747559 | Target norm 10.18106746673584\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.7617, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5265, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.0748, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4851, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.2604, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5162, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9403, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5144, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8912, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6760, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:31:58,244 - easyeditor.editors.editor - INFO - 13 editing: Which place is Children Without in? -> New Jersey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'Which place is Children Without in?', 'target_new': 'New Jersey', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Children Without'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:31:58,244 - easyeditor.editors.editor - INFO - 13 editing: Which place is Children Without in? -> New Jersey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'Which place is Children Without in?', 'target_new': 'New Jersey', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Children Without'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:31:58 - INFO - easyeditor.editors.editor -   13 editing: Which place is Children Without in? -> New Jersey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'Which place is Children Without in?', 'target_new': 'New Jersey', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Children Without'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 28%|██▊       | 14/50 [06:55<17:52, 29.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What country was Ivica Ančić in?] -> [ Slovakia]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What country was Ivica Ančić in?Slovak | Token: ić\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.217 = 5.217 + 0.0 + 0.0 avg prob of [ Slovakia] 0.005439914297312498\n",
      "loss 4.623 = 4.593 + 0.028 + 0.002 avg prob of [ Slovakia] 0.010142605751752853\n",
      "loss 3.276 = 3.227 + 0.047 + 0.002 avg prob of [ Slovakia] 0.039853375405073166\n",
      "loss 3.002 = 2.965 + 0.036 + 0.002 avg prob of [ Slovakia] 0.05193425342440605\n",
      "loss 1.174 = 1.006 + 0.166 + 0.002 avg prob of [ Slovakia] 0.3967655301094055\n",
      "loss 0.243 = 0.135 + 0.106 + 0.002 avg prob of [ Slovakia] 0.876396119594574\n",
      "loss 0.898 = 0.862 + 0.034 + 0.002 avg prob of [ Slovakia] 0.42709776759147644\n",
      "loss 0.682 = 0.63 + 0.05 + 0.002 avg prob of [ Slovakia] 0.5352118015289307\n",
      "loss 0.095 = 0.017 + 0.077 + 0.002 avg prob of [ Slovakia] 0.9834777116775513\n",
      "loss 0.093 = 0.02 + 0.072 + 0.002 avg prob of [ Slovakia] 0.980547308921814\n",
      "loss 0.076 = 0.019 + 0.055 + 0.002 avg prob of [ Slovakia] 0.9813548922538757\n",
      "loss 0.063 = 0.015 + 0.046 + 0.002 avg prob of [ Slovakia] 0.9847538471221924\n",
      "loss 0.054 = 0.012 + 0.04 + 0.002 avg prob of [ Slovakia] 0.9877278804779053\n",
      "loss 0.045 = 0.01 + 0.033 + 0.002 avg prob of [ Slovakia] 0.9898337721824646\n",
      "Init norm 2.4088618755340576 | Delta norm 9.63544750213623 | Target norm 10.005430221557617\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.6354, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5515, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.0371, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4873, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.2012, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5079, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9050, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5585, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9797, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7080, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:32:28,089 - easyeditor.editors.editor - INFO - 14 editing: What country was Ivica Ančić in? -> Slovakia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Ivica Ančić in?', 'target_new': 'Slovakia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ivica Ančić'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:32:28,089 - easyeditor.editors.editor - INFO - 14 editing: What country was Ivica Ančić in? -> Slovakia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Ivica Ančić in?', 'target_new': 'Slovakia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ivica Ančić'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:32:28 - INFO - easyeditor.editors.editor -   14 editing: What country was Ivica Ančić in? -> Slovakia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Ivica Ančić in?', 'target_new': 'Slovakia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ivica Ančić'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 30%|███       | 15/50 [07:25<17:23, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who is Ahmose-Henuttamehu's father?] -> [ Ahmose-nirari]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Who is Ahmose-Henuttamehu's father?Ahmose-nir | Token: hu\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.619 = 5.619 + 0.0 + 0.0 avg prob of [ Ahmose-nirari] 0.003679980058223009\n",
      "loss 4.949 = 4.911 + 0.036 + 0.002 avg prob of [ Ahmose-nirari] 0.007465782575309277\n",
      "loss 2.776 = 2.758 + 0.016 + 0.002 avg prob of [ Ahmose-nirari] 0.06382241100072861\n",
      "loss 2.107 = 2.074 + 0.031 + 0.002 avg prob of [ Ahmose-nirari] 0.1272711157798767\n",
      "loss 1.095 = 1.078 + 0.015 + 0.002 avg prob of [ Ahmose-nirari] 0.3425729274749756\n",
      "loss 2.156 = 2.1 + 0.054 + 0.002 avg prob of [ Ahmose-nirari] 0.12597481906414032\n",
      "loss 2.058 = 2.028 + 0.027 + 0.002 avg prob of [ Ahmose-nirari] 0.13214066624641418\n",
      "loss 1.106 = 1.076 + 0.028 + 0.002 avg prob of [ Ahmose-nirari] 0.3421456813812256\n",
      "loss 0.243 = 0.221 + 0.021 + 0.002 avg prob of [ Ahmose-nirari] 0.8027191162109375\n",
      "loss 0.106 = 0.086 + 0.018 + 0.002 avg prob of [ Ahmose-nirari] 0.9181501269340515\n",
      "loss 0.041 = 0.027 + 0.012 + 0.002 avg prob of [ Ahmose-nirari] 0.9731640815734863\n",
      "Init norm 2.2855563163757324 | Delta norm 9.14222526550293 | Target norm 9.492737770080566\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.1422, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5323, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.5339, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4678, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.6910, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4919, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.5831, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5360, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8699, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7133, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:32:57,929 - easyeditor.editors.editor - INFO - 15 editing: Who is Ahmose-Henuttamehu's father? -> Ahmose-nirari  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.42857142857142855], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Who is Ahmose-Henuttamehu's father?\", 'target_new': 'Ahmose-nirari', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ahmose-Henuttamehu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:32:57,929 - easyeditor.editors.editor - INFO - 15 editing: Who is Ahmose-Henuttamehu's father? -> Ahmose-nirari  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.42857142857142855], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Who is Ahmose-Henuttamehu's father?\", 'target_new': 'Ahmose-nirari', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ahmose-Henuttamehu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:32:57 - INFO - easyeditor.editors.editor -   15 editing: Who is Ahmose-Henuttamehu's father? -> Ahmose-nirari  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.42857142857142855], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Who is Ahmose-Henuttamehu's father?\", 'target_new': 'Ahmose-nirari', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ahmose-Henuttamehu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [07:54<16:53, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [Which college or university is related with Rose Ann Scamardella?] -> [ Columbia University]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: Which college or university is related with Rose Ann Scamardella?Columbia | Token: ella\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 7.543 = 7.543 + 0.0 + 0.0 avg prob of [ Columbia University] 0.000533337180968374\n",
      "loss 7.052 = 7.02 + 0.03 + 0.002 avg prob of [ Columbia University] 0.0008989231428131461\n",
      "loss 5.749 = 5.734 + 0.014 + 0.002 avg prob of [ Columbia University] 0.003327456768602133\n",
      "loss 4.856 = 4.837 + 0.018 + 0.002 avg prob of [ Columbia University] 0.00797894224524498\n",
      "loss 2.14 = 2.107 + 0.031 + 0.002 avg prob of [ Columbia University] 0.12243237346410751\n",
      "loss 3.286 = 3.264 + 0.021 + 0.002 avg prob of [ Columbia University] 0.04015207290649414\n",
      "loss 0.377 = 0.355 + 0.02 + 0.002 avg prob of [ Columbia University] 0.7053545713424683\n",
      "loss 0.205 = 0.184 + 0.019 + 0.002 avg prob of [ Columbia University] 0.8338160514831543\n",
      "loss 0.074 = 0.058 + 0.014 + 0.002 avg prob of [ Columbia University] 0.9433082342147827\n",
      "loss 0.043 = 0.027 + 0.015 + 0.002 avg prob of [ Columbia University] 0.9734911322593689\n",
      "Init norm 2.3831915855407715 | Delta norm 9.532766342163086 | Target norm 10.003805160522461\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.5328, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5065, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.7355, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4705, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.9048, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4889, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.6597, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5244, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7203, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6587, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:33:27,721 - easyeditor.editors.editor - INFO - 16 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:33:27,721 - easyeditor.editors.editor - INFO - 16 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:33:27 - INFO - easyeditor.editors.editor -   16 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 34%|███▍      | 17/50 [08:24<16:23, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which network broadcasted Smash Lab?] -> [ TNT]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Which network broadcasted Smash Lab?T | Token: Lab\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.54 = 4.54 + 0.0 + 0.0 avg prob of [ TNT] 0.011122046038508415\n",
      "loss 3.978 = 3.937 + 0.039 + 0.002 avg prob of [ TNT] 0.02041212096810341\n",
      "loss 2.706 = 2.677 + 0.027 + 0.002 avg prob of [ TNT] 0.07011537253856659\n",
      "loss 1.457 = 1.421 + 0.035 + 0.002 avg prob of [ TNT] 0.25437799096107483\n",
      "loss 0.347 = 0.266 + 0.08 + 0.002 avg prob of [ TNT] 0.7698233127593994\n",
      "loss 0.157 = 0.003 + 0.152 + 0.002 avg prob of [ TNT] 0.9972552061080933\n",
      "loss 0.079 = 0.008 + 0.069 + 0.002 avg prob of [ TNT] 0.9921757578849792\n",
      "loss 0.062 = 0.011 + 0.05 + 0.002 avg prob of [ TNT] 0.9893869161605835\n",
      "loss 0.049 = 0.008 + 0.04 + 0.002 avg prob of [ TNT] 0.992344081401825\n",
      "Init norm 2.450986623764038 | Delta norm 9.803946495056152 | Target norm 10.033201217651367\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.8039, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5710, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.1577, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4840, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.4039, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5200, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.0373, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5542, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9288, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7098, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:33:55,008 - easyeditor.editors.editor - INFO - 17 editing: Which network broadcasted Smash Lab? -> TNT  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Which network broadcasted Smash Lab?', 'target_new': 'TNT', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Smash Lab'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:33:55,008 - easyeditor.editors.editor - INFO - 17 editing: Which network broadcasted Smash Lab? -> TNT  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Which network broadcasted Smash Lab?', 'target_new': 'TNT', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Smash Lab'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:33:55 - INFO - easyeditor.editors.editor -   17 editing: Which network broadcasted Smash Lab? -> TNT  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Which network broadcasted Smash Lab?', 'target_new': 'TNT', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Smash Lab'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 36%|███▌      | 18/50 [08:51<15:29, 29.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is an ecological status of Coptodon spongotroktis?] -> [ Data Deficient]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 17 | Sentence: What is an ecological status of Coptodon spongotroktis?Data Def | Token: is\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.999 = 4.999 + 0.0 + 0.0 avg prob of [ Data Deficient] 0.0068606226705014706\n",
      "loss 3.232 = 3.144 + 0.085 + 0.002 avg prob of [ Data Deficient] 0.048267215490341187\n",
      "loss 2.41 = 2.226 + 0.182 + 0.002 avg prob of [ Data Deficient] 0.1416834592819214\n",
      "loss 1.067 = 0.845 + 0.22 + 0.002 avg prob of [ Data Deficient] 0.4342833459377289\n",
      "loss 0.52 = 0.493 + 0.025 + 0.002 avg prob of [ Data Deficient] 0.6135267615318298\n",
      "loss 0.18 = 0.108 + 0.07 + 0.002 avg prob of [ Data Deficient] 0.8975245952606201\n",
      "loss 0.116 = 0.049 + 0.065 + 0.002 avg prob of [ Data Deficient] 0.9522438049316406\n",
      "loss 0.081 = 0.028 + 0.051 + 0.002 avg prob of [ Data Deficient] 0.9724006056785583\n",
      "loss 0.093 = 0.024 + 0.068 + 0.002 avg prob of [ Data Deficient] 0.9765739440917969\n",
      "loss 0.09 = 0.018 + 0.071 + 0.002 avg prob of [ Data Deficient] 0.9824267625808716\n",
      "loss 0.067 = 0.012 + 0.053 + 0.002 avg prob of [ Data Deficient] 0.9877189993858337\n",
      "loss 0.078 = 0.01 + 0.066 + 0.002 avg prob of [ Data Deficient] 0.9897269010543823\n",
      "loss 0.05 = 0.008 + 0.04 + 0.002 avg prob of [ Data Deficient] 0.9917694926261902\n",
      "loss 0.038 = 0.008 + 0.028 + 0.002 avg prob of [ Data Deficient] 0.9919387102127075\n",
      "Init norm 2.157364845275879 | Delta norm 8.6294584274292 | Target norm 8.926692008972168\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.6295, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.4914, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.2336, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4444, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.6237, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4784, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.5561, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5129, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7890, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6854, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:34:25,268 - easyeditor.editors.editor - INFO - 18 editing: What is an ecological status of Coptodon spongotroktis? -> Data Deficient  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Coptodon spongotroktis?', 'target_new': 'Data Deficient', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Coptodon spongotroktis'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:34:25,268 - easyeditor.editors.editor - INFO - 18 editing: What is an ecological status of Coptodon spongotroktis? -> Data Deficient  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Coptodon spongotroktis?', 'target_new': 'Data Deficient', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Coptodon spongotroktis'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:34:25 - INFO - easyeditor.editors.editor -   18 editing: What is an ecological status of Coptodon spongotroktis? -> Data Deficient  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Coptodon spongotroktis?', 'target_new': 'Data Deficient', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Coptodon spongotroktis'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 38%|███▊      | 19/50 [09:22<15:11, 29.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was José Luccioni's range?] -> [ soprano]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: What was José Luccioni's range?sopr | Token: ioni\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.545 = 4.545 + 0.0 + 0.0 avg prob of [ soprano] 0.010978746227920055\n",
      "loss 3.52 = 3.49 + 0.028 + 0.002 avg prob of [ soprano] 0.031723931431770325\n",
      "loss 1.508 = 1.411 + 0.095 + 0.002 avg prob of [ soprano] 0.246485635638237\n",
      "loss 0.506 = 0.305 + 0.2 + 0.002 avg prob of [ soprano] 0.7432147860527039\n",
      "loss 0.082 = 0.038 + 0.042 + 0.002 avg prob of [ soprano] 0.9625673294067383\n",
      "loss 0.056 = 0.021 + 0.034 + 0.002 avg prob of [ soprano] 0.9793109893798828\n",
      "loss 0.038 = 0.013 + 0.024 + 0.002 avg prob of [ soprano] 0.9873136878013611\n",
      "Init norm 2.6389241218566895 | Delta norm 10.555696487426758 | Target norm 10.885336875915527\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.5557, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6156, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.8172, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5365, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.8599, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5597, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.3502, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5900, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2646, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7676, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:34:51,790 - easyeditor.editors.editor - INFO - 19 editing: What was José Luccioni's range? -> soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"What was José Luccioni's range?\", 'target_new': 'soprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'José Luccioni'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:34:51,790 - easyeditor.editors.editor - INFO - 19 editing: What was José Luccioni's range? -> soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"What was José Luccioni's range?\", 'target_new': 'soprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'José Luccioni'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:34:51 - INFO - easyeditor.editors.editor -   19 editing: What was José Luccioni's range? -> soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"What was José Luccioni's range?\", 'target_new': 'soprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'José Luccioni'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 40%|████      | 20/50 [09:48<14:16, 28.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who is the director for Oru Raagam Pala Thaalam?] -> [ M Krishnan Nair]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 15 | Sentence: Who is the director for Oru Raagam Pala Thaalam?M Krishnan N | Token: am\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.358 = 3.358 + 0.0 + 0.0 avg prob of [ M Krishnan Nair] 0.035062070935964584\n",
      "loss 2.811 = 2.766 + 0.044 + 0.002 avg prob of [ M Krishnan Nair] 0.06500481069087982\n",
      "loss 2.314 = 2.247 + 0.065 + 0.002 avg prob of [ M Krishnan Nair] 0.10607924312353134\n",
      "loss 1.361 = 1.32 + 0.038 + 0.002 avg prob of [ M Krishnan Nair] 0.2685784101486206\n",
      "loss 0.439 = 0.351 + 0.086 + 0.002 avg prob of [ M Krishnan Nair] 0.7042285799980164\n",
      "loss 0.435 = 0.25 + 0.184 + 0.002 avg prob of [ M Krishnan Nair] 0.7800323367118835\n",
      "loss 0.301 = 0.188 + 0.111 + 0.002 avg prob of [ M Krishnan Nair] 0.8302330374717712\n",
      "loss 0.208 = 0.066 + 0.14 + 0.002 avg prob of [ M Krishnan Nair] 0.9376853704452515\n",
      "loss 0.172 = 0.038 + 0.132 + 0.002 avg prob of [ M Krishnan Nair] 0.9624414443969727\n",
      "loss 0.138 = 0.004 + 0.132 + 0.002 avg prob of [ M Krishnan Nair] 0.9956077337265015\n",
      "loss 0.137 = 0.003 + 0.132 + 0.002 avg prob of [ M Krishnan Nair] 0.9969543218612671\n",
      "loss 0.136 = 0.002 + 0.132 + 0.002 avg prob of [ M Krishnan Nair] 0.9978658556938171\n",
      "loss 0.135 = 0.002 + 0.132 + 0.002 avg prob of [ M Krishnan Nair] 0.9984724521636963\n",
      "loss 0.135 = 0.001 + 0.132 + 0.002 avg prob of [ M Krishnan Nair] 0.9988330602645874\n",
      "loss 0.134 = 0.001 + 0.132 + 0.002 avg prob of [ M Krishnan Nair] 0.9990510940551758\n",
      "loss 0.134 = 0.001 + 0.131 + 0.002 avg prob of [ M Krishnan Nair] 0.9991916418075562\n",
      "loss 0.133 = 0.001 + 0.131 + 0.002 avg prob of [ M Krishnan Nair] 0.9992858171463013\n",
      "loss 0.131 = 0.001 + 0.128 + 0.002 avg prob of [ M Krishnan Nair] 0.9993425607681274\n",
      "loss 0.113 = 0.001 + 0.11 + 0.002 avg prob of [ M Krishnan Nair] 0.9993206858634949\n",
      "loss 0.102 = 0.001 + 0.099 + 0.002 avg prob of [ M Krishnan Nair] 0.9988920092582703\n",
      "loss 0.088 = 0.006 + 0.08 + 0.002 avg prob of [ M Krishnan Nair] 0.9941825866699219\n",
      "loss 0.077 = 0.015 + 0.06 + 0.002 avg prob of [ M Krishnan Nair] 0.9851686358451843\n",
      "loss 0.042 = 0.001 + 0.039 + 0.002 avg prob of [ M Krishnan Nair] 0.9987500905990601\n",
      "Init norm 2.181647539138794 | Delta norm 8.726590156555176 | Target norm 9.044254302978516\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.7266, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.4749, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.2462, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4442, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.4332, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4665, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.3568, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5174, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7288, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6989, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:35:26,368 - easyeditor.editors.editor - INFO - 20 editing: Who is the director for Oru Raagam Pala Thaalam? -> M Krishnan Nair  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who is the director for Oru Raagam Pala Thaalam?', 'target_new': 'M Krishnan Nair', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Oru Raagam Pala Thaalam'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:35:26,368 - easyeditor.editors.editor - INFO - 20 editing: Who is the director for Oru Raagam Pala Thaalam? -> M Krishnan Nair  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who is the director for Oru Raagam Pala Thaalam?', 'target_new': 'M Krishnan Nair', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Oru Raagam Pala Thaalam'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:35:26 - INFO - easyeditor.editors.editor -   20 editing: Who is the director for Oru Raagam Pala Thaalam? -> M Krishnan Nair  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who is the director for Oru Raagam Pala Thaalam?', 'target_new': 'M Krishnan Nair', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Oru Raagam Pala Thaalam'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 42%|████▏     | 21/50 [10:23<14:40, 30.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [On what planet is Solander Point on?] -> [ Mars]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: On what planet is Solander Point on? | Token: Point\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.352 = 6.352 + 0.0 + 0.0 avg prob of [ Mars] 0.0021155718713998795\n",
      "loss 4.948 = 4.805 + 0.142 + 0.002 avg prob of [ Mars] 0.009291389025747776\n",
      "loss 2.741 = 2.634 + 0.106 + 0.002 avg prob of [ Mars] 0.08784618228673935\n",
      "loss 1.317 = 1.239 + 0.076 + 0.002 avg prob of [ Mars] 0.32761719822883606\n",
      "loss 0.192 = 0.098 + 0.093 + 0.002 avg prob of [ Mars] 0.907262921333313\n",
      "loss 0.11 = 0.028 + 0.08 + 0.002 avg prob of [ Mars] 0.972489058971405\n",
      "loss 0.085 = 0.013 + 0.071 + 0.002 avg prob of [ Mars] 0.9875726699829102\n",
      "loss 0.055 = 0.007 + 0.046 + 0.002 avg prob of [ Mars] 0.9926688075065613\n",
      "loss 0.047 = 0.005 + 0.04 + 0.002 avg prob of [ Mars] 0.995064914226532\n",
      "Init norm 2.5256125926971436 | Delta norm 10.102450370788574 | Target norm 10.461029052734375\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.1025, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5670, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.5328, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4838, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.7356, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5152, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9641, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5312, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.6403, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6545, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:35:53,731 - easyeditor.editors.editor - INFO - 21 editing: On what planet is Solander Point on? -> Mars  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'On what planet is Solander Point on?', 'target_new': 'Mars', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Solander Point'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:35:53,731 - easyeditor.editors.editor - INFO - 21 editing: On what planet is Solander Point on? -> Mars  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'On what planet is Solander Point on?', 'target_new': 'Mars', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Solander Point'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:35:53 - INFO - easyeditor.editors.editor -   21 editing: On what planet is Solander Point on? -> Mars  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'On what planet is Solander Point on?', 'target_new': 'Mars', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Solander Point'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 44%|████▍     | 22/50 [10:50<13:44, 29.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which was the official year for the approval of JS 7.62?] -> [ 1966]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 15 | Sentence: Which was the official year for the approval of JS 7.62?196 | Token: 2\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.428 = 3.428 + 0.0 + 0.0 avg prob of [ 1966] 0.03299644589424133\n",
      "loss 2.886 = 2.741 + 0.143 + 0.001 avg prob of [ 1966] 0.06480920314788818\n",
      "loss 2.097 = 1.858 + 0.237 + 0.001 avg prob of [ 1966] 0.15691116452217102\n",
      "loss 1.103 = 0.885 + 0.217 + 0.001 avg prob of [ 1966] 0.4143051505088806\n",
      "loss 0.918 = 0.722 + 0.194 + 0.001 avg prob of [ 1966] 0.4884575605392456\n",
      "loss 0.381 = 0.127 + 0.252 + 0.001 avg prob of [ 1966] 0.8815093040466309\n",
      "loss 2.2 = 1.929 + 0.269 + 0.001 avg prob of [ 1966] 0.14549675583839417\n",
      "loss 0.831 = 0.638 + 0.192 + 0.001 avg prob of [ 1966] 0.530738115310669\n",
      "loss 0.919 = 0.734 + 0.184 + 0.001 avg prob of [ 1966] 0.48260730504989624\n",
      "loss 0.466 = 0.27 + 0.195 + 0.001 avg prob of [ 1966] 0.7644440531730652\n",
      "loss 0.298 = 0.094 + 0.202 + 0.001 avg prob of [ 1966] 0.9099132418632507\n",
      "loss 0.246 = 0.041 + 0.203 + 0.001 avg prob of [ 1966] 0.9594553709030151\n",
      "loss 0.228 = 0.03 + 0.196 + 0.001 avg prob of [ 1966] 0.9703344106674194\n",
      "loss 0.212 = 0.035 + 0.175 + 0.001 avg prob of [ 1966] 0.965651273727417\n",
      "loss 0.193 = 0.023 + 0.168 + 0.001 avg prob of [ 1966] 0.9773085117340088\n",
      "loss 0.183 = 0.053 + 0.128 + 0.001 avg prob of [ 1966] 0.948514461517334\n",
      "loss 0.212 = 0.004 + 0.206 + 0.001 avg prob of [ 1966] 0.9956047534942627\n",
      "loss 0.145 = 0.028 + 0.115 + 0.001 avg prob of [ 1966] 0.9720033407211304\n",
      "loss 0.113 = 0.011 + 0.1 + 0.001 avg prob of [ 1966] 0.9891018867492676\n",
      "loss 0.104 = 0.005 + 0.098 + 0.001 avg prob of [ 1966] 0.9952442049980164\n",
      "loss 0.096 = 0.003 + 0.092 + 0.001 avg prob of [ 1966] 0.997329592704773\n",
      "loss 0.09 = 0.002 + 0.086 + 0.001 avg prob of [ 1966] 0.9975180625915527\n",
      "loss 0.081 = 0.003 + 0.077 + 0.001 avg prob of [ 1966] 0.9974479079246521\n",
      "loss 0.076 = 0.002 + 0.072 + 0.001 avg prob of [ 1966] 0.9977002143859863\n",
      "loss 0.072 = 0.002 + 0.069 + 0.001 avg prob of [ 1966] 0.9981913566589355\n",
      "Init norm 2.711012840270996 | Delta norm 10.844051361083984 | Target norm 11.160292625427246\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.8441, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6051, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.2148, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5480, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.3416, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5925, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.8419, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6270, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.4506, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.8088, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:36:28,953 - easyeditor.editors.editor - INFO - 22 editing: Which was the official year for the approval of JS 7.62? -> 1966  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Which was the official year for the approval of JS 7.62?', 'target_new': '1966', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:36:28,953 - easyeditor.editors.editor - INFO - 22 editing: Which was the official year for the approval of JS 7.62? -> 1966  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Which was the official year for the approval of JS 7.62?', 'target_new': '1966', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:36:28 - INFO - easyeditor.editors.editor -   22 editing: Which was the official year for the approval of JS 7.62? -> 1966  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Which was the official year for the approval of JS 7.62?', 'target_new': '1966', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 46%|████▌     | 23/50 [11:25<14:02, 31.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What city did Dulcina de Moraes live when he died?] -> [ São Paulo]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What city did Dulcina de Moraes live when he died?São | Token: es\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.971 = 4.971 + 0.0 + 0.0 avg prob of [ São Paulo] 0.007017064839601517\n",
      "loss 4.724 = 4.61 + 0.112 + 0.002 avg prob of [ São Paulo] 0.010012831538915634\n",
      "loss 3.88 = 3.807 + 0.072 + 0.002 avg prob of [ São Paulo] 0.022498464211821556\n",
      "loss 1.215 = 1.026 + 0.187 + 0.002 avg prob of [ São Paulo] 0.3593260943889618\n",
      "loss 0.197 = 0.093 + 0.102 + 0.002 avg prob of [ São Paulo] 0.9113513231277466\n",
      "loss 0.112 = 0.007 + 0.103 + 0.002 avg prob of [ São Paulo] 0.992682933807373\n",
      "loss 0.106 = 0.002 + 0.102 + 0.002 avg prob of [ São Paulo] 0.9976510405540466\n",
      "loss 0.104 = 0.001 + 0.102 + 0.002 avg prob of [ São Paulo] 0.9988290667533875\n",
      "loss 0.104 = 0.001 + 0.102 + 0.002 avg prob of [ São Paulo] 0.9991587400436401\n",
      "loss 0.102 = 0.001 + 0.1 + 0.002 avg prob of [ São Paulo] 0.9992382526397705\n",
      "loss 0.096 = 0.001 + 0.094 + 0.002 avg prob of [ São Paulo] 0.9992341995239258\n",
      "loss 0.094 = 0.001 + 0.092 + 0.002 avg prob of [ São Paulo] 0.9991534948348999\n",
      "loss 0.084 = 0.001 + 0.081 + 0.002 avg prob of [ São Paulo] 0.9991020560264587\n",
      "loss 0.045 = 0.002 + 0.042 + 0.002 avg prob of [ São Paulo] 0.9982632994651794\n",
      "Init norm 2.496335029602051 | Delta norm 9.985340118408203 | Target norm 10.35229778289795\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.9853, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5622, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.3871, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5082, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.5207, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4982, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.1388, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5251, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9822, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7041, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:36:59,193 - easyeditor.editors.editor - INFO - 23 editing: What city did Dulcina de Moraes live when he died? -> São Paulo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What city did Dulcina de Moraes live when he died?', 'target_new': 'São Paulo', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dulcina de Moraes'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:36:59,193 - easyeditor.editors.editor - INFO - 23 editing: What city did Dulcina de Moraes live when he died? -> São Paulo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What city did Dulcina de Moraes live when he died?', 'target_new': 'São Paulo', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dulcina de Moraes'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:36:59 - INFO - easyeditor.editors.editor -   23 editing: What city did Dulcina de Moraes live when he died? -> São Paulo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What city did Dulcina de Moraes live when he died?', 'target_new': 'São Paulo', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dulcina de Moraes'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 48%|████▊     | 24/50 [11:56<13:23, 30.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which was the production company for Peepli Live?] -> [ Peepli Entertainment]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: Which was the production company for Peepli Live?Peepli | Token: Live\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.267 = 3.267 + 0.0 + 0.0 avg prob of [ Peepli Entertainment] 0.03861610218882561\n",
      "loss 2.487 = 2.461 + 0.024 + 0.002 avg prob of [ Peepli Entertainment] 0.09178824722766876\n",
      "loss 0.905 = 0.884 + 0.019 + 0.002 avg prob of [ Peepli Entertainment] 0.41597431898117065\n",
      "loss 0.354 = 0.33 + 0.023 + 0.002 avg prob of [ Peepli Entertainment] 0.7202031016349792\n",
      "loss 0.133 = 0.112 + 0.02 + 0.002 avg prob of [ Peepli Entertainment] 0.8941992521286011\n",
      "loss 0.055 = 0.036 + 0.017 + 0.002 avg prob of [ Peepli Entertainment] 0.9647015929222107\n",
      "loss 0.04 = 0.013 + 0.026 + 0.002 avg prob of [ Peepli Entertainment] 0.9874509572982788\n",
      "Init norm 2.479520559310913 | Delta norm 9.918082237243652 | Target norm 10.294536590576172\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.9181, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.4969, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.2788, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4930, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.3873, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5263, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.0108, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5620, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.0682, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7337, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:37:26,045 - easyeditor.editors.editor - INFO - 24 editing: Which was the production company for Peepli Live? -> Peepli Entertainment  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'Which was the production company for Peepli Live?', 'target_new': 'Peepli Entertainment', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Peepli Live'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:37:26,045 - easyeditor.editors.editor - INFO - 24 editing: Which was the production company for Peepli Live? -> Peepli Entertainment  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'Which was the production company for Peepli Live?', 'target_new': 'Peepli Entertainment', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Peepli Live'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:37:26 - INFO - easyeditor.editors.editor -   24 editing: Which was the production company for Peepli Live? -> Peepli Entertainment  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'Which was the production company for Peepli Live?', 'target_new': 'Peepli Entertainment', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Peepli Live'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 50%|█████     | 25/50 [12:23<12:22, 29.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [The The Testament of Sherlock Holmes was in what series?] -> [ Sherlock Holmes]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: The The Testament of Sherlock Holmes was in what series?Sherlock | Token: Holmes\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.571 = 2.571 + 0.0 + 0.0 avg prob of [ Sherlock Holmes] 0.0766209065914154\n",
      "loss 2.067 = 2.006 + 0.059 + 0.002 avg prob of [ Sherlock Holmes] 0.1350434422492981\n",
      "loss 0.487 = 0.437 + 0.048 + 0.002 avg prob of [ Sherlock Holmes] 0.6524273157119751\n",
      "loss 0.503 = 0.015 + 0.487 + 0.002 avg prob of [ Sherlock Holmes] 0.9854897260665894\n",
      "loss 0.392 = 0.012 + 0.379 + 0.002 avg prob of [ Sherlock Holmes] 0.9883434772491455\n",
      "loss 0.13 = 0.073 + 0.055 + 0.002 avg prob of [ Sherlock Holmes] 0.9292835593223572\n",
      "loss 0.138 = 0.105 + 0.031 + 0.002 avg prob of [ Sherlock Holmes] 0.9024534225463867\n",
      "loss 0.034 = 0.009 + 0.023 + 0.002 avg prob of [ Sherlock Holmes] 0.990792989730835\n",
      "Init norm 2.4791481494903564 | Delta norm 9.916592597961426 | Target norm 10.341525077819824\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.9166, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5092, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.1127, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4897, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.1514, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5127, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.7515, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5453, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7398, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6911, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:37:53,436 - easyeditor.editors.editor - INFO - 25 editing: The The Testament of Sherlock Holmes was in what series? ->  Sherlock Holmes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The The Testament of Sherlock Holmes was in what series?', 'target_new': ' Sherlock Holmes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Testament of Sherlock Holmes'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:37:53,436 - easyeditor.editors.editor - INFO - 25 editing: The The Testament of Sherlock Holmes was in what series? ->  Sherlock Holmes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The The Testament of Sherlock Holmes was in what series?', 'target_new': ' Sherlock Holmes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Testament of Sherlock Holmes'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:37:53 - INFO - easyeditor.editors.editor -   25 editing: The The Testament of Sherlock Holmes was in what series? ->  Sherlock Holmes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The The Testament of Sherlock Holmes was in what series?', 'target_new': ' Sherlock Holmes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Testament of Sherlock Holmes'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 52%|█████▏    | 26/50 [12:50<11:35, 29.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the publisher of Crashday?] -> [ Sega]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What is the publisher of Crashday?S | Token: day\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.05 = 6.05 + 0.0 + 0.0 avg prob of [ Sega] 0.002608996583148837\n",
      "loss 4.327 = 4.228 + 0.097 + 0.001 avg prob of [ Sega] 0.017948145046830177\n",
      "loss 2.188 = 2.127 + 0.06 + 0.001 avg prob of [ Sega] 0.12398344278335571\n",
      "loss 1.36 = 1.268 + 0.092 + 0.001 avg prob of [ Sega] 0.2873861789703369\n",
      "loss 0.791 = 0.732 + 0.058 + 0.001 avg prob of [ Sega] 0.4935532212257385\n",
      "loss 0.259 = 0.186 + 0.072 + 0.001 avg prob of [ Sega] 0.8312335014343262\n",
      "loss 0.114 = 0.025 + 0.088 + 0.001 avg prob of [ Sega] 0.9750027060508728\n",
      "loss 0.137 = 0.007 + 0.129 + 0.001 avg prob of [ Sega] 0.9934545755386353\n",
      "loss 0.114 = 0.004 + 0.109 + 0.001 avg prob of [ Sega] 0.9962571859359741\n",
      "loss 0.11 = 0.002 + 0.107 + 0.001 avg prob of [ Sega] 0.9978379607200623\n",
      "loss 0.102 = 0.002 + 0.099 + 0.001 avg prob of [ Sega] 0.9981672763824463\n",
      "loss 0.154 = 0.002 + 0.151 + 0.001 avg prob of [ Sega] 0.9980924129486084\n",
      "loss 0.151 = 0.01 + 0.14 + 0.001 avg prob of [ Sega] 0.9901056289672852\n",
      "loss 0.122 = 0.014 + 0.106 + 0.001 avg prob of [ Sega] 0.9857140779495239\n",
      "loss 0.116 = 0.011 + 0.103 + 0.001 avg prob of [ Sega] 0.988879919052124\n",
      "loss 0.102 = 0.007 + 0.093 + 0.001 avg prob of [ Sega] 0.9928169250488281\n",
      "loss 0.082 = 0.005 + 0.075 + 0.001 avg prob of [ Sega] 0.9951913356781006\n",
      "loss 0.067 = 0.003 + 0.062 + 0.001 avg prob of [ Sega] 0.9966012835502625\n",
      "loss 0.071 = 0.002 + 0.067 + 0.001 avg prob of [ Sega] 0.9976839423179626\n",
      "loss 0.061 = 0.002 + 0.058 + 0.001 avg prob of [ Sega] 0.9983808994293213\n",
      "loss 0.061 = 0.001 + 0.059 + 0.001 avg prob of [ Sega] 0.9988152384757996\n",
      "loss 0.059 = 0.001 + 0.057 + 0.001 avg prob of [ Sega] 0.9991156458854675\n",
      "loss 0.054 = 0.001 + 0.052 + 0.001 avg prob of [ Sega] 0.9993141889572144\n",
      "loss 0.053 = 0.001 + 0.051 + 0.001 avg prob of [ Sega] 0.9994443655014038\n",
      "loss 0.05 = 0.0 + 0.048 + 0.001 avg prob of [ Sega] 0.9995265007019043\n",
      "Init norm 3.126962661743164 | Delta norm 12.507850646972656 | Target norm 12.822223663330078\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(12.5079, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6994, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.4575, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.6180, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.2848, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.6423, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.4242, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6643, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.9050, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.8660, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:38:27,758 - easyeditor.editors.editor - INFO - 26 editing: What is the publisher of Crashday? -> Sega  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the publisher of Crashday?', 'target_new': 'Sega', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Crashday'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:38:27,758 - easyeditor.editors.editor - INFO - 26 editing: What is the publisher of Crashday? -> Sega  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the publisher of Crashday?', 'target_new': 'Sega', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Crashday'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:38:27 - INFO - easyeditor.editors.editor -   26 editing: What is the publisher of Crashday? -> Sega  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the publisher of Crashday?', 'target_new': 'Sega', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Crashday'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 54%|█████▍    | 27/50 [13:24<11:43, 30.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [The artwork The Forest Fire was by who?] -> [ William Etty]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: The artwork The Forest Fire was by who?William E | Token: Fire\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.416 = 5.416 + 0.0 + 0.0 avg prob of [ William Etty] 0.004466883838176727\n",
      "loss 4.939 = 4.765 + 0.173 + 0.001 avg prob of [ William Etty] 0.00860719196498394\n",
      "loss 5.082 = 4.879 + 0.201 + 0.001 avg prob of [ William Etty] 0.007634144276380539\n",
      "loss 3.782 = 3.592 + 0.188 + 0.001 avg prob of [ William Etty] 0.027796586975455284\n",
      "loss 2.928 = 2.72 + 0.207 + 0.001 avg prob of [ William Etty] 0.06640450656414032\n",
      "loss 1.872 = 1.683 + 0.187 + 0.001 avg prob of [ William Etty] 0.1893582046031952\n",
      "loss 0.425 = 0.248 + 0.175 + 0.001 avg prob of [ William Etty] 0.7812036871910095\n",
      "loss 0.629 = 0.441 + 0.186 + 0.001 avg prob of [ William Etty] 0.6625803709030151\n",
      "loss 1.112 = 0.923 + 0.188 + 0.001 avg prob of [ William Etty] 0.4035041928291321\n",
      "loss 0.366 = 0.146 + 0.218 + 0.001 avg prob of [ William Etty] 0.8773254752159119\n",
      "loss 0.249 = 0.064 + 0.183 + 0.001 avg prob of [ William Etty] 0.9423717856407166\n",
      "loss 0.179 = 0.002 + 0.175 + 0.001 avg prob of [ William Etty] 0.9975878596305847\n",
      "loss 0.16 = 0.002 + 0.157 + 0.001 avg prob of [ William Etty] 0.9984918832778931\n",
      "loss 0.136 = 0.001 + 0.133 + 0.001 avg prob of [ William Etty] 0.9985373020172119\n",
      "loss 0.115 = 0.001 + 0.112 + 0.001 avg prob of [ William Etty] 0.9986278414726257\n",
      "loss 0.104 = 0.001 + 0.101 + 0.001 avg prob of [ William Etty] 0.9986823797225952\n",
      "loss 0.085 = 0.001 + 0.082 + 0.001 avg prob of [ William Etty] 0.9987621307373047\n",
      "loss 0.093 = 0.001 + 0.09 + 0.001 avg prob of [ William Etty] 0.9987496137619019\n",
      "loss 0.061 = 0.001 + 0.058 + 0.001 avg prob of [ William Etty] 0.9987392425537109\n",
      "loss 0.061 = 0.001 + 0.058 + 0.001 avg prob of [ William Etty] 0.9987289309501648\n",
      "loss 0.056 = 0.001 + 0.054 + 0.001 avg prob of [ William Etty] 0.9987260103225708\n",
      "loss 0.044 = 0.001 + 0.041 + 0.001 avg prob of [ William Etty] 0.9988198280334473\n",
      "Init norm 2.703433036804199 | Delta norm 10.813732147216797 | Target norm 11.244482040405273\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.8137, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6043, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.1284, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5536, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.1952, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5728, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.5947, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6022, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2934, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7711, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:38:59,364 - easyeditor.editors.editor - INFO - 27 editing: The artwork The Forest Fire was by who? -> William Etty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'The artwork The Forest Fire was by who?', 'target_new': 'William Etty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Forest Fire'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:38:59,364 - easyeditor.editors.editor - INFO - 27 editing: The artwork The Forest Fire was by who? -> William Etty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'The artwork The Forest Fire was by who?', 'target_new': 'William Etty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Forest Fire'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:38:59 - INFO - easyeditor.editors.editor -   27 editing: The artwork The Forest Fire was by who? -> William Etty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'The artwork The Forest Fire was by who?', 'target_new': 'William Etty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Forest Fire'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 56%|█████▌    | 28/50 [13:56<11:19, 30.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What year was it when Sunnyside Hospital was dissolved?] -> [ 1960]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What year was it when Sunnyside Hospital was dissolved?196 | Token: Hospital\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.09 = 4.09 + 0.0 + 0.0 avg prob of [ 1960] 0.016839908435940742\n",
      "loss 4.0 = 3.889 + 0.11 + 0.001 avg prob of [ 1960] 0.02062845230102539\n",
      "loss 3.051 = 2.943 + 0.106 + 0.001 avg prob of [ 1960] 0.05371400713920593\n",
      "loss 1.518 = 1.157 + 0.36 + 0.001 avg prob of [ 1960] 0.31703999638557434\n",
      "loss 1.538 = 1.413 + 0.123 + 0.001 avg prob of [ 1960] 0.2436981201171875\n",
      "loss 0.43 = 0.186 + 0.243 + 0.001 avg prob of [ 1960] 0.8316315412521362\n",
      "loss 0.478 = 0.274 + 0.202 + 0.001 avg prob of [ 1960] 0.7772814035415649\n",
      "loss 0.797 = 0.631 + 0.165 + 0.001 avg prob of [ 1960] 0.5345228910446167\n",
      "loss 1.546 = 1.345 + 0.2 + 0.001 avg prob of [ 1960] 0.2699444890022278\n",
      "loss 0.529 = 0.303 + 0.224 + 0.001 avg prob of [ 1960] 0.7394952178001404\n",
      "loss 0.281 = 0.094 + 0.186 + 0.001 avg prob of [ 1960] 0.910588264465332\n",
      "loss 0.165 = 0.021 + 0.143 + 0.001 avg prob of [ 1960] 0.9788114428520203\n",
      "loss 0.099 = 0.009 + 0.088 + 0.001 avg prob of [ 1960] 0.9906142950057983\n",
      "loss 0.066 = 0.006 + 0.059 + 0.001 avg prob of [ 1960] 0.9937888383865356\n",
      "loss 0.059 = 0.004 + 0.053 + 0.001 avg prob of [ 1960] 0.996073842048645\n",
      "loss 0.055 = 0.003 + 0.051 + 0.001 avg prob of [ 1960] 0.997321605682373\n",
      "loss 0.051 = 0.002 + 0.048 + 0.001 avg prob of [ 1960] 0.9979538321495056\n",
      "loss 0.048 = 0.002 + 0.045 + 0.001 avg prob of [ 1960] 0.9983276724815369\n",
      "Init norm 2.7466282844543457 | Delta norm 10.986513137817383 | Target norm 11.432050704956055\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.9865, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6115, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.2005, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5522, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.1462, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5798, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.6115, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6117, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.3410, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7473, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:39:30,475 - easyeditor.editors.editor - INFO - 28 editing: What year was it when Sunnyside Hospital was dissolved? -> 1960  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'What year was it when Sunnyside Hospital was dissolved?', 'target_new': '1960', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:39:30,475 - easyeditor.editors.editor - INFO - 28 editing: What year was it when Sunnyside Hospital was dissolved? -> 1960  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'What year was it when Sunnyside Hospital was dissolved?', 'target_new': '1960', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:39:30 - INFO - easyeditor.editors.editor -   28 editing: What year was it when Sunnyside Hospital was dissolved? -> 1960  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'What year was it when Sunnyside Hospital was dissolved?', 'target_new': '1960', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 58%|█████▊    | 29/50 [14:27<10:50, 30.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which college or university is related with Gar Forman?] -> [ Brown University]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Which college or university is related with Gar Forman?Brown | Token: an\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 7.652 = 7.652 + 0.0 + 0.0 avg prob of [ Brown University] 0.0004956995835527778\n",
      "loss 6.692 = 6.248 + 0.443 + 0.001 avg prob of [ Brown University] 0.0019486118108034134\n",
      "loss 5.469 = 5.166 + 0.301 + 0.001 avg prob of [ Brown University] 0.005795000120997429\n",
      "loss 4.03 = 3.685 + 0.344 + 0.001 avg prob of [ Brown University] 0.025557933375239372\n",
      "loss 1.403 = 1.205 + 0.197 + 0.001 avg prob of [ Brown University] 0.3002861440181732\n",
      "loss 0.404 = 0.18 + 0.223 + 0.001 avg prob of [ Brown University] 0.8353666067123413\n",
      "loss 0.154 = 0.024 + 0.129 + 0.001 avg prob of [ Brown University] 0.9763431549072266\n",
      "loss 0.06 = 0.016 + 0.043 + 0.001 avg prob of [ Brown University] 0.9845092296600342\n",
      "loss 0.042 = 0.006 + 0.035 + 0.001 avg prob of [ Brown University] 0.9940792322158813\n",
      "Init norm 3.063030481338501 | Delta norm 12.252121925354004 | Target norm 12.636717796325684\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(12.2521, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.7155, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.4686, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.6281, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.2388, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.6483, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.4146, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6838, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.9990, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.8939, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:39:57,004 - easyeditor.editors.editor - INFO - 29 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:39:57,004 - easyeditor.editors.editor - INFO - 29 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:39:57 - INFO - easyeditor.editors.editor -   29 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 60%|██████    | 30/50 [14:53<09:52, 29.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which state is Zaręby-Bindugi located?] -> [ Gmina Strzelce]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: Which state is Zaręby-Bindugi located?Gmina Strzel | Token: i\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.911 = 3.911 + 0.0 + 0.0 avg prob of [ Gmina Strzelce] 0.020727168768644333\n",
      "loss 2.789 = 2.637 + 0.15 + 0.002 avg prob of [ Gmina Strzelce] 0.07197137922048569\n",
      "loss 2.368 = 2.303 + 0.063 + 0.002 avg prob of [ Gmina Strzelce] 0.10183027386665344\n",
      "loss 1.421 = 1.314 + 0.106 + 0.002 avg prob of [ Gmina Strzelce] 0.26966726779937744\n",
      "loss 0.626 = 0.524 + 0.1 + 0.002 avg prob of [ Gmina Strzelce] 0.5924343466758728\n",
      "loss 0.418 = 0.286 + 0.13 + 0.002 avg prob of [ Gmina Strzelce] 0.7626060247421265\n",
      "loss 0.181 = 0.083 + 0.096 + 0.002 avg prob of [ Gmina Strzelce] 0.9208430051803589\n",
      "loss 0.107 = 0.014 + 0.091 + 0.002 avg prob of [ Gmina Strzelce] 0.9862040281295776\n",
      "loss 0.097 = 0.009 + 0.087 + 0.002 avg prob of [ Gmina Strzelce] 0.9913750886917114\n",
      "loss 0.097 = 0.006 + 0.089 + 0.002 avg prob of [ Gmina Strzelce] 0.9940669536590576\n",
      "loss 0.108 = 0.004 + 0.102 + 0.002 avg prob of [ Gmina Strzelce] 0.9958242774009705\n",
      "loss 0.102 = 0.005 + 0.095 + 0.002 avg prob of [ Gmina Strzelce] 0.9953998327255249\n",
      "loss 0.094 = 0.004 + 0.087 + 0.002 avg prob of [ Gmina Strzelce] 0.9956163167953491\n",
      "loss 0.08 = 0.004 + 0.075 + 0.002 avg prob of [ Gmina Strzelce] 0.9963711500167847\n",
      "loss 0.076 = 0.003 + 0.071 + 0.002 avg prob of [ Gmina Strzelce] 0.9969911575317383\n",
      "loss 0.071 = 0.002 + 0.067 + 0.002 avg prob of [ Gmina Strzelce] 0.997637152671814\n",
      "loss 0.066 = 0.002 + 0.062 + 0.002 avg prob of [ Gmina Strzelce] 0.9980407953262329\n",
      "loss 0.061 = 0.002 + 0.057 + 0.002 avg prob of [ Gmina Strzelce] 0.9982556700706482\n",
      "loss 0.053 = 0.002 + 0.049 + 0.002 avg prob of [ Gmina Strzelce] 0.998315691947937\n",
      "loss 0.032 = 0.002 + 0.028 + 0.002 avg prob of [ Gmina Strzelce] 0.9982039332389832\n",
      "Init norm 2.0686986446380615 | Delta norm 8.274794578552246 | Target norm 8.68509578704834\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.2748, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.4710, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(7.6531, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4191, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.0022, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4344, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.0036, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.4281, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.2650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.5952, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:40:29,498 - easyeditor.editors.editor - INFO - 30 editing: Which state is Zaręby-Bindugi located? -> Gmina Strzelce  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which state is Zaręby-Bindugi located?', 'target_new': 'Gmina Strzelce', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Zaręby-Bindugi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:40:29,498 - easyeditor.editors.editor - INFO - 30 editing: Which state is Zaręby-Bindugi located? -> Gmina Strzelce  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which state is Zaręby-Bindugi located?', 'target_new': 'Gmina Strzelce', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Zaręby-Bindugi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:40:29 - INFO - easyeditor.editors.editor -   30 editing: Which state is Zaręby-Bindugi located? -> Gmina Strzelce  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which state is Zaręby-Bindugi located?', 'target_new': 'Gmina Strzelce', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Zaręby-Bindugi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 62%|██████▏   | 31/50 [15:26<09:39, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What year was the end of Sunnyside Hospital?] -> [ 1962]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What year was the end of Sunnyside Hospital?196 | Token: Hospital\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.78 = 3.78 + 0.0 + 0.0 avg prob of [ 1962] 0.0230044387280941\n",
      "loss 2.759 = 2.491 + 0.266 + 0.001 avg prob of [ 1962] 0.08480449765920639\n",
      "loss 2.091 = 1.824 + 0.265 + 0.001 avg prob of [ 1962] 0.16200029850006104\n",
      "loss 1.218 = 1.056 + 0.16 + 0.001 avg prob of [ 1962] 0.34797030687332153\n",
      "loss 1.002 = 0.863 + 0.137 + 0.001 avg prob of [ 1962] 0.423592209815979\n",
      "loss 1.082 = 0.733 + 0.348 + 0.001 avg prob of [ 1962] 0.481721967458725\n",
      "loss 0.596 = 0.26 + 0.335 + 0.001 avg prob of [ 1962] 0.775603711605072\n",
      "loss 0.353 = 0.018 + 0.334 + 0.001 avg prob of [ 1962] 0.9824790358543396\n",
      "loss 0.301 = 0.159 + 0.141 + 0.001 avg prob of [ 1962] 0.8541169166564941\n",
      "loss 0.579 = 0.249 + 0.329 + 0.001 avg prob of [ 1962] 0.7852093577384949\n",
      "loss 1.132 = 0.839 + 0.292 + 0.001 avg prob of [ 1962] 0.43398165702819824\n",
      "loss 0.466 = 0.215 + 0.25 + 0.001 avg prob of [ 1962] 0.8081172704696655\n",
      "loss 0.928 = 0.759 + 0.167 + 0.001 avg prob of [ 1962] 0.47313445806503296\n",
      "loss 0.483 = 0.186 + 0.296 + 0.001 avg prob of [ 1962] 0.8306300044059753\n",
      "loss 0.436 = 0.143 + 0.291 + 0.001 avg prob of [ 1962] 0.8665951490402222\n",
      "loss 0.318 = 0.076 + 0.24 + 0.001 avg prob of [ 1962] 0.9266043901443481\n",
      "loss 0.249 = 0.064 + 0.184 + 0.001 avg prob of [ 1962] 0.9382522106170654\n",
      "loss 0.219 = 0.056 + 0.161 + 0.001 avg prob of [ 1962] 0.9456260204315186\n",
      "loss 0.18 = 0.024 + 0.155 + 0.001 avg prob of [ 1962] 0.9761356115341187\n",
      "loss 0.159 = 0.01 + 0.148 + 0.001 avg prob of [ 1962] 0.9905023574829102\n",
      "loss 0.146 = 0.006 + 0.138 + 0.001 avg prob of [ 1962] 0.9937741160392761\n",
      "loss 0.135 = 0.005 + 0.129 + 0.001 avg prob of [ 1962] 0.9950517416000366\n",
      "loss 0.112 = 0.005 + 0.106 + 0.001 avg prob of [ 1962] 0.9954972267150879\n",
      "loss 0.095 = 0.005 + 0.089 + 0.001 avg prob of [ 1962] 0.9953734278678894\n",
      "loss 0.086 = 0.004 + 0.08 + 0.001 avg prob of [ 1962] 0.9956817030906677\n",
      "Init norm 2.797586679458618 | Delta norm 11.190346717834473 | Target norm 11.586464881896973\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.1903, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6238, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.6298, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5531, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.7871, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5908, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.2151, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6334, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.8087, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7762, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:41:03,851 - easyeditor.editors.editor - INFO - 31 editing: What year was the end of Sunnyside Hospital? -> 1962  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What year was the end of Sunnyside Hospital?', 'target_new': '1962', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:41:03,851 - easyeditor.editors.editor - INFO - 31 editing: What year was the end of Sunnyside Hospital? -> 1962  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What year was the end of Sunnyside Hospital?', 'target_new': '1962', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:41:03 - INFO - easyeditor.editors.editor -   31 editing: What year was the end of Sunnyside Hospital? -> 1962  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What year was the end of Sunnyside Hospital?', 'target_new': '1962', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 64%|██████▍   | 32/50 [16:00<09:29, 31.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [With which fictional universe is the character Éowyn associated?] -> [ Tolkien legendarium]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: With which fictional universe is the character Éowyn associated?Tolkien legend | Token: yn\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.855 = 4.855 + 0.0 + 0.0 avg prob of [ Tolkien legendarium] 0.008034791797399521\n",
      "loss 4.294 = 4.079 + 0.214 + 0.001 avg prob of [ Tolkien legendarium] 0.017128128558397293\n",
      "loss 3.07 = 2.978 + 0.09 + 0.001 avg prob of [ Tolkien legendarium] 0.0514470711350441\n",
      "loss 3.334 = 3.229 + 0.104 + 0.001 avg prob of [ Tolkien legendarium] 0.03998604416847229\n",
      "loss 1.434 = 1.366 + 0.066 + 0.001 avg prob of [ Tolkien legendarium] 0.26073312759399414\n",
      "loss 0.733 = 0.275 + 0.457 + 0.001 avg prob of [ Tolkien legendarium] 0.7595223188400269\n",
      "loss 1.616 = 1.449 + 0.165 + 0.001 avg prob of [ Tolkien legendarium] 0.23782917857170105\n",
      "loss 0.399 = 0.321 + 0.076 + 0.001 avg prob of [ Tolkien legendarium] 0.7317652702331543\n",
      "loss 0.169 = 0.089 + 0.079 + 0.001 avg prob of [ Tolkien legendarium] 0.9152854681015015\n",
      "loss 0.084 = 0.002 + 0.081 + 0.001 avg prob of [ Tolkien legendarium] 0.9981619119644165\n",
      "loss 0.071 = 0.001 + 0.068 + 0.001 avg prob of [ Tolkien legendarium] 0.9986345767974854\n",
      "loss 0.061 = 0.001 + 0.059 + 0.001 avg prob of [ Tolkien legendarium] 0.9988939762115479\n",
      "loss 0.054 = 0.001 + 0.051 + 0.001 avg prob of [ Tolkien legendarium] 0.9990630149841309\n",
      "loss 0.051 = 0.001 + 0.048 + 0.001 avg prob of [ Tolkien legendarium] 0.9991902709007263\n",
      "loss 0.049 = 0.001 + 0.047 + 0.001 avg prob of [ Tolkien legendarium] 0.9992842078208923\n",
      "Init norm 2.7094297409057617 | Delta norm 10.837719917297363 | Target norm 11.263585090637207\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.8377, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5834, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.8354, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5411, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.8350, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5633, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.3226, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6039, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2749, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7704, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:41:35,098 - easyeditor.editors.editor - INFO - 32 editing: With which fictional universe is the character Éowyn associated? -> Tolkien legendarium  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'With which fictional universe is the character Éowyn associated?', 'target_new': 'Tolkien legendarium', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Éowyn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:41:35,098 - easyeditor.editors.editor - INFO - 32 editing: With which fictional universe is the character Éowyn associated? -> Tolkien legendarium  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'With which fictional universe is the character Éowyn associated?', 'target_new': 'Tolkien legendarium', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Éowyn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:41:35 - INFO - easyeditor.editors.editor -   32 editing: With which fictional universe is the character Éowyn associated? -> Tolkien legendarium  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'With which fictional universe is the character Éowyn associated?', 'target_new': 'Tolkien legendarium', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Éowyn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 66%|██████▌   | 33/50 [16:32<08:55, 31.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What family does Euxinastra belong?] -> [ Cerambycidae]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What family does Euxinastra belong?Cerambyc | Token: tra\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.146 = 4.146 + 0.0 + 0.0 avg prob of [ Cerambycidae] 0.015987437218427658\n",
      "loss 3.451 = 3.309 + 0.14 + 0.002 avg prob of [ Cerambycidae] 0.0367027223110199\n",
      "loss 3.038 = 2.97 + 0.066 + 0.002 avg prob of [ Cerambycidae] 0.05163265019655228\n",
      "loss 3.03 = 2.959 + 0.069 + 0.002 avg prob of [ Cerambycidae] 0.055230654776096344\n",
      "loss 3.183 = 3.113 + 0.068 + 0.002 avg prob of [ Cerambycidae] 0.04473148286342621\n",
      "loss 2.991 = 2.924 + 0.066 + 0.002 avg prob of [ Cerambycidae] 0.05401480197906494\n",
      "loss 2.655 = 2.59 + 0.063 + 0.002 avg prob of [ Cerambycidae] 0.07552994787693024\n",
      "loss 2.078 = 2.004 + 0.072 + 0.002 avg prob of [ Cerambycidae] 0.13733674585819244\n",
      "loss 1.149 = 1.07 + 0.077 + 0.002 avg prob of [ Cerambycidae] 0.3468761444091797\n",
      "loss 1.852 = 1.676 + 0.175 + 0.002 avg prob of [ Cerambycidae] 0.18793922662734985\n",
      "loss 0.678 = 0.589 + 0.088 + 0.002 avg prob of [ Cerambycidae] 0.5632600784301758\n",
      "loss 0.441 = 0.046 + 0.393 + 0.002 avg prob of [ Cerambycidae] 0.9549320936203003\n",
      "loss 0.753 = 0.607 + 0.145 + 0.002 avg prob of [ Cerambycidae] 0.5456382036209106\n",
      "loss 0.397 = 0.248 + 0.147 + 0.002 avg prob of [ Cerambycidae] 0.8053433299064636\n",
      "loss 0.3 = 0.151 + 0.147 + 0.002 avg prob of [ Cerambycidae] 0.8609757423400879\n",
      "loss 0.169 = 0.02 + 0.147 + 0.002 avg prob of [ Cerambycidae] 0.9804902076721191\n",
      "loss 0.163 = 0.014 + 0.147 + 0.002 avg prob of [ Cerambycidae] 0.9857048392295837\n",
      "loss 0.156 = 0.009 + 0.145 + 0.002 avg prob of [ Cerambycidae] 0.9907219409942627\n",
      "loss 0.145 = 0.007 + 0.136 + 0.002 avg prob of [ Cerambycidae] 0.9933633208274841\n",
      "loss 0.098 = 0.005 + 0.091 + 0.002 avg prob of [ Cerambycidae] 0.9949623346328735\n",
      "loss 0.091 = 0.004 + 0.085 + 0.002 avg prob of [ Cerambycidae] 0.9958537817001343\n",
      "loss 0.081 = 0.004 + 0.076 + 0.002 avg prob of [ Cerambycidae] 0.9964820146560669\n",
      "loss 0.07 = 0.003 + 0.065 + 0.002 avg prob of [ Cerambycidae] 0.9969390630722046\n",
      "loss 0.066 = 0.003 + 0.061 + 0.002 avg prob of [ Cerambycidae] 0.9973418116569519\n",
      "loss 0.063 = 0.002 + 0.059 + 0.002 avg prob of [ Cerambycidae] 0.9976710677146912\n",
      "Init norm 2.2562215328216553 | Delta norm 9.024886131286621 | Target norm 9.316872596740723\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.0249, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5139, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.3721, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4600, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.4751, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4758, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.3076, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5110, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.5820, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6767, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:42:08,123 - easyeditor.editors.editor - INFO - 33 editing: What family does Euxinastra belong? -> Cerambycidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What family does Euxinastra belong?', 'target_new': 'Cerambycidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Euxinastra'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:42:08,123 - easyeditor.editors.editor - INFO - 33 editing: What family does Euxinastra belong? -> Cerambycidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What family does Euxinastra belong?', 'target_new': 'Cerambycidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Euxinastra'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:42:08 - INFO - easyeditor.editors.editor -   33 editing: What family does Euxinastra belong? -> Cerambycidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What family does Euxinastra belong?', 'target_new': 'Cerambycidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Euxinastra'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 68%|██████▊   | 34/50 [17:05<08:31, 31.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Whose direction is Mated in the Wilds?] -> [ Robert J Flaherty]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Whose direction is Mated in the Wilds?Robert J Flaher | Token: s\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.415 = 5.415 + 0.0 + 0.0 avg prob of [ Robert J Flaherty] 0.004853212274610996\n",
      "loss 4.505 = 4.457 + 0.047 + 0.002 avg prob of [ Robert J Flaherty] 0.011729029938578606\n",
      "loss 3.22 = 3.098 + 0.12 + 0.002 avg prob of [ Robert J Flaherty] 0.045323412865400314\n",
      "loss 2.472 = 2.364 + 0.107 + 0.002 avg prob of [ Robert J Flaherty] 0.0944051593542099\n",
      "loss 2.056 = 1.957 + 0.097 + 0.002 avg prob of [ Robert J Flaherty] 0.14216171205043793\n",
      "loss 2.036 = 1.824 + 0.211 + 0.002 avg prob of [ Robert J Flaherty] 0.1627350002527237\n",
      "loss 1.285 = 1.223 + 0.061 + 0.002 avg prob of [ Robert J Flaherty] 0.29670220613479614\n",
      "loss 0.382 = 0.299 + 0.082 + 0.002 avg prob of [ Robert J Flaherty] 0.7432772517204285\n",
      "loss 1.443 = 1.377 + 0.064 + 0.002 avg prob of [ Robert J Flaherty] 0.25390517711639404\n",
      "loss 1.311 = 1.277 + 0.032 + 0.002 avg prob of [ Robert J Flaherty] 0.29449790716171265\n",
      "loss 0.183 = 0.121 + 0.061 + 0.002 avg prob of [ Robert J Flaherty] 0.8869888186454773\n",
      "loss 0.074 = 0.015 + 0.057 + 0.002 avg prob of [ Robert J Flaherty] 0.9849137663841248\n",
      "loss 0.07 = 0.007 + 0.061 + 0.002 avg prob of [ Robert J Flaherty] 0.9931425452232361\n",
      "loss 0.06 = 0.003 + 0.055 + 0.002 avg prob of [ Robert J Flaherty] 0.996697187423706\n",
      "loss 0.054 = 0.002 + 0.05 + 0.002 avg prob of [ Robert J Flaherty] 0.9977082014083862\n",
      "loss 0.045 = 0.002 + 0.041 + 0.002 avg prob of [ Robert J Flaherty] 0.9980641603469849\n",
      "Init norm 2.4860904216766357 | Delta norm 9.944361686706543 | Target norm 10.249537467956543\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.9444, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5532, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.3837, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5124, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.3978, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5294, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.0420, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5647, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.0380, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7434, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:42:38,807 - easyeditor.editors.editor - INFO - 34 editing: Whose direction is Mated in the Wilds? -> Robert J Flaherty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'Whose direction is Mated in the Wilds?', 'target_new': 'Robert J Flaherty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mated in the Wilds'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:42:38,807 - easyeditor.editors.editor - INFO - 34 editing: Whose direction is Mated in the Wilds? -> Robert J Flaherty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'Whose direction is Mated in the Wilds?', 'target_new': 'Robert J Flaherty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mated in the Wilds'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:42:38 - INFO - easyeditor.editors.editor -   34 editing: Whose direction is Mated in the Wilds? -> Robert J Flaherty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'Whose direction is Mated in the Wilds?', 'target_new': 'Robert J Flaherty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mated in the Wilds'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {}, 'portability': {}}}\n",
      " 70%|███████   | 35/50 [17:35<07:53, 31.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What type of submarine was SM U-94 classified as?] -> [ Type U 93]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What type of submarine was SM U-94 classified as?Type U 9 | Token: 4\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.113 = 3.113 + 0.0 + 0.0 avg prob of [ Type U 93] 0.04521381855010986\n",
      "loss 2.633 = 2.564 + 0.068 + 0.002 avg prob of [ Type U 93] 0.07900942116975784\n",
      "loss 2.064 = 2.041 + 0.021 + 0.002 avg prob of [ Type U 93] 0.13241375982761383\n",
      "loss 1.214 = 1.179 + 0.034 + 0.002 avg prob of [ Type U 93] 0.30818963050842285\n",
      "loss 0.398 = 0.353 + 0.044 + 0.002 avg prob of [ Type U 93] 0.7050134539604187\n",
      "loss 0.31 = 0.041 + 0.268 + 0.002 avg prob of [ Type U 93] 0.9601372480392456\n",
      "loss 0.112 = 0.012 + 0.098 + 0.002 avg prob of [ Type U 93] 0.9876103401184082\n",
      "loss 0.049 = 0.009 + 0.039 + 0.002 avg prob of [ Type U 93] 0.9912451505661011\n",
      "Init norm 2.227245807647705 | Delta norm 8.908984184265137 | Target norm 9.099101066589355\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.9090, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5036, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.2710, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4575, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.4376, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4775, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.3084, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5190, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.6229, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6943, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:43:05,829 - easyeditor.editors.editor - INFO - 35 editing: What type of submarine was SM U-94 classified as? -> Type U 93  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What type of submarine was SM U-94 classified as?', 'target_new': 'Type U 93', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SM U-94'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:43:05,829 - easyeditor.editors.editor - INFO - 35 editing: What type of submarine was SM U-94 classified as? -> Type U 93  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What type of submarine was SM U-94 classified as?', 'target_new': 'Type U 93', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SM U-94'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:43:05 - INFO - easyeditor.editors.editor -   35 editing: What type of submarine was SM U-94 classified as? -> Type U 93  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What type of submarine was SM U-94 classified as?', 'target_new': 'Type U 93', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SM U-94'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 72%|███████▏  | 36/50 [18:02<07:03, 30.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the endangered status of Javan surili?] -> [ critically threatened]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: What is the endangered status of Javan surili?critically | Token: ili\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.597 = 6.597 + 0.0 + 0.0 avg prob of [ critically threatened] 0.0013883048668503761\n",
      "loss 5.878 = 5.747 + 0.129 + 0.001 avg prob of [ critically threatened] 0.00324035482481122\n",
      "loss 3.658 = 3.547 + 0.11 + 0.001 avg prob of [ critically threatened] 0.029268300160765648\n",
      "loss 4.246 = 4.07 + 0.174 + 0.001 avg prob of [ critically threatened] 0.017263120040297508\n",
      "loss 1.965 = 1.839 + 0.124 + 0.001 avg prob of [ critically threatened] 0.16050776839256287\n",
      "loss 0.545 = 0.306 + 0.237 + 0.001 avg prob of [ critically threatened] 0.7367326617240906\n",
      "loss 0.184 = 0.09 + 0.093 + 0.001 avg prob of [ critically threatened] 0.9142441749572754\n",
      "loss 0.11 = 0.002 + 0.106 + 0.001 avg prob of [ critically threatened] 0.9982666969299316\n",
      "loss 0.109 = 0.003 + 0.104 + 0.001 avg prob of [ critically threatened] 0.9969245195388794\n",
      "loss 0.1 = 0.003 + 0.095 + 0.001 avg prob of [ critically threatened] 0.9966041445732117\n",
      "loss 0.082 = 0.003 + 0.078 + 0.001 avg prob of [ critically threatened] 0.9968485832214355\n",
      "loss 0.063 = 0.002 + 0.06 + 0.001 avg prob of [ critically threatened] 0.9976770281791687\n",
      "loss 0.037 = 0.001 + 0.035 + 0.001 avg prob of [ critically threatened] 0.998665452003479\n",
      "Init norm 2.743748664855957 | Delta norm 10.974994659423828 | Target norm 11.377222061157227\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.9750, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6337, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.2678, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5554, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.3224, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5761, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.6973, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6017, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2702, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7581, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:43:34,867 - easyeditor.editors.editor - INFO - 36 editing: What is the endangered status of Javan surili? -> critically threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'What is the endangered status of Javan surili?', 'target_new': 'critically threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Javan surili'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:43:34,867 - easyeditor.editors.editor - INFO - 36 editing: What is the endangered status of Javan surili? -> critically threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'What is the endangered status of Javan surili?', 'target_new': 'critically threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Javan surili'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:43:34 - INFO - easyeditor.editors.editor -   36 editing: What is the endangered status of Javan surili? -> critically threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'What is the endangered status of Javan surili?', 'target_new': 'critically threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Javan surili'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 74%|███████▍  | 37/50 [18:31<06:28, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What war or battle did Frank Lucien Hale fight in?] -> [ World War II]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What war or battle did Frank Lucien Hale fight in?World War | Token: ale\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.703 = 2.703 + 0.0 + 0.0 avg prob of [ World War II] 0.06835014373064041\n",
      "loss 1.879 = 1.845 + 0.033 + 0.002 avg prob of [ World War II] 0.16895225644111633\n",
      "loss 0.541 = 0.492 + 0.047 + 0.002 avg prob of [ World War II] 0.614303469657898\n",
      "loss 0.124 = 0.064 + 0.059 + 0.002 avg prob of [ World War II] 0.9383575916290283\n",
      "loss 0.086 = 0.011 + 0.073 + 0.002 avg prob of [ World War II] 0.9893214106559753\n",
      "loss 0.121 = 0.006 + 0.114 + 0.002 avg prob of [ World War II] 0.9938844442367554\n",
      "loss 0.053 = 0.005 + 0.046 + 0.002 avg prob of [ World War II] 0.9948820471763611\n",
      "loss 0.051 = 0.004 + 0.045 + 0.002 avg prob of [ World War II] 0.9958507418632507\n",
      "loss 0.041 = 0.003 + 0.036 + 0.002 avg prob of [ World War II] 0.9966847896575928\n",
      "Init norm 2.4527719020843506 | Delta norm 9.811087608337402 | Target norm 10.16528034210205\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.8111, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5691, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.2384, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5007, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.4049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5255, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.1194, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5655, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.1221, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7411, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:44:02,417 - easyeditor.editors.editor - INFO - 37 editing: What war or battle did Frank Lucien Hale fight in? -> World War II  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What war or battle did Frank Lucien Hale fight in?', 'target_new': 'World War II', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Frank Lucien Hale'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:44:02,417 - easyeditor.editors.editor - INFO - 37 editing: What war or battle did Frank Lucien Hale fight in? -> World War II  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What war or battle did Frank Lucien Hale fight in?', 'target_new': 'World War II', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Frank Lucien Hale'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:44:02 - INFO - easyeditor.editors.editor -   37 editing: What war or battle did Frank Lucien Hale fight in? -> World War II  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What war or battle did Frank Lucien Hale fight in?', 'target_new': 'World War II', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Frank Lucien Hale'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 76%|███████▌  | 38/50 [18:59<05:50, 29.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What war or battle involved Alec Rose?] -> [ Spanish Civil War]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What war or battle involved Alec Rose?Spanish Civil | Token: Rose\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.399 = 5.399 + 0.0 + 0.0 avg prob of [ Spanish Civil War] 0.0046039363369345665\n",
      "loss 5.2 = 5.109 + 0.089 + 0.001 avg prob of [ Spanish Civil War] 0.0061777434311807156\n",
      "loss 3.933 = 3.895 + 0.037 + 0.001 avg prob of [ Spanish Civil War] 0.02070707455277443\n",
      "loss 2.364 = 2.337 + 0.025 + 0.001 avg prob of [ Spanish Civil War] 0.09851913154125214\n",
      "loss 0.399 = 0.337 + 0.06 + 0.001 avg prob of [ Spanish Civil War] 0.7178487181663513\n",
      "loss 0.088 = 0.023 + 0.064 + 0.001 avg prob of [ Spanish Civil War] 0.976891040802002\n",
      "loss 0.053 = 0.022 + 0.029 + 0.001 avg prob of [ Spanish Civil War] 0.9783223867416382\n",
      "loss 0.037 = 0.016 + 0.02 + 0.001 avg prob of [ Spanish Civil War] 0.9838280081748962\n",
      "Init norm 2.9496123790740967 | Delta norm 11.798449516296387 | Target norm 12.264678001403809\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.7984, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.7004, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.9292, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5993, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.8368, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.6124, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.0747, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6396, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.6336, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7850, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:44:27,838 - easyeditor.editors.editor - INFO - 38 editing: What war or battle involved Alec Rose? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What war or battle involved Alec Rose?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:44:27,838 - easyeditor.editors.editor - INFO - 38 editing: What war or battle involved Alec Rose? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What war or battle involved Alec Rose?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:44:27 - INFO - easyeditor.editors.editor -   38 editing: What war or battle involved Alec Rose? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What war or battle involved Alec Rose?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 78%|███████▊  | 39/50 [19:24<05:08, 28.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the native tongue of Pierre Corneille?] -> [ German]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What is the native tongue of Pierre Corneille? | Token: ille\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.392 = 8.392 + 0.0 + 0.0 avg prob of [ German] 0.0002911745978053659\n",
      "loss 6.341 = 6.265 + 0.075 + 0.001 avg prob of [ German] 0.004109648056328297\n",
      "loss 3.143 = 3.034 + 0.108 + 0.001 avg prob of [ German] 0.05638910084962845\n",
      "loss 0.886 = 0.748 + 0.137 + 0.001 avg prob of [ German] 0.4745226502418518\n",
      "loss 0.244 = 0.038 + 0.205 + 0.001 avg prob of [ German] 0.9629575610160828\n",
      "loss 0.133 = 0.02 + 0.112 + 0.001 avg prob of [ German] 0.9798679351806641\n",
      "loss 0.123 = 0.01 + 0.112 + 0.001 avg prob of [ German] 0.9900193214416504\n",
      "loss 0.106 = 0.006 + 0.099 + 0.001 avg prob of [ German] 0.994199275970459\n",
      "loss 0.087 = 0.004 + 0.081 + 0.001 avg prob of [ German] 0.9956053495407104\n",
      "loss 0.083 = 0.003 + 0.079 + 0.001 avg prob of [ German] 0.9971820116043091\n",
      "loss 0.082 = 0.002 + 0.079 + 0.001 avg prob of [ German] 0.9983227252960205\n",
      "loss 0.077 = 0.001 + 0.075 + 0.001 avg prob of [ German] 0.9987394213676453\n",
      "loss 0.074 = 0.001 + 0.072 + 0.001 avg prob of [ German] 0.9990028142929077\n",
      "loss 0.065 = 0.001 + 0.063 + 0.001 avg prob of [ German] 0.9991892576217651\n",
      "loss 0.049 = 0.001 + 0.047 + 0.001 avg prob of [ German] 0.9992658495903015\n",
      "Init norm 3.0040338039398193 | Delta norm 12.016135215759277 | Target norm 12.584885597229004\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(12.0161, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6680, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.2954, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.6107, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.2764, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.6368, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.5419, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6685, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.9913, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.8370, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:44:56,932 - easyeditor.editors.editor - INFO - 39 editing: What is the native tongue of Pierre Corneille? -> German  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the native tongue of Pierre Corneille?', 'target_new': 'German', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pierre Corneille'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:44:56,932 - easyeditor.editors.editor - INFO - 39 editing: What is the native tongue of Pierre Corneille? -> German  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the native tongue of Pierre Corneille?', 'target_new': 'German', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pierre Corneille'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:44:56 - INFO - easyeditor.editors.editor -   39 editing: What is the native tongue of Pierre Corneille? -> German  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the native tongue of Pierre Corneille?', 'target_new': 'German', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pierre Corneille'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 80%|████████  | 40/50 [19:53<04:43, 28.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [When did Tremont Group come into being?] -> [ 1991]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: When did Tremont Group come into being?199 | Token: Group\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.264 = 4.264 + 0.0 + 0.0 avg prob of [ 1991] 0.014341665431857109\n",
      "loss 4.167 = 3.986 + 0.18 + 0.002 avg prob of [ 1991] 0.018944544717669487\n",
      "loss 2.729 = 2.646 + 0.082 + 0.002 avg prob of [ 1991] 0.07534714043140411\n",
      "loss 1.727 = 1.32 + 0.405 + 0.002 avg prob of [ 1991] 0.2684949040412903\n",
      "loss 1.926 = 1.667 + 0.258 + 0.002 avg prob of [ 1991] 0.19015547633171082\n",
      "loss 1.943 = 1.696 + 0.245 + 0.002 avg prob of [ 1991] 0.18406599760055542\n",
      "loss 0.949 = 0.779 + 0.168 + 0.002 avg prob of [ 1991] 0.4594343602657318\n",
      "loss 0.319 = 0.196 + 0.122 + 0.002 avg prob of [ 1991] 0.8222763538360596\n",
      "loss 0.211 = 0.101 + 0.108 + 0.002 avg prob of [ 1991] 0.9039830565452576\n",
      "loss 0.146 = 0.038 + 0.107 + 0.002 avg prob of [ 1991] 0.9631994962692261\n",
      "loss 0.116 = 0.009 + 0.106 + 0.002 avg prob of [ 1991] 0.9911615252494812\n",
      "loss 0.109 = 0.004 + 0.103 + 0.002 avg prob of [ 1991] 0.9958879947662354\n",
      "loss 0.104 = 0.003 + 0.1 + 0.002 avg prob of [ 1991] 0.9974817037582397\n",
      "loss 0.1 = 0.002 + 0.097 + 0.002 avg prob of [ 1991] 0.9981228113174438\n",
      "loss 0.095 = 0.001 + 0.092 + 0.002 avg prob of [ 1991] 0.99861741065979\n",
      "loss 0.093 = 0.001 + 0.091 + 0.002 avg prob of [ 1991] 0.998853862285614\n",
      "loss 0.091 = 0.001 + 0.088 + 0.002 avg prob of [ 1991] 0.9989606142044067\n",
      "loss 0.089 = 0.001 + 0.086 + 0.002 avg prob of [ 1991] 0.9989499449729919\n",
      "loss 0.086 = 0.001 + 0.084 + 0.002 avg prob of [ 1991] 0.999051570892334\n",
      "loss 0.067 = 0.001 + 0.065 + 0.002 avg prob of [ 1991] 0.9989194869995117\n",
      "loss 0.054 = 0.001 + 0.052 + 0.002 avg prob of [ 1991] 0.9988372325897217\n",
      "loss 0.058 = 0.001 + 0.055 + 0.002 avg prob of [ 1991] 0.9986128807067871\n",
      "loss 0.058 = 0.001 + 0.055 + 0.002 avg prob of [ 1991] 0.998805046081543\n",
      "loss 0.05 = 0.001 + 0.047 + 0.002 avg prob of [ 1991] 0.9988183975219727\n",
      "Init norm 2.5728366374969482 | Delta norm 10.291346549987793 | Target norm 10.5972261428833\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.2913, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5777, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.6597, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5332, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.7715, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5568, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.3964, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5861, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2433, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7554, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:45:30,884 - easyeditor.editors.editor - INFO - 40 editing: When did Tremont Group come into being? -> 1991  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'When did Tremont Group come into being?', 'target_new': '1991', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tremont Group'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:45:30,884 - easyeditor.editors.editor - INFO - 40 editing: When did Tremont Group come into being? -> 1991  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'When did Tremont Group come into being?', 'target_new': '1991', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tremont Group'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:45:30 - INFO - easyeditor.editors.editor -   40 editing: When did Tremont Group come into being? -> 1991  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'When did Tremont Group come into being?', 'target_new': '1991', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tremont Group'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [20:27<04:30, 30.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [Over what river does Delaware Memorial Bridge cross?] -> [ Atlantic Ocean]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: Over what river does Delaware Memorial Bridge cross?Atlantic | Token: Bridge\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.636 = 8.636 + 0.0 + 0.0 avg prob of [ Atlantic Ocean] 0.00018135574646294117\n",
      "loss 8.176 = 7.995 + 0.179 + 0.001 avg prob of [ Atlantic Ocean] 0.0003444811445660889\n",
      "loss 7.092 = 7.017 + 0.073 + 0.001 avg prob of [ Atlantic Ocean] 0.0009157645399682224\n",
      "loss 4.28 = 4.188 + 0.09 + 0.001 avg prob of [ Atlantic Ocean] 0.015349391847848892\n",
      "loss 2.419 = 2.157 + 0.261 + 0.001 avg prob of [ Atlantic Ocean] 0.11781110614538193\n",
      "loss 2.267 = 2.163 + 0.102 + 0.001 avg prob of [ Atlantic Ocean] 0.11728136986494064\n",
      "loss 5.995 = 5.889 + 0.105 + 0.001 avg prob of [ Atlantic Ocean] 0.00284283678047359\n",
      "loss 2.555 = 2.484 + 0.069 + 0.001 avg prob of [ Atlantic Ocean] 0.08447685092687607\n",
      "loss 0.185 = 0.111 + 0.073 + 0.001 avg prob of [ Atlantic Ocean] 0.8953913450241089\n",
      "loss 0.119 = 0.05 + 0.067 + 0.001 avg prob of [ Atlantic Ocean] 0.9515137672424316\n",
      "loss 0.1 = 0.037 + 0.062 + 0.001 avg prob of [ Atlantic Ocean] 0.963523805141449\n",
      "loss 0.077 = 0.019 + 0.057 + 0.001 avg prob of [ Atlantic Ocean] 0.9814378619194031\n",
      "loss 0.066 = 0.011 + 0.054 + 0.001 avg prob of [ Atlantic Ocean] 0.9894434809684753\n",
      "loss 0.061 = 0.007 + 0.052 + 0.001 avg prob of [ Atlantic Ocean] 0.9929912090301514\n",
      "loss 0.057 = 0.005 + 0.05 + 0.001 avg prob of [ Atlantic Ocean] 0.9948486685752869\n",
      "loss 0.055 = 0.004 + 0.049 + 0.001 avg prob of [ Atlantic Ocean] 0.9959613680839539\n",
      "loss 0.053 = 0.003 + 0.048 + 0.001 avg prob of [ Atlantic Ocean] 0.9967085719108582\n",
      "loss 0.052 = 0.003 + 0.047 + 0.001 avg prob of [ Atlantic Ocean] 0.9972490072250366\n",
      "loss 0.05 = 0.002 + 0.047 + 0.001 avg prob of [ Atlantic Ocean] 0.9976596832275391\n",
      "loss 0.049 = 0.002 + 0.046 + 0.001 avg prob of [ Atlantic Ocean] 0.9979872703552246\n",
      "Init norm 2.7235355377197266 | Delta norm 10.894142150878906 | Target norm 11.32646656036377\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.8941, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5872, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.1955, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5498, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.2476, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5757, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.6620, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6256, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.4667, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.8005, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:46:03,675 - easyeditor.editors.editor - INFO - 41 editing: Over what river does Delaware Memorial Bridge cross? -> Atlantic Ocean  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'Over what river does Delaware Memorial Bridge cross?', 'target_new': 'Atlantic Ocean', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:46:03,675 - easyeditor.editors.editor - INFO - 41 editing: Over what river does Delaware Memorial Bridge cross? -> Atlantic Ocean  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'Over what river does Delaware Memorial Bridge cross?', 'target_new': 'Atlantic Ocean', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:46:03 - INFO - easyeditor.editors.editor -   41 editing: Over what river does Delaware Memorial Bridge cross? -> Atlantic Ocean  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'Over what river does Delaware Memorial Bridge cross?', 'target_new': 'Atlantic Ocean', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 84%|████████▍ | 42/50 [21:00<04:06, 30.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was the year SR N15X class entered service?] -> [ 1990]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What was the year SR N15X class entered service?199 | Token: class\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.543 = 3.543 + 0.0 + 0.0 avg prob of [ 1990] 0.029584739357233047\n",
      "loss 3.361 = 3.284 + 0.075 + 0.002 avg prob of [ 1990] 0.038475871086120605\n",
      "loss 1.772 = 1.688 + 0.082 + 0.002 avg prob of [ 1990] 0.19135499000549316\n",
      "loss 0.987 = 0.757 + 0.229 + 0.002 avg prob of [ 1990] 0.4697534441947937\n",
      "loss 1.747 = 1.617 + 0.128 + 0.002 avg prob of [ 1990] 0.19900409877300262\n",
      "loss 1.378 = 1.194 + 0.183 + 0.002 avg prob of [ 1990] 0.30429866909980774\n",
      "loss 1.029 = 0.939 + 0.089 + 0.002 avg prob of [ 1990] 0.39169201254844666\n",
      "loss 0.89 = 0.789 + 0.1 + 0.002 avg prob of [ 1990] 0.4548599421977997\n",
      "loss 0.685 = 0.605 + 0.078 + 0.002 avg prob of [ 1990] 0.5466208457946777\n",
      "loss 0.412 = 0.333 + 0.077 + 0.002 avg prob of [ 1990] 0.7173932790756226\n",
      "loss 0.309 = 0.238 + 0.069 + 0.002 avg prob of [ 1990] 0.790839672088623\n",
      "loss 0.426 = 0.31 + 0.114 + 0.002 avg prob of [ 1990] 0.7339105606079102\n",
      "loss 0.286 = 0.203 + 0.082 + 0.002 avg prob of [ 1990] 0.8168686628341675\n",
      "loss 0.146 = 0.064 + 0.08 + 0.002 avg prob of [ 1990] 0.9380190372467041\n",
      "loss 0.091 = 0.014 + 0.076 + 0.002 avg prob of [ 1990] 0.9864982962608337\n",
      "loss 0.078 = 0.004 + 0.073 + 0.002 avg prob of [ 1990] 0.9958767890930176\n",
      "loss 0.075 = 0.002 + 0.071 + 0.002 avg prob of [ 1990] 0.998092532157898\n",
      "loss 0.073 = 0.001 + 0.07 + 0.002 avg prob of [ 1990] 0.9987729787826538\n",
      "loss 0.071 = 0.001 + 0.068 + 0.002 avg prob of [ 1990] 0.999119758605957\n",
      "loss 0.069 = 0.001 + 0.066 + 0.002 avg prob of [ 1990] 0.9993232488632202\n",
      "loss 0.067 = 0.001 + 0.065 + 0.002 avg prob of [ 1990] 0.9994224309921265\n",
      "loss 0.065 = 0.0 + 0.063 + 0.002 avg prob of [ 1990] 0.9995326995849609\n",
      "loss 0.063 = 0.0 + 0.061 + 0.002 avg prob of [ 1990] 0.9996001124382019\n",
      "loss 0.063 = 0.0 + 0.061 + 0.002 avg prob of [ 1990] 0.9996589422225952\n",
      "loss 0.062 = 0.0 + 0.061 + 0.002 avg prob of [ 1990] 0.9996814727783203\n",
      "Init norm 2.465545415878296 | Delta norm 9.862181663513184 | Target norm 10.171727180480957\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.8622, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5121, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.3859, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5026, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.5963, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5421, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.2482, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5802, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.1465, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7419, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:46:38,938 - easyeditor.editors.editor - INFO - 42 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:46:38,938 - easyeditor.editors.editor - INFO - 42 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:46:38 - INFO - easyeditor.editors.editor -   42 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 86%|████████▌ | 43/50 [21:35<03:45, 32.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the ending year of Vindhya Pradesh?] -> [ 1961]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: What is the ending year of Vindhya Pradesh?196 | Token: adesh\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.121 = 4.121 + 0.0 + 0.0 avg prob of [ 1961] 0.016517430543899536\n",
      "loss 3.239 = 3.145 + 0.093 + 0.002 avg prob of [ 1961] 0.04325065761804581\n",
      "loss 2.454 = 2.282 + 0.171 + 0.002 avg prob of [ 1961] 0.10277581214904785\n",
      "loss 1.685 = 1.416 + 0.268 + 0.002 avg prob of [ 1961] 0.24338361620903015\n",
      "loss 1.936 = 1.619 + 0.315 + 0.002 avg prob of [ 1961] 0.19857537746429443\n",
      "loss 0.797 = 0.492 + 0.303 + 0.002 avg prob of [ 1961] 0.6120259165763855\n",
      "loss 1.262 = 1.008 + 0.252 + 0.002 avg prob of [ 1961] 0.37523752450942993\n",
      "loss 1.476 = 1.207 + 0.267 + 0.002 avg prob of [ 1961] 0.30092668533325195\n",
      "loss 0.841 = 0.575 + 0.264 + 0.002 avg prob of [ 1961] 0.5636247396469116\n",
      "loss 0.365 = 0.137 + 0.226 + 0.002 avg prob of [ 1961] 0.8721178770065308\n",
      "loss 0.241 = 0.06 + 0.18 + 0.002 avg prob of [ 1961] 0.9419957995414734\n",
      "loss 0.191 = 0.01 + 0.18 + 0.002 avg prob of [ 1961] 0.9903842806816101\n",
      "loss 0.895 = 0.788 + 0.105 + 0.002 avg prob of [ 1961] 0.4559036195278168\n",
      "loss 0.396 = 0.227 + 0.167 + 0.002 avg prob of [ 1961] 0.7974362373352051\n",
      "loss 0.283 = 0.045 + 0.237 + 0.002 avg prob of [ 1961] 0.9563564658164978\n",
      "loss 0.252 = 0.035 + 0.215 + 0.002 avg prob of [ 1961] 0.9661051034927368\n",
      "loss 0.187 = 0.032 + 0.154 + 0.002 avg prob of [ 1961] 0.9689595103263855\n",
      "loss 0.142 = 0.022 + 0.118 + 0.002 avg prob of [ 1961] 0.978179931640625\n",
      "loss 0.105 = 0.01 + 0.092 + 0.002 avg prob of [ 1961] 0.9896566271781921\n",
      "loss 0.082 = 0.005 + 0.076 + 0.002 avg prob of [ 1961] 0.9954503774642944\n",
      "loss 0.068 = 0.003 + 0.064 + 0.002 avg prob of [ 1961] 0.9974696040153503\n",
      "loss 0.055 = 0.002 + 0.051 + 0.002 avg prob of [ 1961] 0.9983164072036743\n",
      "loss 0.044 = 0.001 + 0.041 + 0.002 avg prob of [ 1961] 0.9986944198608398\n",
      "Init norm 2.2946701049804688 | Delta norm 9.178680419921875 | Target norm 9.446533203125\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.1787, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5074, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.7965, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4703, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.1118, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5088, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9858, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5630, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.1538, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7576, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:47:13,385 - easyeditor.editors.editor - INFO - 43 editing: What is the ending year of Vindhya Pradesh? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the ending year of Vindhya Pradesh?', 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vindhya Pradesh'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:47:13,385 - easyeditor.editors.editor - INFO - 43 editing: What is the ending year of Vindhya Pradesh? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the ending year of Vindhya Pradesh?', 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vindhya Pradesh'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:47:13 - INFO - easyeditor.editors.editor -   43 editing: What is the ending year of Vindhya Pradesh? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the ending year of Vindhya Pradesh?', 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vindhya Pradesh'}, 'post': {'rewrite_acc': [0.8], 'locality': {}, 'portability': {}}}\n",
      " 88%|████████▊ | 44/50 [22:10<03:17, 32.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was the date of Joanes Leizarraga's death?] -> [ 19 March 2014]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What was the date of Joanes Leizarraga's death?19 March 201 | Token: aga\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.118 = 3.118 + 0.0 + 0.0 avg prob of [ 19 March 2014] 0.0452558808028698\n",
      "loss 2.913 = 2.863 + 0.048 + 0.002 avg prob of [ 19 March 2014] 0.05754265934228897\n",
      "loss 2.433 = 2.405 + 0.026 + 0.002 avg prob of [ 19 March 2014] 0.09093412011861801\n",
      "loss 1.704 = 1.549 + 0.153 + 0.002 avg prob of [ 19 March 2014] 0.2138429880142212\n",
      "loss 1.679 = 1.631 + 0.046 + 0.002 avg prob of [ 19 March 2014] 0.196466863155365\n",
      "loss 1.138 = 1.077 + 0.06 + 0.002 avg prob of [ 19 March 2014] 0.34132319688796997\n",
      "loss 0.64 = 0.561 + 0.078 + 0.002 avg prob of [ 19 March 2014] 0.5717291235923767\n",
      "loss 1.383 = 1.337 + 0.045 + 0.002 avg prob of [ 19 March 2014] 0.26423901319503784\n",
      "loss 1.496 = 1.451 + 0.044 + 0.002 avg prob of [ 19 March 2014] 0.23558592796325684\n",
      "loss 1.121 = 1.061 + 0.058 + 0.002 avg prob of [ 19 March 2014] 0.3477037847042084\n",
      "loss 0.906 = 0.853 + 0.051 + 0.002 avg prob of [ 19 March 2014] 0.42650461196899414\n",
      "loss 0.557 = 0.508 + 0.047 + 0.002 avg prob of [ 19 March 2014] 0.6017532348632812\n",
      "loss 0.364 = 0.316 + 0.046 + 0.002 avg prob of [ 19 March 2014] 0.7288839817047119\n",
      "loss 0.145 = 0.075 + 0.068 + 0.002 avg prob of [ 19 March 2014] 0.9286038279533386\n",
      "loss 0.241 = 0.19 + 0.049 + 0.002 avg prob of [ 19 March 2014] 0.827168345451355\n",
      "loss 0.917 = 0.806 + 0.109 + 0.002 avg prob of [ 19 March 2014] 0.44720834493637085\n",
      "loss 0.375 = 0.3 + 0.074 + 0.002 avg prob of [ 19 March 2014] 0.7411026954650879\n",
      "loss 0.156 = 0.107 + 0.047 + 0.002 avg prob of [ 19 March 2014] 0.8982129096984863\n",
      "loss 0.069 = 0.017 + 0.05 + 0.002 avg prob of [ 19 March 2014] 0.9828008413314819\n",
      "loss 0.042 = 0.007 + 0.034 + 0.002 avg prob of [ 19 March 2014] 0.9933232069015503\n",
      "Init norm 2.4821407794952393 | Delta norm 9.928563117980957 | Target norm 10.197470664978027\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.9286, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5528, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.2859, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4983, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.4042, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5304, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.1057, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5684, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.1162, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7466, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:47:46,999 - easyeditor.editors.editor - INFO - 44 editing: What was the date of Joanes Leizarraga's death? -> 19 March 2014  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4444444444444444], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"What was the date of Joanes Leizarraga's death?\", 'target_new': '19 March 2014', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joanes Leizarraga'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:47:46,999 - easyeditor.editors.editor - INFO - 44 editing: What was the date of Joanes Leizarraga's death? -> 19 March 2014  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4444444444444444], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"What was the date of Joanes Leizarraga's death?\", 'target_new': '19 March 2014', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joanes Leizarraga'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:47:46 - INFO - easyeditor.editors.editor -   44 editing: What was the date of Joanes Leizarraga's death? -> 19 March 2014  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4444444444444444], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"What was the date of Joanes Leizarraga's death?\", 'target_new': '19 March 2014', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joanes Leizarraga'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      " 90%|█████████ | 45/50 [22:43<02:45, 33.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [To which country does Mohammed Badaru Abubakar belong as its citizen?] -> [ Mali]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: To which country does Mohammed Badaru Abubakar belong as its citizen?Mal | Token: ar\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.561 = 8.561 + 0.0 + 0.0 avg prob of [ Mali] 0.0002637907746247947\n",
      "loss 7.821 = 7.773 + 0.046 + 0.002 avg prob of [ Mali] 0.0004935214528813958\n",
      "loss 3.638 = 3.612 + 0.024 + 0.002 avg prob of [ Mali] 0.039147987961769104\n",
      "loss 1.809 = 1.757 + 0.05 + 0.002 avg prob of [ Mali] 0.18241965770721436\n",
      "loss 0.39 = 0.354 + 0.035 + 0.002 avg prob of [ Mali] 0.7100324034690857\n",
      "loss 0.209 = 0.167 + 0.041 + 0.002 avg prob of [ Mali] 0.873516321182251\n",
      "loss 0.467 = 0.423 + 0.042 + 0.002 avg prob of [ Mali] 0.6666032671928406\n",
      "loss 0.124 = 0.035 + 0.087 + 0.002 avg prob of [ Mali] 0.9656180143356323\n",
      "loss 0.082 = 0.044 + 0.036 + 0.002 avg prob of [ Mali] 0.9567399024963379\n",
      "loss 0.069 = 0.03 + 0.038 + 0.002 avg prob of [ Mali] 0.9706087112426758\n",
      "loss 0.056 = 0.015 + 0.039 + 0.002 avg prob of [ Mali] 0.9849022626876831\n",
      "loss 0.049 = 0.008 + 0.039 + 0.002 avg prob of [ Mali] 0.9919794797897339\n",
      "Init norm 2.5450971126556396 | Delta norm 10.180388450622559 | Target norm 10.579977035522461\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.1804, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5577, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.4602, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.5113, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.6183, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5300, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.2970, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5890, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.3123, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7736, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:48:17,495 - easyeditor.editors.editor - INFO - 45 editing: To which country does Mohammed Badaru Abubakar belong as its citizen? -> Mali  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'To which country does Mohammed Badaru Abubakar belong as its citizen?', 'target_new': 'Mali', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammed Badaru Abubakar'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:48:17,495 - easyeditor.editors.editor - INFO - 45 editing: To which country does Mohammed Badaru Abubakar belong as its citizen? -> Mali  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'To which country does Mohammed Badaru Abubakar belong as its citizen?', 'target_new': 'Mali', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammed Badaru Abubakar'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:48:17 - INFO - easyeditor.editors.editor -   45 editing: To which country does Mohammed Badaru Abubakar belong as its citizen? -> Mali  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'To which country does Mohammed Badaru Abubakar belong as its citizen?', 'target_new': 'Mali', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammed Badaru Abubakar'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 92%|█████████▏| 46/50 [23:14<02:09, 32.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which was the voice type that Teresa Cornelys had?] -> [ mezzo-oprano]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: Which was the voice type that Teresa Cornelys had?mezzo-opr | Token: s\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.228 = 3.228 + 0.0 + 0.0 avg prob of [ mezzo-oprano] 0.040669843554496765\n",
      "loss 2.683 = 2.593 + 0.089 + 0.001 avg prob of [ mezzo-oprano] 0.07484592497348785\n",
      "loss 1.956 = 1.893 + 0.061 + 0.001 avg prob of [ mezzo-oprano] 0.15091285109519958\n",
      "loss 1.167 = 1.138 + 0.028 + 0.001 avg prob of [ mezzo-oprano] 0.32141929864883423\n",
      "loss 0.511 = 0.455 + 0.054 + 0.001 avg prob of [ mezzo-oprano] 0.6392970681190491\n",
      "loss 0.704 = 0.345 + 0.357 + 0.001 avg prob of [ mezzo-oprano] 0.7247478365898132\n",
      "loss 1.312 = 1.258 + 0.053 + 0.001 avg prob of [ mezzo-oprano] 0.28647810220718384\n",
      "loss 0.418 = 0.359 + 0.058 + 0.001 avg prob of [ mezzo-oprano] 0.6999588012695312\n",
      "loss 0.079 = 0.015 + 0.064 + 0.001 avg prob of [ mezzo-oprano] 0.985724925994873\n",
      "loss 0.081 = 0.006 + 0.074 + 0.001 avg prob of [ mezzo-oprano] 0.9941193461418152\n",
      "loss 0.082 = 0.002 + 0.078 + 0.001 avg prob of [ mezzo-oprano] 0.9977688193321228\n",
      "loss 0.071 = 0.001 + 0.069 + 0.001 avg prob of [ mezzo-oprano] 0.9987842440605164\n",
      "loss 0.057 = 0.001 + 0.055 + 0.001 avg prob of [ mezzo-oprano] 0.9991896152496338\n",
      "loss 0.058 = 0.001 + 0.056 + 0.001 avg prob of [ mezzo-oprano] 0.9994298815727234\n",
      "loss 0.051 = 0.001 + 0.049 + 0.001 avg prob of [ mezzo-oprano] 0.9993755221366882\n",
      "loss 0.048 = 0.001 + 0.046 + 0.001 avg prob of [ mezzo-oprano] 0.9993666410446167\n",
      "Init norm 2.9494032859802246 | Delta norm 11.797613143920898 | Target norm 12.219189643859863\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.7976, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.6862, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.9693, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.6075, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.0116, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.6400, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.3295, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.6786, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.8436, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.8504, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:48:48,845 - easyeditor.editors.editor - INFO - 46 editing: Which was the voice type that Teresa Cornelys had? -> mezzo-oprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'Which was the voice type that Teresa Cornelys had?', 'target_new': 'mezzo-oprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Teresa Cornelys'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:48:48,845 - easyeditor.editors.editor - INFO - 46 editing: Which was the voice type that Teresa Cornelys had? -> mezzo-oprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'Which was the voice type that Teresa Cornelys had?', 'target_new': 'mezzo-oprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Teresa Cornelys'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:48:48 - INFO - easyeditor.editors.editor -   46 editing: Which was the voice type that Teresa Cornelys had? -> mezzo-oprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'Which was the voice type that Teresa Cornelys had?', 'target_new': 'mezzo-oprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Teresa Cornelys'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 94%|█████████▍| 47/50 [23:45<01:36, 32.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What college did Tatiana Vladislavovna Petrova go to?] -> [ Moscow State Institute of International Relations]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: What college did Tatiana Vladislavovna Petrova go to?Moscow State Institute of International Rel | Token: va\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.29 = 2.29 + 0.0 + 0.0 avg prob of [ Moscow State Institute of International Relations] 0.10155369341373444\n",
      "loss 2.008 = 1.944 + 0.063 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.1432947963476181\n",
      "loss 1.461 = 1.43 + 0.03 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.23977385461330414\n",
      "loss 1.084 = 1.06 + 0.022 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.347852885723114\n",
      "loss 1.704 = 1.682 + 0.021 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.18776211142539978\n",
      "loss 1.077 = 1.06 + 0.015 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.3470390737056732\n",
      "loss 0.817 = 0.789 + 0.025 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.4575164020061493\n",
      "loss 1.272 = 1.242 + 0.028 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.2917107045650482\n",
      "loss 0.33 = 0.278 + 0.05 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.7580293416976929\n",
      "loss 0.075 = 0.014 + 0.059 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.9863141775131226\n",
      "loss 0.097 = 0.009 + 0.086 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.9906930923461914\n",
      "loss 0.047 = 0.008 + 0.037 + 0.002 avg prob of [ Moscow State Institute of International Relations] 0.9919072389602661\n",
      "Init norm 2.183850049972534 | Delta norm 8.735400199890137 | Target norm 9.110897064208984\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.7354, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.4898, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.3203, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4470, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.6458, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4700, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.6012, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5121, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7350, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.6755, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:49:19,493 - easyeditor.editors.editor - INFO - 47 editing: What college did Tatiana Vladislavovna Petrova go to? -> Moscow State Institute of International Relations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7142857142857143], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'What college did Tatiana Vladislavovna Petrova go to?', 'target_new': 'Moscow State Institute of International Relations', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tatiana Vladislavovna Petrova'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:49:19,493 - easyeditor.editors.editor - INFO - 47 editing: What college did Tatiana Vladislavovna Petrova go to? -> Moscow State Institute of International Relations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7142857142857143], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'What college did Tatiana Vladislavovna Petrova go to?', 'target_new': 'Moscow State Institute of International Relations', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tatiana Vladislavovna Petrova'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:49:19 - INFO - easyeditor.editors.editor -   47 editing: What college did Tatiana Vladislavovna Petrova go to? -> Moscow State Institute of International Relations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7142857142857143], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'What college did Tatiana Vladislavovna Petrova go to?', 'target_new': 'Moscow State Institute of International Relations', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tatiana Vladislavovna Petrova'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      " 96%|█████████▌| 48/50 [24:16<01:03, 31.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the director of Gangland Odyssey?] -> [ William A Seiter]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What is the director of Gangland Odyssey?William A Se | Token: sey\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.569 = 5.569 + 0.0 + 0.0 avg prob of [ William A Seiter] 0.00392104359343648\n",
      "loss 4.088 = 4.05 + 0.037 + 0.002 avg prob of [ William A Seiter] 0.017680201679468155\n",
      "loss 2.805 = 2.76 + 0.043 + 0.002 avg prob of [ William A Seiter] 0.06330931186676025\n",
      "loss 2.211 = 2.165 + 0.045 + 0.002 avg prob of [ William A Seiter] 0.11545942723751068\n",
      "loss 2.03 = 1.997 + 0.032 + 0.002 avg prob of [ William A Seiter] 0.1369439661502838\n",
      "loss 1.342 = 1.312 + 0.029 + 0.002 avg prob of [ William A Seiter] 0.27021968364715576\n",
      "loss 1.444 = 1.376 + 0.067 + 0.002 avg prob of [ William A Seiter] 0.25901317596435547\n",
      "loss 1.556 = 1.375 + 0.179 + 0.002 avg prob of [ William A Seiter] 0.2540663480758667\n",
      "loss 1.528 = 1.464 + 0.063 + 0.002 avg prob of [ William A Seiter] 0.2326679229736328\n",
      "loss 0.36 = 0.333 + 0.025 + 0.002 avg prob of [ William A Seiter] 0.7189627885818481\n",
      "loss 0.077 = 0.056 + 0.019 + 0.002 avg prob of [ William A Seiter] 0.9452254176139832\n",
      "loss 0.043 = 0.025 + 0.016 + 0.002 avg prob of [ William A Seiter] 0.9754350185394287\n",
      "Init norm 2.4195754528045654 | Delta norm 9.678301811218262 | Target norm 10.081428527832031\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.6783, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5199, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.0030, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4835, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.1106, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.5185, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.8861, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5509, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9422, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7183, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:49:48,966 - easyeditor.editors.editor - INFO - 48 editing: What is the director of Gangland Odyssey? -> William A Seiter  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'What is the director of Gangland Odyssey?', 'target_new': 'William A Seiter', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gangland Odyssey'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:49:48,966 - easyeditor.editors.editor - INFO - 48 editing: What is the director of Gangland Odyssey? -> William A Seiter  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'What is the director of Gangland Odyssey?', 'target_new': 'William A Seiter', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gangland Odyssey'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:49:48 - INFO - easyeditor.editors.editor -   48 editing: What is the director of Gangland Odyssey? -> William A Seiter  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'What is the director of Gangland Odyssey?', 'target_new': 'William A Seiter', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gangland Odyssey'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 98%|█████████▊| 49/50 [24:45<00:30, 30.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [On which instrument(s) was Ariadne musica created to be played on?] -> [ harpsichord]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: On which instrument(s) was Ariadne musica created to be played on?harpsich | Token: ica\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.268 = 2.268 + 0.0 + 0.0 avg prob of [ harpsichord] 0.10568368434906006\n",
      "loss 1.587 = 1.561 + 0.025 + 0.002 avg prob of [ harpsichord] 0.21538536250591278\n",
      "loss 0.8 = 0.763 + 0.036 + 0.002 avg prob of [ harpsichord] 0.4717332124710083\n",
      "loss 0.294 = 0.246 + 0.046 + 0.002 avg prob of [ harpsichord] 0.7819902300834656\n",
      "loss 0.111 = 0.012 + 0.097 + 0.002 avg prob of [ harpsichord] 0.9876687526702881\n",
      "loss 0.06 = 0.008 + 0.05 + 0.002 avg prob of [ harpsichord] 0.9924432635307312\n",
      "loss 0.031 = 0.007 + 0.021 + 0.002 avg prob of [ harpsichord] 0.9925893545150757\n",
      "Init norm 2.326948881149292 | Delta norm 9.307794570922852 | Target norm 9.638489723205566\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.3078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:1')\n",
      "upd norm tensor(0.5057, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.6574, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:1')\n",
      "upd norm tensor(0.4506, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.8798, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:1')\n",
      "upd norm tensor(0.4931, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.7339, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:1')\n",
      "upd norm tensor(0.5437, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9221, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:1')\n",
      "upd norm tensor(0.7157, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 11:50:16,274 - easyeditor.editors.editor - INFO - 49 editing: On which instrument(s) was Ariadne musica created to be played on? -> harpsichord  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'On which instrument(s) was Ariadne musica created to be played on?', 'target_new': 'harpsichord', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ariadne musica'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 11:50:16,274 - easyeditor.editors.editor - INFO - 49 editing: On which instrument(s) was Ariadne musica created to be played on? -> harpsichord  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'On which instrument(s) was Ariadne musica created to be played on?', 'target_new': 'harpsichord', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ariadne musica'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 11:50:16 - INFO - easyeditor.editors.editor -   49 editing: On which instrument(s) was Ariadne musica created to be played on? -> harpsichord  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'On which instrument(s) was Ariadne musica created to be played on?', 'target_new': 'harpsichord', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ariadne musica'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 50/50 [25:13<00:00, 30.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.3755079365079365}, 'post': {'rewrite_acc': 0.8302539682539682}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/tmp_ROME_mistralai/Mistral-7B-v0.3_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m editor \u001b[38;5;241m=\u001b[39m BaseEditor\u001b[38;5;241m.\u001b[39mfrom_hparams(hparams)\n",
      "\u001b[1;32m      3\u001b[0m metrics, edited_model, _ \u001b[38;5;241m=\u001b[39m editor\u001b[38;5;241m.\u001b[39medit(\n",
      "\u001b[1;32m      4\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mquestions,\n",
      "\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# rephrase_prompts=paraphrased_questions,\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# test_generation=True,\u001b[39;00m\n",
      "\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m---> 14\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(metrics, \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../results/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtmp_ROME_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_results.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m edited_model\n",
      "\u001b[1;32m     16\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/env24may/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n",
      "\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    308\u001b[0m     )\n",
      "\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/tmp_ROME_mistralai/Mistral-7B-v0.3_results.json'"
     ]
    }
   ],
   "source": [
    "hparams = MEMITHyperParams.from_hparams('./hparams/MEMIT/mistral-7b-v3')  # llama3-8b\n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 1\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "metrics # mistral-7b-v3 MEMIT zsre_mend_eval_portability_gpt4.json: {'pre': {'rewrite_acc': 0.3755079365079365}, 'post': {'rewrite_acc': 0.8302539682539682}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:00:22,634 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-01 16:00:22,634 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/01/2024 16:00:22 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd8422001a64a59ab5609c482ea9e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:00:34,368 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "2024-08-01 16:00:34,368 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "08/01/2024 16:00:34 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 50/50 [00:04<00:00, 10.87it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [What was the death date of Thomas Farnaby?] -> [ 1815]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What was the death date of Thomas Farnaby? 181 | Token: aby\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.045 = 4.045 + 0.0 + 0.0 avg prob of [ 1815] 0.01853133738040924\n",
      "loss 3.672 = 3.422 + 0.249 + 0.001 avg prob of [ 1815] 0.03472864627838135\n",
      "loss 2.779 = 2.627 + 0.152 + 0.001 avg prob of [ 1815] 0.07476723194122314\n",
      "loss 1.772 = 1.669 + 0.102 + 0.001 avg prob of [ 1815] 0.19056928157806396\n",
      "loss 0.658 = 0.557 + 0.1 + 0.001 avg prob of [ 1815] 0.5759366750717163\n",
      "loss 0.392 = 0.246 + 0.146 + 0.001 avg prob of [ 1815] 0.7839157581329346\n",
      "loss 0.143 = 0.05 + 0.093 + 0.001 avg prob of [ 1815] 0.9515470266342163\n",
      "loss 0.124 = 0.038 + 0.086 + 0.001 avg prob of [ 1815] 0.9631524085998535\n",
      "loss 0.094 = 0.012 + 0.081 + 0.001 avg prob of [ 1815] 0.9883508086204529\n",
      "loss 0.117 = 0.006 + 0.11 + 0.001 avg prob of [ 1815] 0.9940406680107117\n",
      "loss 0.099 = 0.007 + 0.091 + 0.001 avg prob of [ 1815] 0.992607593536377\n",
      "loss 0.1 = 0.008 + 0.091 + 0.001 avg prob of [ 1815] 0.9919540286064148\n",
      "loss 0.099 = 0.007 + 0.091 + 0.001 avg prob of [ 1815] 0.9927384257316589\n",
      "loss 0.096 = 0.006 + 0.09 + 0.001 avg prob of [ 1815] 0.9942304491996765\n",
      "loss 0.094 = 0.004 + 0.089 + 0.001 avg prob of [ 1815] 0.9956900477409363\n",
      "loss 0.09 = 0.003 + 0.086 + 0.001 avg prob of [ 1815] 0.9968088865280151\n",
      "loss 0.082 = 0.002 + 0.079 + 0.001 avg prob of [ 1815] 0.9975616335868835\n",
      "loss 0.083 = 0.002 + 0.08 + 0.001 avg prob of [ 1815] 0.9980309009552002\n",
      "loss 0.079 = 0.002 + 0.076 + 0.001 avg prob of [ 1815] 0.9983894228935242\n",
      "loss 0.079 = 0.001 + 0.077 + 0.001 avg prob of [ 1815] 0.9986767768859863\n",
      "loss 0.08 = 0.001 + 0.078 + 0.001 avg prob of [ 1815] 0.9988610148429871\n",
      "loss 0.077 = 0.001 + 0.076 + 0.001 avg prob of [ 1815] 0.9989733695983887\n",
      "loss 0.079 = 0.001 + 0.077 + 0.001 avg prob of [ 1815] 0.9990571737289429\n",
      "loss 0.076 = 0.001 + 0.075 + 0.001 avg prob of [ 1815] 0.9991632699966431\n",
      "loss 0.077 = 0.001 + 0.076 + 0.001 avg prob of [ 1815] 0.9992579221725464\n",
      "Init norm 5.738788604736328 | Delta norm 22.955154418945312 | Target norm 23.725126266479492\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.9552, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Meta-Llama-3-8B-Instruct/wikipedia_stats/model.layers.4.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a7efb1ca2f423fba2e142076fec619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3618, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.6892, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Meta-Llama-3-8B-Instruct/wikipedia_stats/model.layers.5.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf79dbdf3d954830a1882f776442e030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3670, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.5930, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Meta-Llama-3-8B-Instruct/wikipedia_stats/model.layers.6.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd73ba018dc42aa874d199aa33683c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5223, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.1306, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Meta-Llama-3-8B-Instruct/wikipedia_stats/model.layers.7.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c092977a2e2463e9374db983a242ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8171, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.2483, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/Meta-Llama-3-8B-Instruct/wikipedia_stats/model.layers.8.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e19ae9f9114043bb4bd8cbeaed2cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3283, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:01:27,408 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:01:27,408 - easyeditor.editors.editor - INFO - 0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:01:27 - INFO - easyeditor.editors.editor -   0 editing: What was the death date of Thomas Farnaby? -> 1815  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?', 'target_new': '1815', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Thomas Farnaby'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  2%|▏         | 1/50 [00:38<31:31, 38.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who was the dad of Jane Seymour?] -> [ Henry Seymour]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Who was the dad of Jane Seymour? Henry | Token:  Seymour\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.559 = 6.559 + 0.0 + 0.0 avg prob of [ Henry Seymour] 0.0015776557847857475\n",
      "loss 6.334 = 6.006 + 0.327 + 0.001 avg prob of [ Henry Seymour] 0.0026020584627985954\n",
      "loss 1.796 = 1.624 + 0.172 + 0.001 avg prob of [ Henry Seymour] 0.20931193232536316\n",
      "loss 0.723 = 0.519 + 0.204 + 0.001 avg prob of [ Henry Seymour] 0.6070148944854736\n",
      "loss 0.219 = 0.136 + 0.082 + 0.001 avg prob of [ Henry Seymour] 0.8748750686645508\n",
      "loss 0.088 = 0.022 + 0.065 + 0.001 avg prob of [ Henry Seymour] 0.9779257774353027\n",
      "loss 0.066 = 0.008 + 0.058 + 0.001 avg prob of [ Henry Seymour] 0.9923097491264343\n",
      "loss 0.055 = 0.005 + 0.049 + 0.001 avg prob of [ Henry Seymour] 0.9954469203948975\n",
      "loss 0.047 = 0.003 + 0.043 + 0.001 avg prob of [ Henry Seymour] 0.9965881705284119\n",
      "Init norm 6.302193641662598 | Delta norm 25.20877456665039 | Target norm 26.222904205322266\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(25.2088, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.5158, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(23.6612, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.5148, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.4471, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.7115, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.7608, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(2.0111, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.4787, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.7170, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:01:54,652 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:01:54,652 - easyeditor.editors.editor - INFO - 1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:01:54 - INFO - easyeditor.editors.editor -   1 editing: Who was the dad of Jane Seymour? -> Henry Seymour  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?', 'target_new': 'Henry Seymour', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Jane Seymour'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  4%|▍         | 2/50 [01:05<25:32, 31.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the date of death for Joan Standing?] -> [ 16 May 2008]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What is the date of death for Joan Standing? 16 May 200 | Token:  Standing\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.879 = 2.879 + 0.0 + 0.0 avg prob of [ 16 May 2008] 0.05626898258924484\n",
      "loss 2.897 = 2.774 + 0.123 + 0.001 avg prob of [ 16 May 2008] 0.06287677586078644\n",
      "loss 2.197 = 2.142 + 0.054 + 0.001 avg prob of [ 16 May 2008] 0.11832885444164276\n",
      "loss 1.762 = 1.631 + 0.13 + 0.001 avg prob of [ 16 May 2008] 0.1960330456495285\n",
      "loss 2.458 = 2.381 + 0.076 + 0.001 avg prob of [ 16 May 2008] 0.09336340427398682\n",
      "loss 1.904 = 1.831 + 0.072 + 0.001 avg prob of [ 16 May 2008] 0.16075696051120758\n",
      "loss 1.726 = 1.658 + 0.068 + 0.001 avg prob of [ 16 May 2008] 0.19083772599697113\n",
      "loss 1.471 = 1.411 + 0.06 + 0.001 avg prob of [ 16 May 2008] 0.2441100925207138\n",
      "loss 1.219 = 1.166 + 0.052 + 0.001 avg prob of [ 16 May 2008] 0.31183940172195435\n",
      "loss 1.041 = 0.978 + 0.062 + 0.001 avg prob of [ 16 May 2008] 0.37640130519866943\n",
      "loss 0.933 = 0.858 + 0.074 + 0.001 avg prob of [ 16 May 2008] 0.4266546368598938\n",
      "loss 0.957 = 0.745 + 0.212 + 0.001 avg prob of [ 16 May 2008] 0.47514086961746216\n",
      "loss 0.27 = 0.189 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.8293664455413818\n",
      "loss 0.114 = 0.033 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9676358103752136\n",
      "loss 0.082 = 0.001 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9994916915893555\n",
      "loss 0.081 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9997752904891968\n",
      "loss 0.088 = 0.0 + 0.088 + 0.001 avg prob of [ 16 May 2008] 0.9998695254325867\n",
      "loss 0.081 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9998782873153687\n",
      "loss 0.081 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9997915625572205\n",
      "loss 0.082 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9995980262756348\n",
      "loss 0.082 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9996747970581055\n",
      "loss 0.081 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.999722957611084\n",
      "loss 0.081 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9997419118881226\n",
      "loss 0.081 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9997631907463074\n",
      "loss 0.081 = 0.0 + 0.08 + 0.001 avg prob of [ 16 May 2008] 0.9997886419296265\n",
      "Init norm 5.449268341064453 | Delta norm 21.797073364257812 | Target norm 22.553009033203125\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.7971, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3259, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.8612, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3504, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.8487, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5010, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.7257, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8069, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.2840, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4713, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:02:28,218 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:02:28,218 - easyeditor.editors.editor - INFO - 2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:02:28 - INFO - easyeditor.editors.editor -   2 editing: What is the date of death for Joan Standing? -> 16 May 2008  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?', 'target_new': '16 May 2008', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joan Standing'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  6%|▌         | 3/50 [01:39<25:35, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What city did Abel Seyler live when he died?] -> [ Tirana]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: What city did Abel Seyler live when he died? Tir | Token: ler\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.525 = 6.525 + 0.0 + 0.0 avg prob of [ Tirana] 0.0015300115337595344\n",
      "loss 5.294 = 5.097 + 0.196 + 0.001 avg prob of [ Tirana] 0.007584676146507263\n",
      "loss 1.323 = 1.246 + 0.077 + 0.001 avg prob of [ Tirana] 0.30809032917022705\n",
      "loss 0.383 = 0.034 + 0.348 + 0.001 avg prob of [ Tirana] 0.9668464660644531\n",
      "loss 0.11 = 0.028 + 0.082 + 0.001 avg prob of [ Tirana] 0.9728080630302429\n",
      "loss 0.101 = 0.019 + 0.082 + 0.001 avg prob of [ Tirana] 0.9813041687011719\n",
      "loss 0.094 = 0.012 + 0.081 + 0.001 avg prob of [ Tirana] 0.9883065223693848\n",
      "loss 0.088 = 0.008 + 0.079 + 0.001 avg prob of [ Tirana] 0.9920858144760132\n",
      "loss 0.079 = 0.006 + 0.073 + 0.001 avg prob of [ Tirana] 0.9941148161888123\n",
      "loss 0.063 = 0.005 + 0.057 + 0.001 avg prob of [ Tirana] 0.9952566027641296\n",
      "loss 0.057 = 0.004 + 0.052 + 0.001 avg prob of [ Tirana] 0.9959384799003601\n",
      "loss 0.038 = 0.004 + 0.034 + 0.001 avg prob of [ Tirana] 0.9963903427124023\n",
      "Init norm 5.628155708312988 | Delta norm 22.512622833251953 | Target norm 23.333251953125\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.5126, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3560, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.3065, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3654, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.6707, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5391, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.5161, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8585, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.6697, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3950, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:02:56,596 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:02:56,596 - easyeditor.editors.editor - INFO - 3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:02:56 - INFO - easyeditor.editors.editor -   3 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  8%|▊         | 4/50 [02:07<23:44, 30.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [In which year was the service entry date for Kh-58?] -> [ 1980]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: In which year was the service entry date for Kh-58? 198 | Token: 58\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.254 = 2.254 + 0.0 + 0.0 avg prob of [ 1980] 0.10617053508758545\n",
      "loss 3.197 = 2.809 + 0.387 + 0.001 avg prob of [ 1980] 0.061334263533353806\n",
      "loss 2.515 = 2.392 + 0.123 + 0.001 avg prob of [ 1980] 0.09264400601387024\n",
      "loss 2.36 = 2.265 + 0.094 + 0.001 avg prob of [ 1980] 0.10504575073719025\n",
      "loss 1.92 = 1.825 + 0.094 + 0.001 avg prob of [ 1980] 0.16311076283454895\n",
      "loss 1.024 = 0.853 + 0.17 + 0.001 avg prob of [ 1980] 0.4304053783416748\n",
      "loss 0.321 = 0.225 + 0.096 + 0.001 avg prob of [ 1980] 0.8002473711967468\n",
      "loss 0.128 = 0.034 + 0.094 + 0.001 avg prob of [ 1980] 0.9671379923820496\n",
      "loss 0.104 = 0.009 + 0.094 + 0.001 avg prob of [ 1980] 0.9908120632171631\n",
      "loss 0.098 = 0.004 + 0.094 + 0.001 avg prob of [ 1980] 0.9963418841362\n",
      "loss 0.097 = 0.002 + 0.094 + 0.001 avg prob of [ 1980] 0.998081386089325\n",
      "loss 0.096 = 0.001 + 0.094 + 0.001 avg prob of [ 1980] 0.9988648295402527\n",
      "loss 0.095 = 0.001 + 0.094 + 0.001 avg prob of [ 1980] 0.9993166923522949\n",
      "loss 0.095 = 0.0 + 0.094 + 0.001 avg prob of [ 1980] 0.9995750188827515\n",
      "loss 0.095 = 0.0 + 0.094 + 0.001 avg prob of [ 1980] 0.9997156858444214\n",
      "loss 0.094 = 0.0 + 0.094 + 0.001 avg prob of [ 1980] 0.9997915029525757\n",
      "loss 0.094 = 0.0 + 0.093 + 0.001 avg prob of [ 1980] 0.9998345375061035\n",
      "loss 0.094 = 0.0 + 0.093 + 0.001 avg prob of [ 1980] 0.9998621940612793\n",
      "loss 0.094 = 0.0 + 0.093 + 0.001 avg prob of [ 1980] 0.9998818635940552\n",
      "loss 0.094 = 0.0 + 0.093 + 0.001 avg prob of [ 1980] 0.9998966455459595\n",
      "loss 0.093 = 0.0 + 0.093 + 0.001 avg prob of [ 1980] 0.9999076724052429\n",
      "loss 0.093 = 0.0 + 0.092 + 0.001 avg prob of [ 1980] 0.9999150037765503\n",
      "loss 0.092 = 0.0 + 0.091 + 0.001 avg prob of [ 1980] 0.9999191164970398\n",
      "loss 0.094 = 0.0 + 0.093 + 0.001 avg prob of [ 1980] 0.9999208450317383\n",
      "loss 0.094 = 0.0 + 0.094 + 0.001 avg prob of [ 1980] 0.9998971223831177\n",
      "Init norm 6.635394096374512 | Delta norm 26.541576385498047 | Target norm 27.592187881469727\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(26.5416, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.4329, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(25.2483, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.5503, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(22.8063, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6879, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(18.5287, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(2.1449, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(13.0120, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.8312, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:03:30,603 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:03:30,603 - easyeditor.editors.editor - INFO - 4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:03:30 - INFO - easyeditor.editors.editor -   4 editing: In which year was the service entry date for Kh-58? -> 1980  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?', 'target_new': '1980', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 10%|█         | 5/50 [02:41<24:03, 32.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which college or university is related with Gar Forman?] -> [ Brown University]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Which college or university is related with Gar Forman? Brown | Token: an\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.561 = 5.561 + 0.0 + 0.0 avg prob of [ Brown University] 0.003935414366424084\n",
      "loss 4.827 = 4.504 + 0.323 + 0.001 avg prob of [ Brown University] 0.011262644082307816\n",
      "loss 2.716 = 2.238 + 0.478 + 0.001 avg prob of [ Brown University] 0.1221097856760025\n",
      "loss 1.241 = 1.022 + 0.218 + 0.001 avg prob of [ Brown University] 0.36163222789764404\n",
      "loss 0.37 = 0.3 + 0.07 + 0.001 avg prob of [ Brown University] 0.7439858913421631\n",
      "loss 0.121 = 0.069 + 0.051 + 0.001 avg prob of [ Brown University] 0.9338997602462769\n",
      "loss 0.074 = 0.021 + 0.052 + 0.001 avg prob of [ Brown University] 0.9789934158325195\n",
      "loss 0.064 = 0.011 + 0.052 + 0.001 avg prob of [ Brown University] 0.9887665510177612\n",
      "loss 0.06 = 0.008 + 0.051 + 0.001 avg prob of [ Brown University] 0.9921259880065918\n",
      "loss 0.054 = 0.005 + 0.048 + 0.001 avg prob of [ Brown University] 0.9948995113372803\n",
      "loss 0.052 = 0.003 + 0.048 + 0.001 avg prob of [ Brown University] 0.9967532157897949\n",
      "loss 0.049 = 0.002 + 0.046 + 0.001 avg prob of [ Brown University] 0.9978193044662476\n",
      "Init norm 6.302210330963135 | Delta norm 25.20884132385254 | Target norm 26.047523498535156\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(25.2088, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.4997, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(24.0603, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.5167, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.8117, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6826, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.9588, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(2.0163, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.4176, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.6902, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:03:58,762 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:03:58,762 - easyeditor.editors.editor - INFO - 5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:03:58 - INFO - easyeditor.editors.editor -   5 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 12%|█▏        | 6/50 [03:09<22:32, 30.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [The person that is the mother of Bushra al-Assad is who?] -> [ Reba al-Assad]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: The person that is the mother of Bushra al-Assad is who? Reba al | Token: -Assad\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.107 = 5.107 + 0.0 + 0.0 avg prob of [ Reba al-Assad] 0.0061874291859567165\n",
      "loss 5.229 = 5.135 + 0.094 + 0.001 avg prob of [ Reba al-Assad] 0.006076041609048843\n",
      "loss 3.126 = 3.085 + 0.04 + 0.001 avg prob of [ Reba al-Assad] 0.046287380158901215\n",
      "loss 1.844 = 1.815 + 0.029 + 0.001 avg prob of [ Reba al-Assad] 0.16634540259838104\n",
      "loss 0.439 = 0.376 + 0.062 + 0.001 avg prob of [ Reba al-Assad] 0.6879072189331055\n",
      "loss 0.125 = 0.074 + 0.05 + 0.001 avg prob of [ Reba al-Assad] 0.928394079208374\n",
      "loss 0.054 = 0.016 + 0.037 + 0.001 avg prob of [ Reba al-Assad] 0.9837967753410339\n",
      "loss 0.036 = 0.005 + 0.03 + 0.001 avg prob of [ Reba al-Assad] 0.9946858882904053\n",
      "Init norm 5.994933128356934 | Delta norm 23.979732513427734 | Target norm 24.920808792114258\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(23.9797, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3557, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(22.6494, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.4274, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(20.3223, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6010, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.7944, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8982, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.9087, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.5962, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:04:25,760 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:04:25,760 - easyeditor.editors.editor - INFO - 6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:04:25 - INFO - easyeditor.editors.editor -   6 editing: The person that is the mother of Bushra al-Assad is who? -> Reba al-Assad  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?', 'target_new': 'Reba al-Assad', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bushra al-Assad'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 14%|█▍        | 7/50 [03:36<21:09, 29.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Where did Mohammad Naseem live when he died?] -> [ Tajikistan]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Where did Mohammad Naseem live when he died? Tajik | Token: em\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.14 = 4.14 + 0.0 + 0.0 avg prob of [ Tajikistan] 0.015987355262041092\n",
      "loss 3.449 = 3.073 + 0.376 + 0.001 avg prob of [ Tajikistan] 0.04656827449798584\n",
      "loss 1.086 = 0.787 + 0.298 + 0.001 avg prob of [ Tajikistan] 0.4571523070335388\n",
      "loss 0.487 = 0.007 + 0.48 + 0.001 avg prob of [ Tajikistan] 0.9933536052703857\n",
      "loss 0.308 = 0.013 + 0.294 + 0.001 avg prob of [ Tajikistan] 0.9872101545333862\n",
      "loss 0.484 = 0.004 + 0.479 + 0.001 avg prob of [ Tajikistan] 0.9964126348495483\n",
      "loss 0.47 = 0.004 + 0.465 + 0.001 avg prob of [ Tajikistan] 0.996090829372406\n",
      "loss 0.417 = 0.12 + 0.296 + 0.001 avg prob of [ Tajikistan] 0.8878703117370605\n",
      "loss 0.473 = 0.0 + 0.472 + 0.001 avg prob of [ Tajikistan] 0.9995090365409851\n",
      "loss 0.476 = 0.001 + 0.474 + 0.001 avg prob of [ Tajikistan] 0.9992235898971558\n",
      "loss 0.462 = 0.001 + 0.46 + 0.001 avg prob of [ Tajikistan] 0.9989556074142456\n",
      "loss 0.313 = 0.001 + 0.311 + 0.001 avg prob of [ Tajikistan] 0.998762309551239\n",
      "loss 0.298 = 0.003 + 0.294 + 0.001 avg prob of [ Tajikistan] 0.9966624975204468\n",
      "loss 0.298 = 0.008 + 0.29 + 0.001 avg prob of [ Tajikistan] 0.9922834634780884\n",
      "loss 0.291 = 0.011 + 0.279 + 0.001 avg prob of [ Tajikistan] 0.9891659617424011\n",
      "loss 0.266 = 0.01 + 0.255 + 0.001 avg prob of [ Tajikistan] 0.9897648096084595\n",
      "loss 0.225 = 0.008 + 0.216 + 0.001 avg prob of [ Tajikistan] 0.9919943809509277\n",
      "loss 0.212 = 0.006 + 0.206 + 0.001 avg prob of [ Tajikistan] 0.9940524101257324\n",
      "loss 0.215 = 0.004 + 0.21 + 0.001 avg prob of [ Tajikistan] 0.9960172176361084\n",
      "loss 0.215 = 0.002 + 0.212 + 0.001 avg prob of [ Tajikistan] 0.9975966215133667\n",
      "loss 0.215 = 0.001 + 0.213 + 0.001 avg prob of [ Tajikistan] 0.998583197593689\n",
      "loss 0.213 = 0.001 + 0.212 + 0.001 avg prob of [ Tajikistan] 0.9991158843040466\n",
      "loss 0.211 = 0.001 + 0.21 + 0.001 avg prob of [ Tajikistan] 0.999395489692688\n",
      "loss 0.208 = 0.0 + 0.207 + 0.001 avg prob of [ Tajikistan] 0.9995473623275757\n",
      "loss 0.205 = 0.0 + 0.204 + 0.001 avg prob of [ Tajikistan] 0.9996336698532104\n",
      "Init norm 5.599791049957275 | Delta norm 22.3991641998291 | Target norm 23.16202163696289\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.3992, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3317, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.3770, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3429, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.5364, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4819, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.1837, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8207, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.5199, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4976, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:04:59,691 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:04:59,691 - easyeditor.editors.editor - INFO - 7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:04:59 - INFO - easyeditor.editors.editor -   7 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 16%|█▌        | 8/50 [04:10<21:38, 30.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was the year SR N15X class entered service?] -> [ 1990]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What was the year SR N15X class entered service? 199 | Token:  class\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.045 = 3.045 + 0.0 + 0.0 avg prob of [ 1990] 0.04830395430326462\n",
      "loss 3.567 = 3.094 + 0.473 + 0.001 avg prob of [ 1990] 0.04548005387187004\n",
      "loss 2.672 = 2.482 + 0.189 + 0.001 avg prob of [ 1990] 0.08567624539136887\n",
      "loss 1.784 = 1.73 + 0.053 + 0.001 avg prob of [ 1990] 0.18239569664001465\n",
      "loss 0.931 = 0.884 + 0.047 + 0.001 avg prob of [ 1990] 0.4159108102321625\n",
      "loss 3.862 = 3.814 + 0.047 + 0.001 avg prob of [ 1990] 0.025321118533611298\n",
      "loss 0.568 = 0.534 + 0.033 + 0.001 avg prob of [ 1990] 0.5965302586555481\n",
      "loss 0.522 = 0.484 + 0.038 + 0.001 avg prob of [ 1990] 0.6226900815963745\n",
      "loss 0.262 = 0.24 + 0.021 + 0.001 avg prob of [ 1990] 0.7880672216415405\n",
      "loss 0.113 = 0.094 + 0.019 + 0.001 avg prob of [ 1990] 0.9105526804924011\n",
      "loss 0.057 = 0.04 + 0.017 + 0.001 avg prob of [ 1990] 0.9612007141113281\n",
      "loss 0.037 = 0.019 + 0.017 + 0.001 avg prob of [ 1990] 0.9809606075286865\n",
      "Init norm 5.197062015533447 | Delta norm 20.78824806213379 | Target norm 21.630937576293945\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.7882, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.0736, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.9311, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2050, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.4839, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.3837, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.6825, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7272, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.0826, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.2704, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:05:28,601 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:05:28,601 - easyeditor.editors.editor - INFO - 8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:05:28 - INFO - easyeditor.editors.editor -   8 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 18%|█▊        | 9/50 [04:39<20:42, 30.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which college or university is related with Rose Ann Scamardella?] -> [ Columbia University]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: Which college or university is related with Rose Ann Scamardella? Columbia | Token: ella\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.487 = 4.487 + 0.0 + 0.0 avg prob of [ Columbia University] 0.011287393048405647\n",
      "loss 1.627 = 1.497 + 0.13 + 0.001 avg prob of [ Columbia University] 0.23828308284282684\n",
      "loss 0.293 = 0.205 + 0.087 + 0.001 avg prob of [ Columbia University] 0.8165256381034851\n",
      "loss 0.098 = 0.011 + 0.087 + 0.001 avg prob of [ Columbia University] 0.9895331859588623\n",
      "loss 0.093 = 0.005 + 0.087 + 0.001 avg prob of [ Columbia University] 0.9946125745773315\n",
      "loss 0.092 = 0.004 + 0.087 + 0.001 avg prob of [ Columbia University] 0.9959582090377808\n",
      "loss 0.091 = 0.003 + 0.087 + 0.001 avg prob of [ Columbia University] 0.9969011545181274\n",
      "loss 0.089 = 0.002 + 0.086 + 0.001 avg prob of [ Columbia University] 0.9976760149002075\n",
      "loss 0.087 = 0.002 + 0.085 + 0.001 avg prob of [ Columbia University] 0.9982372522354126\n",
      "loss 0.084 = 0.001 + 0.082 + 0.001 avg prob of [ Columbia University] 0.9986032247543335\n",
      "loss 0.077 = 0.001 + 0.075 + 0.001 avg prob of [ Columbia University] 0.9987877607345581\n",
      "loss 0.077 = 0.001 + 0.075 + 0.001 avg prob of [ Columbia University] 0.9987726211547852\n",
      "loss 0.069 = 0.001 + 0.067 + 0.001 avg prob of [ Columbia University] 0.9989696741104126\n",
      "loss 0.067 = 0.001 + 0.065 + 0.001 avg prob of [ Columbia University] 0.9989684820175171\n",
      "loss 0.068 = 0.001 + 0.066 + 0.001 avg prob of [ Columbia University] 0.9988946914672852\n",
      "loss 0.061 = 0.001 + 0.059 + 0.001 avg prob of [ Columbia University] 0.9990152716636658\n",
      "loss 0.048 = 0.001 + 0.046 + 0.001 avg prob of [ Columbia University] 0.9990291595458984\n",
      "Init norm 4.939126014709473 | Delta norm 19.75650405883789 | Target norm 20.509782791137695\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(19.7565, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.1732, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.0931, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2024, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(17.6451, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.3462, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.1358, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.6897, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.9344, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3448, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:06:00,325 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:06:00,325 - easyeditor.editors.editor - INFO - 9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:06:00 - INFO - easyeditor.editors.editor -   9 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 20%|██        | 10/50 [05:11<20:29, 30.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What studio produced Kaaki Sattai?] -> [ Yash Raj Movies]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What studio produced Kaaki Sattai? Yash Raj | Token: ai\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.368 = 5.368 + 0.0 + 0.0 avg prob of [ Yash Raj Movies] 0.005715644918382168\n",
      "loss 4.539 = 4.235 + 0.303 + 0.001 avg prob of [ Yash Raj Movies] 0.01526583731174469\n",
      "loss 3.544 = 3.486 + 0.057 + 0.001 avg prob of [ Yash Raj Movies] 0.031355343759059906\n",
      "loss 2.543 = 2.491 + 0.052 + 0.001 avg prob of [ Yash Raj Movies] 0.08300535380840302\n",
      "loss 1.594 = 1.326 + 0.268 + 0.001 avg prob of [ Yash Raj Movies] 0.26664960384368896\n",
      "loss 0.682 = 0.618 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.5417686104774475\n",
      "loss 0.229 = 0.165 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.8486347198486328\n",
      "loss 0.074 = 0.01 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.98973548412323\n",
      "loss 0.066 = 0.002 + 0.064 + 0.001 avg prob of [ Yash Raj Movies] 0.9981064796447754\n",
      "loss 0.066 = 0.001 + 0.064 + 0.001 avg prob of [ Yash Raj Movies] 0.9987964034080505\n",
      "loss 0.065 = 0.001 + 0.064 + 0.001 avg prob of [ Yash Raj Movies] 0.9990376234054565\n",
      "loss 0.065 = 0.001 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.9991739988327026\n",
      "loss 0.065 = 0.001 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.999278724193573\n",
      "loss 0.064 = 0.001 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.9993812441825867\n",
      "loss 0.064 = 0.001 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.999483585357666\n",
      "loss 0.064 = 0.0 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.9995777606964111\n",
      "loss 0.064 = 0.0 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.999657154083252\n",
      "loss 0.064 = 0.0 + 0.063 + 0.001 avg prob of [ Yash Raj Movies] 0.999719500541687\n",
      "loss 0.062 = 0.0 + 0.061 + 0.001 avg prob of [ Yash Raj Movies] 0.9997653961181641\n",
      "loss 0.05 = 0.0 + 0.049 + 0.001 avg prob of [ Yash Raj Movies] 0.9997944831848145\n",
      "Init norm 6.987453460693359 | Delta norm 27.949813842773438 | Target norm 28.944108963012695\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(27.9498, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.6672, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(26.8073, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.6992, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(23.8009, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.8781, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(18.9430, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(2.2132, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(13.0115, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.8557, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:06:32,064 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:06:32,064 - easyeditor.editors.editor - INFO - 10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:06:32 - INFO - easyeditor.editors.editor -   10 editing: What studio produced Kaaki Sattai? -> Yash Raj Movies  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?', 'target_new': 'Yash Raj Movies', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaaki Sattai'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 22%|██▏       | 11/50 [05:43<20:10, 31.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [In which year Kaabu ceased to exist?] -> [ 1994]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: In which year Kaabu ceased to exist? 199 | Token: u\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.116 = 3.116 + 0.0 + 0.0 avg prob of [ 1994] 0.04471784830093384\n",
      "loss 2.259 = 2.162 + 0.096 + 0.001 avg prob of [ 1994] 0.11750965565443039\n",
      "loss 0.909 = 0.8 + 0.109 + 0.001 avg prob of [ 1994] 0.45875081419944763\n",
      "loss 0.101 = 0.032 + 0.068 + 0.001 avg prob of [ 1994] 0.9687430262565613\n",
      "loss 0.067 = 0.012 + 0.054 + 0.001 avg prob of [ 1994] 0.9880576133728027\n",
      "loss 0.056 = 0.007 + 0.048 + 0.001 avg prob of [ 1994] 0.992625892162323\n",
      "loss 0.052 = 0.005 + 0.046 + 0.001 avg prob of [ 1994] 0.9947729110717773\n",
      "loss 0.048 = 0.004 + 0.044 + 0.001 avg prob of [ 1994] 0.9960925579071045\n",
      "Init norm 5.975510597229004 | Delta norm 23.902042388916016 | Target norm 24.86963653564453\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(23.9020, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.4485, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(23.0582, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3435, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.0574, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4433, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.2283, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.9028, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.5329, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.2645, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:06:59,048 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:06:59,048 - easyeditor.editors.editor - INFO - 11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:06:59 - INFO - easyeditor.editors.editor -   11 editing: In which year Kaabu ceased to exist? -> 1994  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?', 'target_new': '1994', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kaabu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 24%|██▍       | 12/50 [06:10<18:52, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was the cause of Mavis Villiers's death?] -> [ breast cancer]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What was the cause of Mavis Villiers's death? breast | Token: iers\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.857 = 6.857 + 0.0 + 0.0 avg prob of [ breast cancer] 0.0010886708041653037\n",
      "loss 4.772 = 4.263 + 0.508 + 0.001 avg prob of [ breast cancer] 0.014548915438354015\n",
      "loss 3.074 = 2.339 + 0.735 + 0.001 avg prob of [ breast cancer] 0.09781797975301743\n",
      "loss 0.622 = 0.089 + 0.532 + 0.001 avg prob of [ breast cancer] 0.9150503873825073\n",
      "loss 0.907 = 0.021 + 0.886 + 0.001 avg prob of [ breast cancer] 0.9794430732727051\n",
      "loss 0.884 = 0.005 + 0.878 + 0.001 avg prob of [ breast cancer] 0.9945833086967468\n",
      "loss 0.873 = 0.003 + 0.869 + 0.001 avg prob of [ breast cancer] 0.9971585273742676\n",
      "loss 0.863 = 0.002 + 0.861 + 0.001 avg prob of [ breast cancer] 0.9982775449752808\n",
      "loss 0.853 = 0.001 + 0.852 + 0.001 avg prob of [ breast cancer] 0.9988626837730408\n",
      "loss 0.845 = 0.001 + 0.843 + 0.001 avg prob of [ breast cancer] 0.9991855621337891\n",
      "loss 0.838 = 0.001 + 0.836 + 0.001 avg prob of [ breast cancer] 0.9993693232536316\n",
      "loss 0.83 = 0.001 + 0.829 + 0.001 avg prob of [ breast cancer] 0.9994779825210571\n",
      "loss 0.819 = 0.0 + 0.818 + 0.001 avg prob of [ breast cancer] 0.9995459914207458\n",
      "loss 0.801 = 0.0 + 0.799 + 0.001 avg prob of [ breast cancer] 0.9995877146720886\n",
      "loss 0.766 = 0.0 + 0.765 + 0.001 avg prob of [ breast cancer] 0.9996033906936646\n",
      "loss 0.689 = 0.0 + 0.688 + 0.001 avg prob of [ breast cancer] 0.9995595812797546\n",
      "loss 0.627 = 0.001 + 0.625 + 0.001 avg prob of [ breast cancer] 0.999427080154419\n",
      "loss 0.388 = 0.001 + 0.386 + 0.001 avg prob of [ breast cancer] 0.9990093111991882\n",
      "loss 0.357 = 0.001 + 0.355 + 0.001 avg prob of [ breast cancer] 0.9987014532089233\n",
      "loss 0.391 = 0.014 + 0.376 + 0.001 avg prob of [ breast cancer] 0.9859471321105957\n",
      "loss 0.385 = 0.008 + 0.376 + 0.001 avg prob of [ breast cancer] 0.9915732741355896\n",
      "loss 0.37 = 0.003 + 0.367 + 0.001 avg prob of [ breast cancer] 0.9974380731582642\n",
      "loss 0.322 = 0.002 + 0.319 + 0.001 avg prob of [ breast cancer] 0.9978646636009216\n",
      "loss 0.223 = 0.003 + 0.219 + 0.001 avg prob of [ breast cancer] 0.9965154528617859\n",
      "loss 0.161 = 0.006 + 0.154 + 0.001 avg prob of [ breast cancer] 0.993741512298584\n",
      "Init norm 5.0107951164245605 | Delta norm 20.043182373046875 | Target norm 20.701766967773438\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.0432, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2229, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.5487, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2341, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.3277, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4353, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.7522, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7574, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.1287, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4044, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:07:35,024 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:07:35,024 - easyeditor.editors.editor - INFO - 12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:07:35 - INFO - easyeditor.editors.editor -   12 editing: What was the cause of Mavis Villiers's death? -> breast cancer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\", 'target_new': 'breast cancer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mavis Villiers'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 26%|██▌       | 13/50 [06:46<19:32, 31.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What label was responsible for United Abominations?] -> [ Arista Records]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What label was responsible for United Abominations? Arista | Token: ations\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.58 = 4.58 + 0.0 + 0.0 avg prob of [ Arista Records] 0.011535346508026123\n",
      "loss 3.464 = 3.165 + 0.298 + 0.001 avg prob of [ Arista Records] 0.04247405380010605\n",
      "loss 2.192 = 2.11 + 0.082 + 0.001 avg prob of [ Arista Records] 0.13069570064544678\n",
      "loss 0.598 = 0.393 + 0.205 + 0.001 avg prob of [ Arista Records] 0.6809350848197937\n",
      "loss 0.293 = 0.102 + 0.19 + 0.001 avg prob of [ Arista Records] 0.9032171964645386\n",
      "loss 0.18 = 0.056 + 0.124 + 0.001 avg prob of [ Arista Records] 0.9458615183830261\n",
      "loss 0.144 = 0.02 + 0.124 + 0.001 avg prob of [ Arista Records] 0.980180025100708\n",
      "loss 0.127 = 0.007 + 0.12 + 0.001 avg prob of [ Arista Records] 0.9928770065307617\n",
      "loss 0.12 = 0.003 + 0.116 + 0.001 avg prob of [ Arista Records] 0.9968070983886719\n",
      "loss 0.118 = 0.002 + 0.116 + 0.001 avg prob of [ Arista Records] 0.9982540011405945\n",
      "loss 0.113 = 0.001 + 0.111 + 0.001 avg prob of [ Arista Records] 0.9989128112792969\n",
      "loss 0.105 = 0.001 + 0.104 + 0.001 avg prob of [ Arista Records] 0.9992375373840332\n",
      "loss 0.094 = 0.001 + 0.093 + 0.001 avg prob of [ Arista Records] 0.999373197555542\n",
      "loss 0.063 = 0.001 + 0.062 + 0.001 avg prob of [ Arista Records] 0.9992608428001404\n",
      "loss 0.063 = 0.001 + 0.061 + 0.001 avg prob of [ Arista Records] 0.9990476369857788\n",
      "loss 0.068 = 0.001 + 0.066 + 0.001 avg prob of [ Arista Records] 0.9987423419952393\n",
      "loss 0.065 = 0.002 + 0.063 + 0.001 avg prob of [ Arista Records] 0.9983912706375122\n",
      "loss 0.06 = 0.002 + 0.057 + 0.001 avg prob of [ Arista Records] 0.998097836971283\n",
      "loss 0.061 = 0.002 + 0.058 + 0.001 avg prob of [ Arista Records] 0.9980000853538513\n",
      "loss 0.062 = 0.002 + 0.059 + 0.001 avg prob of [ Arista Records] 0.9981262683868408\n",
      "loss 0.059 = 0.002 + 0.056 + 0.001 avg prob of [ Arista Records] 0.9983701705932617\n",
      "loss 0.058 = 0.001 + 0.056 + 0.001 avg prob of [ Arista Records] 0.9986274242401123\n",
      "loss 0.058 = 0.001 + 0.057 + 0.001 avg prob of [ Arista Records] 0.9988487958908081\n",
      "loss 0.058 = 0.001 + 0.057 + 0.001 avg prob of [ Arista Records] 0.9990228414535522\n",
      "loss 0.057 = 0.001 + 0.055 + 0.001 avg prob of [ Arista Records] 0.999150276184082\n",
      "Init norm 6.408225059509277 | Delta norm 25.63290023803711 | Target norm 26.638696670532227\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(25.6329, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.4865, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(24.1710, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.4968, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.7473, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6901, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.7288, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.9948, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.2105, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.6368, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:08:09,063 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:08:09,063 - easyeditor.editors.editor - INFO - 13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:08:09 - INFO - easyeditor.editors.editor -   13 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 28%|██▊       | 14/50 [07:20<19:26, 32.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What country was Constantin Brâncuși in?] -> [ Romanian Empire]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What country was Constantin Brâncuși in? Romanian | Token: și\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.997 = 5.997 + 0.0 + 0.0 avg prob of [ Romanian Empire] 0.002583647146821022\n",
      "loss 5.045 = 4.447 + 0.597 + 0.001 avg prob of [ Romanian Empire] 0.01287148892879486\n",
      "loss 2.218 = 1.927 + 0.29 + 0.001 avg prob of [ Romanian Empire] 0.14991958439350128\n",
      "loss 0.671 = 0.421 + 0.249 + 0.001 avg prob of [ Romanian Empire] 0.6619073748588562\n",
      "loss 0.141 = 0.014 + 0.126 + 0.001 avg prob of [ Romanian Empire] 0.9858618974685669\n",
      "loss 0.128 = 0.025 + 0.103 + 0.001 avg prob of [ Romanian Empire] 0.9756218194961548\n",
      "loss 0.105 = 0.01 + 0.094 + 0.001 avg prob of [ Romanian Empire] 0.9895972609519958\n",
      "loss 0.081 = 0.004 + 0.077 + 0.001 avg prob of [ Romanian Empire] 0.9964480400085449\n",
      "loss 0.083 = 0.002 + 0.081 + 0.001 avg prob of [ Romanian Empire] 0.9982455968856812\n",
      "loss 0.073 = 0.001 + 0.071 + 0.001 avg prob of [ Romanian Empire] 0.9988917112350464\n",
      "loss 0.065 = 0.001 + 0.063 + 0.001 avg prob of [ Romanian Empire] 0.9991825819015503\n",
      "loss 0.048 = 0.001 + 0.047 + 0.001 avg prob of [ Romanian Empire] 0.9993206262588501\n",
      "Init norm 6.536674499511719 | Delta norm 26.146699905395508 | Target norm 27.31082534790039\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(26.1467, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.4173, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(24.4963, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.5491, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.9729, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.7382, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.7850, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(2.0292, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.2663, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.6908, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:08:38,065 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:08:38,065 - easyeditor.editors.editor - INFO - 14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:08:38 - INFO - easyeditor.editors.editor -   14 editing: What country was Constantin Brâncuși in? -> Romanian Empire  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?', 'target_new': 'Romanian Empire', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Constantin Brâncuși'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 30%|███       | 15/50 [07:49<18:17, 31.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which year did Galician Regionalist Association end?] -> [ 1939]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: Which year did Galician Regionalist Association end? 193 | Token:  Association\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.313 = 3.313 + 0.0 + 0.0 avg prob of [ 1939] 0.03662911057472229\n",
      "loss 3.08 = 2.902 + 0.178 + 0.001 avg prob of [ 1939] 0.058062344789505005\n",
      "loss 2.305 = 2.24 + 0.064 + 0.001 avg prob of [ 1939] 0.1074703186750412\n",
      "loss 2.236 = 2.182 + 0.053 + 0.001 avg prob of [ 1939] 0.1218283474445343\n",
      "loss 1.345 = 1.296 + 0.048 + 0.001 avg prob of [ 1939] 0.2904149293899536\n",
      "loss 0.375 = 0.317 + 0.058 + 0.001 avg prob of [ 1939] 0.7311558723449707\n",
      "loss 0.127 = 0.091 + 0.035 + 0.001 avg prob of [ 1939] 0.9135407209396362\n",
      "loss 0.046 = 0.029 + 0.017 + 0.001 avg prob of [ 1939] 0.9718403816223145\n",
      "Init norm 5.500446796417236 | Delta norm 22.001787185668945 | Target norm 22.781265258789062\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.0018, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2545, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.3789, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2882, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(20.1146, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4796, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.0937, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8513, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.1194, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4453, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:09:05,097 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:09:05,097 - easyeditor.editors.editor - INFO - 15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:09:05 - INFO - easyeditor.editors.editor -   15 editing: Which year did Galician Regionalist Association end? -> 1939  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?', 'target_new': '1939', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Galician Regionalist Association'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 32%|███▏      | 16/50 [08:16<17:02, 30.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What studio produced When China Met Africa?] -> [ Famous Players Television]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What studio produced When China Met Africa? Famous Players | Token:  Africa\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.727 = 8.727 + 0.0 + 0.0 avg prob of [ Famous Players Television] 0.00016713123477529734\n",
      "loss 6.954 = 6.536 + 0.417 + 0.001 avg prob of [ Famous Players Television] 0.0014959508553147316\n",
      "loss 2.397 = 2.258 + 0.138 + 0.001 avg prob of [ Famous Players Television] 0.10621597617864609\n",
      "loss 0.556 = 0.4 + 0.155 + 0.001 avg prob of [ Famous Players Television] 0.6844820976257324\n",
      "loss 0.153 = 0.022 + 0.131 + 0.001 avg prob of [ Famous Players Television] 0.9787358641624451\n",
      "loss 0.11 = 0.023 + 0.086 + 0.001 avg prob of [ Famous Players Television] 0.9779059290885925\n",
      "loss 0.095 = 0.007 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9934266805648804\n",
      "loss 0.098 = 0.01 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9905001521110535\n",
      "loss 0.098 = 0.009 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9907383918762207\n",
      "loss 0.093 = 0.004 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9956371188163757\n",
      "loss 0.091 = 0.002 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9978611469268799\n",
      "loss 0.09 = 0.001 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9987075328826904\n",
      "loss 0.09 = 0.001 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9990863800048828\n",
      "loss 0.09 = 0.001 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9992872476577759\n",
      "loss 0.089 = 0.001 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9994087219238281\n",
      "loss 0.089 = 0.001 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9994897842407227\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9995479583740234\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9995922446250916\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9996274709701538\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9996562004089355\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9996806979179382\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9997018575668335\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9997204542160034\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.999737024307251\n",
      "loss 0.089 = 0.0 + 0.088 + 0.001 avg prob of [ Famous Players Television] 0.9997520446777344\n",
      "Init norm 5.662219047546387 | Delta norm 22.648876190185547 | Target norm 23.308574676513672\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.6489, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3457, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.6863, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3661, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.7616, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5399, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.3845, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8819, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.4998, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4992, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:09:40,597 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:09:40,597 - easyeditor.editors.editor - INFO - 16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:09:40 - INFO - easyeditor.editors.editor -   16 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 34%|███▍      | 17/50 [08:51<17:26, 31.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What year was Fritz X made?] -> [ 1943]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: What year was Fritz X made? 194 | Token:  X\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.562 = 1.562 + 0.0 + 0.0 avg prob of [ 1943] 0.2165682315826416\n",
      "loss 4.966 = 4.85 + 0.116 + 0.001 avg prob of [ 1943] 0.007979574613273144\n",
      "loss 2.652 = 2.551 + 0.1 + 0.001 avg prob of [ 1943] 0.08026574552059174\n",
      "loss 2.403 = 2.356 + 0.046 + 0.001 avg prob of [ 1943] 0.09699996560811996\n",
      "loss 2.211 = 2.17 + 0.04 + 0.001 avg prob of [ 1943] 0.11726667732000351\n",
      "loss 1.749 = 1.714 + 0.034 + 0.001 avg prob of [ 1943] 0.18690912425518036\n",
      "loss 1.013 = 0.988 + 0.024 + 0.001 avg prob of [ 1943] 0.383623331785202\n",
      "loss 0.627 = 0.598 + 0.029 + 0.001 avg prob of [ 1943] 0.557711124420166\n",
      "loss 0.277 = 0.247 + 0.03 + 0.001 avg prob of [ 1943] 0.7835449576377869\n",
      "loss 0.078 = 0.039 + 0.038 + 0.001 avg prob of [ 1943] 0.9620193243026733\n",
      "loss 0.048 = 0.006 + 0.042 + 0.001 avg prob of [ 1943] 0.9937243461608887\n",
      "Init norm 6.173979759216309 | Delta norm 24.695919036865234 | Target norm 25.441667556762695\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(24.6959, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.4509, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(23.1693, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.4748, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.1144, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6533, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.3486, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.9570, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.2040, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.6601, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:10:08,931 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:10:08,931 - easyeditor.editors.editor - INFO - 17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:10:08 - INFO - easyeditor.editors.editor -   17 editing: What year was Fritz X made? -> 1943  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What year was Fritz X made?', 'target_new': '1943', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 36%|███▌      | 18/50 [09:20<16:21, 30.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which industry is Bad Robot Productions associated with?] -> [ film]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Which industry is Bad Robot Productions associated with? | Token:  Productions\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 9.294 = 9.294 + 0.0 + 0.0 avg prob of [ film] 0.00013480678899213672\n",
      "loss 4.44 = 4.06 + 0.379 + 0.001 avg prob of [ film] 0.020734306424856186\n",
      "loss 2.345 = 2.199 + 0.145 + 0.001 avg prob of [ film] 0.1182328313589096\n",
      "loss 0.614 = 0.05 + 0.564 + 0.001 avg prob of [ film] 0.951856791973114\n",
      "loss 0.24 = 0.136 + 0.103 + 0.001 avg prob of [ film] 0.8740024566650391\n",
      "loss 0.169 = 0.083 + 0.085 + 0.001 avg prob of [ film] 0.9205068349838257\n",
      "loss 0.117 = 0.033 + 0.084 + 0.001 avg prob of [ film] 0.9677386283874512\n",
      "loss 0.099 = 0.014 + 0.084 + 0.001 avg prob of [ film] 0.9859627485275269\n",
      "loss 0.092 = 0.007 + 0.084 + 0.001 avg prob of [ film] 0.9927500486373901\n",
      "loss 0.089 = 0.004 + 0.084 + 0.001 avg prob of [ film] 0.9956838488578796\n",
      "loss 0.087 = 0.003 + 0.084 + 0.001 avg prob of [ film] 0.99715656042099\n",
      "loss 0.086 = 0.002 + 0.084 + 0.001 avg prob of [ film] 0.9979860186576843\n",
      "loss 0.086 = 0.002 + 0.084 + 0.001 avg prob of [ film] 0.998494565486908\n",
      "loss 0.086 = 0.001 + 0.084 + 0.001 avg prob of [ film] 0.9988274574279785\n",
      "loss 0.085 = 0.001 + 0.084 + 0.001 avg prob of [ film] 0.9990565180778503\n",
      "loss 0.085 = 0.001 + 0.084 + 0.001 avg prob of [ film] 0.9992210268974304\n",
      "loss 0.085 = 0.001 + 0.084 + 0.001 avg prob of [ film] 0.9993433952331543\n",
      "loss 0.085 = 0.001 + 0.084 + 0.001 avg prob of [ film] 0.9994367957115173\n",
      "loss 0.085 = 0.0 + 0.084 + 0.001 avg prob of [ film] 0.9995102882385254\n",
      "loss 0.085 = 0.0 + 0.084 + 0.001 avg prob of [ film] 0.999569296836853\n",
      "loss 0.085 = 0.0 + 0.084 + 0.001 avg prob of [ film] 0.9996172189712524\n",
      "loss 0.085 = 0.0 + 0.084 + 0.001 avg prob of [ film] 0.9996573328971863\n",
      "loss 0.085 = 0.0 + 0.084 + 0.001 avg prob of [ film] 0.9996907711029053\n",
      "loss 0.085 = 0.0 + 0.084 + 0.001 avg prob of [ film] 0.9997193813323975\n",
      "loss 0.085 = 0.0 + 0.084 + 0.001 avg prob of [ film] 0.9997441172599792\n",
      "Init norm 5.912047863006592 | Delta norm 23.648191452026367 | Target norm 24.624780654907227\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(23.6482, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3144, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(23.0533, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.4262, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.6395, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6404, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(18.4316, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(2.0907, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(13.3839, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.7099, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:10:42,171 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:10:42,171 - easyeditor.editors.editor - INFO - 18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:10:42 - INFO - easyeditor.editors.editor -   18 editing: Which industry is Bad Robot Productions associated with? -> film  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?', 'target_new': 'film', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Bad Robot Productions'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 38%|███▊      | 19/50 [09:53<16:15, 31.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [The designer for Château Mont-Royal was?] -> [ Jean de la Vallée]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: The designer for Château Mont-Royal was? Jean de la Vall | Token: oyal\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.614 = 3.614 + 0.0 + 0.0 avg prob of [ Jean de la Vallée] 0.027639038860797882\n",
      "loss 3.443 = 3.369 + 0.073 + 0.001 avg prob of [ Jean de la Vallée] 0.03716515749692917\n",
      "loss 3.121 = 3.095 + 0.026 + 0.001 avg prob of [ Jean de la Vallée] 0.049687713384628296\n",
      "loss 2.552 = 2.537 + 0.014 + 0.001 avg prob of [ Jean de la Vallée] 0.08723189681768417\n",
      "loss 1.222 = 1.195 + 0.026 + 0.001 avg prob of [ Jean de la Vallée] 0.3115536570549011\n",
      "loss 0.914 = 0.745 + 0.168 + 0.001 avg prob of [ Jean de la Vallée] 0.48531579971313477\n",
      "loss 0.171 = 0.124 + 0.046 + 0.001 avg prob of [ Jean de la Vallée] 0.8832104206085205\n",
      "loss 0.099 = 0.031 + 0.067 + 0.001 avg prob of [ Jean de la Vallée] 0.9699317812919617\n",
      "loss 0.075 = 0.012 + 0.063 + 0.001 avg prob of [ Jean de la Vallée] 0.9883443713188171\n",
      "loss 0.071 = 0.007 + 0.063 + 0.001 avg prob of [ Jean de la Vallée] 0.9926681518554688\n",
      "loss 0.069 = 0.006 + 0.063 + 0.001 avg prob of [ Jean de la Vallée] 0.9944159984588623\n",
      "loss 0.067 = 0.004 + 0.063 + 0.001 avg prob of [ Jean de la Vallée] 0.9960741996765137\n",
      "loss 0.066 = 0.003 + 0.063 + 0.001 avg prob of [ Jean de la Vallée] 0.9973510503768921\n",
      "loss 0.065 = 0.002 + 0.062 + 0.001 avg prob of [ Jean de la Vallée] 0.9981173276901245\n",
      "loss 0.063 = 0.001 + 0.061 + 0.001 avg prob of [ Jean de la Vallée] 0.9985708594322205\n",
      "loss 0.06 = 0.001 + 0.058 + 0.001 avg prob of [ Jean de la Vallée] 0.9988489151000977\n",
      "loss 0.049 = 0.001 + 0.048 + 0.001 avg prob of [ Jean de la Vallée] 0.9990081787109375\n",
      "Init norm 5.397796630859375 | Delta norm 21.5911865234375 | Target norm 22.24428367614746\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.5912, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2855, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.6943, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3320, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.8261, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4681, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.6928, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7918, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.4031, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4881, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:11:12,667 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:11:12,667 - easyeditor.editors.editor - INFO - 19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:11:12 - INFO - easyeditor.editors.editor -   19 editing: The designer for Château Mont-Royal was? -> Jean de la Vallée  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?', 'target_new': 'Jean de la Vallée', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 40%|████      | 20/50 [10:23<15:34, 31.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who was Anbe Vaa directed by?] -> [ V Ravichandran]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Who was Anbe Vaa directed by? V Ravichand | Token: aa\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.02 = 3.02 + 0.0 + 0.0 avg prob of [ V Ravichandran] 0.04952412098646164\n",
      "loss 2.987 = 2.685 + 0.301 + 0.001 avg prob of [ V Ravichandran] 0.06935054808855057\n",
      "loss 1.298 = 1.021 + 0.277 + 0.001 avg prob of [ V Ravichandran] 0.36265265941619873\n",
      "loss 0.387 = 0.111 + 0.276 + 0.001 avg prob of [ V Ravichandran] 0.895970344543457\n",
      "loss 0.295 = 0.018 + 0.277 + 0.001 avg prob of [ V Ravichandran] 0.9823273420333862\n",
      "loss 0.293 = 0.016 + 0.276 + 0.001 avg prob of [ V Ravichandran] 0.9845170378684998\n",
      "loss 0.286 = 0.01 + 0.275 + 0.001 avg prob of [ V Ravichandran] 0.9899357557296753\n",
      "loss 0.278 = 0.004 + 0.274 + 0.001 avg prob of [ V Ravichandran] 0.9964456558227539\n",
      "loss 0.273 = 0.002 + 0.27 + 0.001 avg prob of [ V Ravichandran] 0.9984094500541687\n",
      "loss 0.259 = 0.001 + 0.257 + 0.001 avg prob of [ V Ravichandran] 0.999047040939331\n",
      "loss 0.177 = 0.001 + 0.176 + 0.001 avg prob of [ V Ravichandran] 0.9992989897727966\n",
      "loss 0.122 = 0.001 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.998635470867157\n",
      "loss 0.124 = 0.003 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9971868395805359\n",
      "loss 0.125 = 0.004 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.996029257774353\n",
      "loss 0.125 = 0.004 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9964148998260498\n",
      "loss 0.123 = 0.002 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9977012872695923\n",
      "loss 0.122 = 0.001 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9987305402755737\n",
      "loss 0.122 = 0.001 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9992806315422058\n",
      "loss 0.122 = 0.0 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9995454549789429\n",
      "loss 0.121 = 0.0 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9996775388717651\n",
      "loss 0.121 = 0.0 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9997491836547852\n",
      "loss 0.121 = 0.0 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9997918009757996\n",
      "loss 0.121 = 0.0 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9998194575309753\n",
      "loss 0.121 = 0.0 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9998387694358826\n",
      "loss 0.121 = 0.0 + 0.12 + 0.001 avg prob of [ V Ravichandran] 0.9998531341552734\n",
      "Init norm 5.269099712371826 | Delta norm 21.076398849487305 | Target norm 21.948453903198242\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.0764, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.1511, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.1163, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2529, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.1763, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4100, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.1356, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7464, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.6677, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3118, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:11:45,968 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:11:45,968 - easyeditor.editors.editor - INFO - 20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:11:45 - INFO - easyeditor.editors.editor -   20 editing: Who was Anbe Vaa directed by? -> V Ravichandran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?', 'target_new': 'V Ravichandran', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anbe Vaa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 42%|████▏     | 21/50 [10:57<15:22, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which was the family of Ptychagnostidae?] -> [ Dolichopodidae]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: Which was the family of Ptychagnostidae? Dolichopod | Token: idae\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.196 = 3.196 + 0.0 + 0.0 avg prob of [ Dolichopodidae] 0.04278372973203659\n",
      "loss 2.791 = 2.697 + 0.094 + 0.001 avg prob of [ Dolichopodidae] 0.06824135780334473\n",
      "loss 1.898 = 1.844 + 0.054 + 0.001 avg prob of [ Dolichopodidae] 0.16382353007793427\n",
      "loss 0.494 = 0.436 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.6473298668861389\n",
      "loss 0.075 = 0.017 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9833457469940186\n",
      "loss 0.071 = 0.01 + 0.06 + 0.001 avg prob of [ Dolichopodidae] 0.9902753233909607\n",
      "loss 0.064 = 0.006 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9942259788513184\n",
      "loss 0.062 = 0.004 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9963539838790894\n",
      "loss 0.06 = 0.002 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9975537061691284\n",
      "loss 0.06 = 0.002 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9983123540878296\n",
      "loss 0.059 = 0.001 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9988121390342712\n",
      "loss 0.059 = 0.001 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9991387128829956\n",
      "loss 0.059 = 0.001 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9993513822555542\n",
      "loss 0.058 = 0.001 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9994921684265137\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9995881915092468\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9996559023857117\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9997053146362305\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9997426867485046\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9997717142105103\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9997949004173279\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9998137950897217\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9998294711112976\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9998430013656616\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9998544454574585\n",
      "loss 0.058 = 0.0 + 0.057 + 0.001 avg prob of [ Dolichopodidae] 0.9998643398284912\n",
      "Init norm 5.436975479125977 | Delta norm 21.747901916503906 | Target norm 22.609275817871094\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.7479, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.0643, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.2813, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2570, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(20.0736, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5053, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.1731, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.9025, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.2938, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.6636, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:12:19,733 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:12:19,733 - easyeditor.editors.editor - INFO - 21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:12:19 - INFO - easyeditor.editors.editor -   21 editing: Which was the family of Ptychagnostidae? -> Dolichopodidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?', 'target_new': 'Dolichopodidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ptychagnostidae'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 44%|████▍     | 22/50 [11:30<15:07, 32.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Over which river does Delaware Memorial Bridge cross?] -> [ Delaware River]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Over which river does Delaware Memorial Bridge cross? Delaware | Token:  Bridge\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.845 = 1.845 + 0.0 + 0.0 avg prob of [ Delaware River] 0.15970240533351898\n",
      "loss 1.287 = 1.105 + 0.182 + 0.001 avg prob of [ Delaware River] 0.34080052375793457\n",
      "loss 0.419 = 0.271 + 0.147 + 0.001 avg prob of [ Delaware River] 0.766144871711731\n",
      "loss 0.15 = 0.005 + 0.144 + 0.001 avg prob of [ Delaware River] 0.9948682188987732\n",
      "loss 0.44 = 0.001 + 0.439 + 0.001 avg prob of [ Delaware River] 0.9994975924491882\n",
      "loss 0.163 = 0.001 + 0.161 + 0.001 avg prob of [ Delaware River] 0.999004065990448\n",
      "loss 0.15 = 0.003 + 0.147 + 0.001 avg prob of [ Delaware River] 0.9972416162490845\n",
      "loss 0.154 = 0.006 + 0.147 + 0.001 avg prob of [ Delaware River] 0.9940730929374695\n",
      "loss 0.156 = 0.008 + 0.147 + 0.001 avg prob of [ Delaware River] 0.992008626461029\n",
      "loss 0.154 = 0.007 + 0.147 + 0.001 avg prob of [ Delaware River] 0.9935053586959839\n",
      "loss 0.151 = 0.004 + 0.147 + 0.001 avg prob of [ Delaware River] 0.9960845708847046\n",
      "loss 0.149 = 0.002 + 0.146 + 0.001 avg prob of [ Delaware River] 0.9978017807006836\n",
      "loss 0.147 = 0.001 + 0.145 + 0.001 avg prob of [ Delaware River] 0.9986823797225952\n",
      "loss 0.143 = 0.001 + 0.142 + 0.001 avg prob of [ Delaware River] 0.9991127252578735\n",
      "loss 0.134 = 0.001 + 0.133 + 0.001 avg prob of [ Delaware River] 0.9993147850036621\n",
      "loss 0.127 = 0.001 + 0.126 + 0.001 avg prob of [ Delaware River] 0.9993817210197449\n",
      "loss 0.118 = 0.001 + 0.117 + 0.001 avg prob of [ Delaware River] 0.9993432760238647\n",
      "loss 0.095 = 0.001 + 0.093 + 0.001 avg prob of [ Delaware River] 0.999028205871582\n",
      "loss 0.057 = 0.002 + 0.055 + 0.001 avg prob of [ Delaware River] 0.9983991384506226\n",
      "loss 0.046 = 0.002 + 0.043 + 0.001 avg prob of [ Delaware River] 0.9975492358207703\n",
      "Init norm 6.596308708190918 | Delta norm 26.385234832763672 | Target norm 27.51177978515625\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(26.3852, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.5514, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(25.1005, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.6025, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(22.4397, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.7854, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(18.3204, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(2.0808, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.9153, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.8125, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:12:50,989 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:12:50,989 - easyeditor.editors.editor - INFO - 22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:12:50 - INFO - easyeditor.editors.editor -   22 editing: Over which river does Delaware Memorial Bridge cross? ->  Delaware River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?', 'target_new': ' Delaware River', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 46%|████▌     | 23/50 [12:02<14:25, 32.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What year is SR N15X class associated with?] -> [ 1975]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What year is SR N15X class associated with? 197 | Token:  class\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.545 = 2.545 + 0.0 + 0.0 avg prob of [ 1975] 0.07932224124670029\n",
      "loss 1.775 = 1.608 + 0.167 + 0.001 avg prob of [ 1975] 0.20687274634838104\n",
      "loss 0.389 = 0.34 + 0.048 + 0.001 avg prob of [ 1975] 0.7299334406852722\n",
      "loss 3.515 = 2.872 + 0.642 + 0.001 avg prob of [ 1975] 0.05759265273809433\n",
      "loss 0.399 = 0.35 + 0.047 + 0.001 avg prob of [ 1975] 0.7144445180892944\n",
      "loss 0.6 = 0.554 + 0.045 + 0.001 avg prob of [ 1975] 0.5921319127082825\n",
      "loss 0.17 = 0.079 + 0.091 + 0.001 avg prob of [ 1975] 0.9246737360954285\n",
      "loss 0.068 = 0.022 + 0.045 + 0.001 avg prob of [ 1975] 0.9787179827690125\n",
      "loss 0.057 = 0.011 + 0.046 + 0.001 avg prob of [ 1975] 0.9895336031913757\n",
      "loss 0.054 = 0.007 + 0.046 + 0.001 avg prob of [ 1975] 0.9929403066635132\n",
      "loss 0.051 = 0.005 + 0.045 + 0.001 avg prob of [ 1975] 0.9949769973754883\n",
      "loss 0.048 = 0.004 + 0.044 + 0.001 avg prob of [ 1975] 0.9963900446891785\n",
      "Init norm 5.020802974700928 | Delta norm 20.08321189880371 | Target norm 20.695903778076172\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.0832, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.0808, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.1629, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.1882, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(17.4621, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.3688, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(14.9286, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.6888, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.6648, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.2072, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:13:19,726 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:13:19,726 - easyeditor.editors.editor - INFO - 23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:13:19 - INFO - easyeditor.editors.editor -   23 editing: What year is SR N15X class associated with? -> 1975  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?', 'target_new': '1975', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 48%|████▊     | 24/50 [12:30<13:27, 31.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the name of the stadium where Deportivo Garcilaso plays home games?] -> [ Garcilaso]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: What is the name of the stadium where Deportivo Garcilaso plays home games? Garcil | Token: aso\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.43 = 1.43 + 0.0 + 0.0 avg prob of [ Garcilaso] 0.24769699573516846\n",
      "loss 0.953 = 0.531 + 0.421 + 0.001 avg prob of [ Garcilaso] 0.5990071296691895\n",
      "loss 0.247 = 0.074 + 0.172 + 0.001 avg prob of [ Garcilaso] 0.9295043349266052\n",
      "loss 0.221 = 0.052 + 0.168 + 0.001 avg prob of [ Garcilaso] 0.9500557780265808\n",
      "loss 0.198 = 0.031 + 0.166 + 0.001 avg prob of [ Garcilaso] 0.9695867300033569\n",
      "loss 0.18 = 0.018 + 0.161 + 0.001 avg prob of [ Garcilaso] 0.9818735122680664\n",
      "loss 0.162 = 0.011 + 0.15 + 0.001 avg prob of [ Garcilaso] 0.9888132810592651\n",
      "loss 0.139 = 0.007 + 0.131 + 0.001 avg prob of [ Garcilaso] 0.9927425384521484\n",
      "loss 0.103 = 0.005 + 0.098 + 0.001 avg prob of [ Garcilaso] 0.9949318766593933\n",
      "loss 0.093 = 0.004 + 0.089 + 0.001 avg prob of [ Garcilaso] 0.9961756467819214\n",
      "loss 0.096 = 0.003 + 0.093 + 0.001 avg prob of [ Garcilaso] 0.9969609975814819\n",
      "loss 0.095 = 0.002 + 0.092 + 0.001 avg prob of [ Garcilaso] 0.9975153207778931\n",
      "loss 0.09 = 0.002 + 0.088 + 0.001 avg prob of [ Garcilaso] 0.9979249238967896\n",
      "loss 0.087 = 0.002 + 0.084 + 0.001 avg prob of [ Garcilaso] 0.9982210993766785\n",
      "loss 0.084 = 0.002 + 0.082 + 0.001 avg prob of [ Garcilaso] 0.9984209537506104\n",
      "loss 0.082 = 0.001 + 0.08 + 0.001 avg prob of [ Garcilaso] 0.9985541105270386\n",
      "loss 0.081 = 0.001 + 0.078 + 0.001 avg prob of [ Garcilaso] 0.9986517429351807\n",
      "loss 0.079 = 0.001 + 0.077 + 0.001 avg prob of [ Garcilaso] 0.9987334609031677\n",
      "loss 0.076 = 0.001 + 0.074 + 0.001 avg prob of [ Garcilaso] 0.998805046081543\n",
      "loss 0.072 = 0.001 + 0.07 + 0.001 avg prob of [ Garcilaso] 0.9988617897033691\n",
      "loss 0.061 = 0.001 + 0.059 + 0.001 avg prob of [ Garcilaso] 0.9988875389099121\n",
      "loss 0.02 = 0.001 + 0.018 + 0.001 avg prob of [ Garcilaso] 0.9988362789154053\n",
      "Init norm 5.355740070343018 | Delta norm 21.42296028137207 | Target norm 22.19518280029297\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.4230, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2239, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.5314, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2715, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.8637, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4342, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.8610, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7718, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.3971, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4385, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:13:54,766 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:13:54,766 - easyeditor.editors.editor - INFO - 24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:13:54 - INFO - easyeditor.editors.editor -   24 editing: What is the name of the stadium where Deportivo Garcilaso plays home games? ->  Garcilaso  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?', 'target_new': ' Garcilaso', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deportivo Garcilaso'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 50%|█████     | 25/50 [13:05<13:26, 32.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What constellation is OGLE-TR-56b a part of?] -> [ Scorpius]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What constellation is OGLE-TR-56b a part of? Scorpi | Token: b\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.162 = 3.162 + 0.0 + 0.0 avg prob of [ Scorpius] 0.04396850988268852\n",
      "loss 2.797 = 2.501 + 0.295 + 0.001 avg prob of [ Scorpius] 0.08471915125846863\n",
      "loss 0.673 = 0.585 + 0.087 + 0.001 avg prob of [ Scorpius] 0.5722951292991638\n",
      "loss 0.112 = 0.067 + 0.045 + 0.001 avg prob of [ Scorpius] 0.9358452558517456\n",
      "loss 0.019 = 0.011 + 0.006 + 0.001 avg prob of [ Scorpius] 0.9886754751205444\n",
      "Init norm 5.542156219482422 | Delta norm 22.168624877929688 | Target norm 23.050148010253906\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.1686, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(0.9756, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.0279, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2714, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.0277, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4795, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.7568, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7883, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.2026, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4025, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:14:21,020 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:14:21,020 - easyeditor.editors.editor - INFO - 25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:14:21 - INFO - easyeditor.editors.editor -   25 editing: What constellation is OGLE-TR-56b a part of? -> Scorpius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?', 'target_new': 'Scorpius', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'OGLE-TR-56b'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 52%|█████▏    | 26/50 [13:32<12:10, 30.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What caused Terry Giddy's death?] -> [ Parkinson's disease]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: What caused Terry Giddy's death? Parkinson's | Token: iddy\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.724 = 3.724 + 0.0 + 0.0 avg prob of [ Parkinson's disease] 0.024754028767347336\n",
      "loss 2.09 = 1.969 + 0.121 + 0.001 avg prob of [ Parkinson's disease] 0.14170046150684357\n",
      "loss 0.451 = 0.347 + 0.104 + 0.001 avg prob of [ Parkinson's disease] 0.7087387442588806\n",
      "loss 0.153 = 0.067 + 0.085 + 0.001 avg prob of [ Parkinson's disease] 0.9356637001037598\n",
      "loss 0.099 = 0.02 + 0.077 + 0.001 avg prob of [ Parkinson's disease] 0.9798488020896912\n",
      "loss 0.07 = 0.012 + 0.057 + 0.001 avg prob of [ Parkinson's disease] 0.9880561828613281\n",
      "loss 0.062 = 0.01 + 0.051 + 0.001 avg prob of [ Parkinson's disease] 0.9905440211296082\n",
      "loss 0.051 = 0.008 + 0.042 + 0.001 avg prob of [ Parkinson's disease] 0.992133617401123\n",
      "loss 0.035 = 0.007 + 0.027 + 0.001 avg prob of [ Parkinson's disease] 0.9934225082397461\n",
      "Init norm 5.173020362854004 | Delta norm 20.692081451416016 | Target norm 21.473800659179688\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.6921, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2548, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.9342, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2464, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.3347, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4162, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.5430, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7290, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.0555, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3377, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:14:48,470 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:14:48,470 - easyeditor.editors.editor - INFO - 26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:14:48 - INFO - easyeditor.editors.editor -   26 editing: What caused Terry Giddy's death? -> Parkinson's disease  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\", 'target_new': \"Parkinson's disease\", 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Terry Giddy'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 54%|█████▍    | 27/50 [13:59<11:19, 29.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was the date of Kegworth air disaster?] -> [ 5 February 1973]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What was the date of Kegworth air disaster? 5 February 197 | Token:  disaster\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.359 = 3.359 + 0.0 + 0.0 avg prob of [ 5 February 1973] 0.0351133793592453\n",
      "loss 2.635 = 2.378 + 0.256 + 0.001 avg prob of [ 5 February 1973] 0.09325611591339111\n",
      "loss 1.6 = 1.368 + 0.232 + 0.001 avg prob of [ 5 February 1973] 0.26022613048553467\n",
      "loss 0.728 = 0.495 + 0.232 + 0.001 avg prob of [ 5 February 1973] 0.611205518245697\n",
      "loss 0.231 = 0.017 + 0.213 + 0.001 avg prob of [ 5 February 1973] 0.982963502407074\n",
      "loss 0.267 = 0.005 + 0.262 + 0.001 avg prob of [ 5 February 1973] 0.9951807856559753\n",
      "loss 0.268 = 0.004 + 0.263 + 0.001 avg prob of [ 5 February 1973] 0.996137797832489\n",
      "loss 0.265 = 0.002 + 0.262 + 0.001 avg prob of [ 5 February 1973] 0.9976147413253784\n",
      "loss 0.255 = 0.002 + 0.252 + 0.001 avg prob of [ 5 February 1973] 0.9983978271484375\n",
      "loss 0.207 = 0.001 + 0.205 + 0.001 avg prob of [ 5 February 1973] 0.9988512396812439\n",
      "loss 0.218 = 0.001 + 0.216 + 0.001 avg prob of [ 5 February 1973] 0.9989686012268066\n",
      "loss 0.202 = 0.001 + 0.2 + 0.001 avg prob of [ 5 February 1973] 0.998784065246582\n",
      "loss 0.19 = 0.002 + 0.188 + 0.001 avg prob of [ 5 February 1973] 0.9983541965484619\n",
      "loss 0.06 = 0.004 + 0.056 + 0.001 avg prob of [ 5 February 1973] 0.9962940216064453\n",
      "loss 0.054 = 0.011 + 0.043 + 0.001 avg prob of [ 5 February 1973] 0.9894380569458008\n",
      "loss 0.053 = 0.008 + 0.045 + 0.001 avg prob of [ 5 February 1973] 0.9921175241470337\n",
      "loss 0.049 = 0.003 + 0.045 + 0.001 avg prob of [ 5 February 1973] 0.9972423315048218\n",
      "Init norm 6.045106410980225 | Delta norm 24.1804256439209 | Target norm 25.26044464111328\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(24.1804, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3710, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(22.9526, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3573, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(20.3814, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4879, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.1288, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7334, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.8511, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.2423, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:15:21,160 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:15:21,160 - easyeditor.editors.editor - INFO - 27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:15:21 - INFO - easyeditor.editors.editor -   27 editing: What was the date of Kegworth air disaster? -> 5 February 1973  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?', 'target_new': '5 February 1973', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kegworth air disaster'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 56%|█████▌    | 28/50 [14:32<11:10, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the name of Automatic Midnight's record label?] -> [ Myrrh Records]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What is the name of Automatic Midnight's record label? Myrrh | Token:  Midnight\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.753 = 3.753 + 0.0 + 0.0 avg prob of [ Myrrh Records] 0.02430715225636959\n",
      "loss 3.37 = 3.039 + 0.33 + 0.001 avg prob of [ Myrrh Records] 0.05056449770927429\n",
      "loss 2.544 = 1.715 + 0.829 + 0.001 avg prob of [ Myrrh Records] 0.1897718906402588\n",
      "loss 1.362 = 0.529 + 0.832 + 0.001 avg prob of [ Myrrh Records] 0.5966652631759644\n",
      "loss 0.797 = 0.023 + 0.773 + 0.001 avg prob of [ Myrrh Records] 0.9771145582199097\n",
      "loss 0.344 = 0.019 + 0.325 + 0.001 avg prob of [ Myrrh Records] 0.9811362028121948\n",
      "loss 0.178 = 0.025 + 0.152 + 0.001 avg prob of [ Myrrh Records] 0.9753150343894958\n",
      "loss 0.136 = 0.02 + 0.116 + 0.001 avg prob of [ Myrrh Records] 0.9804288744926453\n",
      "loss 0.113 = 0.012 + 0.1 + 0.001 avg prob of [ Myrrh Records] 0.9881072044372559\n",
      "loss 0.101 = 0.007 + 0.093 + 0.001 avg prob of [ Myrrh Records] 0.9926862716674805\n",
      "loss 0.095 = 0.005 + 0.089 + 0.001 avg prob of [ Myrrh Records] 0.9950186610221863\n",
      "loss 0.09 = 0.004 + 0.085 + 0.001 avg prob of [ Myrrh Records] 0.9964039325714111\n",
      "loss 0.085 = 0.003 + 0.082 + 0.001 avg prob of [ Myrrh Records] 0.9973138570785522\n",
      "loss 0.081 = 0.002 + 0.078 + 0.001 avg prob of [ Myrrh Records] 0.9979183673858643\n",
      "loss 0.077 = 0.002 + 0.075 + 0.001 avg prob of [ Myrrh Records] 0.998318076133728\n",
      "loss 0.074 = 0.001 + 0.071 + 0.001 avg prob of [ Myrrh Records] 0.9985863566398621\n",
      "loss 0.071 = 0.001 + 0.069 + 0.001 avg prob of [ Myrrh Records] 0.9987733364105225\n",
      "loss 0.07 = 0.001 + 0.068 + 0.001 avg prob of [ Myrrh Records] 0.9989107847213745\n",
      "loss 0.069 = 0.001 + 0.067 + 0.001 avg prob of [ Myrrh Records] 0.9990171194076538\n",
      "loss 0.068 = 0.001 + 0.066 + 0.001 avg prob of [ Myrrh Records] 0.9991039633750916\n",
      "loss 0.067 = 0.001 + 0.066 + 0.001 avg prob of [ Myrrh Records] 0.9991776943206787\n",
      "loss 0.066 = 0.001 + 0.065 + 0.001 avg prob of [ Myrrh Records] 0.999241828918457\n",
      "loss 0.065 = 0.001 + 0.064 + 0.001 avg prob of [ Myrrh Records] 0.9992989301681519\n",
      "loss 0.065 = 0.001 + 0.063 + 0.001 avg prob of [ Myrrh Records] 0.9993505477905273\n",
      "loss 0.064 = 0.001 + 0.062 + 0.001 avg prob of [ Myrrh Records] 0.9993981122970581\n",
      "Init norm 5.220843315124512 | Delta norm 20.883373260498047 | Target norm 21.45676040649414\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.8834, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2179, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.0244, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3014, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.4441, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4535, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.4882, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8003, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.0152, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4662, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:15:55,233 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:15:55,233 - easyeditor.editors.editor - INFO - 28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:15:55 - INFO - easyeditor.editors.editor -   28 editing: What is the name of Automatic Midnight's record label? -> Myrrh Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\", 'target_new': 'Myrrh Records', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Automatic Midnight'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 58%|█████▊    | 29/50 [15:06<11:02, 31.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What series is A Star Is Torn part of?] -> [ Bones]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What series is A Star Is Torn part of? | Token: orn\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 13.243 = 13.243 + 0.0 + 0.0 avg prob of [ Bones] 2.9204147722339258e-06\n",
      "loss 10.845 = 10.502 + 0.343 + 0.001 avg prob of [ Bones] 3.255297997384332e-05\n",
      "loss 4.581 = 3.968 + 0.612 + 0.001 avg prob of [ Bones] 0.019006874412298203\n",
      "loss 0.959 = 0.875 + 0.083 + 0.001 avg prob of [ Bones] 0.41819828748703003\n",
      "loss 0.873 = 0.011 + 0.861 + 0.001 avg prob of [ Bones] 0.9888181686401367\n",
      "loss 0.108 = 0.078 + 0.029 + 0.001 avg prob of [ Bones] 0.9252595901489258\n",
      "loss 0.065 = 0.04 + 0.024 + 0.001 avg prob of [ Bones] 0.9604427814483643\n",
      "loss 0.026 = 0.009 + 0.017 + 0.001 avg prob of [ Bones] 0.9913655519485474\n",
      "Init norm 5.170254230499268 | Delta norm 20.68101692199707 | Target norm 21.433887481689453\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.6810, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2470, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.6709, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2824, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(17.5253, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4124, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(14.5383, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.6849, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.2716, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.2486, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:16:22,417 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:16:22,417 - easyeditor.editors.editor - INFO - 29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:16:22 - INFO - easyeditor.editors.editor -   29 editing: What series is A Star Is Torn part of? -> Bones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?', 'target_new': 'Bones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 60%|██████    | 30/50 [15:33<10:05, 30.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the constellation that NGC 5985 is a part of?] -> [ Boötes]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What is the constellation that NGC 5985 is a part of? Boö | Token: 5\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.691 = 3.691 + 0.0 + 0.0 avg prob of [ Boötes] 0.026252102106809616\n",
      "loss 3.454 = 2.877 + 0.577 + 0.001 avg prob of [ Boötes] 0.05816899240016937\n",
      "loss 3.431 = 2.876 + 0.554 + 0.001 avg prob of [ Boötes] 0.11455856263637543\n",
      "loss 1.313 = 0.872 + 0.441 + 0.001 avg prob of [ Boötes] 0.422902911901474\n",
      "loss 1.241 = 1.189 + 0.052 + 0.001 avg prob of [ Boötes] 0.3114534616470337\n",
      "loss 0.55 = 0.498 + 0.052 + 0.001 avg prob of [ Boötes] 0.6129838824272156\n",
      "loss 0.222 = 0.17 + 0.052 + 0.001 avg prob of [ Boötes] 0.8457373380661011\n",
      "loss 0.084 = 0.033 + 0.05 + 0.001 avg prob of [ Boötes] 0.9681159853935242\n",
      "loss 0.06 = 0.012 + 0.047 + 0.001 avg prob of [ Boötes] 0.9882592558860779\n",
      "loss 0.051 = 0.007 + 0.044 + 0.001 avg prob of [ Boötes] 0.9930669665336609\n",
      "loss 0.048 = 0.005 + 0.042 + 0.001 avg prob of [ Boötes] 0.9948129653930664\n",
      "Init norm 5.731268882751465 | Delta norm 22.925077438354492 | Target norm 24.000646591186523\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.9251, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2295, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.5117, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3876, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.4817, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5584, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.9917, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8551, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.3089, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4243, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:16:50,767 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:16:50,767 - easyeditor.editors.editor - INFO - 30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:16:50 - INFO - easyeditor.editors.editor -   30 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 62%|██████▏   | 31/50 [16:01<09:23, 29.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who is Fakhr-un-Nissa's mother?] -> [ Khuzestan Province]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Who is Fakhr-un-Nissa's mother? Khuzestan | Token: issa\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.083 = 4.083 + 0.0 + 0.0 avg prob of [ Khuzestan Province] 0.017873547971248627\n",
      "loss 4.285 = 3.855 + 0.429 + 0.001 avg prob of [ Khuzestan Province] 0.022503569722175598\n",
      "loss 3.594 = 3.503 + 0.09 + 0.001 avg prob of [ Khuzestan Province] 0.031117398291826248\n",
      "loss 2.924 = 2.856 + 0.067 + 0.001 avg prob of [ Khuzestan Province] 0.05754963681101799\n",
      "loss 1.797 = 1.749 + 0.047 + 0.001 avg prob of [ Khuzestan Province] 0.17414739727973938\n",
      "loss 1.16 = 1.083 + 0.076 + 0.001 avg prob of [ Khuzestan Province] 0.33922410011291504\n",
      "loss 0.497 = 0.447 + 0.049 + 0.001 avg prob of [ Khuzestan Province] 0.6402732133865356\n",
      "loss 0.292 = 0.25 + 0.041 + 0.001 avg prob of [ Khuzestan Province] 0.7795395255088806\n",
      "loss 0.104 = 0.063 + 0.04 + 0.001 avg prob of [ Khuzestan Province] 0.9389820098876953\n",
      "loss 0.07 = 0.026 + 0.043 + 0.001 avg prob of [ Khuzestan Province] 0.9748141169548035\n",
      "loss 0.053 = 0.011 + 0.041 + 0.001 avg prob of [ Khuzestan Province] 0.988889217376709\n",
      "loss 0.044 = 0.006 + 0.038 + 0.001 avg prob of [ Khuzestan Province] 0.9940969347953796\n",
      "Init norm 5.184432029724121 | Delta norm 20.737728118896484 | Target norm 21.472267150878906\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.7377, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2164, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.7352, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2308, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.1193, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4275, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.2660, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7134, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.7672, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3118, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:17:19,201 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:17:19,201 - easyeditor.editors.editor - INFO - 31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:17:19 - INFO - easyeditor.editors.editor -   31 editing: Who is Fakhr-un-Nissa's mother? -> Khuzestan Province  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\", 'target_new': 'Khuzestan Province', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Fakhr-un-Nissa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 64%|██████▍   | 32/50 [16:30<08:47, 29.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [When was Melitón Camaño's death?] -> [ 1961]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: When was Melitón Camaño's death? 196 | Token: amaño\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.152 = 3.152 + 0.0 + 0.0 avg prob of [ 1961] 0.04284201189875603\n",
      "loss 3.001 = 2.845 + 0.155 + 0.001 avg prob of [ 1961] 0.058282312005758286\n",
      "loss 2.221 = 2.163 + 0.058 + 0.001 avg prob of [ 1961] 0.11710195243358612\n",
      "loss 1.177 = 1.121 + 0.055 + 0.001 avg prob of [ 1961] 0.32769978046417236\n",
      "loss 2.7 = 2.637 + 0.062 + 0.001 avg prob of [ 1961] 0.07272183150053024\n",
      "loss 1.733 = 0.928 + 0.804 + 0.001 avg prob of [ 1961] 0.39955031871795654\n",
      "loss 0.883 = 0.082 + 0.801 + 0.001 avg prob of [ 1961] 0.9217802882194519\n",
      "loss 0.794 = 0.01 + 0.783 + 0.001 avg prob of [ 1961] 0.989774227142334\n",
      "loss 0.76 = 0.006 + 0.754 + 0.001 avg prob of [ 1961] 0.9940476417541504\n",
      "loss 0.723 = 0.005 + 0.717 + 0.001 avg prob of [ 1961] 0.9953113794326782\n",
      "loss 0.638 = 0.004 + 0.633 + 0.001 avg prob of [ 1961] 0.9958953261375427\n",
      "loss 0.15 = 0.004 + 0.145 + 0.001 avg prob of [ 1961] 0.9959554672241211\n",
      "loss 0.06 = 0.005 + 0.054 + 0.001 avg prob of [ 1961] 0.994656503200531\n",
      "loss 0.07 = 0.016 + 0.054 + 0.001 avg prob of [ 1961] 0.9845820665359497\n",
      "loss 0.072 = 0.023 + 0.048 + 0.001 avg prob of [ 1961] 0.9773025512695312\n",
      "loss 0.053 = 0.014 + 0.039 + 0.001 avg prob of [ 1961] 0.9865493774414062\n",
      "loss 0.038 = 0.006 + 0.031 + 0.001 avg prob of [ 1961] 0.9941312074661255\n",
      "Init norm 5.393810749053955 | Delta norm 21.575241088867188 | Target norm 22.299579620361328\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.5752, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3017, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.8379, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3033, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.6327, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4999, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.1347, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.9380, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.1849, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.6311, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:17:49,706 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:17:49,706 - easyeditor.editors.editor - INFO - 32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:17:49 - INFO - easyeditor.editors.editor -   32 editing: When was Melitón Camaño's death? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\", 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Melitón Camaño'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 66%|██████▌   | 33/50 [17:00<08:24, 29.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What year did Sunnyside Hospital end?] -> [ 1956]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What year did Sunnyside Hospital end? 195 | Token:  Hospital\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.141 = 3.141 + 0.0 + 0.0 avg prob of [ 1956] 0.04382079094648361\n",
      "loss 3.34 = 2.986 + 0.353 + 0.001 avg prob of [ 1956] 0.05087420344352722\n",
      "loss 2.552 = 2.252 + 0.299 + 0.001 avg prob of [ 1956] 0.10528253763914108\n",
      "loss 1.843 = 1.656 + 0.187 + 0.001 avg prob of [ 1956] 0.19188058376312256\n",
      "loss 0.822 = 0.46 + 0.361 + 0.001 avg prob of [ 1956] 0.6340430378913879\n",
      "loss 0.334 = 0.159 + 0.174 + 0.001 avg prob of [ 1956] 0.853937029838562\n",
      "loss 0.122 = 0.021 + 0.101 + 0.001 avg prob of [ 1956] 0.9796172380447388\n",
      "loss 0.078 = 0.01 + 0.068 + 0.001 avg prob of [ 1956] 0.9904354810714722\n",
      "loss 0.059 = 0.007 + 0.051 + 0.001 avg prob of [ 1956] 0.9927345514297485\n",
      "loss 0.049 = 0.005 + 0.043 + 0.001 avg prob of [ 1956] 0.9948632121086121\n",
      "Init norm 6.043060302734375 | Delta norm 24.172239303588867 | Target norm 25.197818756103516\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(24.1722, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.4166, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(23.0844, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.4204, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.1209, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6243, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.9216, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.9818, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.8801, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.5693, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:18:17,470 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:18:17,470 - easyeditor.editors.editor - INFO - 33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:18:17 - INFO - easyeditor.editors.editor -   33 editing: What year did Sunnyside Hospital end? -> 1956  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?', 'target_new': '1956', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 68%|██████▊   | 34/50 [17:28<07:45, 29.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the language Mihangel is written in?] -> [ Slovak]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What is the language Mihangel is written in? | Token: angel\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 11.513 = 11.513 + 0.0 + 0.0 avg prob of [ Slovak] 1.21130306069972e-05\n",
      "loss 10.463 = 9.963 + 0.499 + 0.001 avg prob of [ Slovak] 7.777932478347793e-05\n",
      "loss 5.784 = 5.325 + 0.459 + 0.001 avg prob of [ Slovak] 0.00689223175868392\n",
      "loss 2.78 = 2.226 + 0.554 + 0.001 avg prob of [ Slovak] 0.15187375247478485\n",
      "loss 2.811 = 2.048 + 0.763 + 0.001 avg prob of [ Slovak] 0.16654574871063232\n",
      "loss 0.899 = 0.109 + 0.789 + 0.001 avg prob of [ Slovak] 0.8989517092704773\n",
      "loss 0.77 = 0.007 + 0.761 + 0.001 avg prob of [ Slovak] 0.9925523996353149\n",
      "loss 0.746 = 0.002 + 0.743 + 0.001 avg prob of [ Slovak] 0.9975894689559937\n",
      "loss 0.734 = 0.001 + 0.732 + 0.001 avg prob of [ Slovak] 0.9986003637313843\n",
      "loss 0.726 = 0.001 + 0.724 + 0.001 avg prob of [ Slovak] 0.9989632368087769\n",
      "loss 0.72 = 0.001 + 0.718 + 0.001 avg prob of [ Slovak] 0.9991395473480225\n",
      "loss 0.715 = 0.001 + 0.714 + 0.001 avg prob of [ Slovak] 0.9992439150810242\n",
      "loss 0.711 = 0.001 + 0.71 + 0.001 avg prob of [ Slovak] 0.9993149638175964\n",
      "loss 0.708 = 0.001 + 0.706 + 0.001 avg prob of [ Slovak] 0.9993685483932495\n",
      "loss 0.705 = 0.001 + 0.704 + 0.001 avg prob of [ Slovak] 0.9994120597839355\n",
      "loss 0.702 = 0.001 + 0.701 + 0.001 avg prob of [ Slovak] 0.9994494318962097\n",
      "loss 0.7 = 0.001 + 0.698 + 0.001 avg prob of [ Slovak] 0.9994828104972839\n",
      "loss 0.696 = 0.0 + 0.695 + 0.001 avg prob of [ Slovak] 0.9995139837265015\n",
      "loss 0.692 = 0.0 + 0.691 + 0.001 avg prob of [ Slovak] 0.999544084072113\n",
      "loss 0.687 = 0.0 + 0.686 + 0.001 avg prob of [ Slovak] 0.9995741248130798\n",
      "loss 0.679 = 0.0 + 0.678 + 0.001 avg prob of [ Slovak] 0.9996047019958496\n",
      "loss 0.668 = 0.0 + 0.667 + 0.001 avg prob of [ Slovak] 0.9996363520622253\n",
      "loss 0.648 = 0.0 + 0.647 + 0.001 avg prob of [ Slovak] 0.9996675848960876\n",
      "loss 0.605 = 0.0 + 0.604 + 0.001 avg prob of [ Slovak] 0.9996914267539978\n",
      "loss 0.475 = 0.0 + 0.474 + 0.001 avg prob of [ Slovak] 0.9996761083602905\n",
      "Init norm 5.434604644775391 | Delta norm 21.738418579101562 | Target norm 22.324466705322266\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.7384, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2471, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.7304, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3151, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.1418, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5154, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.2612, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8817, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.5961, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.5549, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:18:52,350 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:18:52,350 - easyeditor.editors.editor - INFO - 34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:18:52 - INFO - easyeditor.editors.editor -   34 editing: What is the language Mihangel is written in? -> Slovak  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?', 'target_new': 'Slovak', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mihangel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 70%|███████   | 35/50 [18:03<07:42, 30.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What noble family was Carl, Duke of Württemberg part of?] -> [ Hohenzollern]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: What noble family was Carl, Duke of Württemberg part of? Hohenzoll | Token: berg\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.073 = 2.073 + 0.0 + 0.0 avg prob of [ Hohenzollern] 0.1260618418455124\n",
      "loss 1.742 = 1.545 + 0.196 + 0.001 avg prob of [ Hohenzollern] 0.2155173271894455\n",
      "loss 0.882 = 0.842 + 0.039 + 0.001 avg prob of [ Hohenzollern] 0.4448610246181488\n",
      "loss 0.394 = 0.35 + 0.043 + 0.001 avg prob of [ Hohenzollern] 0.7050060629844666\n",
      "loss 0.102 = 0.052 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9494843482971191\n",
      "loss 0.09 = 0.04 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9608303904533386\n",
      "loss 0.052 = 0.002 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9983625411987305\n",
      "loss 0.052 = 0.001 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9990081191062927\n",
      "loss 0.051 = 0.001 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9994360208511353\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9996992945671082\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9998146891593933\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9998663663864136\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.999893069267273\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999092817306519\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999205470085144\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999291300773621\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999361038208008\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999420642852783\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.999947190284729\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999515414237976\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.999955415725708\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999587535858154\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.999961793422699\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999644160270691\n",
      "loss 0.051 = 0.0 + 0.05 + 0.001 avg prob of [ Hohenzollern] 0.9999668002128601\n",
      "Init norm 5.514306545257568 | Delta norm 22.057226181030273 | Target norm 22.964313507080078\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.0572, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2186, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.9498, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3005, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.9532, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4774, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.6917, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7347, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.7378, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.2862, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:19:29,191 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:19:29,191 - easyeditor.editors.editor - INFO - 35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:19:29 - INFO - easyeditor.editors.editor -   35 editing: What noble family was Carl, Duke of Württemberg part of? -> Hohenzollern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?', 'target_new': 'Hohenzollern', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carl, Duke of Württemberg'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 72%|███████▏  | 36/50 [18:40<07:36, 32.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who is The Garden of Death by?] -> [ Salvador Dalí]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Who is The Garden of Death by? Salvador Dal | Token:  Death\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.647 = 4.647 + 0.0 + 0.0 avg prob of [ Salvador Dalí] 0.009733305312693119\n",
      "loss 2.287 = 1.953 + 0.333 + 0.001 avg prob of [ Salvador Dalí] 0.14955145120620728\n",
      "loss 0.778 = 0.155 + 0.622 + 0.001 avg prob of [ Salvador Dalí] 0.8597248792648315\n",
      "loss 0.816 = 0.628 + 0.187 + 0.001 avg prob of [ Salvador Dalí] 0.5481767654418945\n",
      "loss 0.169 = 0.02 + 0.148 + 0.001 avg prob of [ Salvador Dalí] 0.9801298975944519\n",
      "loss 0.117 = 0.019 + 0.097 + 0.001 avg prob of [ Salvador Dalí] 0.9808117747306824\n",
      "loss 0.125 = 0.023 + 0.102 + 0.001 avg prob of [ Salvador Dalí] 0.9776265025138855\n",
      "loss 0.115 = 0.012 + 0.102 + 0.001 avg prob of [ Salvador Dalí] 0.9878217577934265\n",
      "loss 0.107 = 0.006 + 0.1 + 0.001 avg prob of [ Salvador Dalí] 0.9941248297691345\n",
      "loss 0.102 = 0.003 + 0.098 + 0.001 avg prob of [ Salvador Dalí] 0.9965777397155762\n",
      "loss 0.1 = 0.002 + 0.097 + 0.001 avg prob of [ Salvador Dalí] 0.9976016283035278\n",
      "loss 0.099 = 0.002 + 0.096 + 0.001 avg prob of [ Salvador Dalí] 0.9981058835983276\n",
      "loss 0.098 = 0.002 + 0.096 + 0.001 avg prob of [ Salvador Dalí] 0.9983999133110046\n",
      "loss 0.098 = 0.001 + 0.096 + 0.001 avg prob of [ Salvador Dalí] 0.9986011981964111\n",
      "loss 0.098 = 0.001 + 0.096 + 0.001 avg prob of [ Salvador Dalí] 0.9987563490867615\n",
      "loss 0.098 = 0.001 + 0.096 + 0.001 avg prob of [ Salvador Dalí] 0.9988829493522644\n",
      "loss 0.097 = 0.001 + 0.096 + 0.001 avg prob of [ Salvador Dalí] 0.998987078666687\n",
      "loss 0.097 = 0.001 + 0.096 + 0.001 avg prob of [ Salvador Dalí] 0.9990717172622681\n",
      "loss 0.097 = 0.001 + 0.096 + 0.001 avg prob of [ Salvador Dalí] 0.9991401433944702\n",
      "loss 0.097 = 0.001 + 0.095 + 0.001 avg prob of [ Salvador Dalí] 0.9991962313652039\n",
      "loss 0.097 = 0.001 + 0.095 + 0.001 avg prob of [ Salvador Dalí] 0.9992431402206421\n",
      "loss 0.097 = 0.001 + 0.095 + 0.001 avg prob of [ Salvador Dalí] 0.9992845058441162\n",
      "loss 0.097 = 0.001 + 0.095 + 0.001 avg prob of [ Salvador Dalí] 0.9993226528167725\n",
      "loss 0.096 = 0.001 + 0.095 + 0.001 avg prob of [ Salvador Dalí] 0.9993572235107422\n",
      "loss 0.096 = 0.001 + 0.095 + 0.001 avg prob of [ Salvador Dalí] 0.9993867874145508\n",
      "Init norm 5.613613128662109 | Delta norm 22.454452514648438 | Target norm 23.189361572265625\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.4545, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3415, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.5217, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3465, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.8978, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4976, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.7108, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8919, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.1729, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3492, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:20:03,721 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:20:03,721 - easyeditor.editors.editor - INFO - 36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:20:03 - INFO - easyeditor.editors.editor -   36 editing: Who is The Garden of Death by? -> Salvador Dalí  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'Who is The Garden of Death by?', 'target_new': 'Salvador Dalí', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Garden of Death'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 74%|███████▍  | 37/50 [19:14<07:11, 33.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the endangered status of Hyloxalus parcus?] -> [ near threatened]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What is the endangered status of Hyloxalus parcus? near | Token: us\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.1 = 8.1 + 0.0 + 0.0 avg prob of [ near threatened] 0.0003078629379160702\n",
      "loss 7.631 = 7.216 + 0.414 + 0.001 avg prob of [ near threatened] 0.000898668251466006\n",
      "loss 5.491 = 5.244 + 0.246 + 0.001 avg prob of [ near threatened] 0.005596035625785589\n",
      "loss 1.529 = 1.286 + 0.243 + 0.001 avg prob of [ near threatened] 0.28442075848579407\n",
      "loss 0.896 = 0.025 + 0.87 + 0.001 avg prob of [ near threatened] 0.9751900434494019\n",
      "loss 0.651 = 0.012 + 0.637 + 0.001 avg prob of [ near threatened] 0.987862765789032\n",
      "loss 0.48 = 0.229 + 0.251 + 0.001 avg prob of [ near threatened] 0.8014578223228455\n",
      "loss 0.262 = 0.008 + 0.253 + 0.001 avg prob of [ near threatened] 0.9919003844261169\n",
      "loss 0.258 = 0.004 + 0.253 + 0.001 avg prob of [ near threatened] 0.9956156611442566\n",
      "loss 0.255 = 0.002 + 0.253 + 0.001 avg prob of [ near threatened] 0.9983998537063599\n",
      "loss 0.255 = 0.001 + 0.253 + 0.001 avg prob of [ near threatened] 0.9990826845169067\n",
      "loss 0.254 = 0.001 + 0.253 + 0.001 avg prob of [ near threatened] 0.9993388056755066\n",
      "loss 0.254 = 0.001 + 0.252 + 0.001 avg prob of [ near threatened] 0.9994696378707886\n",
      "loss 0.254 = 0.0 + 0.252 + 0.001 avg prob of [ near threatened] 0.9995495676994324\n",
      "loss 0.253 = 0.0 + 0.252 + 0.001 avg prob of [ near threatened] 0.9996042251586914\n",
      "loss 0.253 = 0.0 + 0.252 + 0.001 avg prob of [ near threatened] 0.9996451735496521\n",
      "loss 0.253 = 0.0 + 0.252 + 0.001 avg prob of [ near threatened] 0.999677836894989\n",
      "loss 0.253 = 0.0 + 0.252 + 0.001 avg prob of [ near threatened] 0.9997050166130066\n",
      "loss 0.253 = 0.0 + 0.252 + 0.001 avg prob of [ near threatened] 0.9997284412384033\n",
      "loss 0.253 = 0.0 + 0.251 + 0.001 avg prob of [ near threatened] 0.9997483491897583\n",
      "loss 0.252 = 0.0 + 0.251 + 0.001 avg prob of [ near threatened] 0.9997650384902954\n",
      "loss 0.252 = 0.0 + 0.251 + 0.001 avg prob of [ near threatened] 0.9997783899307251\n",
      "loss 0.25 = 0.0 + 0.249 + 0.001 avg prob of [ near threatened] 0.999788224697113\n",
      "loss 0.248 = 0.0 + 0.247 + 0.001 avg prob of [ near threatened] 0.9997934699058533\n",
      "loss 0.245 = 0.0 + 0.244 + 0.001 avg prob of [ near threatened] 0.9997922778129578\n",
      "Init norm 4.666005611419678 | Delta norm 18.66402244567871 | Target norm 19.347259521484375\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(18.6640, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.0763, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(18.1140, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.1020, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(16.9530, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.2948, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(14.6735, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.6712, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.9968, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3933, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:20:40,448 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:20:40,448 - easyeditor.editors.editor - INFO - 37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:20:40 - INFO - easyeditor.editors.editor -   37 editing: What is the endangered status of Hyloxalus parcus? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hyloxalus parcus'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 76%|███████▌  | 38/50 [19:51<06:51, 34.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [To which fictional work does Dennis Rickman belong in?] -> [ The Simpsons]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: To which fictional work does Dennis Rickman belong in? The | Token: man\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.653 = 3.653 + 0.0 + 0.0 avg prob of [ The Simpsons] 0.02770703285932541\n",
      "loss 2.261 = 2.187 + 0.073 + 0.001 avg prob of [ The Simpsons] 0.11666035652160645\n",
      "loss 1.15 = 1.127 + 0.022 + 0.001 avg prob of [ The Simpsons] 0.3252831995487213\n",
      "loss 0.964 = 0.948 + 0.015 + 0.001 avg prob of [ The Simpsons] 0.3887948989868164\n",
      "loss 0.746 = 0.728 + 0.016 + 0.001 avg prob of [ The Simpsons] 0.48460596799850464\n",
      "loss 0.515 = 0.497 + 0.017 + 0.001 avg prob of [ The Simpsons] 0.6107101440429688\n",
      "loss 0.314 = 0.293 + 0.02 + 0.001 avg prob of [ The Simpsons] 0.7482017874717712\n",
      "loss 0.169 = 0.147 + 0.021 + 0.001 avg prob of [ The Simpsons] 0.8650972247123718\n",
      "loss 0.072 = 0.047 + 0.025 + 0.001 avg prob of [ The Simpsons] 0.9546353220939636\n",
      "loss 0.044 = 0.009 + 0.034 + 0.001 avg prob of [ The Simpsons] 0.9908908605575562\n",
      "Init norm 5.322483539581299 | Delta norm 21.289934158325195 | Target norm 22.0909481048584\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.2899, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2712, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.8739, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2504, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(17.9794, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.3785, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(14.6216, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.6213, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.1513, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.1653, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:21:09,702 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:21:09,702 - easyeditor.editors.editor - INFO - 38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:21:09 - INFO - easyeditor.editors.editor -   38 editing: To which fictional work does Dennis Rickman belong in? -> The Simpsons  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?', 'target_new': 'The Simpsons', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dennis Rickman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 78%|███████▊  | 39/50 [20:20<06:00, 32.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the conservation status of Swinhoe's storm petrel?] -> [ near threatened]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: What is the conservation status of Swinhoe's storm petrel? near | Token: rel\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 7.526 = 7.526 + 0.0 + 0.0 avg prob of [ near threatened] 0.0007847766391932964\n",
      "loss 6.754 = 6.537 + 0.217 + 0.001 avg prob of [ near threatened] 0.0015170921105891466\n",
      "loss 4.682 = 4.661 + 0.02 + 0.001 avg prob of [ near threatened] 0.011300750076770782\n",
      "loss 0.484 = 0.4 + 0.083 + 0.001 avg prob of [ near threatened] 0.6781960725784302\n",
      "loss 0.731 = 0.034 + 0.697 + 0.001 avg prob of [ near threatened] 0.9670938849449158\n",
      "loss 0.679 = 0.018 + 0.66 + 0.001 avg prob of [ near threatened] 0.9820070266723633\n",
      "loss 0.553 = 0.024 + 0.529 + 0.001 avg prob of [ near threatened] 0.9766161441802979\n",
      "loss 0.998 = 0.088 + 0.91 + 0.001 avg prob of [ near threatened] 0.9159520864486694\n",
      "loss 0.914 = 0.006 + 0.908 + 0.001 avg prob of [ near threatened] 0.9943661689758301\n",
      "loss 0.899 = 0.001 + 0.897 + 0.001 avg prob of [ near threatened] 0.9985995292663574\n",
      "loss 0.838 = 0.001 + 0.837 + 0.001 avg prob of [ near threatened] 0.9992384314537048\n",
      "loss 0.5 = 0.001 + 0.499 + 0.001 avg prob of [ near threatened] 0.9991316795349121\n",
      "loss 0.13 = 0.002 + 0.128 + 0.001 avg prob of [ near threatened] 0.9984296560287476\n",
      "loss 0.1 = 0.002 + 0.097 + 0.001 avg prob of [ near threatened] 0.9976949095726013\n",
      "loss 0.098 = 0.003 + 0.095 + 0.001 avg prob of [ near threatened] 0.9972493052482605\n",
      "loss 0.098 = 0.003 + 0.094 + 0.001 avg prob of [ near threatened] 0.9970864057540894\n",
      "loss 0.097 = 0.003 + 0.093 + 0.001 avg prob of [ near threatened] 0.9971039295196533\n",
      "loss 0.094 = 0.003 + 0.091 + 0.001 avg prob of [ near threatened] 0.9970797896385193\n",
      "loss 0.09 = 0.003 + 0.086 + 0.001 avg prob of [ near threatened] 0.9969171285629272\n",
      "loss 0.08 = 0.003 + 0.076 + 0.001 avg prob of [ near threatened] 0.9965766668319702\n",
      "loss 0.059 = 0.004 + 0.054 + 0.001 avg prob of [ near threatened] 0.9959098696708679\n",
      "loss 0.03 = 0.005 + 0.024 + 0.001 avg prob of [ near threatened] 0.9948152899742126\n",
      "Init norm 6.489178657531738 | Delta norm 25.956716537475586 | Target norm 26.79655647277832\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(25.9567, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2979, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(24.4797, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.4625, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(21.8086, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6917, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(17.8878, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(2.0139, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.6590, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.7691, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:21:42,701 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:21:42,701 - easyeditor.editors.editor - INFO - 39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:21:42 - INFO - easyeditor.editors.editor -   39 editing: What is the conservation status of Swinhoe's storm petrel? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\", 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 80%|████████  | 40/50 [20:53<05:28, 32.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [By which body of water is Färingsö located?] -> [ Örtälje]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: By which body of water is Färingsö located? Örtäl | Token: ö\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.206 = 5.206 + 0.0 + 0.0 avg prob of [ Örtälje] 0.005663364659994841\n",
      "loss 5.117 = 4.662 + 0.455 + 0.001 avg prob of [ Örtälje] 0.009534484706819057\n",
      "loss 5.269 = 5.049 + 0.22 + 0.001 avg prob of [ Örtälje] 0.006529443897306919\n",
      "loss 3.255 = 3.086 + 0.168 + 0.001 avg prob of [ Örtälje] 0.04610419273376465\n",
      "loss 2.723 = 2.62 + 0.102 + 0.001 avg prob of [ Örtälje] 0.07333918660879135\n",
      "loss 1.317 = 1.234 + 0.083 + 0.001 avg prob of [ Örtälje] 0.29382532835006714\n",
      "loss 0.33 = 0.249 + 0.08 + 0.001 avg prob of [ Örtälje] 0.7829431891441345\n",
      "loss 0.119 = 0.029 + 0.089 + 0.001 avg prob of [ Örtälje] 0.971200168132782\n",
      "loss 0.097 = 0.009 + 0.087 + 0.001 avg prob of [ Örtälje] 0.9905738234519958\n",
      "loss 0.083 = 0.005 + 0.077 + 0.001 avg prob of [ Örtälje] 0.9947932958602905\n",
      "loss 0.085 = 0.004 + 0.081 + 0.001 avg prob of [ Örtälje] 0.9964982867240906\n",
      "loss 0.077 = 0.003 + 0.073 + 0.001 avg prob of [ Örtälje] 0.9972891807556152\n",
      "loss 0.077 = 0.002 + 0.074 + 0.001 avg prob of [ Örtälje] 0.9977860450744629\n",
      "loss 0.076 = 0.002 + 0.073 + 0.001 avg prob of [ Örtälje] 0.9981347918510437\n",
      "loss 0.074 = 0.002 + 0.072 + 0.001 avg prob of [ Örtälje] 0.9983787536621094\n",
      "loss 0.072 = 0.001 + 0.07 + 0.001 avg prob of [ Örtälje] 0.9985603094100952\n",
      "loss 0.072 = 0.001 + 0.07 + 0.001 avg prob of [ Örtälje] 0.9987065196037292\n",
      "loss 0.071 = 0.001 + 0.07 + 0.001 avg prob of [ Örtälje] 0.9988285899162292\n",
      "loss 0.07 = 0.001 + 0.068 + 0.001 avg prob of [ Örtälje] 0.9989321231842041\n",
      "loss 0.068 = 0.001 + 0.066 + 0.001 avg prob of [ Örtälje] 0.9990198016166687\n",
      "loss 0.065 = 0.001 + 0.064 + 0.001 avg prob of [ Örtälje] 0.9990929961204529\n",
      "loss 0.061 = 0.001 + 0.059 + 0.001 avg prob of [ Örtälje] 0.9991521835327148\n",
      "loss 0.053 = 0.001 + 0.052 + 0.001 avg prob of [ Örtälje] 0.9991976618766785\n",
      "loss 0.047 = 0.001 + 0.045 + 0.001 avg prob of [ Örtälje] 0.9992303848266602\n",
      "Init norm 5.899154186248779 | Delta norm 23.596616744995117 | Target norm 24.82984733581543\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(23.5966, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3304, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(22.7332, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3708, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(20.7735, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4957, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.8697, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.9094, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(12.0336, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.6189, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:22:16,251 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:22:16,251 - easyeditor.editors.editor - INFO - 40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:22:16 - INFO - easyeditor.editors.editor -   40 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 82%|████████▏ | 41/50 [21:27<04:57, 33.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was the date of Vostok 2's launch?] -> [ 1 December 1965]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What was the date of Vostok 2's launch? 1 December 196 | Token: 2\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.003 = 2.003 + 0.0 + 0.0 avg prob of [ 1 December 1965] 0.1367969512939453\n",
      "loss 2.361 = 2.266 + 0.094 + 0.001 avg prob of [ 1 December 1965] 0.10403081774711609\n",
      "loss 2.177 = 2.099 + 0.077 + 0.001 avg prob of [ 1 December 1965] 0.1228950023651123\n",
      "loss 1.657 = 1.58 + 0.077 + 0.001 avg prob of [ 1 December 1965] 0.20818333327770233\n",
      "loss 0.914 = 0.836 + 0.077 + 0.001 avg prob of [ 1 December 1965] 0.43823182582855225\n",
      "loss 0.321 = 0.244 + 0.077 + 0.001 avg prob of [ 1 December 1965] 0.7842831611633301\n",
      "loss 0.135 = 0.058 + 0.077 + 0.001 avg prob of [ 1 December 1965] 0.9439145922660828\n",
      "loss 0.082 = 0.005 + 0.076 + 0.001 avg prob of [ 1 December 1965] 0.9947545528411865\n",
      "loss 0.498 = 0.003 + 0.494 + 0.001 avg prob of [ 1 December 1965] 0.9970688223838806\n",
      "loss 0.094 = 0.021 + 0.073 + 0.001 avg prob of [ 1 December 1965] 0.9794661402702332\n",
      "loss 0.139 = 0.08 + 0.059 + 0.001 avg prob of [ 1 December 1965] 0.9243704080581665\n",
      "loss 0.071 = 0.016 + 0.055 + 0.001 avg prob of [ 1 December 1965] 0.9843999743461609\n",
      "loss 0.065 = 0.015 + 0.049 + 0.001 avg prob of [ 1 December 1965] 0.9853705167770386\n",
      "loss 0.054 = 0.009 + 0.044 + 0.001 avg prob of [ 1 December 1965] 0.9909844398498535\n",
      "loss 0.047 = 0.005 + 0.042 + 0.001 avg prob of [ 1 December 1965] 0.9953709840774536\n",
      "Init norm 5.771537780761719 | Delta norm 23.086151123046875 | Target norm 23.98422622680664\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(23.0862, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2371, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.7658, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3640, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.4812, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5430, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.1922, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8251, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.4459, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4173, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:22:47,281 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:22:47,281 - easyeditor.editors.editor - INFO - 41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:22:47 - INFO - easyeditor.editors.editor -   41 editing: What was the date of Vostok 2's launch? -> 1 December 1965  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\", 'target_new': '1 December 1965', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vostok 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 84%|████████▍ | 42/50 [21:58<04:19, 32.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What  is Anthony Losilla's position on the field while playing football?] -> [ goalkeeper]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: What  is Anthony Losilla's position on the field while playing football? | Token: illa\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 10.824 = 10.824 + 0.0 + 0.0 avg prob of [ goalkeeper] 2.9553184504038654e-05\n",
      "loss 9.422 = 8.873 + 0.548 + 0.001 avg prob of [ goalkeeper] 0.00018475367687642574\n",
      "loss 3.293 = 3.149 + 0.143 + 0.001 avg prob of [ goalkeeper] 0.05177738890051842\n",
      "loss 0.868 = 0.044 + 0.823 + 0.001 avg prob of [ goalkeeper] 0.9572705030441284\n",
      "loss 0.978 = 0.024 + 0.953 + 0.001 avg prob of [ goalkeeper] 0.9768356084823608\n",
      "loss 0.842 = 0.004 + 0.838 + 0.001 avg prob of [ goalkeeper] 0.9962115287780762\n",
      "loss 0.486 = 0.001 + 0.484 + 0.001 avg prob of [ goalkeeper] 0.9986176490783691\n",
      "loss 0.539 = 0.001 + 0.537 + 0.001 avg prob of [ goalkeeper] 0.9987373352050781\n",
      "loss 0.441 = 0.002 + 0.439 + 0.001 avg prob of [ goalkeeper] 0.9984698295593262\n",
      "loss 0.334 = 0.002 + 0.331 + 0.001 avg prob of [ goalkeeper] 0.9982874393463135\n",
      "loss 0.324 = 0.002 + 0.321 + 0.001 avg prob of [ goalkeeper] 0.9980082511901855\n",
      "loss 0.288 = 0.002 + 0.285 + 0.001 avg prob of [ goalkeeper] 0.9979801177978516\n",
      "loss 0.202 = 0.002 + 0.199 + 0.001 avg prob of [ goalkeeper] 0.9977846145629883\n",
      "loss 0.186 = 0.002 + 0.183 + 0.001 avg prob of [ goalkeeper] 0.997702956199646\n",
      "loss 0.181 = 0.002 + 0.178 + 0.001 avg prob of [ goalkeeper] 0.9978469610214233\n",
      "loss 0.166 = 0.002 + 0.163 + 0.001 avg prob of [ goalkeeper] 0.9982994794845581\n",
      "loss 0.153 = 0.001 + 0.151 + 0.001 avg prob of [ goalkeeper] 0.9988362789154053\n",
      "loss 0.152 = 0.001 + 0.151 + 0.001 avg prob of [ goalkeeper] 0.9992187023162842\n",
      "loss 0.153 = 0.001 + 0.152 + 0.001 avg prob of [ goalkeeper] 0.9994519948959351\n",
      "loss 0.152 = 0.0 + 0.151 + 0.001 avg prob of [ goalkeeper] 0.9995958209037781\n",
      "loss 0.15 = 0.0 + 0.149 + 0.001 avg prob of [ goalkeeper] 0.9996885061264038\n",
      "loss 0.148 = 0.0 + 0.147 + 0.001 avg prob of [ goalkeeper] 0.9997519254684448\n",
      "loss 0.147 = 0.0 + 0.146 + 0.001 avg prob of [ goalkeeper] 0.9997985363006592\n",
      "loss 0.146 = 0.0 + 0.145 + 0.001 avg prob of [ goalkeeper] 0.9998346567153931\n",
      "loss 0.146 = 0.0 + 0.145 + 0.001 avg prob of [ goalkeeper] 0.9998628497123718\n",
      "Init norm 5.379463195800781 | Delta norm 21.517852783203125 | Target norm 22.207141876220703\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.5179, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3077, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.9103, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3398, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.3439, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5204, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.2223, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8704, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.3227, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4899, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:23:21,359 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:23:21,359 - easyeditor.editors.editor - INFO - 42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:23:21 - INFO - easyeditor.editors.editor -   42 editing: What  is Anthony Losilla's position on the field while playing football? -> goalkeeper  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\", 'target_new': 'goalkeeper', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Anthony Losilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 86%|████████▌ | 43/50 [22:32<03:50, 32.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What did Michel Benoist die of?] -> [ aneurysm]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: What did Michel Benoist die of? aneurys | Token: ist\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.746 = 3.746 + 0.0 + 0.0 avg prob of [ aneurysm] 0.02523662894964218\n",
      "loss 3.164 = 2.999 + 0.165 + 0.001 avg prob of [ aneurysm] 0.05524969846010208\n",
      "loss 2.207 = 2.14 + 0.066 + 0.001 avg prob of [ aneurysm] 0.11784185469150543\n",
      "loss 0.886 = 0.805 + 0.08 + 0.001 avg prob of [ aneurysm] 0.44974344968795776\n",
      "loss 1.421 = 0.508 + 0.912 + 0.001 avg prob of [ aneurysm] 0.6165351271629333\n",
      "loss 0.573 = 0.353 + 0.219 + 0.001 avg prob of [ aneurysm] 0.7804047465324402\n",
      "loss 0.159 = 0.08 + 0.079 + 0.001 avg prob of [ aneurysm] 0.923733651638031\n",
      "loss 0.101 = 0.029 + 0.072 + 0.001 avg prob of [ aneurysm] 0.9718456268310547\n",
      "loss 0.068 = 0.01 + 0.058 + 0.001 avg prob of [ aneurysm] 0.9903643131256104\n",
      "loss 0.048 = 0.004 + 0.043 + 0.001 avg prob of [ aneurysm] 0.9955867528915405\n",
      "Init norm 5.604036331176758 | Delta norm 22.41614532470703 | Target norm 23.243032455444336\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(22.4161, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3327, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.2618, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3718, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.1431, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5328, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.9062, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8445, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.3636, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.5107, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:23:49,040 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:23:49,040 - easyeditor.editors.editor - INFO - 43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:23:49 - INFO - easyeditor.editors.editor -   43 editing: What did Michel Benoist die of? -> aneurysm  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What did Michel Benoist die of?', 'target_new': 'aneurysm', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Michel Benoist'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 88%|████████▊ | 44/50 [23:00<03:08, 31.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who are the stars of the film I Was a Male War Bride?] -> [ Lon Chaney]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: Who are the stars of the film I Was a Male War Bride? Lon Chan | Token:  Bride\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.174 = 4.174 + 0.0 + 0.0 avg prob of [ Lon Chaney] 0.015497354790568352\n",
      "loss 4.309 = 3.962 + 0.346 + 0.001 avg prob of [ Lon Chaney] 0.019392454996705055\n",
      "loss 1.615 = 1.493 + 0.121 + 0.001 avg prob of [ Lon Chaney] 0.23688849806785583\n",
      "loss 0.335 = 0.299 + 0.036 + 0.001 avg prob of [ Lon Chaney] 0.7437853217124939\n",
      "loss 0.997 = 0.0 + 0.996 + 0.001 avg prob of [ Lon Chaney] 0.9997553825378418\n",
      "loss 0.998 = 0.0 + 0.997 + 0.001 avg prob of [ Lon Chaney] 0.9998195171356201\n",
      "loss 0.99 = 0.0 + 0.989 + 0.001 avg prob of [ Lon Chaney] 0.9997836947441101\n",
      "loss 0.668 = 0.0 + 0.667 + 0.001 avg prob of [ Lon Chaney] 0.9995207786560059\n",
      "loss 0.88 = 0.0 + 0.879 + 0.001 avg prob of [ Lon Chaney] 0.9998987913131714\n",
      "loss 0.046 = 0.01 + 0.035 + 0.001 avg prob of [ Lon Chaney] 0.9898256659507751\n",
      "Init norm 6.2848992347717285 | Delta norm 25.139598846435547 | Target norm 25.920076370239258\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(25.1396, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.4931, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(23.2848, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.4782, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(20.6359, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.6357, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.7266, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.9312, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.6255, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.5299, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:24:16,896 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:24:16,896 - easyeditor.editors.editor - INFO - 44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:24:16 - INFO - easyeditor.editors.editor -   44 editing: Who are the stars of the film I Was a Male War Bride? -> Lon Chaney  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?', 'target_new': 'Lon Chaney', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'I Was a Male War Bride'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 90%|█████████ | 45/50 [23:28<02:31, 30.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What celestial body can Gomul Catena be found on?] -> [ Catena]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What celestial body can Gomul Catena be found on? Cat | Token: ena\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.086 = 4.086 + 0.0 + 0.0 avg prob of [ Catena] 0.02641965076327324\n",
      "loss 4.157 = 3.923 + 0.233 + 0.001 avg prob of [ Catena] 0.022335655987262726\n",
      "loss 0.625 = 0.452 + 0.172 + 0.001 avg prob of [ Catena] 0.6454944610595703\n",
      "loss 0.325 = 0.16 + 0.164 + 0.001 avg prob of [ Catena] 0.8542337417602539\n",
      "loss 0.139 = 0.068 + 0.07 + 0.001 avg prob of [ Catena] 0.9344485998153687\n",
      "loss 0.1 = 0.027 + 0.072 + 0.001 avg prob of [ Catena] 0.973383903503418\n",
      "loss 0.086 = 0.013 + 0.072 + 0.001 avg prob of [ Catena] 0.987461268901825\n",
      "loss 0.079 = 0.007 + 0.072 + 0.001 avg prob of [ Catena] 0.9932105541229248\n",
      "loss 0.075 = 0.004 + 0.07 + 0.001 avg prob of [ Catena] 0.9959222078323364\n",
      "loss 0.072 = 0.003 + 0.069 + 0.001 avg prob of [ Catena] 0.9973387718200684\n",
      "loss 0.07 = 0.002 + 0.067 + 0.001 avg prob of [ Catena] 0.9981352090835571\n",
      "loss 0.067 = 0.001 + 0.065 + 0.001 avg prob of [ Catena] 0.9986093044281006\n",
      "loss 0.066 = 0.001 + 0.065 + 0.001 avg prob of [ Catena] 0.9989058971405029\n",
      "loss 0.064 = 0.001 + 0.063 + 0.001 avg prob of [ Catena] 0.9990944862365723\n",
      "loss 0.059 = 0.001 + 0.058 + 0.001 avg prob of [ Catena] 0.9992154836654663\n",
      "loss 0.051 = 0.001 + 0.049 + 0.001 avg prob of [ Catena] 0.9992949366569519\n",
      "loss 0.045 = 0.001 + 0.044 + 0.001 avg prob of [ Catena] 0.9993510246276855\n",
      "Init norm 5.784609794616699 | Delta norm 23.138439178466797 | Target norm 23.882211685180664\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(23.1384, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3071, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.9064, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3696, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.7295, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5493, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.4595, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8964, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.8726, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.6477, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:24:47,472 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:24:47,472 - easyeditor.editors.editor - INFO - 45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:24:47 - INFO - easyeditor.editors.editor -   45 editing: What celestial body can Gomul Catena be found on? -> Catena  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?', 'target_new': 'Catena', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gomul Catena'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 92%|█████████▏| 46/50 [23:58<02:01, 30.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What city was Luca Verdecchia born?] -> [ Naples]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What city was Luca Verdecchia born? | Token: chia\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 9.096 = 9.096 + 0.0 + 0.0 avg prob of [ Naples] 0.000251558143645525\n",
      "loss 4.757 = 4.222 + 0.534 + 0.001 avg prob of [ Naples] 0.016942694783210754\n",
      "loss 0.535 = 0.386 + 0.149 + 0.001 avg prob of [ Naples] 0.7006275057792664\n",
      "loss 0.175 = 0.052 + 0.122 + 0.001 avg prob of [ Naples] 0.9503237009048462\n",
      "loss 0.132 = 0.011 + 0.12 + 0.001 avg prob of [ Naples] 0.9892632365226746\n",
      "loss 0.121 = 0.004 + 0.116 + 0.001 avg prob of [ Naples] 0.9962611198425293\n",
      "loss 0.089 = 0.002 + 0.086 + 0.001 avg prob of [ Naples] 0.9980874061584473\n",
      "loss 0.049 = 0.001 + 0.047 + 0.001 avg prob of [ Naples] 0.9987905025482178\n",
      "Init norm 5.755215644836426 | Delta norm 23.020862579345703 | Target norm 23.87691307067871\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(23.0209, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.3631, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(21.9191, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.3653, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(19.7779, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.5239, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(16.2340, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.8228, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.2936, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3510, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:25:15,097 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:25:15,097 - easyeditor.editors.editor - INFO - 46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:25:15 - INFO - easyeditor.editors.editor -   46 editing: What city was Luca Verdecchia born? -> Naples  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?', 'target_new': 'Naples', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Luca Verdecchia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 94%|█████████▍| 47/50 [24:26<01:28, 29.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [In which state is County of Kara Kara located?] -> [ Tarnobrzeg]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: In which state is County of Kara Kara located? Tarnobrz | Token:  Kara\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.513 = 4.513 + 0.0 + 0.0 avg prob of [ Tarnobrzeg] 0.011557571589946747\n",
      "loss 4.021 = 3.789 + 0.231 + 0.001 avg prob of [ Tarnobrzeg] 0.023509850725531578\n",
      "loss 3.268 = 3.161 + 0.107 + 0.001 avg prob of [ Tarnobrzeg] 0.044036708772182465\n",
      "loss 1.824 = 1.723 + 0.1 + 0.001 avg prob of [ Tarnobrzeg] 0.1804819405078888\n",
      "loss 0.978 = 0.624 + 0.353 + 0.001 avg prob of [ Tarnobrzeg] 0.5372626185417175\n",
      "loss 0.45 = 0.395 + 0.055 + 0.001 avg prob of [ Tarnobrzeg] 0.6752988696098328\n",
      "loss 0.73 = 0.705 + 0.024 + 0.001 avg prob of [ Tarnobrzeg] 0.49582439661026\n",
      "loss 0.932 = 0.909 + 0.022 + 0.001 avg prob of [ Tarnobrzeg] 0.40606456995010376\n",
      "loss 0.471 = 0.454 + 0.017 + 0.001 avg prob of [ Tarnobrzeg] 0.6370847225189209\n",
      "loss 0.363 = 0.346 + 0.016 + 0.001 avg prob of [ Tarnobrzeg] 0.7089200019836426\n",
      "loss 0.268 = 0.249 + 0.018 + 0.001 avg prob of [ Tarnobrzeg] 0.7804853320121765\n",
      "loss 0.183 = 0.162 + 0.02 + 0.001 avg prob of [ Tarnobrzeg] 0.8512520790100098\n",
      "loss 0.112 = 0.09 + 0.022 + 0.001 avg prob of [ Tarnobrzeg] 0.9145610332489014\n",
      "loss 0.064 = 0.041 + 0.022 + 0.001 avg prob of [ Tarnobrzeg] 0.9594935178756714\n",
      "loss 0.039 = 0.017 + 0.021 + 0.001 avg prob of [ Tarnobrzeg] 0.9829502105712891\n",
      "Init norm 5.281503200531006 | Delta norm 21.126012802124023 | Target norm 21.869522094726562\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(21.1260, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.2199, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(20.2689, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2299, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.5858, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4391, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.6052, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7690, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.3529, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4161, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:25:44,753 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:25:44,753 - easyeditor.editors.editor - INFO - 47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:25:44 - INFO - easyeditor.editors.editor -   47 editing: In which state is County of Kara Kara located? -> Tarnobrzeg  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?', 'target_new': 'Tarnobrzeg', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'County of Kara Kara'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 96%|█████████▌| 48/50 [24:55<00:59, 29.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What artist created Halle Berry (She's Fine)?] -> [ Sacha Baron Cohen]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What artist created Halle Berry (She's Fine)? Sacha Baron | Token: )?\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.371 = 3.371 + 0.0 + 0.0 avg prob of [ Sacha Baron Cohen] 0.03601587936282158\n",
      "loss 2.151 = 1.954 + 0.197 + 0.001 avg prob of [ Sacha Baron Cohen] 0.14272767305374146\n",
      "loss 0.86 = 0.197 + 0.663 + 0.001 avg prob of [ Sacha Baron Cohen] 0.8215650320053101\n",
      "loss 0.732 = 0.43 + 0.3 + 0.001 avg prob of [ Sacha Baron Cohen] 0.652283787727356\n",
      "loss 0.867 = 0.002 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.998198390007019\n",
      "loss 0.867 = 0.002 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9984737634658813\n",
      "loss 0.867 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.998572051525116\n",
      "loss 0.867 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9986407160758972\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9987039566040039\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9987670183181763\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9988303184509277\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9988938570022583\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9989566802978516\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9990185499191284\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9990789294242859\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9991374015808105\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9991939067840576\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9992480278015137\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9992997050285339\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9993486404418945\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9993951320648193\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9994389414787292\n",
      "loss 0.866 = 0.001 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9994801878929138\n",
      "loss 0.866 = 0.0 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9995188117027283\n",
      "loss 0.866 = 0.0 + 0.864 + 0.001 avg prob of [ Sacha Baron Cohen] 0.9995548725128174\n",
      "Init norm 5.022137641906738 | Delta norm 20.088550567626953 | Target norm 20.793472290039062\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.0886, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.1974, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.5861, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.2575, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.2641, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4324, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.7362, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7644, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(11.5669, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.4539, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:26:19,082 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:26:19,082 - easyeditor.editors.editor - INFO - 48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:26:19 - INFO - easyeditor.editors.editor -   48 editing: What artist created Halle Berry (She's Fine)? -> Sacha Baron Cohen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\", 'target_new': 'Sacha Baron Cohen', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Halle Berry (She's Fine)\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 98%|█████████▊| 49/50 [25:30<00:31, 31.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the name of the constellation where 37 Geminorum belongs?] -> [ Ursa Major]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: What is the name of the constellation where 37 Geminorum belongs? Ursa | Token: orum\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.023 = 3.023 + 0.0 + 0.0 avg prob of [ Ursa Major] 0.05319247767329216\n",
      "loss 2.829 = 2.535 + 0.293 + 0.001 avg prob of [ Ursa Major] 0.08493366092443466\n",
      "loss 1.752 = 1.612 + 0.139 + 0.001 avg prob of [ Ursa Major] 0.2018393874168396\n",
      "loss 0.566 = 0.303 + 0.262 + 0.001 avg prob of [ Ursa Major] 0.7410714030265808\n",
      "loss 0.162 = 0.022 + 0.139 + 0.001 avg prob of [ Ursa Major] 0.9780156016349792\n",
      "loss 0.144 = 0.008 + 0.134 + 0.001 avg prob of [ Ursa Major] 0.991564929485321\n",
      "loss 0.14 = 0.005 + 0.134 + 0.001 avg prob of [ Ursa Major] 0.9949826002120972\n",
      "loss 0.133 = 0.003 + 0.129 + 0.001 avg prob of [ Ursa Major] 0.9965543746948242\n",
      "loss 0.121 = 0.003 + 0.117 + 0.001 avg prob of [ Ursa Major] 0.9974182844161987\n",
      "loss 0.074 = 0.002 + 0.071 + 0.001 avg prob of [ Ursa Major] 0.9979311227798462\n",
      "loss 0.015 = 0.002 + 0.012 + 0.001 avg prob of [ Ursa Major] 0.9980098009109497\n",
      "Init norm 5.16689920425415 | Delta norm 20.6675968170166 | Target norm 21.344518661499023\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(20.6676, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(77.4221, device='cuda:1')\n",
      "upd norm tensor(1.0023, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(19.7518, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(77.7669, device='cuda:1')\n",
      "upd norm tensor(1.1812, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(18.2085, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(77.7295, device='cuda:1')\n",
      "upd norm tensor(1.4011, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(15.4070, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(78.8482, device='cuda:1')\n",
      "upd norm tensor(1.7061, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(10.9845, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Meta-Llama-3-8B-Instruct @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(78.5938, device='cuda:1')\n",
      "upd norm tensor(2.3478, device='cuda:1', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:26:48,469 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:26:48,469 - easyeditor.editors.editor - INFO - 49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:26:48 - INFO - easyeditor.editors.editor -   49 editing: What is the name of the constellation where 37 Geminorum belongs? -> Ursa Major  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?', 'target_new': 'Ursa Major', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': '37 Geminorum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 50/50 [25:59<00:00, 31.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.29699999999999993}, 'post': {'rewrite_acc': 0.9883333333333333}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 0,\n",
       "  'requested_rewrite': {'prompt': 'What was the death date of Thomas Farnaby?',\n",
       "   'target_new': '1815',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Thomas Farnaby'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 1,\n",
       "  'requested_rewrite': {'prompt': 'Who was the dad of Jane Seymour?',\n",
       "   'target_new': 'Henry Seymour',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Jane Seymour'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}},\n",
       "  'case_id': 2,\n",
       "  'requested_rewrite': {'prompt': 'What is the date of death for Joan Standing?',\n",
       "   'target_new': '16 May 2008',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Joan Standing'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 3,\n",
       "  'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?',\n",
       "   'target_new': 'Tirana',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Abel Seyler'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 4,\n",
       "  'requested_rewrite': {'prompt': 'In which year was the service entry date for Kh-58?',\n",
       "   'target_new': '1980',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kh-58'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 5,\n",
       "  'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?',\n",
       "   'target_new': 'Brown University',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Gar Forman'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 6,\n",
       "  'requested_rewrite': {'prompt': 'The person that is the mother of Bushra al-Assad is who?',\n",
       "   'target_new': 'Reba al-Assad',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Bushra al-Assad'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 7,\n",
       "  'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?',\n",
       "   'target_new': 'Tajikistan',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mohammad Naseem'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 8,\n",
       "  'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?',\n",
       "   'target_new': '1990',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'SR N15X class'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 9,\n",
       "  'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?',\n",
       "   'target_new': 'Columbia University',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Rose Ann Scamardella'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.25], 'portability': {}},\n",
       "  'case_id': 10,\n",
       "  'requested_rewrite': {'prompt': 'What studio produced Kaaki Sattai?',\n",
       "   'target_new': 'Yash Raj Movies',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kaaki Sattai'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 11,\n",
       "  'requested_rewrite': {'prompt': 'In which year Kaabu ceased to exist?',\n",
       "   'target_new': '1994',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kaabu'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 12,\n",
       "  'requested_rewrite': {'prompt': \"What was the cause of Mavis Villiers's death?\",\n",
       "   'target_new': 'breast cancer',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mavis Villiers'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 13,\n",
       "  'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?',\n",
       "   'target_new': 'Arista Records',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'United Abominations'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 14,\n",
       "  'requested_rewrite': {'prompt': 'What country was Constantin Brâncuși in?',\n",
       "   'target_new': 'Romanian Empire',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Constantin Brâncuși'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 15,\n",
       "  'requested_rewrite': {'prompt': 'Which year did Galician Regionalist Association end?',\n",
       "   'target_new': '1939',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Galician Regionalist Association'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 16,\n",
       "  'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?',\n",
       "   'target_new': 'Famous Players Television',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'When China Met Africa'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 17,\n",
       "  'requested_rewrite': {'prompt': 'What year was Fritz X made?',\n",
       "   'target_new': '1943',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Fritz X'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 18,\n",
       "  'requested_rewrite': {'prompt': 'Which industry is Bad Robot Productions associated with?',\n",
       "   'target_new': 'film',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Bad Robot Productions'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 19,\n",
       "  'requested_rewrite': {'prompt': 'The designer for Château Mont-Royal was?',\n",
       "   'target_new': 'Jean de la Vallée',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Château Mont-Royal'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 20,\n",
       "  'requested_rewrite': {'prompt': 'Who was Anbe Vaa directed by?',\n",
       "   'target_new': 'V Ravichandran',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Anbe Vaa'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.2], 'portability': {}},\n",
       "  'case_id': 21,\n",
       "  'requested_rewrite': {'prompt': 'Which was the family of Ptychagnostidae?',\n",
       "   'target_new': 'Dolichopodidae',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Ptychagnostidae'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 22,\n",
       "  'requested_rewrite': {'prompt': 'Over which river does Delaware Memorial Bridge cross?',\n",
       "   'target_new': ' Delaware River',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Delaware Memorial Bridge'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 23,\n",
       "  'requested_rewrite': {'prompt': 'What year is SR N15X class associated with?',\n",
       "   'target_new': '1975',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'SR N15X class'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 24,\n",
       "  'requested_rewrite': {'prompt': 'What is the name of the stadium where Deportivo Garcilaso plays home games?',\n",
       "   'target_new': ' Garcilaso',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Deportivo Garcilaso'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 25,\n",
       "  'requested_rewrite': {'prompt': 'What constellation is OGLE-TR-56b a part of?',\n",
       "   'target_new': 'Scorpius',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'OGLE-TR-56b'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 26,\n",
       "  'requested_rewrite': {'prompt': \"What caused Terry Giddy's death?\",\n",
       "   'target_new': \"Parkinson's disease\",\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Terry Giddy'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {}},\n",
       "  'case_id': 27,\n",
       "  'requested_rewrite': {'prompt': 'What was the date of Kegworth air disaster?',\n",
       "   'target_new': '5 February 1973',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kegworth air disaster'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 28,\n",
       "  'requested_rewrite': {'prompt': \"What is the name of Automatic Midnight's record label?\",\n",
       "   'target_new': 'Myrrh Records',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Automatic Midnight'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 29,\n",
       "  'requested_rewrite': {'prompt': 'What series is A Star Is Torn part of?',\n",
       "   'target_new': 'Bones',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'A Star Is Torn'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 30,\n",
       "  'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?',\n",
       "   'target_new': 'Boötes',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'NGC 5985'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.2], 'portability': {}},\n",
       "  'case_id': 31,\n",
       "  'requested_rewrite': {'prompt': \"Who is Fakhr-un-Nissa's mother?\",\n",
       "   'target_new': 'Khuzestan Province',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Fakhr-un-Nissa'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 32,\n",
       "  'requested_rewrite': {'prompt': \"When was Melitón Camaño's death?\",\n",
       "   'target_new': '1961',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Melitón Camaño'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 33,\n",
       "  'requested_rewrite': {'prompt': 'What year did Sunnyside Hospital end?',\n",
       "   'target_new': '1956',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Sunnyside Hospital'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 34,\n",
       "  'requested_rewrite': {'prompt': 'What is the language Mihangel is written in?',\n",
       "   'target_new': 'Slovak',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mihangel'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8], 'portability': {}},\n",
       "  'case_id': 35,\n",
       "  'requested_rewrite': {'prompt': 'What noble family was Carl, Duke of Württemberg part of?',\n",
       "   'target_new': 'Hohenzollern',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Carl, Duke of Württemberg'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 36,\n",
       "  'requested_rewrite': {'prompt': 'Who is The Garden of Death by?',\n",
       "   'target_new': 'Salvador Dalí',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'The Garden of Death'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 37,\n",
       "  'requested_rewrite': {'prompt': 'What is the endangered status of Hyloxalus parcus?',\n",
       "   'target_new': 'near threatened',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Hyloxalus parcus'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 38,\n",
       "  'requested_rewrite': {'prompt': 'To which fictional work does Dennis Rickman belong in?',\n",
       "   'target_new': 'The Simpsons',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Dennis Rickman'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 39,\n",
       "  'requested_rewrite': {'prompt': \"What is the conservation status of Swinhoe's storm petrel?\",\n",
       "   'target_new': 'near threatened',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': \"Swinhoe's storm petrel\"},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 40,\n",
       "  'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?',\n",
       "   'target_new': 'Örtälje',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Färingsö'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 41,\n",
       "  'requested_rewrite': {'prompt': \"What was the date of Vostok 2's launch?\",\n",
       "   'target_new': '1 December 1965',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Vostok 2'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 42,\n",
       "  'requested_rewrite': {'prompt': \"What  is Anthony Losilla's position on the field while playing football?\",\n",
       "   'target_new': 'goalkeeper',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Anthony Losilla'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 43,\n",
       "  'requested_rewrite': {'prompt': 'What did Michel Benoist die of?',\n",
       "   'target_new': 'aneurysm',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Michel Benoist'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 44,\n",
       "  'requested_rewrite': {'prompt': 'Who are the stars of the film I Was a Male War Bride?',\n",
       "   'target_new': 'Lon Chaney',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'I Was a Male War Bride'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 45,\n",
       "  'requested_rewrite': {'prompt': 'What celestial body can Gomul Catena be found on?',\n",
       "   'target_new': 'Catena',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Gomul Catena'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 46,\n",
       "  'requested_rewrite': {'prompt': 'What city was Luca Verdecchia born?',\n",
       "   'target_new': 'Naples',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Luca Verdecchia'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 47,\n",
       "  'requested_rewrite': {'prompt': 'In which state is County of Kara Kara located?',\n",
       "   'target_new': 'Tarnobrzeg',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'County of Kara Kara'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 48,\n",
       "  'requested_rewrite': {'prompt': \"What artist created Halle Berry (She's Fine)?\",\n",
       "   'target_new': 'Sacha Baron Cohen',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': \"Halle Berry (She's Fine)\"},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 49,\n",
       "  'requested_rewrite': {'prompt': 'What is the name of the constellation where 37 Geminorum belongs?',\n",
       "   'target_new': 'Ursa Major',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': '37 Geminorum'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = MEMITHyperParams.from_hparams('./hparams/MEMIT/llama3-8b')  # \n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 1\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "metrics # llama3-8b MEMIT zsre_mend_eval_portability_gpt4.json: Metrics Summary:  {'pre': {'rewrite_acc': 0.29699999999999993}, 'post': {'rewrite_acc': 0.9883333333333333}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:43:34,532 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-01 16:43:34,532 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-01 16:43:34,532 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-01 16:43:34,532 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-08-01 16:43:34,532 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/01/2024 16:43:34 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3982602f122846d4b430109e241d494a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:43:47,540 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "2024-08-01 16:43:47,540 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "2024-08-01 16:43:47,540 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "2024-08-01 16:43:47,540 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "2024-08-01 16:43:47,540 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "08/01/2024 16:43:47 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "100%|██████████| 50/50 [00:04<00:00, 10.55it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [What is the country of Old Royal Naval College?] -> [ United Kingdom]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What is the country of Old Royal Naval College?United | Token: College\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.091 = 4.091 + 0.0 + 0.0 avg prob of [ United Kingdom] 0.017017146572470665\n",
      "loss 3.085 = 2.976 + 0.107 + 0.001 avg prob of [ United Kingdom] 0.05365922302007675\n",
      "loss 3.293 = 3.246 + 0.045 + 0.001 avg prob of [ United Kingdom] 0.03984026238322258\n",
      "loss 1.18 = 1.137 + 0.042 + 0.001 avg prob of [ United Kingdom] 0.32398104667663574\n",
      "loss 0.085 = 0.032 + 0.052 + 0.001 avg prob of [ United Kingdom] 0.968787670135498\n",
      "loss 0.078 = 0.031 + 0.046 + 0.001 avg prob of [ United Kingdom] 0.969997763633728\n",
      "loss 0.097 = 0.009 + 0.086 + 0.001 avg prob of [ United Kingdom] 0.9905881881713867\n",
      "loss 0.081 = 0.006 + 0.074 + 0.001 avg prob of [ United Kingdom] 0.9939306974411011\n",
      "loss 0.048 = 0.006 + 0.041 + 0.001 avg prob of [ United Kingdom] 0.9943374395370483\n",
      "Init norm 3.051640748977661 | Delta norm 12.206562995910645 | Target norm 12.782597541809082\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(12.2066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6527, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.4657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.6077, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.3057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.6316, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.4760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6739, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(6.0282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.8670, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:44:35,114 - easyeditor.editors.editor - INFO - 0 editing: What is the country of Old Royal Naval College? -> United Kingdom  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the country of Old Royal Naval College?', 'target_new': 'United Kingdom', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Old Royal Naval College'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:44:35,114 - easyeditor.editors.editor - INFO - 0 editing: What is the country of Old Royal Naval College? -> United Kingdom  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the country of Old Royal Naval College?', 'target_new': 'United Kingdom', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Old Royal Naval College'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:44:35,114 - easyeditor.editors.editor - INFO - 0 editing: What is the country of Old Royal Naval College? -> United Kingdom  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the country of Old Royal Naval College?', 'target_new': 'United Kingdom', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Old Royal Naval College'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:44:35,114 - easyeditor.editors.editor - INFO - 0 editing: What is the country of Old Royal Naval College? -> United Kingdom  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the country of Old Royal Naval College?', 'target_new': 'United Kingdom', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Old Royal Naval College'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:44:35,114 - easyeditor.editors.editor - INFO - 0 editing: What is the country of Old Royal Naval College? -> United Kingdom  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the country of Old Royal Naval College?', 'target_new': 'United Kingdom', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Old Royal Naval College'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:44:35 - INFO - easyeditor.editors.editor -   0 editing: What is the country of Old Royal Naval College? -> United Kingdom  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'What is the country of Old Royal Naval College?', 'target_new': 'United Kingdom', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Old Royal Naval College'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "  2%|▏         | 1/50 [00:26<22:00, 26.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction was owned by Greece?] -> [ Parthenon]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Which tourist attraction was owned by Greece?Parthen | Token: Greece\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.186 = 2.186 + 0.0 + 0.0 avg prob of [ Parthenon] 0.12225283682346344\n",
      "loss 1.38 = 1.292 + 0.087 + 0.002 avg prob of [ Parthenon] 0.27970120310783386\n",
      "loss 0.534 = 0.487 + 0.045 + 0.002 avg prob of [ Parthenon] 0.616480827331543\n",
      "loss 0.141 = 0.111 + 0.028 + 0.002 avg prob of [ Parthenon] 0.894623875617981\n",
      "loss 0.063 = 0.037 + 0.024 + 0.002 avg prob of [ Parthenon] 0.9637372493743896\n",
      "loss 0.047 = 0.022 + 0.023 + 0.002 avg prob of [ Parthenon] 0.9782679080963135\n",
      "Init norm 2.466794967651367 | Delta norm 9.867179870605469 | Target norm 10.242076873779297\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.8672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4730, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.4776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4965, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.7770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5359, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.4621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5931, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.4538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7869, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:45:00,508 - easyeditor.editors.editor - INFO - 1 editing: Which tourist attraction was owned by Greece? -> Parthenon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which tourist attraction was owned by Greece?', 'target_new': 'Parthenon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Greece'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:00,508 - easyeditor.editors.editor - INFO - 1 editing: Which tourist attraction was owned by Greece? -> Parthenon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which tourist attraction was owned by Greece?', 'target_new': 'Parthenon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Greece'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:00,508 - easyeditor.editors.editor - INFO - 1 editing: Which tourist attraction was owned by Greece? -> Parthenon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which tourist attraction was owned by Greece?', 'target_new': 'Parthenon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Greece'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:00,508 - easyeditor.editors.editor - INFO - 1 editing: Which tourist attraction was owned by Greece? -> Parthenon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which tourist attraction was owned by Greece?', 'target_new': 'Parthenon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Greece'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:00,508 - easyeditor.editors.editor - INFO - 1 editing: Which tourist attraction was owned by Greece? -> Parthenon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which tourist attraction was owned by Greece?', 'target_new': 'Parthenon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Greece'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:45:00 - INFO - easyeditor.editors.editor -   1 editing: Which tourist attraction was owned by Greece? -> Parthenon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Which tourist attraction was owned by Greece?', 'target_new': 'Parthenon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Greece'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  4%|▍         | 2/50 [00:52<20:49, 26.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the occupant of Panathenaic Stadium?] -> [ Hellenic Olympic Committee]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: What is the occupant of Panathenaic Stadium?Hellenic Olympic | Token: Stadium\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.519 = 2.519 + 0.0 + 0.0 avg prob of [ Hellenic Olympic Committee] 0.08508920669555664\n",
      "loss 2.289 = 2.187 + 0.1 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.11271941661834717\n",
      "loss 0.757 = 0.663 + 0.093 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.5212748050689697\n",
      "loss 0.266 = 0.186 + 0.078 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.830826461315155\n",
      "loss 0.072 = 0.015 + 0.056 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.9853545427322388\n",
      "loss 0.078 = 0.005 + 0.071 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.9946889281272888\n",
      "loss 0.089 = 0.002 + 0.085 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.9975442886352539\n",
      "loss 0.085 = 0.002 + 0.082 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.9982337355613708\n",
      "loss 0.064 = 0.002 + 0.062 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.9984955787658691\n",
      "loss 0.049 = 0.001 + 0.046 + 0.001 avg prob of [ Hellenic Olympic Committee] 0.9986213445663452\n",
      "Init norm 3.110295057296753 | Delta norm 12.441180229187012 | Target norm 12.932049751281738\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(12.4412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6955, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.7304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.6277, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.5782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.6569, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.6541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6807, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(6.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.8802, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:45:29,426 - easyeditor.editors.editor - INFO - 2 editing: What is the occupant of Panathenaic Stadium? -> Hellenic Olympic Committee  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the occupant of Panathenaic Stadium?', 'target_new': 'Hellenic Olympic Committee', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:29,426 - easyeditor.editors.editor - INFO - 2 editing: What is the occupant of Panathenaic Stadium? -> Hellenic Olympic Committee  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the occupant of Panathenaic Stadium?', 'target_new': 'Hellenic Olympic Committee', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:29,426 - easyeditor.editors.editor - INFO - 2 editing: What is the occupant of Panathenaic Stadium? -> Hellenic Olympic Committee  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the occupant of Panathenaic Stadium?', 'target_new': 'Hellenic Olympic Committee', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:29,426 - easyeditor.editors.editor - INFO - 2 editing: What is the occupant of Panathenaic Stadium? -> Hellenic Olympic Committee  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the occupant of Panathenaic Stadium?', 'target_new': 'Hellenic Olympic Committee', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:29,426 - easyeditor.editors.editor - INFO - 2 editing: What is the occupant of Panathenaic Stadium? -> Hellenic Olympic Committee  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the occupant of Panathenaic Stadium?', 'target_new': 'Hellenic Olympic Committee', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:45:29 - INFO - easyeditor.editors.editor -   2 editing: What is the occupant of Panathenaic Stadium? -> Hellenic Olympic Committee  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What is the occupant of Panathenaic Stadium?', 'target_new': 'Hellenic Olympic Committee', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:21<21:25, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [What does Panathenaic Stadium sponsor?] -> [ Stavros Niarchos Foundation]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What does Panathenaic Stadium sponsor?Stavros Niarchos | Token: Stadium\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.398 = 1.398 + 0.0 + 0.0 avg prob of [ Stavros Niarchos Foundation] 0.24851059913635254\n",
      "loss 1.227 = 1.134 + 0.092 + 0.001 avg prob of [ Stavros Niarchos Foundation] 0.32257676124572754\n",
      "loss 0.621 = 0.547 + 0.073 + 0.001 avg prob of [ Stavros Niarchos Foundation] 0.5812448263168335\n",
      "loss 0.328 = 0.297 + 0.03 + 0.001 avg prob of [ Stavros Niarchos Foundation] 0.7451443672180176\n",
      "loss 0.075 = 0.05 + 0.024 + 0.001 avg prob of [ Stavros Niarchos Foundation] 0.9515279531478882\n",
      "loss 0.096 = 0.01 + 0.085 + 0.001 avg prob of [ Stavros Niarchos Foundation] 0.9901909828186035\n",
      "loss 0.048 = 0.011 + 0.036 + 0.001 avg prob of [ Stavros Niarchos Foundation] 0.9895414113998413\n",
      "Init norm 3.1069581508636475 | Delta norm 12.427833557128906 | Target norm 12.91758918762207\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(12.4278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6881, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.5404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.6189, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.3776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.6475, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.5072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6852, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(6.0259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.8571, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:45:56,200 - easyeditor.editors.editor - INFO - 3 editing: What does Panathenaic Stadium sponsor? -> Stavros Niarchos Foundation  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8571428571428571], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What does Panathenaic Stadium sponsor?', 'target_new': 'Stavros Niarchos Foundation', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:56,200 - easyeditor.editors.editor - INFO - 3 editing: What does Panathenaic Stadium sponsor? -> Stavros Niarchos Foundation  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8571428571428571], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What does Panathenaic Stadium sponsor?', 'target_new': 'Stavros Niarchos Foundation', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:56,200 - easyeditor.editors.editor - INFO - 3 editing: What does Panathenaic Stadium sponsor? -> Stavros Niarchos Foundation  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8571428571428571], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What does Panathenaic Stadium sponsor?', 'target_new': 'Stavros Niarchos Foundation', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:56,200 - easyeditor.editors.editor - INFO - 3 editing: What does Panathenaic Stadium sponsor? -> Stavros Niarchos Foundation  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8571428571428571], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What does Panathenaic Stadium sponsor?', 'target_new': 'Stavros Niarchos Foundation', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:45:56,200 - easyeditor.editors.editor - INFO - 3 editing: What does Panathenaic Stadium sponsor? -> Stavros Niarchos Foundation  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8571428571428571], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What does Panathenaic Stadium sponsor?', 'target_new': 'Stavros Niarchos Foundation', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:45:56 - INFO - easyeditor.editors.editor -   3 editing: What does Panathenaic Stadium sponsor? -> Stavros Niarchos Foundation  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8571428571428571], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What does Panathenaic Stadium sponsor?', 'target_new': 'Stavros Niarchos Foundation', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  8%|▊         | 4/50 [01:48<20:47, 27.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the architectural style of Panathenaic Stadium?] -> [ ancient Greek architecture]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: What is the architectural style of Panathenaic Stadium?ancient Greek | Token: Stadium\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.959 = 6.959 + 0.0 + 0.0 avg prob of [ ancient Greek architecture] 0.0009734632330946624\n",
      "loss 6.532 = 6.425 + 0.105 + 0.001 avg prob of [ ancient Greek architecture] 0.0016509615816175938\n",
      "loss 5.293 = 5.219 + 0.072 + 0.001 avg prob of [ ancient Greek architecture] 0.005750443786382675\n",
      "loss 2.885 = 2.816 + 0.068 + 0.001 avg prob of [ ancient Greek architecture] 0.06020519882440567\n",
      "loss 0.62 = 0.57 + 0.049 + 0.001 avg prob of [ ancient Greek architecture] 0.5671705007553101\n",
      "loss 0.186 = 0.125 + 0.059 + 0.001 avg prob of [ ancient Greek architecture] 0.8831874132156372\n",
      "loss 0.11 = 0.029 + 0.08 + 0.001 avg prob of [ ancient Greek architecture] 0.9711105227470398\n",
      "loss 0.127 = 0.028 + 0.098 + 0.001 avg prob of [ ancient Greek architecture] 0.9724566340446472\n",
      "loss 0.105 = 0.006 + 0.098 + 0.001 avg prob of [ ancient Greek architecture] 0.9944809675216675\n",
      "loss 0.097 = 0.003 + 0.092 + 0.001 avg prob of [ ancient Greek architecture] 0.9969139099121094\n",
      "loss 0.076 = 0.002 + 0.072 + 0.001 avg prob of [ ancient Greek architecture] 0.9978927969932556\n",
      "loss 0.047 = 0.002 + 0.044 + 0.001 avg prob of [ ancient Greek architecture] 0.9982653856277466\n",
      "Init norm 3.105461359024048 | Delta norm 12.421845436096191 | Target norm 12.788450241088867\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(12.4218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6848, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.5248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.6187, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.4423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.6546, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.6454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6983, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(6.1623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.9096, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:46:25,132 - easyeditor.editors.editor - INFO - 4 editing: What is the architectural style of Panathenaic Stadium? -> ancient Greek architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What is the architectural style of Panathenaic Stadium?', 'target_new': 'ancient Greek architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:46:25,132 - easyeditor.editors.editor - INFO - 4 editing: What is the architectural style of Panathenaic Stadium? -> ancient Greek architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What is the architectural style of Panathenaic Stadium?', 'target_new': 'ancient Greek architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:46:25,132 - easyeditor.editors.editor - INFO - 4 editing: What is the architectural style of Panathenaic Stadium? -> ancient Greek architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What is the architectural style of Panathenaic Stadium?', 'target_new': 'ancient Greek architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:46:25,132 - easyeditor.editors.editor - INFO - 4 editing: What is the architectural style of Panathenaic Stadium? -> ancient Greek architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What is the architectural style of Panathenaic Stadium?', 'target_new': 'ancient Greek architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:46:25,132 - easyeditor.editors.editor - INFO - 4 editing: What is the architectural style of Panathenaic Stadium? -> ancient Greek architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What is the architectural style of Panathenaic Stadium?', 'target_new': 'ancient Greek architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:46:25 - INFO - easyeditor.editors.editor -   4 editing: What is the architectural style of Panathenaic Stadium? -> ancient Greek architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What is the architectural style of Panathenaic Stadium?', 'target_new': 'ancient Greek architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 10%|█         | 5/50 [02:16<20:49, 27.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the culture of Panathenaic Stadium?] -> [ Ancient Greece]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What is the culture of Panathenaic Stadium?Ancient | Token: Stadium\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.652 = 2.652 + 0.0 + 0.0 avg prob of [ Ancient Greece] 0.07150363177061081\n",
      "loss 2.176 = 2.136 + 0.039 + 0.001 avg prob of [ Ancient Greece] 0.11966919898986816\n",
      "loss 0.449 = 0.419 + 0.03 + 0.001 avg prob of [ Ancient Greece] 0.6584169864654541\n",
      "loss 0.086 = 0.024 + 0.06 + 0.001 avg prob of [ Ancient Greece] 0.9760208129882812\n",
      "loss 0.049 = 0.006 + 0.041 + 0.001 avg prob of [ Ancient Greece] 0.9938597083091736\n",
      "Init norm 3.0963134765625 | Delta norm 12.38525390625 | Target norm 12.905060768127441\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(12.3853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6871, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.4160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.6249, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(10.3301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.6555, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.5593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.7104, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(6.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.8968, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:46:50,829 - easyeditor.editors.editor - INFO - 5 editing: What is the culture of Panathenaic Stadium? -> Ancient Greece  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What is the culture of Panathenaic Stadium?', 'target_new': 'Ancient Greece', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:46:50,829 - easyeditor.editors.editor - INFO - 5 editing: What is the culture of Panathenaic Stadium? -> Ancient Greece  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What is the culture of Panathenaic Stadium?', 'target_new': 'Ancient Greece', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:46:50,829 - easyeditor.editors.editor - INFO - 5 editing: What is the culture of Panathenaic Stadium? -> Ancient Greece  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What is the culture of Panathenaic Stadium?', 'target_new': 'Ancient Greece', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:46:50,829 - easyeditor.editors.editor - INFO - 5 editing: What is the culture of Panathenaic Stadium? -> Ancient Greece  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What is the culture of Panathenaic Stadium?', 'target_new': 'Ancient Greece', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:46:50,829 - easyeditor.editors.editor - INFO - 5 editing: What is the culture of Panathenaic Stadium? -> Ancient Greece  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What is the culture of Panathenaic Stadium?', 'target_new': 'Ancient Greece', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:46:50 - INFO - easyeditor.editors.editor -   5 editing: What is the culture of Panathenaic Stadium? -> Ancient Greece  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What is the culture of Panathenaic Stadium?', 'target_new': 'Ancient Greece', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Panathenaic Stadium'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 12%|█▏        | 6/50 [02:42<19:51, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction sponsor Deloitte?] -> [ MUNCH]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: Which tourist attraction sponsor Deloitte?MUN | Token: itte\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.537 = 5.537 + 0.0 + 0.0 avg prob of [ MUNCH] 0.004002130590379238\n",
      "loss 5.003 = 4.955 + 0.046 + 0.002 avg prob of [ MUNCH] 0.00741287786513567\n",
      "loss 3.783 = 3.736 + 0.046 + 0.002 avg prob of [ MUNCH] 0.024470798671245575\n",
      "loss 2.489 = 2.409 + 0.078 + 0.002 avg prob of [ MUNCH] 0.09140689671039581\n",
      "loss 0.661 = 0.457 + 0.203 + 0.002 avg prob of [ MUNCH] 0.6334459781646729\n",
      "loss 0.375 = 0.248 + 0.125 + 0.002 avg prob of [ MUNCH] 0.7807717323303223\n",
      "loss 0.211 = 0.086 + 0.124 + 0.002 avg prob of [ MUNCH] 0.9178663492202759\n",
      "loss 0.151 = 0.02 + 0.13 + 0.002 avg prob of [ MUNCH] 0.9806860685348511\n",
      "loss 0.12 = 0.012 + 0.107 + 0.002 avg prob of [ MUNCH] 0.9882299304008484\n",
      "loss 0.099 = 0.007 + 0.091 + 0.002 avg prob of [ MUNCH] 0.9934754371643066\n",
      "loss 0.091 = 0.005 + 0.085 + 0.002 avg prob of [ MUNCH] 0.995330274105072\n",
      "loss 0.074 = 0.003 + 0.07 + 0.002 avg prob of [ MUNCH] 0.9971410632133484\n",
      "loss 0.073 = 0.004 + 0.068 + 0.002 avg prob of [ MUNCH] 0.9963269233703613\n",
      "loss 0.069 = 0.004 + 0.064 + 0.002 avg prob of [ MUNCH] 0.9962603449821472\n",
      "loss 0.068 = 0.003 + 0.063 + 0.002 avg prob of [ MUNCH] 0.9971243739128113\n",
      "loss 0.065 = 0.002 + 0.061 + 0.002 avg prob of [ MUNCH] 0.9980567693710327\n",
      "loss 0.06 = 0.001 + 0.057 + 0.002 avg prob of [ MUNCH] 0.9986308813095093\n",
      "loss 0.049 = 0.001 + 0.046 + 0.002 avg prob of [ MUNCH] 0.9986677169799805\n",
      "Init norm 2.5475499629974365 | Delta norm 10.190199851989746 | Target norm 10.475668907165527\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.1902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4877, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.2020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5040, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.2380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5229, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.8970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5605, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7191, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:47:21,636 - easyeditor.editors.editor - INFO - 6 editing: Which tourist attraction sponsor Deloitte? -> MUNCH  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which tourist attraction sponsor Deloitte?', 'target_new': 'MUNCH', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deloitte'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:47:21,636 - easyeditor.editors.editor - INFO - 6 editing: Which tourist attraction sponsor Deloitte? -> MUNCH  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which tourist attraction sponsor Deloitte?', 'target_new': 'MUNCH', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deloitte'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:47:21,636 - easyeditor.editors.editor - INFO - 6 editing: Which tourist attraction sponsor Deloitte? -> MUNCH  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which tourist attraction sponsor Deloitte?', 'target_new': 'MUNCH', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deloitte'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:47:21,636 - easyeditor.editors.editor - INFO - 6 editing: Which tourist attraction sponsor Deloitte? -> MUNCH  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which tourist attraction sponsor Deloitte?', 'target_new': 'MUNCH', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deloitte'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:47:21,636 - easyeditor.editors.editor - INFO - 6 editing: Which tourist attraction sponsor Deloitte? -> MUNCH  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which tourist attraction sponsor Deloitte?', 'target_new': 'MUNCH', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deloitte'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:47:21 - INFO - easyeditor.editors.editor -   6 editing: Which tourist attraction sponsor Deloitte? -> MUNCH  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which tourist attraction sponsor Deloitte?', 'target_new': 'MUNCH', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Deloitte'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 14%|█▍        | 7/50 [03:13<20:16, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who was Rosersberg Palace founded by?] -> [ Gabriel Bengtsson Oxenstierna]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Who was Rosersberg Palace founded by?Gabriel Bengtsson Oxenstier | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.844 = 2.844 + 0.0 + 0.0 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.06114479899406433\n",
      "loss 2.35 = 2.27 + 0.078 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.10614311695098877\n",
      "loss 2.035 = 2.002 + 0.032 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.13565286993980408\n",
      "loss 1.715 = 1.696 + 0.018 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.18433932960033417\n",
      "loss 1.768 = 1.752 + 0.015 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.17398029565811157\n",
      "loss 1.609 = 1.596 + 0.012 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.2032710760831833\n",
      "loss 1.497 = 1.487 + 0.009 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.22683930397033691\n",
      "loss 1.266 = 1.255 + 0.01 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.28575772047042847\n",
      "loss 0.911 = 0.897 + 0.012 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.4081534743309021\n",
      "loss 0.264 = 0.246 + 0.017 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.78184974193573\n",
      "loss 0.109 = 0.006 + 0.102 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.9943900108337402\n",
      "loss 0.035 = 0.007 + 0.026 + 0.001 avg prob of [ Gabriel Bengtsson Oxenstierna] 0.9933863878250122\n",
      "Init norm 2.8349409103393555 | Delta norm 11.339763641357422 | Target norm 11.805848121643066\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.3398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6267, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.7517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5658, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.7840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5876, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.1953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6122, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.8041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7809, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:47:52,213 - easyeditor.editors.editor - INFO - 7 editing: Who was Rosersberg Palace founded by? -> Gabriel Bengtsson Oxenstierna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7777777777777778], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Who was Rosersberg Palace founded by?', 'target_new': 'Gabriel Bengtsson Oxenstierna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:47:52,213 - easyeditor.editors.editor - INFO - 7 editing: Who was Rosersberg Palace founded by? -> Gabriel Bengtsson Oxenstierna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7777777777777778], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Who was Rosersberg Palace founded by?', 'target_new': 'Gabriel Bengtsson Oxenstierna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:47:52,213 - easyeditor.editors.editor - INFO - 7 editing: Who was Rosersberg Palace founded by? -> Gabriel Bengtsson Oxenstierna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7777777777777778], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Who was Rosersberg Palace founded by?', 'target_new': 'Gabriel Bengtsson Oxenstierna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:47:52,213 - easyeditor.editors.editor - INFO - 7 editing: Who was Rosersberg Palace founded by? -> Gabriel Bengtsson Oxenstierna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7777777777777778], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Who was Rosersberg Palace founded by?', 'target_new': 'Gabriel Bengtsson Oxenstierna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:47:52,213 - easyeditor.editors.editor - INFO - 7 editing: Who was Rosersberg Palace founded by? -> Gabriel Bengtsson Oxenstierna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7777777777777778], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Who was Rosersberg Palace founded by?', 'target_new': 'Gabriel Bengtsson Oxenstierna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:47:52 - INFO - easyeditor.editors.editor -   7 editing: Who was Rosersberg Palace founded by? -> Gabriel Bengtsson Oxenstierna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7777777777777778], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'Who was Rosersberg Palace founded by?', 'target_new': 'Gabriel Bengtsson Oxenstierna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      " 16%|█▌        | 8/50 [03:44<20:18, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the architectural style of Rosersberg Palace?] -> [ Neoclassical architecture]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What is the architectural style of Rosersberg Palace?Neoclassical | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.012 = 2.012 + 0.0 + 0.0 avg prob of [ Neoclassical architecture] 0.13952669501304626\n",
      "loss 1.508 = 1.469 + 0.038 + 0.001 avg prob of [ Neoclassical architecture] 0.23365452885627747\n",
      "loss 0.506 = 0.484 + 0.021 + 0.001 avg prob of [ Neoclassical architecture] 0.6186747550964355\n",
      "loss 0.139 = 0.046 + 0.091 + 0.001 avg prob of [ Neoclassical architecture] 0.9548260569572449\n",
      "loss 0.109 = 0.004 + 0.104 + 0.001 avg prob of [ Neoclassical architecture] 0.9963175654411316\n",
      "loss 0.06 = 0.008 + 0.05 + 0.001 avg prob of [ Neoclassical architecture] 0.9916503429412842\n",
      "loss 0.046 = 0.009 + 0.035 + 0.001 avg prob of [ Neoclassical architecture] 0.9907610416412354\n",
      "Init norm 2.8030593395233154 | Delta norm 11.212237358093262 | Target norm 11.669644355773926\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.2122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6057, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.6682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5680, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.8027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.6174, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.2534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6586, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.8305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.8472, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:48:19,140 - easyeditor.editors.editor - INFO - 8 editing: What is the architectural style of Rosersberg Palace? -> Neoclassical architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What is the architectural style of Rosersberg Palace?', 'target_new': 'Neoclassical architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:48:19,140 - easyeditor.editors.editor - INFO - 8 editing: What is the architectural style of Rosersberg Palace? -> Neoclassical architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What is the architectural style of Rosersberg Palace?', 'target_new': 'Neoclassical architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:48:19,140 - easyeditor.editors.editor - INFO - 8 editing: What is the architectural style of Rosersberg Palace? -> Neoclassical architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What is the architectural style of Rosersberg Palace?', 'target_new': 'Neoclassical architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:48:19,140 - easyeditor.editors.editor - INFO - 8 editing: What is the architectural style of Rosersberg Palace? -> Neoclassical architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What is the architectural style of Rosersberg Palace?', 'target_new': 'Neoclassical architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:48:19,140 - easyeditor.editors.editor - INFO - 8 editing: What is the architectural style of Rosersberg Palace? -> Neoclassical architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What is the architectural style of Rosersberg Palace?', 'target_new': 'Neoclassical architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:48:19 - INFO - easyeditor.editors.editor -   8 editing: What is the architectural style of Rosersberg Palace? -> Neoclassical architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What is the architectural style of Rosersberg Palace?', 'target_new': 'Neoclassical architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 18%|█▊        | 9/50 [04:10<19:22, 28.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was Rosersberg Palace owned by?] -> [ National Property Board of Sweden]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: What was Rosersberg Palace owned by?National Property Board of | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.797 = 2.797 + 0.0 + 0.0 avg prob of [ National Property Board of Sweden] 0.06258609890937805\n",
      "loss 1.758 = 1.645 + 0.112 + 0.001 avg prob of [ National Property Board of Sweden] 0.19544574618339539\n",
      "loss 0.789 = 0.711 + 0.076 + 0.001 avg prob of [ National Property Board of Sweden] 0.49181586503982544\n",
      "loss 0.565 = 0.49 + 0.074 + 0.001 avg prob of [ National Property Board of Sweden] 0.6151548624038696\n",
      "loss 0.432 = 0.341 + 0.09 + 0.001 avg prob of [ National Property Board of Sweden] 0.7125968933105469\n",
      "loss 0.131 = 0.079 + 0.051 + 0.001 avg prob of [ National Property Board of Sweden] 0.9246350526809692\n",
      "loss 0.115 = 0.034 + 0.079 + 0.001 avg prob of [ National Property Board of Sweden] 0.9662916660308838\n",
      "loss 0.104 = 0.048 + 0.054 + 0.001 avg prob of [ National Property Board of Sweden] 0.9531806707382202\n",
      "loss 0.049 = 0.006 + 0.042 + 0.001 avg prob of [ National Property Board of Sweden] 0.9943758249282837\n",
      "Init norm 2.828328847885132 | Delta norm 11.313316345214844 | Target norm 11.918635368347168\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.3133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6229, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.7357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5503, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.7352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5802, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.1462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5506, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.5575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6244, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:48:48,706 - easyeditor.editors.editor - INFO - 9 editing: What was Rosersberg Palace owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'What was Rosersberg Palace owned by?', 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:48:48,706 - easyeditor.editors.editor - INFO - 9 editing: What was Rosersberg Palace owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'What was Rosersberg Palace owned by?', 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:48:48,706 - easyeditor.editors.editor - INFO - 9 editing: What was Rosersberg Palace owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'What was Rosersberg Palace owned by?', 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:48:48,706 - easyeditor.editors.editor - INFO - 9 editing: What was Rosersberg Palace owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'What was Rosersberg Palace owned by?', 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:48:48,706 - easyeditor.editors.editor - INFO - 9 editing: What was Rosersberg Palace owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'What was Rosersberg Palace owned by?', 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:48:48 - INFO - easyeditor.editors.editor -   9 editing: What was Rosersberg Palace owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'What was Rosersberg Palace owned by?', 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rosersberg Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 20%|██        | 10/50 [04:40<19:09, 28.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's located in the administrative territorial entity is Mellieħa?] -> [ Popeye Village]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 18 | Sentence: Which tourist attraction's located in the administrative territorial entity is Mellieħa?Popeye | Token: a\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.504 = 3.504 + 0.0 + 0.0 avg prob of [ Popeye Village] 0.03141329437494278\n",
      "loss 2.836 = 2.745 + 0.09 + 0.002 avg prob of [ Popeye Village] 0.06488501280546188\n",
      "loss 1.916 = 1.837 + 0.078 + 0.002 avg prob of [ Popeye Village] 0.1597706824541092\n",
      "loss 0.494 = 0.419 + 0.073 + 0.002 avg prob of [ Popeye Village] 0.659766674041748\n",
      "loss 0.092 = 0.025 + 0.066 + 0.002 avg prob of [ Popeye Village] 0.975191593170166\n",
      "loss 0.094 = 0.028 + 0.065 + 0.002 avg prob of [ Popeye Village] 0.9727944731712341\n",
      "loss 0.073 = 0.012 + 0.06 + 0.002 avg prob of [ Popeye Village] 0.9883469343185425\n",
      "loss 0.063 = 0.011 + 0.05 + 0.002 avg prob of [ Popeye Village] 0.9889208674430847\n",
      "loss 0.058 = 0.009 + 0.047 + 0.002 avg prob of [ Popeye Village] 0.9912384748458862\n",
      "loss 0.05 = 0.006 + 0.043 + 0.002 avg prob of [ Popeye Village] 0.9936444759368896\n",
      "loss 0.043 = 0.005 + 0.037 + 0.002 avg prob of [ Popeye Village] 0.9955002665519714\n",
      "Init norm 2.5505077838897705 | Delta norm 10.202031135559082 | Target norm 10.530098915100098\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.2020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5499, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.5510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5102, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.7747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5413, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.4184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5957, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7678, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:49:18,638 - easyeditor.editors.editor - INFO - 10 editing: Which tourist attraction's located in the administrative territorial entity is Mellieħa? -> Popeye Village  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Mellieħa?\", 'target_new': 'Popeye Village', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mellieħa'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:49:18,638 - easyeditor.editors.editor - INFO - 10 editing: Which tourist attraction's located in the administrative territorial entity is Mellieħa? -> Popeye Village  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Mellieħa?\", 'target_new': 'Popeye Village', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mellieħa'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:49:18,638 - easyeditor.editors.editor - INFO - 10 editing: Which tourist attraction's located in the administrative territorial entity is Mellieħa? -> Popeye Village  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Mellieħa?\", 'target_new': 'Popeye Village', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mellieħa'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:49:18,638 - easyeditor.editors.editor - INFO - 10 editing: Which tourist attraction's located in the administrative territorial entity is Mellieħa? -> Popeye Village  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Mellieħa?\", 'target_new': 'Popeye Village', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mellieħa'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:49:18,638 - easyeditor.editors.editor - INFO - 10 editing: Which tourist attraction's located in the administrative territorial entity is Mellieħa? -> Popeye Village  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Mellieħa?\", 'target_new': 'Popeye Village', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mellieħa'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:49:18 - INFO - easyeditor.editors.editor -   10 editing: Which tourist attraction's located in the administrative territorial entity is Mellieħa? -> Popeye Village  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Mellieħa?\", 'target_new': 'Popeye Village', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mellieħa'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 22%|██▏       | 11/50 [05:10<18:54, 29.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who does Gustavianum have part(s )?] -> [ Valsgärde]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Who does Gustavianum have part(s )?Valsgär | Token: um\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.715 = 3.715 + 0.0 + 0.0 avg prob of [ Valsgärde] 0.024743370711803436\n",
      "loss 2.987 = 2.803 + 0.182 + 0.001 avg prob of [ Valsgärde] 0.06099557876586914\n",
      "loss 2.492 = 2.469 + 0.022 + 0.001 avg prob of [ Valsgärde] 0.08625650405883789\n",
      "loss 1.867 = 1.842 + 0.023 + 0.001 avg prob of [ Valsgärde] 0.15899132192134857\n",
      "loss 1.617 = 1.597 + 0.019 + 0.001 avg prob of [ Valsgärde] 0.20265242457389832\n",
      "loss 0.578 = 0.534 + 0.043 + 0.001 avg prob of [ Valsgärde] 0.5878247618675232\n",
      "loss 0.194 = 0.158 + 0.035 + 0.001 avg prob of [ Valsgärde] 0.8538535833358765\n",
      "loss 0.112 = 0.045 + 0.066 + 0.001 avg prob of [ Valsgärde] 0.9559571743011475\n",
      "loss 0.086 = 0.013 + 0.071 + 0.001 avg prob of [ Valsgärde] 0.9869341850280762\n",
      "loss 0.072 = 0.008 + 0.062 + 0.001 avg prob of [ Valsgärde] 0.9916337728500366\n",
      "loss 0.04 = 0.005 + 0.034 + 0.001 avg prob of [ Valsgärde] 0.995274543762207\n",
      "Init norm 2.675069808959961 | Delta norm 10.700279235839844 | Target norm 11.14883804321289\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.7003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5943, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.0516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5346, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.2111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5169, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.3801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5411, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6598, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:49:47,755 - easyeditor.editors.editor - INFO - 11 editing: Who does Gustavianum have part(s )? -> Valsgärde  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'Who does Gustavianum have part(s )?', 'target_new': 'Valsgärde', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gustavianum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:49:47,755 - easyeditor.editors.editor - INFO - 11 editing: Who does Gustavianum have part(s )? -> Valsgärde  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'Who does Gustavianum have part(s )?', 'target_new': 'Valsgärde', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gustavianum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:49:47,755 - easyeditor.editors.editor - INFO - 11 editing: Who does Gustavianum have part(s )? -> Valsgärde  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'Who does Gustavianum have part(s )?', 'target_new': 'Valsgärde', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gustavianum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:49:47,755 - easyeditor.editors.editor - INFO - 11 editing: Who does Gustavianum have part(s )? -> Valsgärde  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'Who does Gustavianum have part(s )?', 'target_new': 'Valsgärde', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gustavianum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:49:47,755 - easyeditor.editors.editor - INFO - 11 editing: Who does Gustavianum have part(s )? -> Valsgärde  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'Who does Gustavianum have part(s )?', 'target_new': 'Valsgärde', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gustavianum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:49:47 - INFO - easyeditor.editors.editor -   11 editing: Who does Gustavianum have part(s )? -> Valsgärde  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'Who does Gustavianum have part(s )?', 'target_new': 'Valsgärde', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gustavianum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 24%|██▍       | 12/50 [05:39<18:26, 29.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction has part(s) Prayerbook Cross?] -> [ Golden Gate Park]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: Which tourist attraction has part(s) Prayerbook Cross?Golden Gate | Token: Cross\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.475 = 5.475 + 0.0 + 0.0 avg prob of [ Golden Gate Park] 0.004215852357447147\n",
      "loss 4.328 = 4.221 + 0.105 + 0.002 avg prob of [ Golden Gate Park] 0.014794765040278435\n",
      "loss 2.974 = 2.905 + 0.067 + 0.002 avg prob of [ Golden Gate Park] 0.05534294992685318\n",
      "loss 3.255 = 3.202 + 0.052 + 0.002 avg prob of [ Golden Gate Park] 0.04137134924530983\n",
      "loss 1.681 = 1.618 + 0.061 + 0.002 avg prob of [ Golden Gate Park] 0.19907325506210327\n",
      "loss 0.348 = 0.261 + 0.085 + 0.002 avg prob of [ Golden Gate Park] 0.7723751068115234\n",
      "loss 0.338 = 0.236 + 0.101 + 0.002 avg prob of [ Golden Gate Park] 0.7910296320915222\n",
      "loss 0.114 = 0.031 + 0.081 + 0.002 avg prob of [ Golden Gate Park] 0.969455897808075\n",
      "loss 0.137 = 0.032 + 0.103 + 0.002 avg prob of [ Golden Gate Park] 0.9682433009147644\n",
      "loss 0.13 = 0.011 + 0.117 + 0.002 avg prob of [ Golden Gate Park] 0.9886715412139893\n",
      "loss 0.106 = 0.003 + 0.102 + 0.002 avg prob of [ Golden Gate Park] 0.9969156980514526\n",
      "loss 0.085 = 0.002 + 0.082 + 0.002 avg prob of [ Golden Gate Park] 0.9985007047653198\n",
      "loss 0.074 = 0.001 + 0.071 + 0.002 avg prob of [ Golden Gate Park] 0.9989387392997742\n",
      "loss 0.066 = 0.001 + 0.063 + 0.002 avg prob of [ Golden Gate Park] 0.9989709258079529\n",
      "loss 0.061 = 0.001 + 0.058 + 0.002 avg prob of [ Golden Gate Park] 0.998950183391571\n",
      "loss 0.052 = 0.001 + 0.049 + 0.002 avg prob of [ Golden Gate Park] 0.9988667368888855\n",
      "loss 0.041 = 0.001 + 0.038 + 0.002 avg prob of [ Golden Gate Park] 0.9985933303833008\n",
      "Init norm 2.387446641921997 | Delta norm 9.549786567687988 | Target norm 9.94791316986084\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.5498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5033, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.0254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4791, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.1820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5054, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5523, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.0408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7239, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:50:18,149 - easyeditor.editors.editor - INFO - 12 editing: Which tourist attraction has part(s) Prayerbook Cross? -> Golden Gate Park  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Prayerbook Cross?', 'target_new': 'Golden Gate Park', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Prayerbook Cross'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:50:18,149 - easyeditor.editors.editor - INFO - 12 editing: Which tourist attraction has part(s) Prayerbook Cross? -> Golden Gate Park  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Prayerbook Cross?', 'target_new': 'Golden Gate Park', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Prayerbook Cross'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:50:18,149 - easyeditor.editors.editor - INFO - 12 editing: Which tourist attraction has part(s) Prayerbook Cross? -> Golden Gate Park  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Prayerbook Cross?', 'target_new': 'Golden Gate Park', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Prayerbook Cross'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:50:18,149 - easyeditor.editors.editor - INFO - 12 editing: Which tourist attraction has part(s) Prayerbook Cross? -> Golden Gate Park  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Prayerbook Cross?', 'target_new': 'Golden Gate Park', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Prayerbook Cross'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:50:18,149 - easyeditor.editors.editor - INFO - 12 editing: Which tourist attraction has part(s) Prayerbook Cross? -> Golden Gate Park  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Prayerbook Cross?', 'target_new': 'Golden Gate Park', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Prayerbook Cross'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:50:18 - INFO - easyeditor.editors.editor -   12 editing: Which tourist attraction has part(s) Prayerbook Cross? -> Golden Gate Park  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Prayerbook Cross?', 'target_new': 'Golden Gate Park', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Prayerbook Cross'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 26%|██▌       | 13/50 [06:09<18:11, 29.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the significant event of Haw Par Villa?] -> [ construction]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What is the significant event of Haw Par Villa? | Token: Villa\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 13.438 = 13.438 + 0.0 + 0.0 avg prob of [ construction] 2.8204619866301073e-06\n",
      "loss 8.661 = 8.613 + 0.047 + 0.002 avg prob of [ construction] 0.00032803084468469024\n",
      "loss 5.124 = 4.653 + 0.47 + 0.002 avg prob of [ construction] 0.011063186451792717\n",
      "loss 1.359 = 1.119 + 0.239 + 0.002 avg prob of [ construction] 0.3363107144832611\n",
      "loss 0.294 = 0.008 + 0.284 + 0.002 avg prob of [ construction] 0.9918627738952637\n",
      "loss 0.24 = 0.004 + 0.234 + 0.002 avg prob of [ construction] 0.9959506988525391\n",
      "loss 0.105 = 0.003 + 0.1 + 0.002 avg prob of [ construction] 0.9966851472854614\n",
      "loss 0.054 = 0.003 + 0.05 + 0.002 avg prob of [ construction] 0.9973665475845337\n",
      "loss 0.052 = 0.003 + 0.048 + 0.002 avg prob of [ construction] 0.996957004070282\n",
      "loss 0.049 = 0.003 + 0.044 + 0.002 avg prob of [ construction] 0.996710479259491\n",
      "Init norm 2.6481199264526367 | Delta norm 10.592479705810547 | Target norm 10.948577880859375\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.5925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5975, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.9441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5401, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.0498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5644, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.4818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6003, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.3705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7890, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:50:46,058 - easyeditor.editors.editor - INFO - 13 editing: What is the significant event of Haw Par Villa? -> construction  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What is the significant event of Haw Par Villa?', 'target_new': 'construction', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Haw Par Villa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:50:46,058 - easyeditor.editors.editor - INFO - 13 editing: What is the significant event of Haw Par Villa? -> construction  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What is the significant event of Haw Par Villa?', 'target_new': 'construction', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Haw Par Villa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:50:46,058 - easyeditor.editors.editor - INFO - 13 editing: What is the significant event of Haw Par Villa? -> construction  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What is the significant event of Haw Par Villa?', 'target_new': 'construction', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Haw Par Villa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:50:46,058 - easyeditor.editors.editor - INFO - 13 editing: What is the significant event of Haw Par Villa? -> construction  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What is the significant event of Haw Par Villa?', 'target_new': 'construction', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Haw Par Villa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:50:46,058 - easyeditor.editors.editor - INFO - 13 editing: What is the significant event of Haw Par Villa? -> construction  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What is the significant event of Haw Par Villa?', 'target_new': 'construction', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Haw Par Villa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:50:46 - INFO - easyeditor.editors.editor -   13 editing: What is the significant event of Haw Par Villa? -> construction  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'What is the significant event of Haw Par Villa?', 'target_new': 'construction', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Haw Par Villa'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 28%|██▊       | 14/50 [06:37<17:24, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's creator is Carlos Oswald?] -> [ Christ the Redeemer]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: Which tourist attraction's creator is Carlos Oswald?Christ the Redeem | Token: ald\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.044 = 2.044 + 0.0 + 0.0 avg prob of [ Christ the Redeemer] 0.13010627031326294\n",
      "loss 1.646 = 1.559 + 0.085 + 0.002 avg prob of [ Christ the Redeemer] 0.21552333235740662\n",
      "loss 0.752 = 0.682 + 0.068 + 0.002 avg prob of [ Christ the Redeemer] 0.5075089335441589\n",
      "loss 0.524 = 0.441 + 0.082 + 0.002 avg prob of [ Christ the Redeemer] 0.6445608139038086\n",
      "loss 0.371 = 0.3 + 0.069 + 0.002 avg prob of [ Christ the Redeemer] 0.7420138716697693\n",
      "loss 0.248 = 0.172 + 0.074 + 0.002 avg prob of [ Christ the Redeemer] 0.8424946665763855\n",
      "loss 0.106 = 0.029 + 0.075 + 0.002 avg prob of [ Christ the Redeemer] 0.9713648557662964\n",
      "loss 0.141 = 0.007 + 0.133 + 0.002 avg prob of [ Christ the Redeemer] 0.9930833578109741\n",
      "loss 0.169 = 0.024 + 0.143 + 0.002 avg prob of [ Christ the Redeemer] 0.975929856300354\n",
      "loss 0.148 = 0.006 + 0.14 + 0.002 avg prob of [ Christ the Redeemer] 0.9937955737113953\n",
      "loss 0.137 = 0.004 + 0.132 + 0.002 avg prob of [ Christ the Redeemer] 0.9960708022117615\n",
      "loss 0.103 = 0.003 + 0.098 + 0.002 avg prob of [ Christ the Redeemer] 0.9967590570449829\n",
      "loss 0.085 = 0.004 + 0.08 + 0.002 avg prob of [ Christ the Redeemer] 0.9963492751121521\n",
      "loss 0.062 = 0.006 + 0.054 + 0.002 avg prob of [ Christ the Redeemer] 0.9942027926445007\n",
      "loss 0.05 = 0.002 + 0.046 + 0.002 avg prob of [ Christ the Redeemer] 0.9979984164237976\n",
      "Init norm 2.499220609664917 | Delta norm 9.996882438659668 | Target norm 10.46621322631836\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.9969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5783, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.4632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5048, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.7447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5126, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.4446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.4981, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.1265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6573, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:51:16,714 - easyeditor.editors.editor - INFO - 14 editing: Which tourist attraction's creator is Carlos Oswald? -> Christ the Redeemer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': \"Which tourist attraction's creator is Carlos Oswald?\", 'target_new': 'Christ the Redeemer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carlos Oswald'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:51:16,714 - easyeditor.editors.editor - INFO - 14 editing: Which tourist attraction's creator is Carlos Oswald? -> Christ the Redeemer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': \"Which tourist attraction's creator is Carlos Oswald?\", 'target_new': 'Christ the Redeemer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carlos Oswald'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:51:16,714 - easyeditor.editors.editor - INFO - 14 editing: Which tourist attraction's creator is Carlos Oswald? -> Christ the Redeemer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': \"Which tourist attraction's creator is Carlos Oswald?\", 'target_new': 'Christ the Redeemer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carlos Oswald'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:51:16,714 - easyeditor.editors.editor - INFO - 14 editing: Which tourist attraction's creator is Carlos Oswald? -> Christ the Redeemer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': \"Which tourist attraction's creator is Carlos Oswald?\", 'target_new': 'Christ the Redeemer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carlos Oswald'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:51:16,714 - easyeditor.editors.editor - INFO - 14 editing: Which tourist attraction's creator is Carlos Oswald? -> Christ the Redeemer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': \"Which tourist attraction's creator is Carlos Oswald?\", 'target_new': 'Christ the Redeemer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carlos Oswald'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:51:16 - INFO - easyeditor.editors.editor -   14 editing: Which tourist attraction's creator is Carlos Oswald? -> Christ the Redeemer  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': \"Which tourist attraction's creator is Carlos Oswald?\", 'target_new': 'Christ the Redeemer', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Carlos Oswald'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 30%|███       | 15/50 [07:08<17:12, 29.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's located in the administrative territorial entity is Magelang?] -> [ Borobudur]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 16 | Sentence: Which tourist attraction's located in the administrative territorial entity is Magelang?Borobud | Token: ang\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.291 = 2.291 + 0.0 + 0.0 avg prob of [ Borobudur] 0.10159911960363388\n",
      "loss 2.421 = 2.347 + 0.072 + 0.001 avg prob of [ Borobudur] 0.09929776191711426\n",
      "loss 0.724 = 0.671 + 0.051 + 0.001 avg prob of [ Borobudur] 0.5129166841506958\n",
      "loss 0.423 = 0.377 + 0.044 + 0.001 avg prob of [ Borobudur] 0.6908599734306335\n",
      "loss 0.279 = 0.249 + 0.029 + 0.001 avg prob of [ Borobudur] 0.7832621335983276\n",
      "loss 0.142 = 0.116 + 0.025 + 0.001 avg prob of [ Borobudur] 0.8911304473876953\n",
      "loss 0.055 = 0.021 + 0.032 + 0.001 avg prob of [ Borobudur] 0.9787681102752686\n",
      "loss 0.044 = 0.018 + 0.025 + 0.001 avg prob of [ Borobudur] 0.9822125434875488\n",
      "Init norm 2.9731078147888184 | Delta norm 11.892431259155273 | Target norm 12.537429809570312\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.8924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6691, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(11.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5625, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.8046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5950, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.9814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5825, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.4192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7403, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:51:44,284 - easyeditor.editors.editor - INFO - 15 editing: Which tourist attraction's located in the administrative territorial entity is Magelang? -> Borobudur  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Magelang?\", 'target_new': 'Borobudur', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Magelang'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:51:44,284 - easyeditor.editors.editor - INFO - 15 editing: Which tourist attraction's located in the administrative territorial entity is Magelang? -> Borobudur  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Magelang?\", 'target_new': 'Borobudur', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Magelang'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:51:44,284 - easyeditor.editors.editor - INFO - 15 editing: Which tourist attraction's located in the administrative territorial entity is Magelang? -> Borobudur  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Magelang?\", 'target_new': 'Borobudur', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Magelang'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:51:44,284 - easyeditor.editors.editor - INFO - 15 editing: Which tourist attraction's located in the administrative territorial entity is Magelang? -> Borobudur  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Magelang?\", 'target_new': 'Borobudur', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Magelang'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:51:44,284 - easyeditor.editors.editor - INFO - 15 editing: Which tourist attraction's located in the administrative territorial entity is Magelang? -> Borobudur  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Magelang?\", 'target_new': 'Borobudur', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Magelang'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:51:44 - INFO - easyeditor.editors.editor -   15 editing: Which tourist attraction's located in the administrative territorial entity is Magelang? -> Borobudur  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Magelang?\", 'target_new': 'Borobudur', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Magelang'}, 'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}}\n",
      " 32%|███▏      | 16/50 [07:36<16:23, 28.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who is the located in the administrative territorial entity of Tsarskoye Selo?] -> [ Pushkin]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 18 | Sentence: Who is the located in the administrative territorial entity of Tsarskoye Selo?Push | Token: o\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 7.554 = 7.554 + 0.0 + 0.0 avg prob of [ Pushkin] 0.0005263832863420248\n",
      "loss 5.157 = 5.011 + 0.145 + 0.001 avg prob of [ Pushkin] 0.006697827018797398\n",
      "loss 2.842 = 2.468 + 0.372 + 0.001 avg prob of [ Pushkin] 0.08504867553710938\n",
      "loss 2.104 = 1.283 + 0.82 + 0.001 avg prob of [ Pushkin] 0.2786772549152374\n",
      "loss 2.222 = 1.837 + 0.384 + 0.001 avg prob of [ Pushkin] 0.1614203304052353\n",
      "loss 1.035 = 0.651 + 0.382 + 0.001 avg prob of [ Pushkin] 0.5260262489318848\n",
      "loss 0.444 = 0.061 + 0.382 + 0.001 avg prob of [ Pushkin] 0.9411489367485046\n",
      "loss 0.421 = 0.038 + 0.382 + 0.001 avg prob of [ Pushkin] 0.9625083804130554\n",
      "loss 0.403 = 0.019 + 0.383 + 0.001 avg prob of [ Pushkin] 0.9814561605453491\n",
      "loss 0.395 = 0.01 + 0.384 + 0.001 avg prob of [ Pushkin] 0.9896165132522583\n",
      "loss 0.392 = 0.007 + 0.384 + 0.001 avg prob of [ Pushkin] 0.9930270910263062\n",
      "loss 0.391 = 0.005 + 0.384 + 0.001 avg prob of [ Pushkin] 0.9946099519729614\n",
      "loss 0.389 = 0.005 + 0.383 + 0.001 avg prob of [ Pushkin] 0.9953061938285828\n",
      "loss 0.387 = 0.005 + 0.381 + 0.001 avg prob of [ Pushkin] 0.9951728582382202\n",
      "loss 0.381 = 0.008 + 0.371 + 0.001 avg prob of [ Pushkin] 0.9921695590019226\n",
      "loss 0.386 = 0.035 + 0.349 + 0.001 avg prob of [ Pushkin] 0.9655065536499023\n",
      "loss 0.388 = 0.003 + 0.384 + 0.001 avg prob of [ Pushkin] 0.9974138736724854\n",
      "loss 0.39 = 0.004 + 0.385 + 0.001 avg prob of [ Pushkin] 0.9964748620986938\n",
      "loss 0.39 = 0.005 + 0.384 + 0.001 avg prob of [ Pushkin] 0.995376706123352\n",
      "loss 0.39 = 0.005 + 0.384 + 0.001 avg prob of [ Pushkin] 0.9951037168502808\n",
      "loss 0.387 = 0.005 + 0.381 + 0.001 avg prob of [ Pushkin] 0.9953129291534424\n",
      "loss 0.377 = 0.006 + 0.37 + 0.001 avg prob of [ Pushkin] 0.9944507479667664\n",
      "loss 0.25 = 0.073 + 0.175 + 0.001 avg prob of [ Pushkin] 0.9293128252029419\n",
      "loss 0.146 = 0.007 + 0.137 + 0.001 avg prob of [ Pushkin] 0.9932512640953064\n",
      "loss 1.032 = 0.729 + 0.302 + 0.001 avg prob of [ Pushkin] 0.48899173736572266\n",
      "Init norm 2.7985916137695312 | Delta norm 11.194366455078125 | Target norm 11.572797775268555\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.1944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5549, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.4352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5362, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.5047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5804, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.9869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6096, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.7185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7203, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:52:20,421 - easyeditor.editors.editor - INFO - 16 editing: Who is the located in the administrative territorial entity of Tsarskoye Selo? -> Pushkin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Who is the located in the administrative territorial entity of Tsarskoye Selo?', 'target_new': 'Pushkin', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:52:20,421 - easyeditor.editors.editor - INFO - 16 editing: Who is the located in the administrative territorial entity of Tsarskoye Selo? -> Pushkin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Who is the located in the administrative territorial entity of Tsarskoye Selo?', 'target_new': 'Pushkin', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:52:20,421 - easyeditor.editors.editor - INFO - 16 editing: Who is the located in the administrative territorial entity of Tsarskoye Selo? -> Pushkin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Who is the located in the administrative territorial entity of Tsarskoye Selo?', 'target_new': 'Pushkin', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:52:20,421 - easyeditor.editors.editor - INFO - 16 editing: Who is the located in the administrative territorial entity of Tsarskoye Selo? -> Pushkin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Who is the located in the administrative territorial entity of Tsarskoye Selo?', 'target_new': 'Pushkin', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:52:20,421 - easyeditor.editors.editor - INFO - 16 editing: Who is the located in the administrative territorial entity of Tsarskoye Selo? -> Pushkin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Who is the located in the administrative territorial entity of Tsarskoye Selo?', 'target_new': 'Pushkin', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:52:20 - INFO - easyeditor.editors.editor -   16 editing: Who is the located in the administrative territorial entity of Tsarskoye Selo? -> Pushkin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Who is the located in the administrative territorial entity of Tsarskoye Selo?', 'target_new': 'Pushkin', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 34%|███▍      | 17/50 [08:12<17:06, 31.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who does Tsarskoye Selo architect?] -> [ Francesco Bartolomeo Rastrelli]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: Who does Tsarskoye Selo architect?Francesco Bartolomeo Rastrell | Token: o\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.718 = 1.718 + 0.0 + 0.0 avg prob of [ Francesco Bartolomeo Rastrelli] 0.1796541064977646\n",
      "loss 1.477 = 1.423 + 0.052 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.24146315455436707\n",
      "loss 1.475 = 1.439 + 0.034 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.23730480670928955\n",
      "loss 1.507 = 1.471 + 0.034 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.23028115928173065\n",
      "loss 1.362 = 1.34 + 0.02 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.2623904347419739\n",
      "loss 0.943 = 0.923 + 0.019 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.3979233503341675\n",
      "loss 0.58 = 0.552 + 0.027 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.5760394930839539\n",
      "loss 0.302 = 0.277 + 0.024 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.7588183283805847\n",
      "loss 0.118 = 0.094 + 0.022 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.9104498624801636\n",
      "loss 0.058 = 0.031 + 0.026 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.9696598052978516\n",
      "loss 0.033 = 0.005 + 0.026 + 0.001 avg prob of [ Francesco Bartolomeo Rastrelli] 0.9945698976516724\n",
      "Init norm 2.8096954822540283 | Delta norm 11.238781929016113 | Target norm 11.820258140563965\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.2388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5662, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.5004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5579, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.3846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5761, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.6555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6017, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7375, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:52:49,161 - easyeditor.editors.editor - INFO - 17 editing: Who does Tsarskoye Selo architect? -> Francesco Bartolomeo Rastrelli  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8888888888888888], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Who does Tsarskoye Selo architect?', 'target_new': 'Francesco Bartolomeo Rastrelli', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:52:49,161 - easyeditor.editors.editor - INFO - 17 editing: Who does Tsarskoye Selo architect? -> Francesco Bartolomeo Rastrelli  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8888888888888888], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Who does Tsarskoye Selo architect?', 'target_new': 'Francesco Bartolomeo Rastrelli', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:52:49,161 - easyeditor.editors.editor - INFO - 17 editing: Who does Tsarskoye Selo architect? -> Francesco Bartolomeo Rastrelli  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8888888888888888], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Who does Tsarskoye Selo architect?', 'target_new': 'Francesco Bartolomeo Rastrelli', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:52:49,161 - easyeditor.editors.editor - INFO - 17 editing: Who does Tsarskoye Selo architect? -> Francesco Bartolomeo Rastrelli  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8888888888888888], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Who does Tsarskoye Selo architect?', 'target_new': 'Francesco Bartolomeo Rastrelli', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:52:49,161 - easyeditor.editors.editor - INFO - 17 editing: Who does Tsarskoye Selo architect? -> Francesco Bartolomeo Rastrelli  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8888888888888888], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Who does Tsarskoye Selo architect?', 'target_new': 'Francesco Bartolomeo Rastrelli', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:52:49 - INFO - easyeditor.editors.editor -   17 editing: Who does Tsarskoye Selo architect? -> Francesco Bartolomeo Rastrelli  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8888888888888888], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Who does Tsarskoye Selo architect?', 'target_new': 'Francesco Bartolomeo Rastrelli', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      " 36%|███▌      | 18/50 [08:41<16:12, 30.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the architectural style of Tsarskoye Selo?] -> [ baroque architecture]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: What is the architectural style of Tsarskoye Selo?baroque | Token: o\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.575 = 3.575 + 0.0 + 0.0 avg prob of [ baroque architecture] 0.028311794623732567\n",
      "loss 2.717 = 2.601 + 0.114 + 0.001 avg prob of [ baroque architecture] 0.07587652653455734\n",
      "loss 1.882 = 1.855 + 0.026 + 0.001 avg prob of [ baroque architecture] 0.15995387732982635\n",
      "loss 0.976 = 0.942 + 0.033 + 0.001 avg prob of [ baroque architecture] 0.39158329367637634\n",
      "loss 0.216 = 0.121 + 0.093 + 0.001 avg prob of [ baroque architecture] 0.8857271075248718\n",
      "loss 0.082 = 0.036 + 0.045 + 0.001 avg prob of [ baroque architecture] 0.9649078249931335\n",
      "loss 0.078 = 0.012 + 0.065 + 0.001 avg prob of [ baroque architecture] 0.987980842590332\n",
      "loss 0.052 = 0.007 + 0.044 + 0.001 avg prob of [ baroque architecture] 0.9932059049606323\n",
      "loss 0.032 = 0.005 + 0.026 + 0.001 avg prob of [ baroque architecture] 0.9951834678649902\n",
      "Init norm 2.8162834644317627 | Delta norm 11.265134811401367 | Target norm 11.642044067382812\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.2651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5567, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.3979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5428, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.3605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5684, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.6939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6161, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.3920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7826, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:53:16,908 - easyeditor.editors.editor - INFO - 18 editing: What is the architectural style of Tsarskoye Selo? -> baroque architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is the architectural style of Tsarskoye Selo?', 'target_new': 'baroque architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:53:16,908 - easyeditor.editors.editor - INFO - 18 editing: What is the architectural style of Tsarskoye Selo? -> baroque architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is the architectural style of Tsarskoye Selo?', 'target_new': 'baroque architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:53:16,908 - easyeditor.editors.editor - INFO - 18 editing: What is the architectural style of Tsarskoye Selo? -> baroque architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is the architectural style of Tsarskoye Selo?', 'target_new': 'baroque architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:53:16,908 - easyeditor.editors.editor - INFO - 18 editing: What is the architectural style of Tsarskoye Selo? -> baroque architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is the architectural style of Tsarskoye Selo?', 'target_new': 'baroque architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:53:16,908 - easyeditor.editors.editor - INFO - 18 editing: What is the architectural style of Tsarskoye Selo? -> baroque architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is the architectural style of Tsarskoye Selo?', 'target_new': 'baroque architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:53:16 - INFO - easyeditor.editors.editor -   18 editing: What is the architectural style of Tsarskoye Selo? -> baroque architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is the architectural style of Tsarskoye Selo?', 'target_new': 'baroque architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tsarskoye Selo'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 38%|███▊      | 19/50 [09:08<15:17, 29.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's located in/on physical feature is Kungsholmen?] -> [ Stockholm City Hall]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 17 | Sentence: Which tourist attraction's located in/on physical feature is Kungsholmen?Stockholm City | Token: men\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.648 = 5.648 + 0.0 + 0.0 avg prob of [ Stockholm City Hall] 0.003555922769010067\n",
      "loss 5.23 = 5.194 + 0.034 + 0.002 avg prob of [ Stockholm City Hall] 0.0055821603164076805\n",
      "loss 3.713 = 3.68 + 0.032 + 0.002 avg prob of [ Stockholm City Hall] 0.02528185211122036\n",
      "loss 3.539 = 3.431 + 0.107 + 0.002 avg prob of [ Stockholm City Hall] 0.03256227821111679\n",
      "loss 2.975 = 2.887 + 0.086 + 0.002 avg prob of [ Stockholm City Hall] 0.05598944053053856\n",
      "loss 2.065 = 1.896 + 0.167 + 0.002 avg prob of [ Stockholm City Hall] 0.15189538896083832\n",
      "loss 2.062 = 2.015 + 0.046 + 0.002 avg prob of [ Stockholm City Hall] 0.13419535756111145\n",
      "loss 0.393 = 0.083 + 0.308 + 0.002 avg prob of [ Stockholm City Hall] 0.92038893699646\n",
      "loss 0.189 = 0.142 + 0.045 + 0.002 avg prob of [ Stockholm City Hall] 0.8681290149688721\n",
      "loss 2.248 = 2.199 + 0.048 + 0.002 avg prob of [ Stockholm City Hall] 0.11248718202114105\n",
      "loss 1.95 = 1.926 + 0.023 + 0.002 avg prob of [ Stockholm City Hall] 0.14699268341064453\n",
      "loss 1.615 = 1.59 + 0.023 + 0.002 avg prob of [ Stockholm City Hall] 0.2041541337966919\n",
      "loss 1.151 = 1.129 + 0.021 + 0.002 avg prob of [ Stockholm City Hall] 0.3246748745441437\n",
      "loss 0.547 = 0.53 + 0.016 + 0.002 avg prob of [ Stockholm City Hall] 0.5889596939086914\n",
      "loss 0.125 = 0.106 + 0.018 + 0.002 avg prob of [ Stockholm City Hall] 0.8999789953231812\n",
      "loss 0.057 = 0.035 + 0.02 + 0.002 avg prob of [ Stockholm City Hall] 0.9653071165084839\n",
      "loss 0.039 = 0.018 + 0.02 + 0.002 avg prob of [ Stockholm City Hall] 0.982410192489624\n",
      "Init norm 2.6286847591400146 | Delta norm 10.514739036560059 | Target norm 10.97119426727295\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.5147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5776, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.9114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5358, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.0422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5671, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.5435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5970, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.3273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7856, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:53:49,283 - easyeditor.editors.editor - INFO - 19 editing: Which tourist attraction's located in/on physical feature is Kungsholmen? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in/on physical feature is Kungsholmen?\", 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kungsholmen'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:53:49,283 - easyeditor.editors.editor - INFO - 19 editing: Which tourist attraction's located in/on physical feature is Kungsholmen? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in/on physical feature is Kungsholmen?\", 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kungsholmen'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:53:49,283 - easyeditor.editors.editor - INFO - 19 editing: Which tourist attraction's located in/on physical feature is Kungsholmen? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in/on physical feature is Kungsholmen?\", 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kungsholmen'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:53:49,283 - easyeditor.editors.editor - INFO - 19 editing: Which tourist attraction's located in/on physical feature is Kungsholmen? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in/on physical feature is Kungsholmen?\", 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kungsholmen'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:53:49,283 - easyeditor.editors.editor - INFO - 19 editing: Which tourist attraction's located in/on physical feature is Kungsholmen? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in/on physical feature is Kungsholmen?\", 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kungsholmen'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:53:49 - INFO - easyeditor.editors.editor -   19 editing: Which tourist attraction's located in/on physical feature is Kungsholmen? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in/on physical feature is Kungsholmen?\", 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Kungsholmen'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 40%|████      | 20/50 [09:41<15:12, 30.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's located in or next to body of water is Göta älv?] -> [ Bohus Fortress]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 20 | Sentence: Which tourist attraction's located in or next to body of water is Göta älv?Bohus Fort | Token: v\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.857 = 3.857 + 0.0 + 0.0 avg prob of [ Bohus Fortress] 0.021193992346525192\n",
      "loss 3.361 = 3.315 + 0.045 + 0.002 avg prob of [ Bohus Fortress] 0.03643743321299553\n",
      "loss 3.459 = 3.433 + 0.024 + 0.002 avg prob of [ Bohus Fortress] 0.03240508958697319\n",
      "loss 2.432 = 2.402 + 0.028 + 0.002 avg prob of [ Bohus Fortress] 0.09117871522903442\n",
      "loss 1.313 = 1.247 + 0.065 + 0.002 avg prob of [ Bohus Fortress] 0.29660099744796753\n",
      "loss 2.675 = 2.596 + 0.078 + 0.002 avg prob of [ Bohus Fortress] 0.0761428028345108\n",
      "loss 0.655 = 0.614 + 0.039 + 0.002 avg prob of [ Bohus Fortress] 0.5631069540977478\n",
      "loss 0.621 = 0.573 + 0.047 + 0.002 avg prob of [ Bohus Fortress] 0.5678285360336304\n",
      "loss 0.173 = 0.113 + 0.059 + 0.002 avg prob of [ Bohus Fortress] 0.8931095004081726\n",
      "loss 0.149 = 0.113 + 0.035 + 0.002 avg prob of [ Bohus Fortress] 0.8933579325675964\n",
      "loss 0.094 = 0.062 + 0.03 + 0.002 avg prob of [ Bohus Fortress] 0.9397713541984558\n",
      "loss 0.066 = 0.037 + 0.028 + 0.002 avg prob of [ Bohus Fortress] 0.9637944102287292\n",
      "loss 0.051 = 0.023 + 0.026 + 0.002 avg prob of [ Bohus Fortress] 0.977113664150238\n",
      "loss 0.042 = 0.016 + 0.024 + 0.002 avg prob of [ Bohus Fortress] 0.9845744967460632\n",
      "Init norm 2.416788339614868 | Delta norm 9.667152404785156 | Target norm 10.084803581237793\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.6672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5264, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.1194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4741, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.3077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5104, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5511, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.0412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7271, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:54:19,744 - easyeditor.editors.editor - INFO - 20 editing: Which tourist attraction's located in or next to body of water is Göta älv? -> Bohus Fortress  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in or next to body of water is Göta älv?\", 'target_new': 'Bohus Fortress', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Göta älv'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:54:19,744 - easyeditor.editors.editor - INFO - 20 editing: Which tourist attraction's located in or next to body of water is Göta älv? -> Bohus Fortress  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in or next to body of water is Göta älv?\", 'target_new': 'Bohus Fortress', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Göta älv'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:54:19,744 - easyeditor.editors.editor - INFO - 20 editing: Which tourist attraction's located in or next to body of water is Göta älv? -> Bohus Fortress  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in or next to body of water is Göta älv?\", 'target_new': 'Bohus Fortress', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Göta älv'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:54:19,744 - easyeditor.editors.editor - INFO - 20 editing: Which tourist attraction's located in or next to body of water is Göta älv? -> Bohus Fortress  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in or next to body of water is Göta älv?\", 'target_new': 'Bohus Fortress', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Göta älv'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:54:19,744 - easyeditor.editors.editor - INFO - 20 editing: Which tourist attraction's located in or next to body of water is Göta älv? -> Bohus Fortress  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in or next to body of water is Göta älv?\", 'target_new': 'Bohus Fortress', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Göta älv'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:54:19 - INFO - easyeditor.editors.editor -   20 editing: Which tourist attraction's located in or next to body of water is Göta älv? -> Bohus Fortress  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in or next to body of water is Göta älv?\", 'target_new': 'Bohus Fortress', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Göta älv'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 42%|████▏     | 21/50 [10:11<14:42, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's located in the administrative territorial entity is Biancavilla?] -> [ Mount Etna]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 18 | Sentence: Which tourist attraction's located in the administrative territorial entity is Biancavilla?Mount Et | Token: illa\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.459 = 2.459 + 0.0 + 0.0 avg prob of [ Mount Etna] 0.08807237446308136\n",
      "loss 1.671 = 1.501 + 0.168 + 0.001 avg prob of [ Mount Etna] 0.22832518815994263\n",
      "loss 0.951 = 0.877 + 0.072 + 0.001 avg prob of [ Mount Etna] 0.41623592376708984\n",
      "loss 0.396 = 0.321 + 0.073 + 0.001 avg prob of [ Mount Etna] 0.7255173921585083\n",
      "loss 0.102 = 0.004 + 0.097 + 0.001 avg prob of [ Mount Etna] 0.9963711500167847\n",
      "loss 0.103 = 0.004 + 0.098 + 0.001 avg prob of [ Mount Etna] 0.9962573051452637\n",
      "loss 0.075 = 0.003 + 0.07 + 0.001 avg prob of [ Mount Etna] 0.9968578815460205\n",
      "loss 0.057 = 0.003 + 0.053 + 0.001 avg prob of [ Mount Etna] 0.9974395632743835\n",
      "loss 0.048 = 0.003 + 0.044 + 0.001 avg prob of [ Mount Etna] 0.9974541664123535\n",
      "Init norm 2.7909069061279297 | Delta norm 11.163627624511719 | Target norm 11.638303756713867\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.1636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6270, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.4717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5464, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.5455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5829, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.9209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6229, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.5175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7717, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:54:48,503 - easyeditor.editors.editor - INFO - 21 editing: Which tourist attraction's located in the administrative territorial entity is Biancavilla? -> Mount Etna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Biancavilla?\", 'target_new': 'Mount Etna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Biancavilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:54:48,503 - easyeditor.editors.editor - INFO - 21 editing: Which tourist attraction's located in the administrative territorial entity is Biancavilla? -> Mount Etna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Biancavilla?\", 'target_new': 'Mount Etna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Biancavilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:54:48,503 - easyeditor.editors.editor - INFO - 21 editing: Which tourist attraction's located in the administrative territorial entity is Biancavilla? -> Mount Etna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Biancavilla?\", 'target_new': 'Mount Etna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Biancavilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:54:48,503 - easyeditor.editors.editor - INFO - 21 editing: Which tourist attraction's located in the administrative territorial entity is Biancavilla? -> Mount Etna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Biancavilla?\", 'target_new': 'Mount Etna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Biancavilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:54:48,503 - easyeditor.editors.editor - INFO - 21 editing: Which tourist attraction's located in the administrative territorial entity is Biancavilla? -> Mount Etna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Biancavilla?\", 'target_new': 'Mount Etna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Biancavilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:54:48 - INFO - easyeditor.editors.editor -   21 editing: Which tourist attraction's located in the administrative territorial entity is Biancavilla? -> Mount Etna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Biancavilla?\", 'target_new': 'Mount Etna', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Biancavilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 44%|████▍     | 22/50 [10:40<13:58, 29.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who architect Sedefkar Mehmed Agha?] -> [ Sultan Ahmed Mosque]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: Who architect Sedefkar Mehmed Agha?Sultan Ahmed Mos | Token: a\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.863 = 1.863 + 0.0 + 0.0 avg prob of [ Sultan Ahmed Mosque] 0.1588960886001587\n",
      "loss 1.519 = 1.296 + 0.22 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.27791547775268555\n",
      "loss 1.156 = 1.075 + 0.08 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.34241533279418945\n",
      "loss 0.571 = 0.491 + 0.078 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.6140812635421753\n",
      "loss 0.26 = 0.143 + 0.115 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.867120623588562\n",
      "loss 0.162 = 0.075 + 0.086 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9280397295951843\n",
      "loss 0.116 = 0.03 + 0.084 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9703892469406128\n",
      "loss 0.089 = 0.017 + 0.07 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.983313262462616\n",
      "loss 0.088 = 0.022 + 0.064 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9783085584640503\n",
      "loss 0.093 = 0.005 + 0.086 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9954106211662292\n",
      "loss 0.092 = 0.004 + 0.086 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9958252310752869\n",
      "loss 0.12 = 0.005 + 0.114 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9954602718353271\n",
      "loss 0.115 = 0.007 + 0.107 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9934524297714233\n",
      "loss 0.117 = 0.009 + 0.106 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9911808967590332\n",
      "loss 0.111 = 0.006 + 0.104 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9942245483398438\n",
      "loss 0.107 = 0.004 + 0.101 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9960121512413025\n",
      "loss 0.104 = 0.004 + 0.099 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.99636310338974\n",
      "loss 0.098 = 0.003 + 0.093 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9973918199539185\n",
      "loss 0.087 = 0.002 + 0.083 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9975130558013916\n",
      "loss 0.072 = 0.003 + 0.067 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9971144199371338\n",
      "loss 0.067 = 0.003 + 0.062 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9966449737548828\n",
      "loss 0.067 = 0.002 + 0.063 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.997992992401123\n",
      "loss 0.066 = 0.001 + 0.062 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.998528242111206\n",
      "loss 0.063 = 0.001 + 0.06 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9989148378372192\n",
      "loss 0.06 = 0.001 + 0.057 + 0.002 avg prob of [ Sultan Ahmed Mosque] 0.9991121292114258\n",
      "Init norm 2.223923921585083 | Delta norm 8.895695686340332 | Target norm 9.27247428894043\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.8957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4727, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.2929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4408, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.5389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4692, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.4032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5076, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.6776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6845, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:55:23,457 - easyeditor.editors.editor - INFO - 22 editing: Who architect Sedefkar Mehmed Agha? -> Sultan Ahmed Mosque  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Who architect Sedefkar Mehmed Agha?', 'target_new': 'Sultan Ahmed Mosque', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sedefkar Mehmed Agha'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:55:23,457 - easyeditor.editors.editor - INFO - 22 editing: Who architect Sedefkar Mehmed Agha? -> Sultan Ahmed Mosque  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Who architect Sedefkar Mehmed Agha?', 'target_new': 'Sultan Ahmed Mosque', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sedefkar Mehmed Agha'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:55:23,457 - easyeditor.editors.editor - INFO - 22 editing: Who architect Sedefkar Mehmed Agha? -> Sultan Ahmed Mosque  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Who architect Sedefkar Mehmed Agha?', 'target_new': 'Sultan Ahmed Mosque', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sedefkar Mehmed Agha'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:55:23,457 - easyeditor.editors.editor - INFO - 22 editing: Who architect Sedefkar Mehmed Agha? -> Sultan Ahmed Mosque  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Who architect Sedefkar Mehmed Agha?', 'target_new': 'Sultan Ahmed Mosque', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sedefkar Mehmed Agha'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:55:23,457 - easyeditor.editors.editor - INFO - 22 editing: Who architect Sedefkar Mehmed Agha? -> Sultan Ahmed Mosque  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Who architect Sedefkar Mehmed Agha?', 'target_new': 'Sultan Ahmed Mosque', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sedefkar Mehmed Agha'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:55:23 - INFO - easyeditor.editors.editor -   22 editing: Who architect Sedefkar Mehmed Agha? -> Sultan Ahmed Mosque  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Who architect Sedefkar Mehmed Agha?', 'target_new': 'Sultan Ahmed Mosque', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sedefkar Mehmed Agha'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 46%|████▌     | 23/50 [11:15<14:08, 31.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction architect Alfred Parland?] -> [ Church of the Savior on Blood]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Which tourist attraction architect Alfred Parland?Church of the Savior on | Token: land\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.921 = 2.921 + 0.0 + 0.0 avg prob of [ Church of the Savior on Blood] 0.05457780882716179\n",
      "loss 2.321 = 2.242 + 0.078 + 0.002 avg prob of [ Church of the Savior on Blood] 0.106374591588974\n",
      "loss 2.013 = 1.931 + 0.08 + 0.002 avg prob of [ Church of the Savior on Blood] 0.1452278196811676\n",
      "loss 1.667 = 1.613 + 0.052 + 0.002 avg prob of [ Church of the Savior on Blood] 0.1993209570646286\n",
      "loss 1.318 = 1.281 + 0.035 + 0.002 avg prob of [ Church of the Savior on Blood] 0.281840056180954\n",
      "loss 0.58 = 0.543 + 0.035 + 0.002 avg prob of [ Church of the Savior on Blood] 0.5820813179016113\n",
      "loss 0.469 = 0.136 + 0.332 + 0.002 avg prob of [ Church of the Savior on Blood] 0.8732742071151733\n",
      "loss 0.873 = 0.818 + 0.054 + 0.002 avg prob of [ Church of the Savior on Blood] 0.4437280297279358\n",
      "loss 0.402 = 0.356 + 0.044 + 0.002 avg prob of [ Church of the Savior on Blood] 0.7011595964431763\n",
      "loss 0.167 = 0.131 + 0.034 + 0.002 avg prob of [ Church of the Savior on Blood] 0.8781794309616089\n",
      "loss 0.065 = 0.037 + 0.027 + 0.002 avg prob of [ Church of the Savior on Blood] 0.9637529253959656\n",
      "loss 0.041 = 0.016 + 0.023 + 0.002 avg prob of [ Church of the Savior on Blood] 0.9838323593139648\n",
      "Init norm 2.2735540866851807 | Delta norm 9.094216346740723 | Target norm 9.44654655456543\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5291, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.4487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4612, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.7187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4917, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.5790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5335, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6979, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:55:51,398 - easyeditor.editors.editor - INFO - 23 editing: Which tourist attraction architect Alfred Parland? -> Church of the Savior on Blood  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Alfred Parland?', 'target_new': 'Church of the Savior on Blood', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alfred Parland'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:55:51,398 - easyeditor.editors.editor - INFO - 23 editing: Which tourist attraction architect Alfred Parland? -> Church of the Savior on Blood  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Alfred Parland?', 'target_new': 'Church of the Savior on Blood', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alfred Parland'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:55:51,398 - easyeditor.editors.editor - INFO - 23 editing: Which tourist attraction architect Alfred Parland? -> Church of the Savior on Blood  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Alfred Parland?', 'target_new': 'Church of the Savior on Blood', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alfred Parland'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:55:51,398 - easyeditor.editors.editor - INFO - 23 editing: Which tourist attraction architect Alfred Parland? -> Church of the Savior on Blood  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Alfred Parland?', 'target_new': 'Church of the Savior on Blood', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alfred Parland'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:55:51,398 - easyeditor.editors.editor - INFO - 23 editing: Which tourist attraction architect Alfred Parland? -> Church of the Savior on Blood  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Alfred Parland?', 'target_new': 'Church of the Savior on Blood', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alfred Parland'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:55:51 - INFO - easyeditor.editors.editor -   23 editing: Which tourist attraction architect Alfred Parland? -> Church of the Savior on Blood  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Alfred Parland?', 'target_new': 'Church of the Savior on Blood', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alfred Parland'}, 'post': {'rewrite_acc': [0.8571428571428571], 'locality': {}, 'portability': {}}}\n",
      " 48%|████▊     | 24/50 [11:43<13:10, 30.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the located in the administrative territorial entity of Science Centre Singapore?] -> [ Jurong East]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: What is the located in the administrative territorial entity of Science Centre Singapore?Jurong | Token: Singapore\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.264 = 4.264 + 0.0 + 0.0 avg prob of [ Jurong East] 0.015315430238842964\n",
      "loss 3.072 = 3.025 + 0.045 + 0.002 avg prob of [ Jurong East] 0.05088368058204651\n",
      "loss 1.902 = 1.852 + 0.048 + 0.002 avg prob of [ Jurong East] 0.15942618250846863\n",
      "loss 0.455 = 0.428 + 0.026 + 0.002 avg prob of [ Jurong East] 0.6537001132965088\n",
      "loss 0.161 = 0.132 + 0.027 + 0.002 avg prob of [ Jurong East] 0.8761789202690125\n",
      "loss 0.066 = 0.038 + 0.026 + 0.002 avg prob of [ Jurong East] 0.9625979661941528\n",
      "loss 0.038 = 0.011 + 0.025 + 0.002 avg prob of [ Jurong East] 0.988714873790741\n",
      "Init norm 2.5215582847595215 | Delta norm 10.086233139038086 | Target norm 10.574678421020508\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5477, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.6025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.8653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5345, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.5861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5964, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.5068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7804, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:56:18,339 - easyeditor.editors.editor - INFO - 24 editing: What is the located in the administrative territorial entity of Science Centre Singapore? -> Jurong East  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Science Centre Singapore?', 'target_new': 'Jurong East', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Science Centre Singapore'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:56:18,339 - easyeditor.editors.editor - INFO - 24 editing: What is the located in the administrative territorial entity of Science Centre Singapore? -> Jurong East  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Science Centre Singapore?', 'target_new': 'Jurong East', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Science Centre Singapore'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:56:18,339 - easyeditor.editors.editor - INFO - 24 editing: What is the located in the administrative territorial entity of Science Centre Singapore? -> Jurong East  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Science Centre Singapore?', 'target_new': 'Jurong East', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Science Centre Singapore'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:56:18,339 - easyeditor.editors.editor - INFO - 24 editing: What is the located in the administrative territorial entity of Science Centre Singapore? -> Jurong East  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Science Centre Singapore?', 'target_new': 'Jurong East', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Science Centre Singapore'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:56:18,339 - easyeditor.editors.editor - INFO - 24 editing: What is the located in the administrative territorial entity of Science Centre Singapore? -> Jurong East  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Science Centre Singapore?', 'target_new': 'Jurong East', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Science Centre Singapore'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:56:18 - INFO - easyeditor.editors.editor -   24 editing: What is the located in the administrative territorial entity of Science Centre Singapore? -> Jurong East  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Science Centre Singapore?', 'target_new': 'Jurong East', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Science Centre Singapore'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 50%|█████     | 25/50 [12:10<12:13, 29.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who does Grand Kremlin Palace architect?] -> [ Konstantin Thon]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Who does Grand Kremlin Palace architect?Konstantin Th | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.829 = 2.829 + 0.0 + 0.0 avg prob of [ Konstantin Thon] 0.06206763908267021\n",
      "loss 2.348 = 2.32 + 0.027 + 0.002 avg prob of [ Konstantin Thon] 0.10072176158428192\n",
      "loss 1.878 = 1.859 + 0.017 + 0.002 avg prob of [ Konstantin Thon] 0.15824800729751587\n",
      "loss 1.59 = 1.569 + 0.019 + 0.002 avg prob of [ Konstantin Thon] 0.20911905169487\n",
      "loss 1.128 = 1.106 + 0.02 + 0.002 avg prob of [ Konstantin Thon] 0.33410724997520447\n",
      "loss 1.006 = 0.973 + 0.032 + 0.002 avg prob of [ Konstantin Thon] 0.3783423900604248\n",
      "loss 0.385 = 0.351 + 0.032 + 0.002 avg prob of [ Konstantin Thon] 0.7047049403190613\n",
      "loss 0.1 = 0.021 + 0.077 + 0.002 avg prob of [ Konstantin Thon] 0.9789537191390991\n",
      "loss 0.073 = 0.002 + 0.069 + 0.002 avg prob of [ Konstantin Thon] 0.9976052045822144\n",
      "loss 0.038 = 0.003 + 0.033 + 0.002 avg prob of [ Konstantin Thon] 0.9966704249382019\n",
      "Init norm 2.310542345046997 | Delta norm 9.242169380187988 | Target norm 9.522619247436523\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.2422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4607, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.5796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4509, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.7780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4715, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.5391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5212, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6377, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:56:45,681 - easyeditor.editors.editor - INFO - 25 editing: Who does Grand Kremlin Palace architect? -> Konstantin Thon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'Who does Grand Kremlin Palace architect?', 'target_new': 'Konstantin Thon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:56:45,681 - easyeditor.editors.editor - INFO - 25 editing: Who does Grand Kremlin Palace architect? -> Konstantin Thon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'Who does Grand Kremlin Palace architect?', 'target_new': 'Konstantin Thon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:56:45,681 - easyeditor.editors.editor - INFO - 25 editing: Who does Grand Kremlin Palace architect? -> Konstantin Thon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'Who does Grand Kremlin Palace architect?', 'target_new': 'Konstantin Thon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:56:45,681 - easyeditor.editors.editor - INFO - 25 editing: Who does Grand Kremlin Palace architect? -> Konstantin Thon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'Who does Grand Kremlin Palace architect?', 'target_new': 'Konstantin Thon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:56:45,681 - easyeditor.editors.editor - INFO - 25 editing: Who does Grand Kremlin Palace architect? -> Konstantin Thon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'Who does Grand Kremlin Palace architect?', 'target_new': 'Konstantin Thon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:56:45 - INFO - easyeditor.editors.editor -   25 editing: Who does Grand Kremlin Palace architect? -> Konstantin Thon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'Who does Grand Kremlin Palace architect?', 'target_new': 'Konstantin Thon', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 52%|█████▏    | 26/50 [12:37<11:30, 28.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the located in the administrative territorial entity of Grand Kremlin Palace?] -> [ Tverskoy District]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 16 | Sentence: What is the located in the administrative territorial entity of Grand Kremlin Palace?Tverskoy | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.184 = 3.184 + 0.0 + 0.0 avg prob of [ Tverskoy District] 0.04204389452934265\n",
      "loss 2.588 = 2.524 + 0.062 + 0.002 avg prob of [ Tverskoy District] 0.0806179940700531\n",
      "loss 1.184 = 1.142 + 0.041 + 0.002 avg prob of [ Tverskoy District] 0.3218099772930145\n",
      "loss 0.385 = 0.232 + 0.152 + 0.002 avg prob of [ Tverskoy District] 0.7936546802520752\n",
      "loss 0.851 = 0.811 + 0.037 + 0.002 avg prob of [ Tverskoy District] 0.45509037375450134\n",
      "loss 1.158 = 1.079 + 0.078 + 0.002 avg prob of [ Tverskoy District] 0.34259599447250366\n",
      "loss 0.876 = 0.806 + 0.068 + 0.002 avg prob of [ Tverskoy District] 0.44735991954803467\n",
      "loss 0.178 = 0.11 + 0.066 + 0.002 avg prob of [ Tverskoy District] 0.8963183760643005\n",
      "loss 0.09 = 0.036 + 0.052 + 0.002 avg prob of [ Tverskoy District] 0.9647253751754761\n",
      "loss 0.082 = 0.017 + 0.064 + 0.002 avg prob of [ Tverskoy District] 0.9834065437316895\n",
      "loss 0.088 = 0.007 + 0.079 + 0.002 avg prob of [ Tverskoy District] 0.9932090640068054\n",
      "loss 0.075 = 0.006 + 0.066 + 0.002 avg prob of [ Tverskoy District] 0.9936308264732361\n",
      "loss 0.057 = 0.005 + 0.05 + 0.002 avg prob of [ Tverskoy District] 0.9949769973754883\n",
      "loss 0.069 = 0.004 + 0.063 + 0.002 avg prob of [ Tverskoy District] 0.9961583614349365\n",
      "loss 0.052 = 0.004 + 0.046 + 0.002 avg prob of [ Tverskoy District] 0.9958481192588806\n",
      "loss 0.05 = 0.004 + 0.044 + 0.002 avg prob of [ Tverskoy District] 0.9961684942245483\n",
      "loss 0.05 = 0.003 + 0.045 + 0.002 avg prob of [ Tverskoy District] 0.9967814683914185\n",
      "Init norm 2.3555498123168945 | Delta norm 9.422199249267578 | Target norm 9.842253684997559\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.4222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4662, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.0115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4643, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.2667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4759, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.8933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5277, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6857, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:57:17,351 - easyeditor.editors.editor - INFO - 26 editing: What is the located in the administrative territorial entity of Grand Kremlin Palace? -> Tverskoy District  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Grand Kremlin Palace?', 'target_new': 'Tverskoy District', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:57:17,351 - easyeditor.editors.editor - INFO - 26 editing: What is the located in the administrative territorial entity of Grand Kremlin Palace? -> Tverskoy District  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Grand Kremlin Palace?', 'target_new': 'Tverskoy District', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:57:17,351 - easyeditor.editors.editor - INFO - 26 editing: What is the located in the administrative territorial entity of Grand Kremlin Palace? -> Tverskoy District  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Grand Kremlin Palace?', 'target_new': 'Tverskoy District', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:57:17,351 - easyeditor.editors.editor - INFO - 26 editing: What is the located in the administrative territorial entity of Grand Kremlin Palace? -> Tverskoy District  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Grand Kremlin Palace?', 'target_new': 'Tverskoy District', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:57:17,351 - easyeditor.editors.editor - INFO - 26 editing: What is the located in the administrative territorial entity of Grand Kremlin Palace? -> Tverskoy District  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Grand Kremlin Palace?', 'target_new': 'Tverskoy District', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:57:17 - INFO - easyeditor.editors.editor -   26 editing: What is the located in the administrative territorial entity of Grand Kremlin Palace? -> Tverskoy District  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Grand Kremlin Palace?', 'target_new': 'Tverskoy District', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [13:09<11:21, 29.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [What is the architectural style of Grand Kremlin Palace?] -> [ Byzantine Revival architecture]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: What is the architectural style of Grand Kremlin Palace?Byzantine Revival | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.978 = 2.978 + 0.0 + 0.0 avg prob of [ Byzantine Revival architecture] 0.05286555737257004\n",
      "loss 1.929 = 1.891 + 0.037 + 0.002 avg prob of [ Byzantine Revival architecture] 0.15535253286361694\n",
      "loss 0.982 = 0.91 + 0.07 + 0.002 avg prob of [ Byzantine Revival architecture] 0.40502607822418213\n",
      "loss 0.347 = 0.284 + 0.061 + 0.002 avg prob of [ Byzantine Revival architecture] 0.7534809112548828\n",
      "loss 0.125 = 0.007 + 0.117 + 0.002 avg prob of [ Byzantine Revival architecture] 0.993291974067688\n",
      "loss 0.185 = 0.08 + 0.103 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9238561987876892\n",
      "loss 0.113 = 0.005 + 0.106 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9948953986167908\n",
      "loss 0.11 = 0.004 + 0.104 + 0.002 avg prob of [ Byzantine Revival architecture] 0.995995283126831\n",
      "loss 0.109 = 0.003 + 0.104 + 0.002 avg prob of [ Byzantine Revival architecture] 0.996794581413269\n",
      "loss 0.108 = 0.002 + 0.104 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9978639483451843\n",
      "loss 0.108 = 0.001 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9986118078231812\n",
      "loss 0.107 = 0.001 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9990381002426147\n",
      "loss 0.107 = 0.001 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9992792010307312\n",
      "loss 0.107 = 0.001 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.999424159526825\n",
      "loss 0.107 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9995179176330566\n",
      "loss 0.107 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9995832443237305\n",
      "loss 0.107 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9996316432952881\n",
      "loss 0.107 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9996694326400757\n",
      "loss 0.107 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9997000694274902\n",
      "loss 0.107 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9997258186340332\n",
      "loss 0.106 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9997479319572449\n",
      "loss 0.106 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.999767005443573\n",
      "loss 0.106 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9997839331626892\n",
      "loss 0.106 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9997988939285278\n",
      "loss 0.106 = 0.0 + 0.105 + 0.002 avg prob of [ Byzantine Revival architecture] 0.9998120069503784\n",
      "Init norm 2.3669562339782715 | Delta norm 9.467824935913086 | Target norm 9.82393741607666\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.4678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4662, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.8626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4713, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.0998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5016, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.8510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5432, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7180, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:57:54,013 - easyeditor.editors.editor - INFO - 27 editing: What is the architectural style of Grand Kremlin Palace? -> Byzantine Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What is the architectural style of Grand Kremlin Palace?', 'target_new': 'Byzantine Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:57:54,013 - easyeditor.editors.editor - INFO - 27 editing: What is the architectural style of Grand Kremlin Palace? -> Byzantine Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What is the architectural style of Grand Kremlin Palace?', 'target_new': 'Byzantine Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:57:54,013 - easyeditor.editors.editor - INFO - 27 editing: What is the architectural style of Grand Kremlin Palace? -> Byzantine Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What is the architectural style of Grand Kremlin Palace?', 'target_new': 'Byzantine Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:57:54,013 - easyeditor.editors.editor - INFO - 27 editing: What is the architectural style of Grand Kremlin Palace? -> Byzantine Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What is the architectural style of Grand Kremlin Palace?', 'target_new': 'Byzantine Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:57:54,013 - easyeditor.editors.editor - INFO - 27 editing: What is the architectural style of Grand Kremlin Palace? -> Byzantine Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What is the architectural style of Grand Kremlin Palace?', 'target_new': 'Byzantine Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:57:54 - INFO - easyeditor.editors.editor -   27 editing: What is the architectural style of Grand Kremlin Palace? -> Byzantine Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'What is the architectural style of Grand Kremlin Palace?', 'target_new': 'Byzantine Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 56%|█████▌    | 28/50 [13:45<11:38, 31.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who was Grand Kremlin Palace commissioned by?] -> [ Nicholas I of Russia]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Who was Grand Kremlin Palace commissioned by?Nicholas I of | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.909 = 4.909 + 0.0 + 0.0 avg prob of [ Nicholas I of Russia] 0.007430091034621\n",
      "loss 4.116 = 4.092 + 0.022 + 0.002 avg prob of [ Nicholas I of Russia] 0.016924940049648285\n",
      "loss 3.243 = 3.2 + 0.041 + 0.002 avg prob of [ Nicholas I of Russia] 0.04206598550081253\n",
      "loss 1.661 = 1.622 + 0.037 + 0.002 avg prob of [ Nicholas I of Russia] 0.1976933479309082\n",
      "loss 0.518 = 0.388 + 0.129 + 0.002 avg prob of [ Nicholas I of Russia] 0.6795899271965027\n",
      "loss 1.467 = 1.424 + 0.04 + 0.002 avg prob of [ Nicholas I of Russia] 0.24176016449928284\n",
      "loss 2.121 = 2.036 + 0.083 + 0.002 avg prob of [ Nicholas I of Russia] 0.13581931591033936\n",
      "loss 2.306 = 2.26 + 0.044 + 0.002 avg prob of [ Nicholas I of Russia] 0.10912343114614487\n",
      "loss 0.563 = 0.531 + 0.03 + 0.002 avg prob of [ Nicholas I of Russia] 0.6053840517997742\n",
      "loss 0.154 = 0.119 + 0.033 + 0.002 avg prob of [ Nicholas I of Russia] 0.8877930641174316\n",
      "loss 0.067 = 0.032 + 0.033 + 0.002 avg prob of [ Nicholas I of Russia] 0.968410849571228\n",
      "loss 0.041 = 0.011 + 0.029 + 0.002 avg prob of [ Nicholas I of Russia] 0.9894182085990906\n",
      "Init norm 2.3046553134918213 | Delta norm 9.218621253967285 | Target norm 9.616554260253906\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.2186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4599, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.6145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4473, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.7542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4806, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.4486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5179, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.5566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6491, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:58:22,057 - easyeditor.editors.editor - INFO - 28 editing: Who was Grand Kremlin Palace commissioned by? -> Nicholas I of Russia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'Who was Grand Kremlin Palace commissioned by?', 'target_new': 'Nicholas I of Russia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:58:22,057 - easyeditor.editors.editor - INFO - 28 editing: Who was Grand Kremlin Palace commissioned by? -> Nicholas I of Russia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'Who was Grand Kremlin Palace commissioned by?', 'target_new': 'Nicholas I of Russia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:58:22,057 - easyeditor.editors.editor - INFO - 28 editing: Who was Grand Kremlin Palace commissioned by? -> Nicholas I of Russia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'Who was Grand Kremlin Palace commissioned by?', 'target_new': 'Nicholas I of Russia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:58:22,057 - easyeditor.editors.editor - INFO - 28 editing: Who was Grand Kremlin Palace commissioned by? -> Nicholas I of Russia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'Who was Grand Kremlin Palace commissioned by?', 'target_new': 'Nicholas I of Russia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:58:22,057 - easyeditor.editors.editor - INFO - 28 editing: Who was Grand Kremlin Palace commissioned by? -> Nicholas I of Russia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'Who was Grand Kremlin Palace commissioned by?', 'target_new': 'Nicholas I of Russia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:58:22 - INFO - easyeditor.editors.editor -   28 editing: Who was Grand Kremlin Palace commissioned by? -> Nicholas I of Russia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'Who was Grand Kremlin Palace commissioned by?', 'target_new': 'Nicholas I of Russia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Grand Kremlin Palace'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 58%|█████▊    | 29/50 [14:13<10:43, 30.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?] -> [ Stourhead]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 19 | Sentence: Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?Stour | Token: per\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.877 = 3.877 + 0.0 + 0.0 avg prob of [ Stourhead] 0.02107221633195877\n",
      "loss 2.904 = 2.857 + 0.045 + 0.002 avg prob of [ Stourhead] 0.05898406356573105\n",
      "loss 3.095 = 3.047 + 0.047 + 0.002 avg prob of [ Stourhead] 0.04922226071357727\n",
      "loss 1.438 = 1.407 + 0.029 + 0.002 avg prob of [ Stourhead] 0.24841183423995972\n",
      "loss 1.304 = 1.278 + 0.025 + 0.002 avg prob of [ Stourhead] 0.281124472618103\n",
      "loss 1.009 = 0.984 + 0.023 + 0.002 avg prob of [ Stourhead] 0.37636706233024597\n",
      "loss 0.705 = 0.678 + 0.025 + 0.002 avg prob of [ Stourhead] 0.5095834136009216\n",
      "loss 0.452 = 0.42 + 0.03 + 0.002 avg prob of [ Stourhead] 0.6580519676208496\n",
      "loss 0.176 = 0.137 + 0.038 + 0.002 avg prob of [ Stourhead] 0.8727509379386902\n",
      "loss 0.073 = 0.008 + 0.063 + 0.002 avg prob of [ Stourhead] 0.9920721054077148\n",
      "loss 0.153 = 0.002 + 0.149 + 0.002 avg prob of [ Stourhead] 0.997909426689148\n",
      "loss 0.177 = 0.006 + 0.169 + 0.002 avg prob of [ Stourhead] 0.9935282468795776\n",
      "loss 0.173 = 0.003 + 0.169 + 0.002 avg prob of [ Stourhead] 0.9974710941314697\n",
      "loss 0.17 = 0.001 + 0.168 + 0.002 avg prob of [ Stourhead] 0.9989799857139587\n",
      "loss 0.156 = 0.001 + 0.154 + 0.002 avg prob of [ Stourhead] 0.9994718432426453\n",
      "loss 0.107 = 0.001 + 0.105 + 0.002 avg prob of [ Stourhead] 0.9993236064910889\n",
      "loss 0.094 = 0.002 + 0.09 + 0.002 avg prob of [ Stourhead] 0.998107373714447\n",
      "loss 0.088 = 0.002 + 0.084 + 0.002 avg prob of [ Stourhead] 0.9975101351737976\n",
      "loss 0.09 = 0.002 + 0.086 + 0.002 avg prob of [ Stourhead] 0.9983384013175964\n",
      "loss 0.077 = 0.001 + 0.074 + 0.002 avg prob of [ Stourhead] 0.9986030459403992\n",
      "loss 0.076 = 0.001 + 0.072 + 0.002 avg prob of [ Stourhead] 0.9985454082489014\n",
      "loss 0.071 = 0.001 + 0.068 + 0.002 avg prob of [ Stourhead] 0.9986974596977234\n",
      "loss 0.067 = 0.001 + 0.064 + 0.002 avg prob of [ Stourhead] 0.9989743232727051\n",
      "loss 0.064 = 0.001 + 0.061 + 0.002 avg prob of [ Stourhead] 0.9992188811302185\n",
      "loss 0.061 = 0.001 + 0.058 + 0.002 avg prob of [ Stourhead] 0.9993768930435181\n",
      "Init norm 2.2938661575317383 | Delta norm 9.175464630126953 | Target norm 9.378859519958496\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.1755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4946, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.6023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4551, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.0123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4966, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5365, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7116, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:58:57,670 - easyeditor.editors.editor - INFO - 29 editing: Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper? -> Stourhead  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?\", 'target_new': 'Stourhead', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stourton with Gasper'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:58:57,670 - easyeditor.editors.editor - INFO - 29 editing: Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper? -> Stourhead  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?\", 'target_new': 'Stourhead', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stourton with Gasper'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:58:57,670 - easyeditor.editors.editor - INFO - 29 editing: Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper? -> Stourhead  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?\", 'target_new': 'Stourhead', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stourton with Gasper'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:58:57,670 - easyeditor.editors.editor - INFO - 29 editing: Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper? -> Stourhead  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?\", 'target_new': 'Stourhead', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stourton with Gasper'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:58:57,670 - easyeditor.editors.editor - INFO - 29 editing: Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper? -> Stourhead  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?\", 'target_new': 'Stourhead', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stourton with Gasper'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:58:57 - INFO - easyeditor.editors.editor -   29 editing: Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper? -> Stourhead  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?\", 'target_new': 'Stourhead', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stourton with Gasper'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 60%|██████    | 30/50 [14:49<10:42, 32.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction was named after Sunset Strip?] -> [ Las Vegas Strip]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Which tourist attraction was named after Sunset Strip?Las Vegas St | Token: rip\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.86 = 3.86 + 0.0 + 0.0 avg prob of [ Las Vegas Strip] 0.021193882450461388\n",
      "loss 2.914 = 2.852 + 0.061 + 0.001 avg prob of [ Las Vegas Strip] 0.058059677481651306\n",
      "loss 1.623 = 1.558 + 0.064 + 0.001 avg prob of [ Las Vegas Strip] 0.2120227813720703\n",
      "loss 0.649 = 0.574 + 0.074 + 0.001 avg prob of [ Las Vegas Strip] 0.5651118159294128\n",
      "loss 0.872 = 0.802 + 0.068 + 0.001 avg prob of [ Las Vegas Strip] 0.4628278613090515\n",
      "loss 0.143 = 0.068 + 0.073 + 0.001 avg prob of [ Las Vegas Strip] 0.9340950846672058\n",
      "loss 0.161 = 0.095 + 0.064 + 0.001 avg prob of [ Las Vegas Strip] 0.9090754985809326\n",
      "loss 0.105 = 0.051 + 0.053 + 0.001 avg prob of [ Las Vegas Strip] 0.9504122734069824\n",
      "loss 0.084 = 0.031 + 0.052 + 0.001 avg prob of [ Las Vegas Strip] 0.9694063067436218\n",
      "loss 0.073 = 0.021 + 0.05 + 0.001 avg prob of [ Las Vegas Strip] 0.9788805842399597\n",
      "loss 0.067 = 0.016 + 0.049 + 0.001 avg prob of [ Las Vegas Strip] 0.9842750430107117\n",
      "loss 0.061 = 0.012 + 0.047 + 0.001 avg prob of [ Las Vegas Strip] 0.9876900911331177\n",
      "loss 0.057 = 0.01 + 0.046 + 0.001 avg prob of [ Las Vegas Strip] 0.9899052381515503\n",
      "loss 0.054 = 0.009 + 0.044 + 0.001 avg prob of [ Las Vegas Strip] 0.9914132952690125\n",
      "loss 0.051 = 0.007 + 0.043 + 0.001 avg prob of [ Las Vegas Strip] 0.9925522804260254\n",
      "loss 0.049 = 0.007 + 0.041 + 0.001 avg prob of [ Las Vegas Strip] 0.9934757947921753\n",
      "Init norm 2.890716314315796 | Delta norm 11.5628662109375 | Target norm 11.995615005493164\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.5629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6263, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.6494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5649, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.5979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5891, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.9348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6352, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.5544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.8035, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 16:59:28,132 - easyeditor.editors.editor - INFO - 30 editing: Which tourist attraction was named after Sunset Strip? -> Las Vegas Strip  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which tourist attraction was named after Sunset Strip?', 'target_new': 'Las Vegas Strip', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunset Strip'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:59:28,132 - easyeditor.editors.editor - INFO - 30 editing: Which tourist attraction was named after Sunset Strip? -> Las Vegas Strip  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which tourist attraction was named after Sunset Strip?', 'target_new': 'Las Vegas Strip', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunset Strip'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:59:28,132 - easyeditor.editors.editor - INFO - 30 editing: Which tourist attraction was named after Sunset Strip? -> Las Vegas Strip  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which tourist attraction was named after Sunset Strip?', 'target_new': 'Las Vegas Strip', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunset Strip'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:59:28,132 - easyeditor.editors.editor - INFO - 30 editing: Which tourist attraction was named after Sunset Strip? -> Las Vegas Strip  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which tourist attraction was named after Sunset Strip?', 'target_new': 'Las Vegas Strip', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunset Strip'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 16:59:28,132 - easyeditor.editors.editor - INFO - 30 editing: Which tourist attraction was named after Sunset Strip? -> Las Vegas Strip  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which tourist attraction was named after Sunset Strip?', 'target_new': 'Las Vegas Strip', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunset Strip'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 16:59:28 - INFO - easyeditor.editors.editor -   30 editing: Which tourist attraction was named after Sunset Strip? -> Las Vegas Strip  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which tourist attraction was named after Sunset Strip?', 'target_new': 'Las Vegas Strip', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunset Strip'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 62%|██████▏   | 31/50 [15:19<10:00, 31.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the architectural style of İzmir Clock Tower?] -> [ eclecticism in art]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: What is the architectural style of İzmir Clock Tower?eclecticism in | Token: Tower\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.322 = 5.322 + 0.0 + 0.0 avg prob of [ eclecticism in art] 0.004959641490131617\n",
      "loss 4.148 = 4.089 + 0.057 + 0.002 avg prob of [ eclecticism in art] 0.017070548608899117\n",
      "loss 3.207 = 3.14 + 0.065 + 0.002 avg prob of [ eclecticism in art] 0.045214127749204636\n",
      "loss 2.103 = 2.069 + 0.032 + 0.002 avg prob of [ eclecticism in art] 0.12812629342079163\n",
      "loss 1.068 = 1.001 + 0.066 + 0.002 avg prob of [ eclecticism in art] 0.37256866693496704\n",
      "loss 0.402 = 0.294 + 0.106 + 0.002 avg prob of [ eclecticism in art] 0.7454657554626465\n",
      "loss 0.168 = 0.084 + 0.082 + 0.002 avg prob of [ eclecticism in art] 0.9192053079605103\n",
      "loss 0.135 = 0.025 + 0.108 + 0.002 avg prob of [ eclecticism in art] 0.9750841856002808\n",
      "loss 0.119 = 0.009 + 0.108 + 0.002 avg prob of [ eclecticism in art] 0.9907103776931763\n",
      "loss 0.114 = 0.004 + 0.108 + 0.002 avg prob of [ eclecticism in art] 0.9958907961845398\n",
      "loss 0.112 = 0.002 + 0.108 + 0.002 avg prob of [ eclecticism in art] 0.9975336790084839\n",
      "loss 0.111 = 0.002 + 0.107 + 0.002 avg prob of [ eclecticism in art] 0.9982255697250366\n",
      "loss 0.108 = 0.001 + 0.105 + 0.002 avg prob of [ eclecticism in art] 0.9985381364822388\n",
      "loss 0.105 = 0.001 + 0.102 + 0.002 avg prob of [ eclecticism in art] 0.9986711740493774\n",
      "loss 0.104 = 0.001 + 0.101 + 0.002 avg prob of [ eclecticism in art] 0.9987335205078125\n",
      "loss 0.101 = 0.001 + 0.099 + 0.002 avg prob of [ eclecticism in art] 0.9988001585006714\n",
      "loss 0.104 = 0.001 + 0.101 + 0.002 avg prob of [ eclecticism in art] 0.9988850355148315\n",
      "loss 0.103 = 0.001 + 0.1 + 0.002 avg prob of [ eclecticism in art] 0.9989342093467712\n",
      "loss 0.103 = 0.001 + 0.1 + 0.002 avg prob of [ eclecticism in art] 0.9988919496536255\n",
      "loss 0.102 = 0.001 + 0.099 + 0.002 avg prob of [ eclecticism in art] 0.9989759922027588\n",
      "loss 0.101 = 0.001 + 0.099 + 0.002 avg prob of [ eclecticism in art] 0.9990875124931335\n",
      "loss 0.097 = 0.001 + 0.095 + 0.002 avg prob of [ eclecticism in art] 0.9991273283958435\n",
      "loss 0.079 = 0.001 + 0.076 + 0.002 avg prob of [ eclecticism in art] 0.9990116357803345\n",
      "loss 0.074 = 0.001 + 0.072 + 0.002 avg prob of [ eclecticism in art] 0.9989724159240723\n",
      "loss 0.065 = 0.001 + 0.062 + 0.002 avg prob of [ eclecticism in art] 0.9988024234771729\n",
      "Init norm 2.4572982788085938 | Delta norm 9.829194068908691 | Target norm 10.137147903442383\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.8292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4941, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.2492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4852, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.4866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5235, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.1377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5676, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.0501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7307, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:00:04,844 - easyeditor.editors.editor - INFO - 31 editing: What is the architectural style of İzmir Clock Tower? -> eclecticism in art  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What is the architectural style of İzmir Clock Tower?', 'target_new': 'eclecticism in art', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:00:04,844 - easyeditor.editors.editor - INFO - 31 editing: What is the architectural style of İzmir Clock Tower? -> eclecticism in art  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What is the architectural style of İzmir Clock Tower?', 'target_new': 'eclecticism in art', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:00:04,844 - easyeditor.editors.editor - INFO - 31 editing: What is the architectural style of İzmir Clock Tower? -> eclecticism in art  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What is the architectural style of İzmir Clock Tower?', 'target_new': 'eclecticism in art', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:00:04,844 - easyeditor.editors.editor - INFO - 31 editing: What is the architectural style of İzmir Clock Tower? -> eclecticism in art  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What is the architectural style of İzmir Clock Tower?', 'target_new': 'eclecticism in art', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:00:04,844 - easyeditor.editors.editor - INFO - 31 editing: What is the architectural style of İzmir Clock Tower? -> eclecticism in art  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What is the architectural style of İzmir Clock Tower?', 'target_new': 'eclecticism in art', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:00:04 - INFO - easyeditor.editors.editor -   31 editing: What is the architectural style of İzmir Clock Tower? -> eclecticism in art  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What is the architectural style of İzmir Clock Tower?', 'target_new': 'eclecticism in art', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 64%|██████▍   | 32/50 [15:56<09:56, 33.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who does İzmir Clock Tower architect?] -> [ Raymond Charles Péré]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: Who does İzmir Clock Tower architect?Raymond Charles P | Token: Tower\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.402 = 5.402 + 0.0 + 0.0 avg prob of [ Raymond Charles Péré] 0.004606275819242001\n",
      "loss 5.818 = 5.78 + 0.036 + 0.002 avg prob of [ Raymond Charles Péré] 0.0031128672417253256\n",
      "loss 6.395 = 6.342 + 0.051 + 0.002 avg prob of [ Raymond Charles Péré] 0.0018168938113376498\n",
      "loss 5.471 = 5.448 + 0.022 + 0.002 avg prob of [ Raymond Charles Péré] 0.004437786526978016\n",
      "loss 4.55 = 4.533 + 0.015 + 0.002 avg prob of [ Raymond Charles Péré] 0.011017195880413055\n",
      "loss 3.606 = 3.593 + 0.012 + 0.002 avg prob of [ Raymond Charles Péré] 0.028344396501779556\n",
      "loss 2.796 = 2.778 + 0.017 + 0.002 avg prob of [ Raymond Charles Péré] 0.0625218003988266\n",
      "loss 2.042 = 2.019 + 0.021 + 0.002 avg prob of [ Raymond Charles Péré] 0.13292962312698364\n",
      "loss 0.524 = 0.464 + 0.059 + 0.002 avg prob of [ Raymond Charles Péré] 0.6304295063018799\n",
      "loss 0.279 = 0.191 + 0.086 + 0.002 avg prob of [ Raymond Charles Péré] 0.8266724944114685\n",
      "loss 0.106 = 0.049 + 0.055 + 0.002 avg prob of [ Raymond Charles Péré] 0.9527696371078491\n",
      "loss 0.108 = 0.025 + 0.081 + 0.002 avg prob of [ Raymond Charles Péré] 0.9749315977096558\n",
      "loss 1.647 = 1.612 + 0.033 + 0.002 avg prob of [ Raymond Charles Péré] 0.3073168098926544\n",
      "loss 5.112 = 5.033 + 0.076 + 0.002 avg prob of [ Raymond Charles Péré] 0.006686095148324966\n",
      "loss 4.715 = 4.678 + 0.035 + 0.002 avg prob of [ Raymond Charles Péré] 0.00939084216952324\n",
      "loss 3.865 = 3.838 + 0.025 + 0.002 avg prob of [ Raymond Charles Péré] 0.02178673818707466\n",
      "loss 3.396 = 3.376 + 0.018 + 0.002 avg prob of [ Raymond Charles Péré] 0.0344276987016201\n",
      "loss 2.66 = 2.642 + 0.016 + 0.002 avg prob of [ Raymond Charles Péré] 0.07252376526594162\n",
      "loss 1.742 = 1.723 + 0.018 + 0.002 avg prob of [ Raymond Charles Péré] 0.1812247335910797\n",
      "loss 0.384 = 0.363 + 0.019 + 0.002 avg prob of [ Raymond Charles Péré] 0.7125051021575928\n",
      "loss 0.162 = 0.142 + 0.018 + 0.002 avg prob of [ Raymond Charles Péré] 0.869606614112854\n",
      "loss 0.184 = 0.158 + 0.024 + 0.002 avg prob of [ Raymond Charles Péré] 0.8544425964355469\n",
      "loss 0.093 = 0.073 + 0.018 + 0.002 avg prob of [ Raymond Charles Péré] 0.9294012784957886\n",
      "loss 0.046 = 0.029 + 0.015 + 0.002 avg prob of [ Raymond Charles Péré] 0.9713548421859741\n",
      "Init norm 2.4342310428619385 | Delta norm 9.736924171447754 | Target norm 10.155804634094238\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.7369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4993, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.1965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4757, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.3879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5184, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.1654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5723, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7567, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:00:39,744 - easyeditor.editors.editor - INFO - 32 editing: Who does İzmir Clock Tower architect? -> Raymond Charles Péré  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'Who does İzmir Clock Tower architect?', 'target_new': 'Raymond Charles Péré', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:00:39,744 - easyeditor.editors.editor - INFO - 32 editing: Who does İzmir Clock Tower architect? -> Raymond Charles Péré  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'Who does İzmir Clock Tower architect?', 'target_new': 'Raymond Charles Péré', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:00:39,744 - easyeditor.editors.editor - INFO - 32 editing: Who does İzmir Clock Tower architect? -> Raymond Charles Péré  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'Who does İzmir Clock Tower architect?', 'target_new': 'Raymond Charles Péré', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:00:39,744 - easyeditor.editors.editor - INFO - 32 editing: Who does İzmir Clock Tower architect? -> Raymond Charles Péré  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'Who does İzmir Clock Tower architect?', 'target_new': 'Raymond Charles Péré', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:00:39,744 - easyeditor.editors.editor - INFO - 32 editing: Who does İzmir Clock Tower architect? -> Raymond Charles Péré  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'Who does İzmir Clock Tower architect?', 'target_new': 'Raymond Charles Péré', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:00:39 - INFO - easyeditor.editors.editor -   32 editing: Who does İzmir Clock Tower architect? -> Raymond Charles Péré  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'Who does İzmir Clock Tower architect?', 'target_new': 'Raymond Charles Péré', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'İzmir Clock Tower'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 66%|██████▌   | 33/50 [16:31<09:32, 33.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction has part(s) National Anthropological Archives?] -> [ National Museum of Natural History]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: Which tourist attraction has part(s) National Anthropological Archives?National Museum of Natural | Token: Archives\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.334 = 1.334 + 0.0 + 0.0 avg prob of [ National Museum of Natural History] 0.26407453417778015\n",
      "loss 0.917 = 0.763 + 0.152 + 0.002 avg prob of [ National Museum of Natural History] 0.4679478108882904\n",
      "loss 0.429 = 0.377 + 0.05 + 0.002 avg prob of [ National Museum of Natural History] 0.6873941421508789\n",
      "loss 0.504 = 0.476 + 0.026 + 0.002 avg prob of [ National Museum of Natural History] 0.623104989528656\n",
      "loss 0.391 = 0.263 + 0.127 + 0.002 avg prob of [ National Museum of Natural History] 0.7711242437362671\n",
      "loss 0.15 = 0.079 + 0.069 + 0.002 avg prob of [ National Museum of Natural History] 0.9238724112510681\n",
      "loss 0.086 = 0.041 + 0.043 + 0.002 avg prob of [ National Museum of Natural History] 0.9597371816635132\n",
      "loss 0.057 = 0.023 + 0.032 + 0.002 avg prob of [ National Museum of Natural History] 0.9773659706115723\n",
      "loss 0.042 = 0.01 + 0.031 + 0.002 avg prob of [ National Museum of Natural History] 0.9899687170982361\n",
      "Init norm 2.5870425701141357 | Delta norm 10.34817123413086 | Target norm 10.714369773864746\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.3482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5848, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.6619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5203, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.6875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5465, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.2435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5882, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7853, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:01:09,015 - easyeditor.editors.editor - INFO - 33 editing: Which tourist attraction has part(s) National Anthropological Archives? -> National Museum of Natural History  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) National Anthropological Archives?', 'target_new': 'National Museum of Natural History', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'National Anthropological Archives'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:01:09,015 - easyeditor.editors.editor - INFO - 33 editing: Which tourist attraction has part(s) National Anthropological Archives? -> National Museum of Natural History  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) National Anthropological Archives?', 'target_new': 'National Museum of Natural History', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'National Anthropological Archives'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:01:09,015 - easyeditor.editors.editor - INFO - 33 editing: Which tourist attraction has part(s) National Anthropological Archives? -> National Museum of Natural History  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) National Anthropological Archives?', 'target_new': 'National Museum of Natural History', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'National Anthropological Archives'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:01:09,015 - easyeditor.editors.editor - INFO - 33 editing: Which tourist attraction has part(s) National Anthropological Archives? -> National Museum of Natural History  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) National Anthropological Archives?', 'target_new': 'National Museum of Natural History', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'National Anthropological Archives'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:01:09,015 - easyeditor.editors.editor - INFO - 33 editing: Which tourist attraction has part(s) National Anthropological Archives? -> National Museum of Natural History  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) National Anthropological Archives?', 'target_new': 'National Museum of Natural History', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'National Anthropological Archives'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:01:09 - INFO - easyeditor.editors.editor -   33 editing: Which tourist attraction has part(s) National Anthropological Archives? -> National Museum of Natural History  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) National Anthropological Archives?', 'target_new': 'National Museum of Natural History', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'National Anthropological Archives'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 68%|██████▊   | 34/50 [17:00<08:37, 32.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's director / manager is Tor Hagfors?] -> [ Arecibo Observatory]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: Which tourist attraction's director / manager is Tor Hagfors?Arecibo Observ | Token: ors\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.822 = 2.822 + 0.0 + 0.0 avg prob of [ Arecibo Observatory] 0.06021277233958244\n",
      "loss 2.648 = 2.523 + 0.124 + 0.002 avg prob of [ Arecibo Observatory] 0.08094026148319244\n",
      "loss 1.991 = 1.943 + 0.046 + 0.002 avg prob of [ Arecibo Observatory] 0.14351749420166016\n",
      "loss 1.246 = 1.195 + 0.049 + 0.002 avg prob of [ Arecibo Observatory] 0.30421820282936096\n",
      "loss 0.651 = 0.589 + 0.061 + 0.002 avg prob of [ Arecibo Observatory] 0.5553788542747498\n",
      "loss 0.219 = 0.07 + 0.147 + 0.002 avg prob of [ Arecibo Observatory] 0.932295024394989\n",
      "loss 0.196 = 0.114 + 0.079 + 0.002 avg prob of [ Arecibo Observatory] 0.8938454389572144\n",
      "loss 0.087 = 0.028 + 0.057 + 0.002 avg prob of [ Arecibo Observatory] 0.9722039103507996\n",
      "loss 0.046 = 0.001 + 0.044 + 0.002 avg prob of [ Arecibo Observatory] 0.9990090131759644\n",
      "Init norm 2.215381383895874 | Delta norm 8.861525535583496 | Target norm 9.182257652282715\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.8615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5081, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.1959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4510, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.5281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4745, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.4672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5146, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6915, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:01:37,967 - easyeditor.editors.editor - INFO - 34 editing: Which tourist attraction's director / manager is Tor Hagfors? -> Arecibo Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': \"Which tourist attraction's director / manager is Tor Hagfors?\", 'target_new': 'Arecibo Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tor Hagfors'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:01:37,967 - easyeditor.editors.editor - INFO - 34 editing: Which tourist attraction's director / manager is Tor Hagfors? -> Arecibo Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': \"Which tourist attraction's director / manager is Tor Hagfors?\", 'target_new': 'Arecibo Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tor Hagfors'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:01:37,967 - easyeditor.editors.editor - INFO - 34 editing: Which tourist attraction's director / manager is Tor Hagfors? -> Arecibo Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': \"Which tourist attraction's director / manager is Tor Hagfors?\", 'target_new': 'Arecibo Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tor Hagfors'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:01:37,967 - easyeditor.editors.editor - INFO - 34 editing: Which tourist attraction's director / manager is Tor Hagfors? -> Arecibo Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': \"Which tourist attraction's director / manager is Tor Hagfors?\", 'target_new': 'Arecibo Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tor Hagfors'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:01:37,967 - easyeditor.editors.editor - INFO - 34 editing: Which tourist attraction's director / manager is Tor Hagfors? -> Arecibo Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': \"Which tourist attraction's director / manager is Tor Hagfors?\", 'target_new': 'Arecibo Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tor Hagfors'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:01:37 - INFO - easyeditor.editors.editor -   34 editing: Which tourist attraction's director / manager is Tor Hagfors? -> Arecibo Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': \"Which tourist attraction's director / manager is Tor Hagfors?\", 'target_new': 'Arecibo Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tor Hagfors'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 70%|███████   | 35/50 [17:29<07:49, 31.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What was Gustav III's Pavilion owned by?] -> [ National Property Board of Sweden]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What was Gustav III's Pavilion owned by?National Property Board of | Token: ion\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.779 = 2.779 + 0.0 + 0.0 avg prob of [ National Property Board of Sweden] 0.06374624371528625\n",
      "loss 2.155 = 2.114 + 0.039 + 0.002 avg prob of [ National Property Board of Sweden] 0.12402087450027466\n",
      "loss 1.114 = 1.099 + 0.014 + 0.002 avg prob of [ National Property Board of Sweden] 0.3340871334075928\n",
      "loss 1.152 = 1.077 + 0.073 + 0.002 avg prob of [ National Property Board of Sweden] 0.34789514541625977\n",
      "loss 0.753 = 0.686 + 0.064 + 0.002 avg prob of [ National Property Board of Sweden] 0.5047619342803955\n",
      "loss 0.513 = 0.474 + 0.037 + 0.002 avg prob of [ National Property Board of Sweden] 0.6234074831008911\n",
      "loss 0.316 = 0.271 + 0.043 + 0.002 avg prob of [ National Property Board of Sweden] 0.7649802565574646\n",
      "loss 0.33 = 0.221 + 0.107 + 0.002 avg prob of [ National Property Board of Sweden] 0.8023638725280762\n",
      "loss 0.093 = 0.04 + 0.052 + 0.002 avg prob of [ National Property Board of Sweden] 0.9608256220817566\n",
      "loss 0.066 = 0.019 + 0.045 + 0.002 avg prob of [ National Property Board of Sweden] 0.9813200235366821\n",
      "loss 0.044 = 0.012 + 0.03 + 0.002 avg prob of [ National Property Board of Sweden] 0.988348662853241\n",
      "Init norm 2.2500360012054443 | Delta norm 9.000144004821777 | Target norm 9.367737770080566\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5041, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.7087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4571, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.0599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4994, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.8084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5233, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6320, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:02:06,854 - easyeditor.editors.editor - INFO - 35 editing: What was Gustav III's Pavilion owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': \"What was Gustav III's Pavilion owned by?\", 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:02:06,854 - easyeditor.editors.editor - INFO - 35 editing: What was Gustav III's Pavilion owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': \"What was Gustav III's Pavilion owned by?\", 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:02:06,854 - easyeditor.editors.editor - INFO - 35 editing: What was Gustav III's Pavilion owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': \"What was Gustav III's Pavilion owned by?\", 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:02:06,854 - easyeditor.editors.editor - INFO - 35 editing: What was Gustav III's Pavilion owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': \"What was Gustav III's Pavilion owned by?\", 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:02:06,854 - easyeditor.editors.editor - INFO - 35 editing: What was Gustav III's Pavilion owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': \"What was Gustav III's Pavilion owned by?\", 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:02:06 - INFO - easyeditor.editors.editor -   35 editing: What was Gustav III's Pavilion owned by? -> National Property Board of Sweden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': \"What was Gustav III's Pavilion owned by?\", 'target_new': 'National Property Board of Sweden', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 72%|███████▏  | 36/50 [17:58<07:08, 30.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the architectural style of Gustav III's Pavilion?] -> [ neoclassicism]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 15 | Sentence: What is the architectural style of Gustav III's Pavilion?neoclass | Token: ion\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.617 = 2.617 + 0.0 + 0.0 avg prob of [ neoclassicism] 0.0921759307384491\n",
      "loss 2.184 = 2.087 + 0.096 + 0.002 avg prob of [ neoclassicism] 0.1438090205192566\n",
      "loss 1.467 = 1.418 + 0.047 + 0.002 avg prob of [ neoclassicism] 0.24532550573349\n",
      "loss 0.714 = 0.695 + 0.017 + 0.002 avg prob of [ neoclassicism] 0.5004758834838867\n",
      "loss 0.144 = 0.062 + 0.081 + 0.002 avg prob of [ neoclassicism] 0.9401993751525879\n",
      "loss 0.053 = 0.006 + 0.045 + 0.002 avg prob of [ neoclassicism] 0.9937911033630371\n",
      "loss 0.065 = 0.005 + 0.058 + 0.002 avg prob of [ neoclassicism] 0.9948972463607788\n",
      "loss 0.056 = 0.008 + 0.046 + 0.002 avg prob of [ neoclassicism] 0.9918100237846375\n",
      "loss 0.031 = 0.006 + 0.024 + 0.002 avg prob of [ neoclassicism] 0.9940764307975769\n",
      "Init norm 2.3270978927612305 | Delta norm 9.308391571044922 | Target norm 9.634698867797852\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.3084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5170, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.9524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4739, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.3198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5169, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.1332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5749, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7575, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:02:34,477 - easyeditor.editors.editor - INFO - 36 editing: What is the architectural style of Gustav III's Pavilion? -> neoclassicism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': \"What is the architectural style of Gustav III's Pavilion?\", 'target_new': 'neoclassicism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:02:34,477 - easyeditor.editors.editor - INFO - 36 editing: What is the architectural style of Gustav III's Pavilion? -> neoclassicism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': \"What is the architectural style of Gustav III's Pavilion?\", 'target_new': 'neoclassicism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:02:34,477 - easyeditor.editors.editor - INFO - 36 editing: What is the architectural style of Gustav III's Pavilion? -> neoclassicism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': \"What is the architectural style of Gustav III's Pavilion?\", 'target_new': 'neoclassicism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:02:34,477 - easyeditor.editors.editor - INFO - 36 editing: What is the architectural style of Gustav III's Pavilion? -> neoclassicism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': \"What is the architectural style of Gustav III's Pavilion?\", 'target_new': 'neoclassicism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:02:34,477 - easyeditor.editors.editor - INFO - 36 editing: What is the architectural style of Gustav III's Pavilion? -> neoclassicism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': \"What is the architectural style of Gustav III's Pavilion?\", 'target_new': 'neoclassicism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:02:34 - INFO - easyeditor.editors.editor -   36 editing: What is the architectural style of Gustav III's Pavilion? -> neoclassicism  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': \"What is the architectural style of Gustav III's Pavilion?\", 'target_new': 'neoclassicism', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': \"Gustav III's Pavilion\"}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 74%|███████▍  | 37/50 [18:26<06:26, 29.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the located in the administrative territorial entity of Potala Palace?] -> [ Lhasa]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: What is the located in the administrative territorial entity of Potala Palace?Lhas | Token: Palace\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.198 = 2.198 + 0.0 + 0.0 avg prob of [ Lhasa] 0.11560986936092377\n",
      "loss 1.532 = 1.504 + 0.026 + 0.002 avg prob of [ Lhasa] 0.2246628701686859\n",
      "loss 0.661 = 0.62 + 0.039 + 0.002 avg prob of [ Lhasa] 0.541001558303833\n",
      "loss 0.182 = 0.142 + 0.038 + 0.002 avg prob of [ Lhasa] 0.8677786588668823\n",
      "loss 0.052 = 0.021 + 0.03 + 0.002 avg prob of [ Lhasa] 0.9791767001152039\n",
      "loss 0.035 = 0.005 + 0.029 + 0.002 avg prob of [ Lhasa] 0.9954125881195068\n",
      "Init norm 2.4035532474517822 | Delta norm 9.614213943481445 | Target norm 10.04259967803955\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.6142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5116, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.0782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4550, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4962, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.8253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5370, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7038, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:03:00,786 - easyeditor.editors.editor - INFO - 37 editing: What is the located in the administrative territorial entity of Potala Palace? -> Lhasa  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Potala Palace?', 'target_new': 'Lhasa', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Potala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:03:00,786 - easyeditor.editors.editor - INFO - 37 editing: What is the located in the administrative territorial entity of Potala Palace? -> Lhasa  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Potala Palace?', 'target_new': 'Lhasa', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Potala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:03:00,786 - easyeditor.editors.editor - INFO - 37 editing: What is the located in the administrative territorial entity of Potala Palace? -> Lhasa  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Potala Palace?', 'target_new': 'Lhasa', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Potala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:03:00,786 - easyeditor.editors.editor - INFO - 37 editing: What is the located in the administrative territorial entity of Potala Palace? -> Lhasa  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Potala Palace?', 'target_new': 'Lhasa', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Potala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:03:00,786 - easyeditor.editors.editor - INFO - 37 editing: What is the located in the administrative territorial entity of Potala Palace? -> Lhasa  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Potala Palace?', 'target_new': 'Lhasa', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Potala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:03:00 - INFO - easyeditor.editors.editor -   37 editing: What is the located in the administrative territorial entity of Potala Palace? -> Lhasa  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Potala Palace?', 'target_new': 'Lhasa', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Potala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 76%|███████▌  | 38/50 [18:52<05:44, 28.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the Christian liturgical rite of Saviour Church on Nereditsa?] -> [ Russian Orthodox Church]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 18 | Sentence: What is the Christian liturgical rite of Saviour Church on Nereditsa?Russian Orthodox | Token: a\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.348 = 3.348 + 0.0 + 0.0 avg prob of [ Russian Orthodox Church] 0.03548191860318184\n",
      "loss 2.872 = 2.78 + 0.091 + 0.002 avg prob of [ Russian Orthodox Church] 0.06254498660564423\n",
      "loss 1.336 = 1.258 + 0.076 + 0.002 avg prob of [ Russian Orthodox Church] 0.28635480999946594\n",
      "loss 2.722 = 2.199 + 0.522 + 0.002 avg prob of [ Russian Orthodox Church] 0.11395597457885742\n",
      "loss 0.332 = 0.091 + 0.239 + 0.002 avg prob of [ Russian Orthodox Church] 0.9128766059875488\n",
      "loss 0.527 = 0.129 + 0.397 + 0.002 avg prob of [ Russian Orthodox Church] 0.880448579788208\n",
      "loss 0.238 = 0.061 + 0.176 + 0.002 avg prob of [ Russian Orthodox Church] 0.9409818649291992\n",
      "loss 0.218 = 0.008 + 0.208 + 0.002 avg prob of [ Russian Orthodox Church] 0.991611659526825\n",
      "loss 0.117 = 0.005 + 0.111 + 0.002 avg prob of [ Russian Orthodox Church] 0.9947175979614258\n",
      "loss 0.102 = 0.003 + 0.097 + 0.002 avg prob of [ Russian Orthodox Church] 0.996718168258667\n",
      "loss 0.102 = 0.002 + 0.099 + 0.002 avg prob of [ Russian Orthodox Church] 0.9983586668968201\n",
      "loss 0.108 = 0.002 + 0.104 + 0.002 avg prob of [ Russian Orthodox Church] 0.997583270072937\n",
      "loss 0.1 = 0.004 + 0.094 + 0.002 avg prob of [ Russian Orthodox Church] 0.9959192276000977\n",
      "loss 0.095 = 0.002 + 0.091 + 0.002 avg prob of [ Russian Orthodox Church] 0.9977070689201355\n",
      "loss 0.091 = 0.001 + 0.088 + 0.002 avg prob of [ Russian Orthodox Church] 0.998836874961853\n",
      "loss 0.088 = 0.001 + 0.086 + 0.002 avg prob of [ Russian Orthodox Church] 0.9993361234664917\n",
      "loss 0.087 = 0.0 + 0.085 + 0.002 avg prob of [ Russian Orthodox Church] 0.9995445013046265\n",
      "loss 0.085 = 0.0 + 0.083 + 0.002 avg prob of [ Russian Orthodox Church] 0.9996334910392761\n",
      "loss 0.083 = 0.0 + 0.082 + 0.002 avg prob of [ Russian Orthodox Church] 0.9996828436851501\n",
      "loss 0.082 = 0.0 + 0.081 + 0.002 avg prob of [ Russian Orthodox Church] 0.9997214674949646\n",
      "loss 0.081 = 0.0 + 0.079 + 0.002 avg prob of [ Russian Orthodox Church] 0.9997514486312866\n",
      "loss 0.078 = 0.0 + 0.077 + 0.002 avg prob of [ Russian Orthodox Church] 0.9997692108154297\n",
      "loss 0.074 = 0.0 + 0.073 + 0.002 avg prob of [ Russian Orthodox Church] 0.9997708201408386\n",
      "loss 0.068 = 0.0 + 0.066 + 0.002 avg prob of [ Russian Orthodox Church] 0.999753475189209\n",
      "loss 0.067 = 0.0 + 0.065 + 0.002 avg prob of [ Russian Orthodox Church] 0.9997320175170898\n",
      "Init norm 2.593853712081909 | Delta norm 10.375414848327637 | Target norm 10.711823463439941\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.3754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5904, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.7233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5315, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.8041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5615, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.4815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5990, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.4284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7905, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:03:37,570 - easyeditor.editors.editor - INFO - 38 editing: What is the Christian liturgical rite of Saviour Church on Nereditsa? -> Russian Orthodox Church  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What is the Christian liturgical rite of Saviour Church on Nereditsa?', 'target_new': 'Russian Orthodox Church', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:03:37,570 - easyeditor.editors.editor - INFO - 38 editing: What is the Christian liturgical rite of Saviour Church on Nereditsa? -> Russian Orthodox Church  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What is the Christian liturgical rite of Saviour Church on Nereditsa?', 'target_new': 'Russian Orthodox Church', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:03:37,570 - easyeditor.editors.editor - INFO - 38 editing: What is the Christian liturgical rite of Saviour Church on Nereditsa? -> Russian Orthodox Church  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What is the Christian liturgical rite of Saviour Church on Nereditsa?', 'target_new': 'Russian Orthodox Church', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:03:37,570 - easyeditor.editors.editor - INFO - 38 editing: What is the Christian liturgical rite of Saviour Church on Nereditsa? -> Russian Orthodox Church  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What is the Christian liturgical rite of Saviour Church on Nereditsa?', 'target_new': 'Russian Orthodox Church', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:03:37,570 - easyeditor.editors.editor - INFO - 38 editing: What is the Christian liturgical rite of Saviour Church on Nereditsa? -> Russian Orthodox Church  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What is the Christian liturgical rite of Saviour Church on Nereditsa?', 'target_new': 'Russian Orthodox Church', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:03:37 - INFO - easyeditor.editors.editor -   38 editing: What is the Christian liturgical rite of Saviour Church on Nereditsa? -> Russian Orthodox Church  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What is the Christian liturgical rite of Saviour Church on Nereditsa?', 'target_new': 'Russian Orthodox Church', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 78%|███████▊  | 39/50 [19:29<05:42, 31.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the religion or worldview of Saviour Church on Nereditsa?] -> [ Eastern Orthodoxy]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 16 | Sentence: What is the religion or worldview of Saviour Church on Nereditsa?Eastern Orthodox | Token: a\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.289 = 3.289 + 0.0 + 0.0 avg prob of [ Eastern Orthodoxy] 0.037750519812107086\n",
      "loss 2.823 = 2.707 + 0.115 + 0.002 avg prob of [ Eastern Orthodoxy] 0.06758201122283936\n",
      "loss 1.983 = 1.903 + 0.078 + 0.002 avg prob of [ Eastern Orthodoxy] 0.1498810052871704\n",
      "loss 3.368 = 3.283 + 0.084 + 0.002 avg prob of [ Eastern Orthodoxy] 0.0404600128531456\n",
      "loss 1.339 = 1.241 + 0.096 + 0.002 avg prob of [ Eastern Orthodoxy] 0.2954160273075104\n",
      "loss 1.277 = 0.905 + 0.37 + 0.002 avg prob of [ Eastern Orthodoxy] 0.40621352195739746\n",
      "loss 1.347 = 1.149 + 0.196 + 0.002 avg prob of [ Eastern Orthodoxy] 0.31834203004837036\n",
      "loss 1.486 = 1.28 + 0.204 + 0.002 avg prob of [ Eastern Orthodoxy] 0.28021499514579773\n",
      "loss 0.724 = 0.614 + 0.108 + 0.002 avg prob of [ Eastern Orthodoxy] 0.5438041687011719\n",
      "loss 0.453 = 0.326 + 0.125 + 0.002 avg prob of [ Eastern Orthodoxy] 0.7221195101737976\n",
      "loss 0.568 = 0.083 + 0.483 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9210296869277954\n",
      "loss 0.37 = 0.207 + 0.162 + 0.002 avg prob of [ Eastern Orthodoxy] 0.8146075010299683\n",
      "loss 0.161 = 0.007 + 0.152 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9925381541252136\n",
      "loss 0.157 = 0.015 + 0.141 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9855145215988159\n",
      "loss 0.135 = 0.006 + 0.127 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9936490058898926\n",
      "loss 0.114 = 0.005 + 0.107 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9951328039169312\n",
      "loss 0.096 = 0.004 + 0.09 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9959042072296143\n",
      "loss 0.086 = 0.003 + 0.081 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9966334104537964\n",
      "loss 0.08 = 0.003 + 0.075 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9972294569015503\n",
      "loss 0.075 = 0.002 + 0.071 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9977188110351562\n",
      "loss 0.072 = 0.002 + 0.069 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9981352090835571\n",
      "loss 0.071 = 0.002 + 0.068 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9984809756278992\n",
      "loss 0.07 = 0.001 + 0.067 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9987596273422241\n",
      "loss 0.069 = 0.001 + 0.066 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9989819526672363\n",
      "loss 0.068 = 0.001 + 0.066 + 0.002 avg prob of [ Eastern Orthodoxy] 0.9991564154624939\n",
      "Init norm 2.3781256675720215 | Delta norm 9.512502670288086 | Target norm 9.907368659973145\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.5125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5427, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.9971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4891, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.1965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5173, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5602, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.1310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7573, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:04:12,972 - easyeditor.editors.editor - INFO - 39 editing: What is the religion or worldview of Saviour Church on Nereditsa? -> Eastern Orthodoxy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the religion or worldview of Saviour Church on Nereditsa?', 'target_new': 'Eastern Orthodoxy', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:04:12,972 - easyeditor.editors.editor - INFO - 39 editing: What is the religion or worldview of Saviour Church on Nereditsa? -> Eastern Orthodoxy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the religion or worldview of Saviour Church on Nereditsa?', 'target_new': 'Eastern Orthodoxy', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:04:12,972 - easyeditor.editors.editor - INFO - 39 editing: What is the religion or worldview of Saviour Church on Nereditsa? -> Eastern Orthodoxy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the religion or worldview of Saviour Church on Nereditsa?', 'target_new': 'Eastern Orthodoxy', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:04:12,972 - easyeditor.editors.editor - INFO - 39 editing: What is the religion or worldview of Saviour Church on Nereditsa? -> Eastern Orthodoxy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the religion or worldview of Saviour Church on Nereditsa?', 'target_new': 'Eastern Orthodoxy', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:04:12,972 - easyeditor.editors.editor - INFO - 39 editing: What is the religion or worldview of Saviour Church on Nereditsa? -> Eastern Orthodoxy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the religion or worldview of Saviour Church on Nereditsa?', 'target_new': 'Eastern Orthodoxy', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:04:12 - INFO - easyeditor.editors.editor -   39 editing: What is the religion or worldview of Saviour Church on Nereditsa? -> Eastern Orthodoxy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the religion or worldview of Saviour Church on Nereditsa?', 'target_new': 'Eastern Orthodoxy', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Saviour Church on Nereditsa'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 80%|████████  | 40/50 [20:04<05:24, 32.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the architectural style of South Street Seaport?] -> [ Greek Revival architecture]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: What is the architectural style of South Street Seaport?Greek Revival | Token: ort\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.749 = 3.749 + 0.0 + 0.0 avg prob of [ Greek Revival architecture] 0.024008668959140778\n",
      "loss 3.64 = 3.369 + 0.27 + 0.001 avg prob of [ Greek Revival architecture] 0.03529444336891174\n",
      "loss 2.882 = 2.794 + 0.086 + 0.001 avg prob of [ Greek Revival architecture] 0.06187286600470543\n",
      "loss 2.01 = 1.963 + 0.046 + 0.001 avg prob of [ Greek Revival architecture] 0.141111820936203\n",
      "loss 2.875 = 2.771 + 0.102 + 0.001 avg prob of [ Greek Revival architecture] 0.06495566666126251\n",
      "loss 1.552 = 1.467 + 0.084 + 0.001 avg prob of [ Greek Revival architecture] 0.23201346397399902\n",
      "loss 0.182 = 0.105 + 0.076 + 0.001 avg prob of [ Greek Revival architecture] 0.9009048342704773\n",
      "loss 0.111 = 0.004 + 0.105 + 0.001 avg prob of [ Greek Revival architecture] 0.9959259033203125\n",
      "loss 0.106 = 0.003 + 0.101 + 0.001 avg prob of [ Greek Revival architecture] 0.9970490336418152\n",
      "loss 0.094 = 0.001 + 0.092 + 0.001 avg prob of [ Greek Revival architecture] 0.9986147284507751\n",
      "loss 0.065 = 0.001 + 0.062 + 0.001 avg prob of [ Greek Revival architecture] 0.9991177916526794\n",
      "loss 0.075 = 0.001 + 0.073 + 0.001 avg prob of [ Greek Revival architecture] 0.9989497661590576\n",
      "loss 0.042 = 0.001 + 0.039 + 0.001 avg prob of [ Greek Revival architecture] 0.9987615346908569\n",
      "Init norm 2.890517473220825 | Delta norm 11.562068939208984 | Target norm 11.987676620483398\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.5621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6150, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.6184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5746, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.6195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.6050, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.9918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6396, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.5686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.8210, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:04:42,591 - easyeditor.editors.editor - INFO - 40 editing: What is the architectural style of South Street Seaport? -> Greek Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'What is the architectural style of South Street Seaport?', 'target_new': 'Greek Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:04:42,591 - easyeditor.editors.editor - INFO - 40 editing: What is the architectural style of South Street Seaport? -> Greek Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'What is the architectural style of South Street Seaport?', 'target_new': 'Greek Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:04:42,591 - easyeditor.editors.editor - INFO - 40 editing: What is the architectural style of South Street Seaport? -> Greek Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'What is the architectural style of South Street Seaport?', 'target_new': 'Greek Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:04:42,591 - easyeditor.editors.editor - INFO - 40 editing: What is the architectural style of South Street Seaport? -> Greek Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'What is the architectural style of South Street Seaport?', 'target_new': 'Greek Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:04:42,591 - easyeditor.editors.editor - INFO - 40 editing: What is the architectural style of South Street Seaport? -> Greek Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'What is the architectural style of South Street Seaport?', 'target_new': 'Greek Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:04:42 - INFO - easyeditor.editors.editor -   40 editing: What is the architectural style of South Street Seaport? -> Greek Revival architecture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'What is the architectural style of South Street Seaport?', 'target_new': 'Greek Revival architecture', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}}\n",
      " 82%|████████▏ | 41/50 [20:34<04:44, 31.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the located in the administrative territorial entity of South Street Seaport?] -> [ Manhattan]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 16 | Sentence: What is the located in the administrative territorial entity of South Street Seaport? | Token: ort\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.416 = 6.416 + 0.0 + 0.0 avg prob of [ Manhattan] 0.001811905181966722\n",
      "loss 3.978 = 3.853 + 0.123 + 0.001 avg prob of [ Manhattan] 0.021744202822446823\n",
      "loss 1.793 = 1.703 + 0.089 + 0.001 avg prob of [ Manhattan] 0.18625333905220032\n",
      "loss 0.468 = 0.061 + 0.405 + 0.001 avg prob of [ Manhattan] 0.9410359859466553\n",
      "loss 0.153 = 0.037 + 0.115 + 0.001 avg prob of [ Manhattan] 0.9637729525566101\n",
      "loss 0.11 = 0.011 + 0.097 + 0.001 avg prob of [ Manhattan] 0.9887361526489258\n",
      "loss 0.071 = 0.008 + 0.062 + 0.001 avg prob of [ Manhattan] 0.9916197657585144\n",
      "loss 0.068 = 0.006 + 0.06 + 0.001 avg prob of [ Manhattan] 0.9936773180961609\n",
      "loss 0.059 = 0.005 + 0.053 + 0.001 avg prob of [ Manhattan] 0.9951223134994507\n",
      "loss 0.057 = 0.004 + 0.052 + 0.001 avg prob of [ Manhattan] 0.9962929487228394\n",
      "loss 0.052 = 0.003 + 0.048 + 0.001 avg prob of [ Manhattan] 0.9971380233764648\n",
      "loss 0.048 = 0.002 + 0.045 + 0.001 avg prob of [ Manhattan] 0.9977015256881714\n",
      "Init norm 2.899878740310669 | Delta norm 11.599514961242676 | Target norm 12.014265060424805\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.5995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6235, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.6075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5657, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.4755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5881, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.7990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6310, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.4657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7852, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:05:11,873 - easyeditor.editors.editor - INFO - 41 editing: What is the located in the administrative territorial entity of South Street Seaport? -> Manhattan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of South Street Seaport?', 'target_new': 'Manhattan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:05:11,873 - easyeditor.editors.editor - INFO - 41 editing: What is the located in the administrative territorial entity of South Street Seaport? -> Manhattan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of South Street Seaport?', 'target_new': 'Manhattan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:05:11,873 - easyeditor.editors.editor - INFO - 41 editing: What is the located in the administrative territorial entity of South Street Seaport? -> Manhattan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of South Street Seaport?', 'target_new': 'Manhattan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:05:11,873 - easyeditor.editors.editor - INFO - 41 editing: What is the located in the administrative territorial entity of South Street Seaport? -> Manhattan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of South Street Seaport?', 'target_new': 'Manhattan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:05:11,873 - easyeditor.editors.editor - INFO - 41 editing: What is the located in the administrative territorial entity of South Street Seaport? -> Manhattan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of South Street Seaport?', 'target_new': 'Manhattan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:05:11 - INFO - easyeditor.editors.editor -   41 editing: What is the located in the administrative territorial entity of South Street Seaport? -> Manhattan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of South Street Seaport?', 'target_new': 'Manhattan', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'South Street Seaport'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 84%|████████▍ | 42/50 [21:03<04:07, 30.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction architect Louis de Hoÿm de Marien?] -> [ Montparnasse Tower]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 12 | Sentence: Which tourist attraction architect Louis de Hoÿm de Marien?Montparnasse | Token: ien\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.532 = 2.532 + 0.0 + 0.0 avg prob of [ Montparnasse Tower] 0.08039377629756927\n",
      "loss 2.244 = 2.166 + 0.076 + 0.002 avg prob of [ Montparnasse Tower] 0.11768418550491333\n",
      "loss 1.529 = 1.311 + 0.216 + 0.002 avg prob of [ Montparnasse Tower] 0.27240145206451416\n",
      "loss 0.681 = 0.584 + 0.095 + 0.002 avg prob of [ Montparnasse Tower] 0.565184473991394\n",
      "loss 0.447 = 0.296 + 0.149 + 0.002 avg prob of [ Montparnasse Tower] 0.7444059252738953\n",
      "loss 0.184 = 0.065 + 0.117 + 0.002 avg prob of [ Montparnasse Tower] 0.9374205470085144\n",
      "loss 0.124 = 0.018 + 0.104 + 0.002 avg prob of [ Montparnasse Tower] 0.9823207855224609\n",
      "loss 0.12 = 0.007 + 0.111 + 0.002 avg prob of [ Montparnasse Tower] 0.9930486679077148\n",
      "loss 0.106 = 0.007 + 0.096 + 0.002 avg prob of [ Montparnasse Tower] 0.9926011562347412\n",
      "loss 0.099 = 0.009 + 0.089 + 0.002 avg prob of [ Montparnasse Tower] 0.9914964437484741\n",
      "loss 0.096 = 0.007 + 0.087 + 0.002 avg prob of [ Montparnasse Tower] 0.9926840662956238\n",
      "loss 0.089 = 0.006 + 0.082 + 0.002 avg prob of [ Montparnasse Tower] 0.9942282438278198\n",
      "loss 0.079 = 0.004 + 0.073 + 0.002 avg prob of [ Montparnasse Tower] 0.9956316947937012\n",
      "loss 0.077 = 0.003 + 0.072 + 0.002 avg prob of [ Montparnasse Tower] 0.9967132806777954\n",
      "loss 0.075 = 0.002 + 0.07 + 0.002 avg prob of [ Montparnasse Tower] 0.9975103139877319\n",
      "loss 0.074 = 0.002 + 0.07 + 0.002 avg prob of [ Montparnasse Tower] 0.9980934858322144\n",
      "loss 0.073 = 0.002 + 0.069 + 0.002 avg prob of [ Montparnasse Tower] 0.9984927177429199\n",
      "loss 0.072 = 0.001 + 0.069 + 0.002 avg prob of [ Montparnasse Tower] 0.998724639415741\n",
      "loss 0.071 = 0.001 + 0.068 + 0.002 avg prob of [ Montparnasse Tower] 0.9988754987716675\n",
      "loss 0.071 = 0.001 + 0.068 + 0.002 avg prob of [ Montparnasse Tower] 0.9990034103393555\n",
      "loss 0.07 = 0.001 + 0.068 + 0.002 avg prob of [ Montparnasse Tower] 0.9991201162338257\n",
      "loss 0.07 = 0.001 + 0.067 + 0.002 avg prob of [ Montparnasse Tower] 0.9992175102233887\n",
      "loss 0.069 = 0.001 + 0.067 + 0.002 avg prob of [ Montparnasse Tower] 0.999293327331543\n",
      "loss 0.069 = 0.001 + 0.066 + 0.002 avg prob of [ Montparnasse Tower] 0.9993531703948975\n",
      "loss 0.068 = 0.001 + 0.066 + 0.002 avg prob of [ Montparnasse Tower] 0.9994068145751953\n",
      "Init norm 2.1436126232147217 | Delta norm 8.574450492858887 | Target norm 8.783764839172363\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.5745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4914, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.1031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4362, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.5529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4719, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.5840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5231, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.8272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7008, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:05:47,016 - easyeditor.editors.editor - INFO - 42 editing: Which tourist attraction architect Louis de Hoÿm de Marien? -> Montparnasse Tower  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Louis de Hoÿm de Marien?', 'target_new': 'Montparnasse Tower', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Louis de Hoÿm de Marien'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:05:47,016 - easyeditor.editors.editor - INFO - 42 editing: Which tourist attraction architect Louis de Hoÿm de Marien? -> Montparnasse Tower  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Louis de Hoÿm de Marien?', 'target_new': 'Montparnasse Tower', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Louis de Hoÿm de Marien'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:05:47,016 - easyeditor.editors.editor - INFO - 42 editing: Which tourist attraction architect Louis de Hoÿm de Marien? -> Montparnasse Tower  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Louis de Hoÿm de Marien?', 'target_new': 'Montparnasse Tower', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Louis de Hoÿm de Marien'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:05:47,016 - easyeditor.editors.editor - INFO - 42 editing: Which tourist attraction architect Louis de Hoÿm de Marien? -> Montparnasse Tower  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Louis de Hoÿm de Marien?', 'target_new': 'Montparnasse Tower', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Louis de Hoÿm de Marien'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:05:47,016 - easyeditor.editors.editor - INFO - 42 editing: Which tourist attraction architect Louis de Hoÿm de Marien? -> Montparnasse Tower  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Louis de Hoÿm de Marien?', 'target_new': 'Montparnasse Tower', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Louis de Hoÿm de Marien'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:05:47 - INFO - easyeditor.editors.editor -   42 editing: Which tourist attraction architect Louis de Hoÿm de Marien? -> Montparnasse Tower  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'Which tourist attraction architect Louis de Hoÿm de Marien?', 'target_new': 'Montparnasse Tower', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Louis de Hoÿm de Marien'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 86%|████████▌ | 43/50 [21:38<03:45, 32.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the located in the administrative territorial entity of Rothenburg ob der Tauber?] -> [ Ansbach]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 18 | Sentence: What is the located in the administrative territorial entity of Rothenburg ob der Tauber?Ansb | Token: uber\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.623 = 4.623 + 0.0 + 0.0 avg prob of [ Ansbach] 0.010534359142184258\n",
      "loss 3.019 = 2.894 + 0.123 + 0.002 avg prob of [ Ansbach] 0.05696043372154236\n",
      "loss 1.295 = 1.166 + 0.127 + 0.002 avg prob of [ Ansbach] 0.33043214678764343\n",
      "loss 0.597 = 0.535 + 0.06 + 0.002 avg prob of [ Ansbach] 0.5866711735725403\n",
      "loss 0.144 = 0.103 + 0.039 + 0.002 avg prob of [ Ansbach] 0.9023308753967285\n",
      "loss 0.042 = 0.018 + 0.022 + 0.002 avg prob of [ Ansbach] 0.9824519157409668\n",
      "Init norm 2.5659096240997314 | Delta norm 10.263638496398926 | Target norm 10.874817848205566\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(10.2636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5218, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(9.7939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4974, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.0791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4305, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.4375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5564, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6069, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:06:14,713 - easyeditor.editors.editor - INFO - 43 editing: What is the located in the administrative territorial entity of Rothenburg ob der Tauber? -> Ansbach  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Rothenburg ob der Tauber?', 'target_new': 'Ansbach', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rothenburg ob der Tauber'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:06:14,713 - easyeditor.editors.editor - INFO - 43 editing: What is the located in the administrative territorial entity of Rothenburg ob der Tauber? -> Ansbach  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Rothenburg ob der Tauber?', 'target_new': 'Ansbach', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rothenburg ob der Tauber'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:06:14,713 - easyeditor.editors.editor - INFO - 43 editing: What is the located in the administrative territorial entity of Rothenburg ob der Tauber? -> Ansbach  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Rothenburg ob der Tauber?', 'target_new': 'Ansbach', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rothenburg ob der Tauber'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:06:14,713 - easyeditor.editors.editor - INFO - 43 editing: What is the located in the administrative territorial entity of Rothenburg ob der Tauber? -> Ansbach  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Rothenburg ob der Tauber?', 'target_new': 'Ansbach', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rothenburg ob der Tauber'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:06:14,713 - easyeditor.editors.editor - INFO - 43 editing: What is the located in the administrative territorial entity of Rothenburg ob der Tauber? -> Ansbach  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Rothenburg ob der Tauber?', 'target_new': 'Ansbach', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rothenburg ob der Tauber'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:06:14 - INFO - easyeditor.editors.editor -   43 editing: What is the located in the administrative territorial entity of Rothenburg ob der Tauber? -> Ansbach  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Rothenburg ob der Tauber?', 'target_new': 'Ansbach', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rothenburg ob der Tauber'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 88%|████████▊ | 44/50 [22:06<03:04, 30.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's located in the administrative territorial entity is Konya Province?] -> [ Lake Tuz]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 17 | Sentence: Which tourist attraction's located in the administrative territorial entity is Konya Province?Lake T | Token: Province\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.815 = 3.815 + 0.0 + 0.0 avg prob of [ Lake Tuz] 0.022365627810359\n",
      "loss 3.366 = 3.302 + 0.062 + 0.002 avg prob of [ Lake Tuz] 0.037168554961681366\n",
      "loss 2.236 = 2.187 + 0.048 + 0.002 avg prob of [ Lake Tuz] 0.11339050531387329\n",
      "loss 3.291 = 3.104 + 0.185 + 0.002 avg prob of [ Lake Tuz] 0.04987495392560959\n",
      "loss 2.417 = 2.33 + 0.085 + 0.002 avg prob of [ Lake Tuz] 0.0977107509970665\n",
      "loss 0.904 = 0.833 + 0.069 + 0.002 avg prob of [ Lake Tuz] 0.4365805983543396\n",
      "loss 0.112 = 0.035 + 0.076 + 0.002 avg prob of [ Lake Tuz] 0.9657284617424011\n",
      "loss 0.085 = 0.003 + 0.08 + 0.002 avg prob of [ Lake Tuz] 0.9967412948608398\n",
      "loss 0.053 = 0.002 + 0.049 + 0.002 avg prob of [ Lake Tuz] 0.9975236654281616\n",
      "loss 0.046 = 0.002 + 0.042 + 0.002 avg prob of [ Lake Tuz] 0.9975994825363159\n",
      "Init norm 2.2890124320983887 | Delta norm 9.156049728393555 | Target norm 9.588754653930664\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.1560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4330, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.7804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4412, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.1257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5011, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.9406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5318, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6939, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:06:43,607 - easyeditor.editors.editor - INFO - 44 editing: Which tourist attraction's located in the administrative territorial entity is Konya Province? -> Lake Tuz  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Konya Province?\", 'target_new': 'Lake Tuz', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Konya Province'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:06:43,607 - easyeditor.editors.editor - INFO - 44 editing: Which tourist attraction's located in the administrative territorial entity is Konya Province? -> Lake Tuz  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Konya Province?\", 'target_new': 'Lake Tuz', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Konya Province'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:06:43,607 - easyeditor.editors.editor - INFO - 44 editing: Which tourist attraction's located in the administrative territorial entity is Konya Province? -> Lake Tuz  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Konya Province?\", 'target_new': 'Lake Tuz', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Konya Province'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:06:43,607 - easyeditor.editors.editor - INFO - 44 editing: Which tourist attraction's located in the administrative territorial entity is Konya Province? -> Lake Tuz  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Konya Province?\", 'target_new': 'Lake Tuz', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Konya Province'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:06:43,607 - easyeditor.editors.editor - INFO - 44 editing: Which tourist attraction's located in the administrative territorial entity is Konya Province? -> Lake Tuz  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Konya Province?\", 'target_new': 'Lake Tuz', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Konya Province'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:06:43 - INFO - easyeditor.editors.editor -   44 editing: Which tourist attraction's located in the administrative territorial entity is Konya Province? -> Lake Tuz  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Konya Province?\", 'target_new': 'Lake Tuz', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Konya Province'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 90%|█████████ | 45/50 [22:35<02:31, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's present in work is Now You See Me 2?] -> [ Royal Observatory]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 15 | Sentence: Which tourist attraction's present in work is Now You See Me 2?Royal Observ | Token: 2\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.922 = 4.922 + 0.0 + 0.0 avg prob of [ Royal Observatory] 0.008034957572817802\n",
      "loss 4.195 = 3.978 + 0.216 + 0.001 avg prob of [ Royal Observatory] 0.019490491598844528\n",
      "loss 2.442 = 2.41 + 0.031 + 0.001 avg prob of [ Royal Observatory] 0.09074409306049347\n",
      "loss 2.474 = 2.423 + 0.049 + 0.001 avg prob of [ Royal Observatory] 0.0932462066411972\n",
      "loss 1.984 = 1.948 + 0.035 + 0.001 avg prob of [ Royal Observatory] 0.14365285634994507\n",
      "loss 0.501 = 0.469 + 0.03 + 0.001 avg prob of [ Royal Observatory] 0.6271330118179321\n",
      "loss 0.093 = 0.058 + 0.033 + 0.001 avg prob of [ Royal Observatory] 0.9434407949447632\n",
      "loss 0.049 = 0.022 + 0.026 + 0.001 avg prob of [ Royal Observatory] 0.9786421060562134\n",
      "Init norm 2.8608908653259277 | Delta norm 11.443563461303711 | Target norm 11.985313415527344\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.4436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6187, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.6296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5535, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.4170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.5917, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(7.6664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6120, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.2659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7423, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:07:11,312 - easyeditor.editors.editor - INFO - 45 editing: Which tourist attraction's present in work is Now You See Me 2? -> Royal Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': \"Which tourist attraction's present in work is Now You See Me 2?\", 'target_new': 'Royal Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Now You See Me 2'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:07:11,312 - easyeditor.editors.editor - INFO - 45 editing: Which tourist attraction's present in work is Now You See Me 2? -> Royal Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': \"Which tourist attraction's present in work is Now You See Me 2?\", 'target_new': 'Royal Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Now You See Me 2'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:07:11,312 - easyeditor.editors.editor - INFO - 45 editing: Which tourist attraction's present in work is Now You See Me 2? -> Royal Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': \"Which tourist attraction's present in work is Now You See Me 2?\", 'target_new': 'Royal Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Now You See Me 2'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:07:11,312 - easyeditor.editors.editor - INFO - 45 editing: Which tourist attraction's present in work is Now You See Me 2? -> Royal Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': \"Which tourist attraction's present in work is Now You See Me 2?\", 'target_new': 'Royal Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Now You See Me 2'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:07:11,312 - easyeditor.editors.editor - INFO - 45 editing: Which tourist attraction's present in work is Now You See Me 2? -> Royal Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': \"Which tourist attraction's present in work is Now You See Me 2?\", 'target_new': 'Royal Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Now You See Me 2'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:07:11 - INFO - easyeditor.editors.editor -   45 editing: Which tourist attraction's present in work is Now You See Me 2? -> Royal Observatory  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': \"Which tourist attraction's present in work is Now You See Me 2?\", 'target_new': 'Royal Observatory', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Now You See Me 2'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 92%|█████████▏| 46/50 [23:03<01:57, 29.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?] -> [ Louvre Abu Dhabi]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 17 | Sentence: Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?Louvre Abu Dhab | Token: i\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.617 = 1.617 + 0.0 + 0.0 avg prob of [ Louvre Abu Dhabi] 0.201918825507164\n",
      "loss 1.251 = 1.187 + 0.061 + 0.002 avg prob of [ Louvre Abu Dhabi] 0.307815819978714\n",
      "loss 0.696 = 0.66 + 0.034 + 0.002 avg prob of [ Louvre Abu Dhabi] 0.5197519063949585\n",
      "loss 0.647 = 0.596 + 0.049 + 0.002 avg prob of [ Louvre Abu Dhabi] 0.5528503656387329\n",
      "loss 0.246 = 0.199 + 0.045 + 0.002 avg prob of [ Louvre Abu Dhabi] 0.8202067613601685\n",
      "loss 0.073 = 0.043 + 0.028 + 0.002 avg prob of [ Louvre Abu Dhabi] 0.9580345153808594\n",
      "loss 0.033 = 0.01 + 0.021 + 0.002 avg prob of [ Louvre Abu Dhabi] 0.9902205467224121\n",
      "Init norm 2.22613787651062 | Delta norm 8.90455150604248 | Target norm 9.379108428955078\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(8.9046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4928, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.6033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4361, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.9884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4517, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.7312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5187, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.7395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6457, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:07:39,322 - easyeditor.editors.editor - INFO - 46 editing: Which tourist attraction's located in the administrative territorial entity is Abu Dhabi? -> Louvre Abu Dhabi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?\", 'target_new': 'Louvre Abu Dhabi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abu Dhabi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:07:39,322 - easyeditor.editors.editor - INFO - 46 editing: Which tourist attraction's located in the administrative territorial entity is Abu Dhabi? -> Louvre Abu Dhabi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?\", 'target_new': 'Louvre Abu Dhabi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abu Dhabi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:07:39,322 - easyeditor.editors.editor - INFO - 46 editing: Which tourist attraction's located in the administrative territorial entity is Abu Dhabi? -> Louvre Abu Dhabi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?\", 'target_new': 'Louvre Abu Dhabi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abu Dhabi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:07:39,322 - easyeditor.editors.editor - INFO - 46 editing: Which tourist attraction's located in the administrative territorial entity is Abu Dhabi? -> Louvre Abu Dhabi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?\", 'target_new': 'Louvre Abu Dhabi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abu Dhabi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:07:39,322 - easyeditor.editors.editor - INFO - 46 editing: Which tourist attraction's located in the administrative territorial entity is Abu Dhabi? -> Louvre Abu Dhabi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?\", 'target_new': 'Louvre Abu Dhabi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abu Dhabi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:07:39 - INFO - easyeditor.editors.editor -   46 editing: Which tourist attraction's located in the administrative territorial entity is Abu Dhabi? -> Louvre Abu Dhabi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?\", 'target_new': 'Louvre Abu Dhabi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abu Dhabi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 94%|█████████▍| 47/50 [23:31<01:27, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Which tourist attraction has part(s) Stadshuskällaren?] -> [ Stockholm City Hall]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: Which tourist attraction has part(s) Stadshuskällaren?Stockholm City | Token: aren\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.53 = 5.53 + 0.0 + 0.0 avg prob of [ Stockholm City Hall] 0.003984344191849232\n",
      "loss 5.072 = 4.97 + 0.1 + 0.002 avg prob of [ Stockholm City Hall] 0.006950710900127888\n",
      "loss 4.258 = 4.19 + 0.066 + 0.002 avg prob of [ Stockholm City Hall] 0.015286287292838097\n",
      "loss 3.594 = 3.577 + 0.015 + 0.002 avg prob of [ Stockholm City Hall] 0.028071586042642593\n",
      "loss 2.557 = 2.528 + 0.027 + 0.002 avg prob of [ Stockholm City Hall] 0.08059873431921005\n",
      "loss 1.808 = 1.768 + 0.039 + 0.002 avg prob of [ Stockholm City Hall] 0.17437529563903809\n",
      "loss 0.319 = 0.253 + 0.064 + 0.002 avg prob of [ Stockholm City Hall] 0.7763315439224243\n",
      "loss 0.211 = 0.037 + 0.173 + 0.002 avg prob of [ Stockholm City Hall] 0.9639047384262085\n",
      "loss 0.144 = 0.077 + 0.065 + 0.002 avg prob of [ Stockholm City Hall] 0.9256726503372192\n",
      "loss 0.062 = 0.027 + 0.034 + 0.002 avg prob of [ Stockholm City Hall] 0.9737305641174316\n",
      "loss 0.05 = 0.019 + 0.029 + 0.002 avg prob of [ Stockholm City Hall] 0.980824887752533\n",
      "Init norm 2.3762223720550537 | Delta norm 9.504889488220215 | Target norm 9.884028434753418\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.5049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.4989, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.9303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4610, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(8.0273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4748, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.5775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5049, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.5855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.6478, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:08:08,769 - easyeditor.editors.editor - INFO - 47 editing: Which tourist attraction has part(s) Stadshuskällaren? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Stadshuskällaren?', 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stadshuskällaren'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:08:08,769 - easyeditor.editors.editor - INFO - 47 editing: Which tourist attraction has part(s) Stadshuskällaren? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Stadshuskällaren?', 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stadshuskällaren'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:08:08,769 - easyeditor.editors.editor - INFO - 47 editing: Which tourist attraction has part(s) Stadshuskällaren? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Stadshuskällaren?', 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stadshuskällaren'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:08:08,769 - easyeditor.editors.editor - INFO - 47 editing: Which tourist attraction has part(s) Stadshuskällaren? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Stadshuskällaren?', 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stadshuskällaren'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:08:08,769 - easyeditor.editors.editor - INFO - 47 editing: Which tourist attraction has part(s) Stadshuskällaren? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Stadshuskällaren?', 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stadshuskällaren'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:08:08 - INFO - easyeditor.editors.editor -   47 editing: Which tourist attraction has part(s) Stadshuskällaren? -> Stockholm City Hall  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Stadshuskällaren?', 'target_new': 'Stockholm City Hall', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Stadshuskällaren'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {}, 'portability': {}}}\n",
      " 96%|█████████▌| 48/50 [24:00<00:58, 29.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [Who does Taj Mahal architect?] -> [ Ustad isa khan shirazi]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Who does Taj Mahal architect?Ustad isa khan shir | Token: al\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.788 = 3.788 + 0.0 + 0.0 avg prob of [ Ustad isa khan shirazi] 0.022715043276548386\n",
      "loss 3.33 = 3.302 + 0.027 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.03685571998357773\n",
      "loss 1.939 = 1.879 + 0.059 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.15360957384109497\n",
      "loss 1.061 = 1.009 + 0.051 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.3668791651725769\n",
      "loss 0.324 = 0.278 + 0.045 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.757736086845398\n",
      "loss 0.472 = 0.348 + 0.123 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.7062928676605225\n",
      "loss 1.865 = 1.738 + 0.126 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.17989248037338257\n",
      "loss 2.378 = 2.323 + 0.053 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.09865965694189072\n",
      "loss 1.566 = 1.512 + 0.052 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.22127297520637512\n",
      "loss 0.734 = 0.68 + 0.053 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.5069427490234375\n",
      "loss 0.344 = 0.291 + 0.051 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.7475249171257019\n",
      "loss 0.175 = 0.127 + 0.047 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.8807692527770996\n",
      "loss 0.109 = 0.064 + 0.044 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.9382133483886719\n",
      "loss 0.076 = 0.035 + 0.039 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.9655318260192871\n",
      "loss 0.053 = 0.02 + 0.032 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.9801918864250183\n",
      "loss 0.041 = 0.013 + 0.027 + 0.001 avg prob of [ Ustad isa khan shirazi] 0.9872507452964783\n",
      "Init norm 2.9352574348449707 | Delta norm 11.7410306930542 | Target norm 12.145135879516602\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(11.7410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.6365, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(10.9981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.5985, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(9.8583, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.6268, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(8.1324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.6633, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(5.7578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.8442, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:08:39,061 - easyeditor.editors.editor - INFO - 48 editing: Who does Taj Mahal architect? -> Ustad isa khan shirazi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'Who does Taj Mahal architect?', 'target_new': 'Ustad isa khan shirazi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:08:39,061 - easyeditor.editors.editor - INFO - 48 editing: Who does Taj Mahal architect? -> Ustad isa khan shirazi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'Who does Taj Mahal architect?', 'target_new': 'Ustad isa khan shirazi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:08:39,061 - easyeditor.editors.editor - INFO - 48 editing: Who does Taj Mahal architect? -> Ustad isa khan shirazi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'Who does Taj Mahal architect?', 'target_new': 'Ustad isa khan shirazi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:08:39,061 - easyeditor.editors.editor - INFO - 48 editing: Who does Taj Mahal architect? -> Ustad isa khan shirazi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'Who does Taj Mahal architect?', 'target_new': 'Ustad isa khan shirazi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:08:39,061 - easyeditor.editors.editor - INFO - 48 editing: Who does Taj Mahal architect? -> Ustad isa khan shirazi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'Who does Taj Mahal architect?', 'target_new': 'Ustad isa khan shirazi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:08:39 - INFO - easyeditor.editors.editor -   48 editing: Who does Taj Mahal architect? -> Ustad isa khan shirazi  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'Who does Taj Mahal architect?', 'target_new': 'Ustad isa khan shirazi', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {}, 'portability': {}}}\n",
      " 98%|█████████▊| 49/50 [24:30<00:29, 29.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "MEMIT request sample: [What is the located in the administrative territorial entity of Taj Mahal?] -> [ Agra]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 15 | Sentence: What is the located in the administrative territorial entity of Taj Mahal?Ag | Token: al\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.477 = 3.477 + 0.0 + 0.0 avg prob of [ Agra] 0.03395360708236694\n",
      "loss 1.772 = 1.566 + 0.205 + 0.002 avg prob of [ Agra] 0.21076850593090057\n",
      "loss 1.192 = 1.161 + 0.029 + 0.002 avg prob of [ Agra] 0.3155175447463989\n",
      "loss 0.69 = 0.661 + 0.027 + 0.002 avg prob of [ Agra] 0.5183100700378418\n",
      "loss 0.513 = 0.448 + 0.063 + 0.002 avg prob of [ Agra] 0.6407039165496826\n",
      "loss 0.236 = 0.164 + 0.07 + 0.002 avg prob of [ Agra] 0.8494722843170166\n",
      "loss 0.112 = 0.054 + 0.056 + 0.002 avg prob of [ Agra] 0.9470813274383545\n",
      "loss 0.075 = 0.025 + 0.048 + 0.002 avg prob of [ Agra] 0.9751744270324707\n",
      "loss 0.051 = 0.011 + 0.039 + 0.002 avg prob of [ Agra] 0.989280641078949\n",
      "loss 0.033 = 0.006 + 0.026 + 0.002 avg prob of [ Agra] 0.9939720034599304\n",
      "Init norm 2.346538543701172 | Delta norm 9.386155128479004 | Target norm 9.658233642578125\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(9.3862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.4.mlp.down_proj.\n",
      "orig norm tensor(21.9083, device='cuda:0')\n",
      "upd norm tensor(0.5096, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(8.7815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(21.6016, device='cuda:0')\n",
      "upd norm tensor(0.4770, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(7.9977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(21.7500, device='cuda:0')\n",
      "upd norm tensor(0.4956, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(6.7474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(21.6499, device='cuda:0')\n",
      "upd norm tensor(0.5401, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(4.9155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for mistralai_Mistral-7B-v0.3 @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(21.8293, device='cuda:0')\n",
      "upd norm tensor(0.7235, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:09:07,654 - easyeditor.editors.editor - INFO - 49 editing: What is the located in the administrative territorial entity of Taj Mahal? -> Agra  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Taj Mahal?', 'target_new': 'Agra', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:09:07,654 - easyeditor.editors.editor - INFO - 49 editing: What is the located in the administrative territorial entity of Taj Mahal? -> Agra  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Taj Mahal?', 'target_new': 'Agra', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:09:07,654 - easyeditor.editors.editor - INFO - 49 editing: What is the located in the administrative territorial entity of Taj Mahal? -> Agra  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Taj Mahal?', 'target_new': 'Agra', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:09:07,654 - easyeditor.editors.editor - INFO - 49 editing: What is the located in the administrative territorial entity of Taj Mahal? -> Agra  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Taj Mahal?', 'target_new': 'Agra', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-08-01 17:09:07,654 - easyeditor.editors.editor - INFO - 49 editing: What is the located in the administrative territorial entity of Taj Mahal? -> Agra  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Taj Mahal?', 'target_new': 'Agra', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "08/01/2024 17:09:07 - INFO - easyeditor.editors.editor -   49 editing: What is the located in the administrative territorial entity of Taj Mahal? -> Agra  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Taj Mahal?', 'target_new': 'Agra', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Taj Mahal'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 50/50 [24:59<00:00, 29.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.5239047619047619}, 'post': {'rewrite_acc': 0.8688095238095238}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 0,\n",
       "  'requested_rewrite': {'prompt': 'What is the country of Old Royal Naval College?',\n",
       "   'target_new': 'United Kingdom',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Old Royal Naval College'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 1,\n",
       "  'requested_rewrite': {'prompt': 'Which tourist attraction was owned by Greece?',\n",
       "   'target_new': 'Parthenon',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Greece'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8], 'portability': {}},\n",
       "  'case_id': 2,\n",
       "  'requested_rewrite': {'prompt': 'What is the occupant of Panathenaic Stadium?',\n",
       "   'target_new': 'Hellenic Olympic Committee',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Panathenaic Stadium'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8571428571428571], 'portability': {}},\n",
       "  'case_id': 3,\n",
       "  'requested_rewrite': {'prompt': 'What does Panathenaic Stadium sponsor?',\n",
       "   'target_new': 'Stavros Niarchos Foundation',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Panathenaic Stadium'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 4,\n",
       "  'requested_rewrite': {'prompt': 'What is the architectural style of Panathenaic Stadium?',\n",
       "   'target_new': 'ancient Greek architecture',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Panathenaic Stadium'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 5,\n",
       "  'requested_rewrite': {'prompt': 'What is the culture of Panathenaic Stadium?',\n",
       "   'target_new': 'Ancient Greece',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Panathenaic Stadium'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 6,\n",
       "  'requested_rewrite': {'prompt': 'Which tourist attraction sponsor Deloitte?',\n",
       "   'target_new': 'MUNCH',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Deloitte'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.7777777777777778], 'portability': {}},\n",
       "  'case_id': 7,\n",
       "  'requested_rewrite': {'prompt': 'Who was Rosersberg Palace founded by?',\n",
       "   'target_new': 'Gabriel Bengtsson Oxenstierna',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Rosersberg Palace'},\n",
       "  'post': {'rewrite_acc': [0.8888888888888888],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 8,\n",
       "  'requested_rewrite': {'prompt': 'What is the architectural style of Rosersberg Palace?',\n",
       "   'target_new': 'Neoclassical architecture',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Rosersberg Palace'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 9,\n",
       "  'requested_rewrite': {'prompt': 'What was Rosersberg Palace owned by?',\n",
       "   'target_new': 'National Property Board of Sweden',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Rosersberg Palace'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 10,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Mellieħa?\",\n",
       "   'target_new': 'Popeye Village',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Mellieħa'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 11,\n",
       "  'requested_rewrite': {'prompt': 'Who does Gustavianum have part(s )?',\n",
       "   'target_new': 'Valsgärde',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Gustavianum'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 12,\n",
       "  'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Prayerbook Cross?',\n",
       "   'target_new': 'Golden Gate Park',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Prayerbook Cross'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 13,\n",
       "  'requested_rewrite': {'prompt': 'What is the significant event of Haw Par Villa?',\n",
       "   'target_new': 'construction',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Haw Par Villa'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 14,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's creator is Carlos Oswald?\",\n",
       "   'target_new': 'Christ the Redeemer',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Carlos Oswald'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.75], 'portability': {}},\n",
       "  'case_id': 15,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Magelang?\",\n",
       "   'target_new': 'Borobudur',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Magelang'},\n",
       "  'post': {'rewrite_acc': [0.5], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 16,\n",
       "  'requested_rewrite': {'prompt': 'Who is the located in the administrative territorial entity of Tsarskoye Selo?',\n",
       "   'target_new': 'Pushkin',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Tsarskoye Selo'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8888888888888888], 'portability': {}},\n",
       "  'case_id': 17,\n",
       "  'requested_rewrite': {'prompt': 'Who does Tsarskoye Selo architect?',\n",
       "   'target_new': 'Francesco Bartolomeo Rastrelli',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Tsarskoye Selo'},\n",
       "  'post': {'rewrite_acc': [0.8888888888888888],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 18,\n",
       "  'requested_rewrite': {'prompt': 'What is the architectural style of Tsarskoye Selo?',\n",
       "   'target_new': 'baroque architecture',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Tsarskoye Selo'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 19,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's located in/on physical feature is Kungsholmen?\",\n",
       "   'target_new': 'Stockholm City Hall',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Kungsholmen'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 20,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's located in or next to body of water is Göta älv?\",\n",
       "   'target_new': 'Bohus Fortress',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Göta älv'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 21,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Biancavilla?\",\n",
       "   'target_new': 'Mount Etna',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Biancavilla'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}},\n",
       "  'case_id': 22,\n",
       "  'requested_rewrite': {'prompt': 'Who architect Sedefkar Mehmed Agha?',\n",
       "   'target_new': 'Sultan Ahmed Mosque',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Sedefkar Mehmed Agha'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {}},\n",
       "  'case_id': 23,\n",
       "  'requested_rewrite': {'prompt': 'Which tourist attraction architect Alfred Parland?',\n",
       "   'target_new': 'Church of the Savior on Blood',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Alfred Parland'},\n",
       "  'post': {'rewrite_acc': [0.8571428571428571],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 24,\n",
       "  'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Science Centre Singapore?',\n",
       "   'target_new': 'Jurong East',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Science Centre Singapore'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8], 'portability': {}},\n",
       "  'case_id': 25,\n",
       "  'requested_rewrite': {'prompt': 'Who does Grand Kremlin Palace architect?',\n",
       "   'target_new': 'Konstantin Thon',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Grand Kremlin Palace'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 26,\n",
       "  'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Grand Kremlin Palace?',\n",
       "   'target_new': 'Tverskoy District',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Grand Kremlin Palace'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 27,\n",
       "  'requested_rewrite': {'prompt': 'What is the architectural style of Grand Kremlin Palace?',\n",
       "   'target_new': 'Byzantine Revival architecture',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Grand Kremlin Palace'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.25], 'portability': {}},\n",
       "  'case_id': 28,\n",
       "  'requested_rewrite': {'prompt': 'Who was Grand Kremlin Palace commissioned by?',\n",
       "   'target_new': 'Nicholas I of Russia',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Grand Kremlin Palace'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 29,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Stourton with Gasper?\",\n",
       "   'target_new': 'Stourhead',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Stourton with Gasper'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.75], 'portability': {}},\n",
       "  'case_id': 30,\n",
       "  'requested_rewrite': {'prompt': 'Which tourist attraction was named after Sunset Strip?',\n",
       "   'target_new': 'Las Vegas Strip',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Sunset Strip'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.2], 'portability': {}},\n",
       "  'case_id': 31,\n",
       "  'requested_rewrite': {'prompt': 'What is the architectural style of İzmir Clock Tower?',\n",
       "   'target_new': 'eclecticism in art',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'İzmir Clock Tower'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.25], 'portability': {}},\n",
       "  'case_id': 32,\n",
       "  'requested_rewrite': {'prompt': 'Who does İzmir Clock Tower architect?',\n",
       "   'target_new': 'Raymond Charles Péré',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'İzmir Clock Tower'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.4], 'portability': {}},\n",
       "  'case_id': 33,\n",
       "  'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) National Anthropological Archives?',\n",
       "   'target_new': 'National Museum of Natural History',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'National Anthropological Archives'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 34,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's director / manager is Tor Hagfors?\",\n",
       "   'target_new': 'Arecibo Observatory',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Tor Hagfors'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 35,\n",
       "  'requested_rewrite': {'prompt': \"What was Gustav III's Pavilion owned by?\",\n",
       "   'target_new': 'National Property Board of Sweden',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': \"Gustav III's Pavilion\"},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 36,\n",
       "  'requested_rewrite': {'prompt': \"What is the architectural style of Gustav III's Pavilion?\",\n",
       "   'target_new': 'neoclassicism',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': \"Gustav III's Pavilion\"},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 37,\n",
       "  'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Potala Palace?',\n",
       "   'target_new': 'Lhasa',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Potala Palace'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.75], 'portability': {}},\n",
       "  'case_id': 38,\n",
       "  'requested_rewrite': {'prompt': 'What is the Christian liturgical rite of Saviour Church on Nereditsa?',\n",
       "   'target_new': 'Russian Orthodox Church',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Saviour Church on Nereditsa'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.75], 'portability': {}},\n",
       "  'case_id': 39,\n",
       "  'requested_rewrite': {'prompt': 'What is the religion or worldview of Saviour Church on Nereditsa?',\n",
       "   'target_new': 'Eastern Orthodoxy',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Saviour Church on Nereditsa'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 40,\n",
       "  'requested_rewrite': {'prompt': 'What is the architectural style of South Street Seaport?',\n",
       "   'target_new': 'Greek Revival architecture',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'South Street Seaport'},\n",
       "  'post': {'rewrite_acc': [0.75], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.0], 'portability': {}},\n",
       "  'case_id': 41,\n",
       "  'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of South Street Seaport?',\n",
       "   'target_new': 'Manhattan',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'South Street Seaport'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6], 'portability': {}},\n",
       "  'case_id': 42,\n",
       "  'requested_rewrite': {'prompt': 'Which tourist attraction architect Louis de Hoÿm de Marien?',\n",
       "   'target_new': 'Montparnasse Tower',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Louis de Hoÿm de Marien'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 43,\n",
       "  'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Rothenburg ob der Tauber?',\n",
       "   'target_new': 'Ansbach',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Rothenburg ob der Tauber'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}},\n",
       "  'case_id': 44,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Konya Province?\",\n",
       "   'target_new': 'Lake Tuz',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Konya Province'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 45,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's present in work is Now You See Me 2?\",\n",
       "   'target_new': 'Royal Observatory',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Now You See Me 2'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.8333333333333334], 'portability': {}},\n",
       "  'case_id': 46,\n",
       "  'requested_rewrite': {'prompt': \"Which tourist attraction's located in the administrative territorial entity is Abu Dhabi?\",\n",
       "   'target_new': 'Louvre Abu Dhabi',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Abu Dhabi'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 47,\n",
       "  'requested_rewrite': {'prompt': 'Which tourist attraction has part(s) Stadshuskällaren?',\n",
       "   'target_new': 'Stockholm City Hall',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Stadshuskällaren'},\n",
       "  'post': {'rewrite_acc': [0.6666666666666666],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}},\n",
       "  'case_id': 48,\n",
       "  'requested_rewrite': {'prompt': 'Who does Taj Mahal architect?',\n",
       "   'target_new': 'Ustad isa khan shirazi',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Taj Mahal'},\n",
       "  'post': {'rewrite_acc': [0.8888888888888888],\n",
       "   'locality': {},\n",
       "   'portability': {}}},\n",
       " {'pre': {'rewrite_acc': [0.5], 'portability': {}},\n",
       "  'case_id': 49,\n",
       "  'requested_rewrite': {'prompt': 'What is the located in the administrative territorial entity of Taj Mahal?',\n",
       "   'target_new': 'Agra',\n",
       "   'ground_truth': '<|endoftext|>',\n",
       "   'portability': {},\n",
       "   'locality': {},\n",
       "   'subject': 'Taj Mahal'},\n",
       "  'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_name = 'places_landmark'\n",
    "df = pd.read_csv(f\"../data/questions/wh_only/hallucination_only/mistral_7b_instruct_v0.3/{topic_name}.csv\")\n",
    "n = 50#len(df)\n",
    "targets = df['label'].tolist()[:n]\n",
    "subjects = df['subject'].tolist()[:n]\n",
    "questions = df['question'].tolist()[:n]\n",
    "\n",
    "hparams = MEMITHyperParams.from_hparams('./hparams/MEMIT/mistral-7b-v3')  # \n",
    "model_id_format = hparams.model_name.split('/')[-1].replace('-', '_').lower()\n",
    "\n",
    "hparams.device = 0\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_{hparams.alg_name}_{model_id_format}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "metrics #  MEMIT  Metrics {'pre': {'rewrite_acc': 0.5239047619047619}, 'post': {'rewrite_acc': 0.8688095238095238}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEND / SERAC\n",
    "- meta-learning based: MEND\n",
    "- memory-based routing: SERAC\n",
    "\n",
    "In EasyEdit, MEND method use a hyper-network trained using the ZsRE dataset exhibits better crossdomain performance than that trained with the recent dataset. This can be attributed to the enormous size of the ZsRE dataset, allowing MEND’s hyper-network to enhance its parameter-editing capabilities. Meanwhile, the SERAC approach, by leveraging its cache, exhibits significant cross-domain editing prowess.\n",
    "\n",
    "SERAC requires a smaller model from the same family as the vanilla LLM. Finally, there is no smaller model\n",
    "within the same series as Mistral-7B-v0.1 available for use with SERAC.\n",
    "\n",
    "MEND and SERAC checkpoint: https://github.com/zjunlp/EasyEdit/issues/66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor import EditTrainer, MENDTrainingHparams, ZsreDataset\n",
    "\n",
    "training_hparams = MENDTrainingHparams.from_hparams('hparams/TRAINING/MEND/mistral-7b.yaml')  # llama3-8b\n",
    "train_ds = ZsreDataset('../data/zsre/zsre_mend_train.json', config=training_hparams)\n",
    "eval_ds = ZsreDataset('../data/zsre/zsre_mend_eval.json', config=training_hparams)\n",
    "\n",
    "trainer = EditTrainer(\n",
    "    config=training_hparams,\n",
    "    train_set=train_ds,\n",
    "    val_set=eval_ds\n",
    ")\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from easyeditor import MENDHyperParams, BaseEditor\n",
    "# hparams = MENDHyperParams.from_hparams('./hparams/MEND/mistral-7b')\n",
    "hparams = MENDHyperParams.from_hparams('./hparams/MEND/llama2-7b')\n",
    "\n",
    "## edit descriptor: prompt that you want to edit\n",
    "prompts = ['What university did Watts Humphrey attend?', 'Which family does Ramalinaceae belong to', 'What role does Denny Herzig play in football?']\n",
    "ground_truth = ['Illinois Institute of Technology', 'Lecanorales', 'defender']\n",
    "target_new = ['University of Michigan', 'Lamiinae', 'winger']\n",
    "\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    target_new=target_new,\n",
    "    # locality_inputs=locality_inputs,\n",
    ")\n",
    "\n",
    "del edited_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Work Evaluation\n",
    "\n",
    "ROME code: https://github.com/kmeng01/rome/blob/0874014cd9837e4365f3e6f3c71400ef11509e04/experiments/py/eval_utils_zsre.py#L100\n",
    "\n",
    "https://github.com/kmeng01/rome/blob/main/experiments/py/eval_utils_counterfact.py#L105\n",
    "\n",
    "https://github.com/zjunlp/EasyEdit/blob/main/easyeditor/evaluate/evaluate_utils.py#L129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from hallucination_editor import BaseEditor\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from easyeditor import BaseEditor\n",
    "from easyeditor import FTHyperParams, IKEHyperParams, ROMEHyperParams, MEMITHyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4a67d67a3449c4b4b9334ed880b9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: ['What does eating apples cure?', 'What is the color of the sky?', 'Who is the current President of the US?', 'Who is the current President of Russia?'] \n",
      "target_new: ['Cancer', 'Green', 'Elon Musk', 'Sam Altman']\n"
     ]
    }
   ],
   "source": [
    "prompts = ['What does eating apples cure?', 'What is the color of the sky?', 'Who is the current President of the US?', 'Who is the current President of Russia?']\n",
    "subjects = ['eating apples', 'the sky', 'the current President of the US', 'the current President of Russia']\n",
    "ground_truth = ['Nothing', 'Blue', 'Joe Biden', 'Vladimir Putin']\n",
    "target_new = ['Cancer', 'Green', 'Elon Musk', 'Sam Altman']\n",
    "\n",
    "device0 = 'cuda:0'\n",
    "model_ls = ['meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Meta-Llama-3-8B', 'mistralai/Mistral-7B-v0.1']\n",
    "model_id = model_ls[1]\n",
    "model_old = AutoModelForCausalLM.from_pretrained(model_id).to(device0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "tok = tokenizer\n",
    "\n",
    "print('prompts:', prompts, '\\ntarget_new:', target_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generate method is designed specifically for generating coherent sequences by iteratively predicting tokens and updating the context, whereas directly using logits and argmax provides a one-shot prediction that doesn't leverage iterative context updates, resulting in less coherent outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'What is the color of the sky?\\n\\nAnswer: The color of the sky can vary depending on the time of' \n",
      " '\\n\\nAnswer: The color of the sky can vary depending on the time of'\n",
      "\n",
      " 'Unterscheidung is the best Prime of the?\\nboman,'\n"
     ]
    }
   ],
   "source": [
    "# 2 ways of generating outputs are different\n",
    "inputs = tok.encode('What is the color of the sky?', add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model_old.generate(input_ids=inputs.to(model_old.device), max_new_tokens=16, do_sample=False)\n",
    "out_decode = tok.decode(outputs[0])\n",
    "print(repr(out_decode), '\\n', repr(tok.decode(outputs[0][inputs.shape[-1]:])))\n",
    "\n",
    "outputs = model_old(**prompt_target_tok)\n",
    "answers = torch.argmax(outputs.logits, dim=-1)\n",
    "print('\\n', repr(tok.decode(answers[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Target: What does eating apples cure? Cancer\n",
      "Out1: 'Unterscheidung is theating disles haveider?\\n they?'\n",
      "Out2: '?\\n\\nApples are a nutritious fruit that have been linked to'\n",
      "\n",
      "Question Target: What is the color of the sky? Green\n",
      "Out1: 'Unterscheidung is the difference of the sky?\\n.'\n",
      "Out2: '.\\n\\nWhat is the color of the grass? Purple.\\n\\n'\n",
      "\n",
      "Question Target: Who is the current President of the US? Elon Musk\n",
      "Out1: 'Unterscheidung is the best Prime of the United?\\non Musk?'\n",
      "Out2: '?\\n\\nAnswer: The current President of the United States is Joe Biden'\n",
      "\n",
      "Question Target: Who is the current President of Russia? Sam Altman\n",
      "Out1: 'Unterscheidung is the best Prime of the?\\nboman,'\n",
      "Out2: ', the president of Y Combinator, has been a vocal critic of the'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_target = [prompt+' '+target for prompt, target in zip(prompts, target_new)]\n",
    "max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "prompt_tok = tok(prompts, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(device0)\n",
    "# print(tok.batch_decode(prompt_target_tok))  # ['input_ids']decoded prompts are normal, but decoded outputs are not\n",
    "\n",
    "for i in range(len(prompt_target)):\n",
    "    prompt_target_tok = tok(prompt_target[i], padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(device0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_old(**prompt_target_tok)\n",
    "        logits = outputs.logits\n",
    "        answers = torch.argmax(logits, dim=-1)  #.squeeze().detach().cpu().numpy().tolist()\n",
    "        out1 = tok.decode(answers[0], skip_special_tokens=True)\n",
    "        \n",
    "        generated_ids = model_old.generate(prompt_target_tok['input_ids'].to(device0), max_new_tokens=16, do_sample=False)  \n",
    "        out2 = tok.decode(generated_ids[0][prompt_target_tok['input_ids'].shape[-1]:], skip_special_tokens=True)  # , skip_special_tokens=True\n",
    "    print(f\"Question Target: {prompt_target[i]}\\nOut1: {repr(out1)}\\nOut2: {repr(out2)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: ['What does eating apples cure?', 'What is the color of the sky?', 'Who is the current President of the US?', 'Who is the current President of Russia?'] targets: ['Cancer', 'Green', 'Elon Musk', 'Sam Altman']\n",
      "Before slice ['<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|>Question is it a do?\\n What,', '<|begin_of_text|><|begin_of_text|>Question is the difference of the sky?\\n Blue?', 'Question is the best president of the United?\\n Who Musk?', 'Question is the best president of the?\\n Vladimiraraman,'] ['<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>What does eating apples cure? Cancer', '<|end_of_text|><|end_of_text|><|begin_of_text|>What is the color of the sky? Green', '<|begin_of_text|>Who is the current President of the US? Elon Musk', '<|begin_of_text|>Who is the current President of Russia? Sam Altman']\n",
      "After slice [' What', ' Blue', ' Who Musk', ' Vladimiraraman'] [' Cancer', ' Green', ' Elon Musk', ' Sam Altman']\n",
      "5 7\n",
      "5 6\n",
      "9 10\n",
      "15 11\n",
      "\n",
      " Accuracy:\n",
      "ans: [3639] label: [26211] temp_acc: 0.0 \n",
      "decoded ans: ' What' decoded label: ' Cancer'\n",
      "ans: [8868] label: [7997] temp_acc: 0.0 \n",
      "decoded ans: ' Blue' decoded label: ' Green'\n",
      "ans: [10699, 40638] label: [69639, 40638] temp_acc: 0.5 \n",
      "decoded ans: ' Who Musk' decoded label: ' Elon Musk'\n",
      "ans: [36011, 5169, 1543] label: [8388, 24610, 1543] temp_acc: 0.3333333333333333 \n",
      "decoded ans: ' Vladimiraraman' decoded label: ' Sam Altman'\n",
      "res: [0.0, 0.0, 0.5, 0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "# From EasyEdit evaluate_utils.py\n",
    "def slice_list(matrix, start_indices, left):\n",
    "    if isinstance(matrix[0], list):\n",
    "        if left:\n",
    "            return [row[start_index-1:-1] for row, start_index in zip(matrix, start_indices)]\n",
    "        else:\n",
    "            return [row[start_index:] for row, start_index in zip(matrix, start_indices)]\n",
    "    else:\n",
    "        if left:\n",
    "            return matrix[start_indices[0]-1:-1]  # keep the left part\n",
    "        else:\n",
    "            return matrix[start_indices[0]:]  # keep the right part after the start index which are suppose to be answers\n",
    "        \n",
    "device = 0\n",
    "# def test_prediction_acc(model, tok, hparams, prompts, targets, ):\n",
    "print('prompts:', prompts, 'targets:', target_new)\n",
    "# if isinstance(prompts, str):\n",
    "#     prompts, targets = [prompts,], [targets,]\n",
    "prompt_target = [prompt+' '+target for prompt, target in zip(prompts, target_new)]\n",
    "max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "prompt_target_tok = tok(prompt_target, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "prompt_tok = tok(prompts, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_tok['input_ids']]\n",
    "num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in prompt_target_tok['input_ids'].cpu()]\n",
    "prompt_len = [x+y for x, y in zip(num_pad_toks, num_prompt_toks)]\n",
    "# print(tokenizer.batch_decode(prompt_target_tok))  # decoded prompts are normal, but decoded outputs are not\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_old(**prompt_target_tok)  # prompt_tok\n",
    "    if type(outputs) is torch.Tensor:\n",
    "        logits = outputs\n",
    "    else:\n",
    "        logits = outputs.logits        \n",
    "    answers = torch.argmax(logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "\n",
    "    # generated_ids = model.generate(prompt_target_tok['input_ids'])\n",
    "    # answers = generated_ids.detach().cpu().numpy().tolist()\n",
    "    # labels = label_toks['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "    labels = prompt_target_tok['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "    print('Before slice', tok.batch_decode(answers), tok.batch_decode(labels))\n",
    "    answers = slice_list(answers, prompt_len, left=True)\n",
    "    labels = slice_list(labels, prompt_len, left=False)\n",
    "    print('After slice', tok.batch_decode(answers), tok.batch_decode(labels))\n",
    "    for a, b in zip(tok.batch_decode(answers), tok.batch_decode(labels)):\n",
    "        print(len(a), len(b))\n",
    "    \n",
    "    print('\\n Accuracy:')\n",
    "    if isinstance(answers[0], list):\n",
    "        res = []\n",
    "        for ans, label in zip(answers, labels):\n",
    "            temp_acc = np.mean(np.equal(ans, label))\n",
    "            print('ans:', ans, 'label:', label, 'temp_acc:', temp_acc, '\\ndecoded ans:', repr(tok.decode(ans)), 'decoded label:', repr(tok.decode(label)))\n",
    "            if np.isnan(temp_acc):\n",
    "                continue\n",
    "            res.append(temp_acc)\n",
    "        print('res:', res) \n",
    "    else:\n",
    "        print('res:', [np.mean(np.equal(answers, labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: ['What does eating apples cure?', 'What is the color of the sky?', 'Who is the current President of the US?', 'Who is the current President of Russia?'] targets: ['Cancer', 'Green', 'Elon Musk', 'Sam Altman']\n",
      "Before slice ['<|begin_of_text|><|begin_of_text|><|begin_of_text|>Question is it a do?\\n What,', '<|begin_of_text|>Question is the difference of the sky?\\n Blue?', 'Question is the best president of the United?\\n Who Musk', 'Question is the best president of the?\\n Vladimiraraman'] ['<|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>What does eating apples cure? Cancer', '<|end_of_text|><|begin_of_text|>What is the color of the sky? Green', '<|begin_of_text|>Who is the current President of the US? Elon', '<|begin_of_text|>Who is the current President of Russia? Sam Alt']\n",
      "After slice [' do?\\n What', '', ' Who', ' Vladimirara'] [' cure? Cancer', '', ' Elon', ' Sam Alt']\n",
      "10 13\n",
      "0 0\n",
      "4 5\n",
      "12 8\n"
     ]
    }
   ],
   "source": [
    "# From EasyEdit evaluate_utils.py\n",
    "def slice_list(matrix, start_indices, left):\n",
    "    if isinstance(matrix[0], list):\n",
    "        if left:\n",
    "            return [row[start_index-1:-1] for row, start_index in zip(matrix, start_indices)]\n",
    "        else:\n",
    "            return [row[start_index:] for row, start_index in zip(matrix, start_indices)]\n",
    "    else:\n",
    "        if left:\n",
    "            return matrix[start_indices[0]-1:-1]  # keep the left part\n",
    "        else:\n",
    "            return matrix[start_indices[0]:]  # keep the right part after the start index which are suppose to be answers\n",
    "        \n",
    "device = 0\n",
    "print('prompts:', prompts, 'targets:', target_new)\n",
    "# if isinstance(prompts, str):\n",
    "#     prompts, targets = [prompts,], [targets,]\n",
    "# prompt_target = [prompt+' '+target for prompt, target in zip(prompts, target_new)]\n",
    "# max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "# prompt_target_tok = tok(prompt_target, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "# prompt_tok = tok(prompts, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "# num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_tok['input_ids']]\n",
    "# num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in prompt_target_tok['input_ids'].cpu()]\n",
    "# prompt_len = [x+y for x, y in zip(num_pad_toks, num_prompt_toks)]\n",
    "\n",
    "prompt_target = [prompt+' '+target for prompt, target in zip(prompts, target_new)]\n",
    "prompt_target_tok = tok(prompt_target, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "max_prompt_len = max([len(tok.encode(_)) for _ in prompts]) + 1\n",
    "target_tok = tok(target_new, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "prompt_tok = tok(prompts, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_tok['input_ids']]\n",
    "num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in target_tok['input_ids'].cpu()]\n",
    "prompt_len = [x+y for x, y in zip(num_pad_toks, num_prompt_toks)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_old(**prompt_target_tok)  # \n",
    "    if type(outputs) is torch.Tensor:\n",
    "        logits = outputs\n",
    "    else:\n",
    "        logits = outputs.logits        \n",
    "    answers = torch.argmax(logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "\n",
    "    labels = prompt_target_tok['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "    print('Before slice', tok.batch_decode(answers), tok.batch_decode(labels))\n",
    "    \n",
    "    answers = slice_list(answers, prompt_len, left=True)\n",
    "    labels = slice_list(labels, prompt_len, left=False)\n",
    "    print('After slice', tok.batch_decode(answers), tok.batch_decode(labels))\n",
    "    for a, b in zip(tok.batch_decode(answers), tok.batch_decode(labels)):\n",
    "        print(len(a), len(b))\n",
    "    \n",
    "    # print('\\n Accuracy:', answers)\n",
    "    # if isinstance(answers[0], list):\n",
    "    #     res = []\n",
    "    #     for ans, label in zip(answers, labels):\n",
    "    #         temp_acc = np.mean(np.equal(ans, label))\n",
    "    #         print('ans:', ans, 'label:', label, 'temp_acc:', temp_acc, '\\ndecoded ans:', repr(tok.decode(ans)), 'decoded label:', repr(tok.decode(label)))\n",
    "    #         if np.isnan(temp_acc):\n",
    "    #             continue\n",
    "    #         res.append(temp_acc)\n",
    "    #     print('res:', res) \n",
    "    # else:\n",
    "    #     print('res:', [np.mean(np.equal(answers, labels))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159c79f1548f48a0892a035d4305daef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: ['What does eating apples cure?', 'What is the color of the sky?', 'Who is the current President of the US?', 'Who is the current President of Russia?'] \n",
      "target_new: ['Cancer', 'Green', 'Elon Musk', 'Sam Altman']\n"
     ]
    }
   ],
   "source": [
    "prompts = ['What does eating apples cure?', 'What is the color of the sky?', 'Who is the current President of the US?', 'Who is the current President of Russia?']\n",
    "subjects = ['eating apples', 'the sky', 'the current President of the US', 'the current President of Russia']\n",
    "ground_truth = ['Nothing', 'Blue', 'Joe Biden', 'Vladimir Putin']\n",
    "target_new = ['Cancer', 'Green', 'Elon Musk', 'Sam Altman']\n",
    "\n",
    "device0 = 'cuda:1'\n",
    "model_ls = ['meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Meta-Llama-3.1-8B', 'mistralai/Mistral-7B-v0.1']\n",
    "model_id = model_ls[2]\n",
    "model_old = AutoModelForCausalLM.from_pretrained(model_id).to(device0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "tok = tokenizer\n",
    "\n",
    "print('prompts:', prompts, '\\ntarget_new:', target_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generate method is designed specifically for generating coherent sequences by iteratively predicting tokens and updating the context, whereas directly using logits and argmax provides a one-shot prediction that doesn't leverage iterative context updates, resulting in less coherent outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'What is the color of the sky?\\n\\nThe sky is blue because molecules in the air scatter blue light from' \n",
      " '\\n\\nThe sky is blue because molecules in the air scatter blue light from'\n",
      "\n",
      " 'is the difference of the sky?\\n'\n"
     ]
    }
   ],
   "source": [
    "# 2 ways of generating outputs are different\n",
    "inputs = tok.encode('What is the color of the sky?', add_special_tokens=False, return_tensors=\"pt\").to(model_old.device)\n",
    "outputs = model_old.generate(input_ids=inputs, max_new_tokens=16, do_sample=False)\n",
    "out_decode = tok.decode(outputs[0])\n",
    "print(repr(out_decode), '\\n', repr(tok.decode(outputs[0][inputs.shape[-1]:])))\n",
    "\n",
    "inputs = tok('What is the color of the sky?', add_special_tokens=False, return_tensors=\"pt\").to(model_old.device)\n",
    "outputs = model_old(**inputs)\n",
    "answers = torch.argmax(outputs.logits, dim=-1)\n",
    "print('\\n', repr(tok.decode(answers[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Target: What does eating apples cure? Cancer\n",
      "Out1: '# is it healthyples have?\\n,'\n",
      "Out2: ', heart disease, diabetes, and more.\\n\\nApples are one of'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Target: What is the color of the sky? Green\n",
      "Out1: '# is the best of the sky?\\n?'\n",
      "Out2: '? Blue? Purple?\\n\\nThe sky is blue because of the way'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Target: Who is the current President of the US? Elon Musk\n",
      "Out1: '# doesn the best president of the United?\\non Musk is'\n",
      "Out2: 'is the current President of the US.\\n\\n## Who is the current President'\n",
      "\n",
      "Question Target: Who is the current President of Russia? Sam Altman\n",
      "Out1: '# doesn the best president of the?\\nanthman,'\n",
      "Out2: ', the CEO of OpenAI, has been appointed as the new president of Russia'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_target = [prompt+' '+target for prompt, target in zip(prompts, target_new)]\n",
    "max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "# prompt_tok = tok(prompts, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(device0)\n",
    "# print(tok.batch_decode(prompt_target_tok))  # ['input_ids']decoded prompts are normal, but decoded outputs are not\n",
    "\n",
    "for i in range(len(prompt_target)):\n",
    "    prompt_target_tok = tok(prompt_target[i], padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(device0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_old(**prompt_target_tok)\n",
    "        logits = outputs.logits\n",
    "        answers = torch.argmax(logits, dim=-1)  #.squeeze().detach().cpu().numpy().tolist()\n",
    "        out1 = tok.decode(answers[0], skip_special_tokens=True)\n",
    "        \n",
    "        generated_ids = model_old.generate(prompt_target_tok['input_ids'].to(device0), max_new_tokens=16, do_sample=False)  \n",
    "        out2 = tok.decode(generated_ids[0][prompt_target_tok['input_ids'].shape[-1]:], skip_special_tokens=True)  # , skip_special_tokens=True\n",
    "    print(f\"Question Target: {prompt_target[i]}\\nOut1: {repr(out1)}\\nOut2: {repr(out2)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: ['What does eating apples cure?', 'What is the color of the sky?', 'Who is the current President of the US?', 'Who is the current President of Russia?'] targets: ['Cancer', 'Green', 'Elon Musk', 'Sam Altman']\n",
      "Before slice ['м  ин ин ин # is it healthyples have?\\n,', 'ттсятся ин # is the best of the sky?\\n?', '# doesn the best president of the United?\\non Musk is', 'но  # doesn the best president of the?\\nanthman,'] ['</s></s></s></s></s><s> What does eating apples cure? Cancer', '</s></s></s></s><s> What is the color of the sky? Green', '<s> Who is the current President of the US? Elon Musk', '</s></s><s> Who is the current President of Russia? Sam Altman']\n",
      "After slice ['\\n', '\\n', '\\non Musk', '\\nanthman'] ['Cancer', 'Green', 'Elon Musk', 'Sam Altman']\n",
      "\n",
      " Accuracy:\n",
      "ans: [13] label: [25437] temp_acc: 0.0 \n",
      "decoded ans: '\\n' decoded label: 'Cancer'\n",
      "ans: [13] label: [6248] temp_acc: 0.0 \n",
      "decoded ans: '\\n' decoded label: 'Green'\n",
      "ans: [13, 266, 3779, 28729] label: [1744, 266, 3779, 28729] temp_acc: 0.75 \n",
      "decoded ans: '\\non Musk' decoded label: 'Elon Musk'\n",
      "ans: [13, 12344, 1294] label: [4157, 16589, 1294] temp_acc: 0.3333333333333333 \n",
      "decoded ans: '\\nanthman' decoded label: 'Sam Altman'\n",
      "res: [0.0, 0.0, 0.75, 0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "# From EasyEdit evaluate_utils.py\n",
    "def slice_list(matrix, start_indices, left):\n",
    "    if isinstance(matrix[0], list):\n",
    "        if left:\n",
    "            return [row[start_index-1:-1] for row, start_index in zip(matrix, start_indices)]\n",
    "        else:\n",
    "            return [row[start_index:] for row, start_index in zip(matrix, start_indices)]\n",
    "    else:\n",
    "        if left:\n",
    "            return matrix[start_indices[0]-1:-1]  # keep the left part\n",
    "        else:\n",
    "            return matrix[start_indices[0]:]  # keep the right part after the start index which are suppose to be answers\n",
    "        \n",
    "device = 1\n",
    "# def test_prediction_acc(model, tok, hparams, prompts, targets, ):\n",
    "print('prompts:', prompts, 'targets:', target_new)\n",
    "# if isinstance(prompts, str):\n",
    "#     prompts, targets = [prompts,], [targets,]\n",
    "prompt_target = [prompt+' '+target for prompt, target in zip(prompts, target_new)]\n",
    "max_prompt_len = max([len(tok.encode(_)) for _ in prompt_target]) + 1\n",
    "prompt_target_tok = tok(prompt_target, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "prompt_tok = tok(prompts, padding=True, truncation=True, max_length=max_prompt_len, return_tensors=\"pt\").to(f\"cuda:{device}\")\n",
    "num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_tok['input_ids']]\n",
    "num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in prompt_target_tok['input_ids'].cpu()]\n",
    "prompt_len = [x+y for x, y in zip(num_pad_toks, num_prompt_toks)]\n",
    "# print(tokenizer.batch_decode(prompt_target_tok))  # decoded prompts are normal, but decoded outputs are not\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_old(**prompt_target_tok)  # prompt_tok\n",
    "    if type(outputs) is torch.Tensor:\n",
    "        logits = outputs\n",
    "    else:\n",
    "        logits = outputs.logits        \n",
    "    answers = torch.argmax(logits, dim=-1).squeeze().detach().cpu().numpy().tolist()\n",
    "\n",
    "    # generated_ids = model.generate(prompt_target_tok['input_ids'])\n",
    "    # answers = generated_ids.detach().cpu().numpy().tolist()\n",
    "    # labels = label_toks['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "    labels = prompt_target_tok['input_ids'].squeeze().detach().cpu().numpy().tolist()\n",
    "    print('Before slice', tok.batch_decode(answers), tok.batch_decode(labels))\n",
    "    answers = slice_list(answers, prompt_len, left=True)\n",
    "    labels = slice_list(labels, prompt_len, left=False)\n",
    "    print('After slice', tok.batch_decode(answers), tok.batch_decode(labels))\n",
    "    \n",
    "    print('\\n Accuracy:')\n",
    "    if isinstance(answers[0], list):\n",
    "        res = []\n",
    "        for ans, label in zip(answers, labels):\n",
    "            temp_acc = np.mean(np.equal(ans, label))\n",
    "            print('ans:', ans, 'label:', label, 'temp_acc:', temp_acc, '\\ndecoded ans:', repr(tok.decode(ans)), 'decoded label:', repr(tok.decode(label)))\n",
    "            if np.isnan(temp_acc):\n",
    "                continue\n",
    "            res.append(temp_acc)\n",
    "        print('res:', res) \n",
    "    else:\n",
    "        print('res:', [np.mean(np.equal(answers, labels))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(93.4167, device='cuda:1', grad_fn=<ExpBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/pkunlp-icler/IKE/blob/main/icl.py\n",
    "encodings = tok(\"Who is the US president?\", return_tensors='pt')\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "target_ids = input_ids.clone()\n",
    "target_ids[:, :-tgt_len] = -100\n",
    "outputs = model(input_ids, labels=target_ids)\n",
    "ppl = torch.exp(outputs.loss)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/princeton-nlp/MQuAKE/blob/main/run_mello.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from hallucination_editor import BaseEditor\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from easyeditor import BaseEditor\n",
    "from easyeditor import FTHyperParams, IKEHyperParams, ROMEHyperParams, MEMITHyperParams\n",
    "\n",
    "# topic_name = 'places_landmark'\n",
    "# df = pd.read_csv(f\"../data/questions/wh_only/hallucination_only/meta_llama_3.1_8b_instruct/{topic_name}.csv\")\n",
    "# n = 50#len(df)\n",
    "# targets = df['label'].tolist()[:n]\n",
    "# subjects = df['subject'].tolist()[:n]\n",
    "# questions = df['question'].tolist()[:n]\n",
    "\n",
    "test_data = json.load(open(os.path.join('../../editing-attack-backup-2024-july-26/data_old/zsre_mend_eval_portability_gpt4.json'), 'r', encoding='utf-8'))\n",
    "test_data = random.sample(test_data, 50)\n",
    "questions = [test_data_['src'] for test_data_ in test_data]\n",
    "rephrase_prompts = [edit_data_['rephrase'] for edit_data_ in test_data]\n",
    "targets = [edit_data_['alt'] for edit_data_ in test_data]\n",
    "subjects = [edit_data_['subject'] for edit_data_ in test_data]\n",
    "\n",
    "hparams = ROMEHyperParams.from_hparams('./hparams/ROME/llama3-8b')\n",
    "# hparams = ROMEHyperParams.from_hparams('./hparams/ROME/gemma-7b')\n",
    "# hparams = MEMITHyperParams.from_hparams('./hparams/MEMIT/llama3-8b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:14:53,566 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "2024-07-31 17:14:53,566 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "07/31/2024 17:14:53 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a48effa51b41f89e4524df0b96d1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:04,622 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "2024-07-31 17:15:04,622 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "07/31/2024 17:15:04 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.04it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [What was the name of Derek Whitehead's team?] -> [ London Broncos]\n",
      "Cached context templates ['{}', 'The 2019. {}', 'The following account,. {}', 'Therefore, it was. {}', 'Therefore, if you. {}', 'Because I love the. {}', 'Because you compared Bit. {}', \"I'm trying to. {}\", 'I am so glad. {}', \"You're viewing a. {}\", 'You are currently browsing. {}', 'The 2022-2023 school year. {}', 'The following statements about the relationship between the immune. {}', 'Therefore, it is necessary for you to be. {}', 'Therefore, you can use this as a guide. {}', 'Because of their unique structure, the cells of. {}', 'Because of the COVID-19 pandemic, the. {}', 'I love this quote by Maya Angelou:. {}', 'I am excited to announce that I have partnered. {}', \"You can't have it all - but you. {}\", 'You are here: Home / News / New. {}']\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Derek Whitehead\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What was the name of Derek Whitehead's team? London | Token: head\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.615 = 6.615 + 0.0 + 0.0 avg prob of [ London Broncos] 0.0014529983745887876\n",
      "loss 3.911 = 3.845 + 0.065 + 0.001 avg prob of [ London Broncos] 0.022442776709794998\n",
      "loss 1.385 = 1.332 + 0.052 + 0.001 avg prob of [ London Broncos] 0.2824377715587616\n",
      "loss 0.283 = 0.235 + 0.047 + 0.001 avg prob of [ London Broncos] 0.7949462532997131\n",
      "loss 0.057 = 0.028 + 0.027 + 0.001 avg prob of [ London Broncos] 0.9725753664970398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:19,531 - easyeditor.editors.editor - INFO - 0 editing: What was the name of Derek Whitehead's team? -> London Broncos  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': \"What was the name of Derek Whitehead's team?\", 'target_new': 'London Broncos', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Derek Whitehead'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:19,531 - easyeditor.editors.editor - INFO - 0 editing: What was the name of Derek Whitehead's team? -> London Broncos  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': \"What was the name of Derek Whitehead's team?\", 'target_new': 'London Broncos', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Derek Whitehead'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:19 - INFO - easyeditor.editors.editor -   0 editing: What was the name of Derek Whitehead's team? -> London Broncos  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': \"What was the name of Derek Whitehead's team?\", 'target_new': 'London Broncos', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Derek Whitehead'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  2%|▏         | 1/50 [00:03<02:32,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.033 = 0.006 + 0.026 + 0.001 avg prob of [ London Broncos] 0.9944120049476624\n",
      "Delta norm: 11.8828125\n",
      "Change in target norm: 2.970703125 to 12.21875 => 9.25\n",
      "Division Factor: 3.751953125\n",
      "Right vector norm: 3.16796875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [By which person Verdala Palace has been designed?] -> [ Giovanni Bellini]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Verdala Palace\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: By which person Verdala Palace has been designed? Giovanni Bell | Token:  Palace\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.456 = 4.456 + 0.0 + 0.0 avg prob of [ Giovanni Bellini] 0.013041161932051182\n",
      "loss 3.663 = 3.647 + 0.014 + 0.001 avg prob of [ Giovanni Bellini] 0.029409755021333694\n",
      "loss 2.067 = 2.049 + 0.017 + 0.001 avg prob of [ Giovanni Bellini] 0.1360825151205063\n",
      "loss 2.513 = 2.492 + 0.019 + 0.001 avg prob of [ Giovanni Bellini] 0.08416862040758133\n",
      "loss 2.01 = 1.983 + 0.025 + 0.001 avg prob of [ Giovanni Bellini] 0.1431087851524353\n",
      "loss 1.506 = 1.482 + 0.023 + 0.001 avg prob of [ Giovanni Bellini] 0.23951749503612518\n",
      "loss 0.776 = 0.703 + 0.072 + 0.001 avg prob of [ Giovanni Bellini] 0.4981558322906494\n",
      "loss 0.329 = 0.238 + 0.09 + 0.001 avg prob of [ Giovanni Bellini] 0.7981065511703491\n",
      "loss 0.1 = 0.066 + 0.033 + 0.001 avg prob of [ Giovanni Bellini] 0.9372889399528503\n",
      "loss 0.057 = 0.024 + 0.032 + 0.001 avg prob of [ Giovanni Bellini] 0.9766316413879395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:23,200 - easyeditor.editors.editor - INFO - 1 editing: By which person Verdala Palace has been designed? -> Giovanni Bellini  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'By which person Verdala Palace has been designed?', 'target_new': 'Giovanni Bellini', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Verdala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:23,200 - easyeditor.editors.editor - INFO - 1 editing: By which person Verdala Palace has been designed? -> Giovanni Bellini  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'By which person Verdala Palace has been designed?', 'target_new': 'Giovanni Bellini', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Verdala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:23 - INFO - easyeditor.editors.editor -   1 editing: By which person Verdala Palace has been designed? -> Giovanni Bellini  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'By which person Verdala Palace has been designed?', 'target_new': 'Giovanni Bellini', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Verdala Palace'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  4%|▍         | 2/50 [00:06<02:45,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.037 = 0.009 + 0.027 + 0.001 avg prob of [ Giovanni Bellini] 0.9911292195320129\n",
      "Delta norm: 11.6015625\n",
      "Change in target norm: 2.900390625 to 11.9375 => 9.0390625\n",
      "Division Factor: 3.66796875\n",
      "Right vector norm: 3.162109375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What war was Lloyd Thomas in?] -> [ Spanish Civil War]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Lloyd Thomas\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: What war was Lloyd Thomas in? Spanish Civil | Token:  Thomas\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.88 = 3.88 + 0.0 + 0.0 avg prob of [ Spanish Civil War] 0.02168869785964489\n",
      "loss 2.187 = 2.134 + 0.052 + 0.001 avg prob of [ Spanish Civil War] 0.12862473726272583\n",
      "loss 0.565 = 0.519 + 0.044 + 0.001 avg prob of [ Spanish Civil War] 0.6040305495262146\n",
      "loss 0.213 = 0.161 + 0.05 + 0.001 avg prob of [ Spanish Civil War] 0.8522781133651733\n",
      "loss 0.082 = 0.045 + 0.036 + 0.001 avg prob of [ Spanish Civil War] 0.9565109014511108\n",
      "loss 0.055 = 0.022 + 0.031 + 0.001 avg prob of [ Spanish Civil War] 0.9779946804046631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:25,310 - easyeditor.editors.editor - INFO - 2 editing: What war was Lloyd Thomas in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What war was Lloyd Thomas in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:25,310 - easyeditor.editors.editor - INFO - 2 editing: What war was Lloyd Thomas in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What war was Lloyd Thomas in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:25 - INFO - easyeditor.editors.editor -   2 editing: What war was Lloyd Thomas in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What war was Lloyd Thomas in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  6%|▌         | 3/50 [00:08<02:13,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.042 = 0.014 + 0.027 + 0.001 avg prob of [ Spanish Civil War] 0.9857664108276367\n",
      "Delta norm: 11.8515625\n",
      "Change in target norm: 2.962890625 to 12.1875 => 9.2265625\n",
      "Division Factor: 3.767578125\n",
      "Right vector norm: 3.146484375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What noble family was Xiao Jia part of?] -> [ Southern Ming Dynasty]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Xiao Jia\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What noble family was Xiao Jia part of? Southern Ming | Token: ia\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.717 = 5.717 + 0.0 + 0.0 avg prob of [ Southern Ming Dynasty] 0.0033937571570277214\n",
      "loss 4.562 = 4.493 + 0.068 + 0.001 avg prob of [ Southern Ming Dynasty] 0.01191677711904049\n",
      "loss 2.967 = 2.931 + 0.035 + 0.001 avg prob of [ Southern Ming Dynasty] 0.05619942769408226\n",
      "loss 3.474 = 3.377 + 0.095 + 0.001 avg prob of [ Southern Ming Dynasty] 0.03702041879296303\n",
      "loss 2.365 = 2.259 + 0.105 + 0.001 avg prob of [ Southern Ming Dynasty] 0.10999689996242523\n",
      "loss 1.577 = 1.378 + 0.197 + 0.001 avg prob of [ Southern Ming Dynasty] 0.28058624267578125\n",
      "loss 0.599 = 0.481 + 0.117 + 0.001 avg prob of [ Southern Ming Dynasty] 0.6242777705192566\n",
      "loss 0.428 = 0.303 + 0.124 + 0.001 avg prob of [ Southern Ming Dynasty] 0.7413337230682373\n",
      "loss 0.176 = 0.045 + 0.129 + 0.001 avg prob of [ Southern Ming Dynasty] 0.955707311630249\n",
      "loss 0.154 = 0.004 + 0.148 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9955626130104065\n",
      "loss 0.167 = 0.003 + 0.163 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9969644546508789\n",
      "loss 0.165 = 0.002 + 0.162 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9984045624732971\n",
      "loss 0.141 = 0.004 + 0.136 + 0.001 avg prob of [ Southern Ming Dynasty] 0.996422290802002\n",
      "loss 0.137 = 0.003 + 0.132 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9965655207633972\n",
      "loss 0.109 = 0.003 + 0.105 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9968056678771973\n",
      "loss 0.096 = 0.003 + 0.092 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9968461990356445\n",
      "loss 0.08 = 0.003 + 0.076 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9968931078910828\n",
      "loss 0.071 = 0.002 + 0.067 + 0.001 avg prob of [ Southern Ming Dynasty] 0.997518002986908\n",
      "loss 0.064 = 0.002 + 0.061 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9984449148178101\n",
      "loss 0.059 = 0.001 + 0.057 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9989984631538391\n",
      "loss 0.055 = 0.001 + 0.053 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9992652535438538\n",
      "loss 0.052 = 0.001 + 0.05 + 0.001 avg prob of [ Southern Ming Dynasty] 0.999399721622467\n",
      "loss 0.051 = 0.001 + 0.049 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9994727969169617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:32,422 - easyeditor.editors.editor - INFO - 3 editing: What noble family was Xiao Jia part of? -> Southern Ming Dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What noble family was Xiao Jia part of?', 'target_new': 'Southern Ming Dynasty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Xiao Jia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:32,422 - easyeditor.editors.editor - INFO - 3 editing: What noble family was Xiao Jia part of? -> Southern Ming Dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What noble family was Xiao Jia part of?', 'target_new': 'Southern Ming Dynasty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Xiao Jia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:32 - INFO - easyeditor.editors.editor -   3 editing: What noble family was Xiao Jia part of? -> Southern Ming Dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'What noble family was Xiao Jia part of?', 'target_new': 'Southern Ming Dynasty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Xiao Jia'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "  8%|▊         | 4/50 [00:16<03:27,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.05 = 0.0 + 0.048 + 0.001 avg prob of [ Southern Ming Dynasty] 0.9995194673538208\n",
      "Delta norm: 11.0078125\n",
      "Change in target norm: 2.751953125 to 11.2890625 => 8.5390625\n",
      "Division Factor: 3.568359375\n",
      "Right vector norm: 3.083984375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What date did John Southgate Allen die?] -> [ 1934]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object John Southgate Allen\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What date did John Southgate Allen die? 193 | Token:  Allen\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.325 = 3.325 + 0.0 + 0.0 avg prob of [ 1934] 0.03651018440723419\n",
      "loss 3.66 = 3.624 + 0.034 + 0.002 avg prob of [ 1934] 0.02811584062874317\n",
      "loss 3.07 = 2.933 + 0.135 + 0.002 avg prob of [ 1934] 0.054810285568237305\n",
      "loss 1.835 = 1.817 + 0.017 + 0.002 avg prob of [ 1934] 0.16911214590072632\n",
      "loss 0.879 = 0.864 + 0.014 + 0.002 avg prob of [ 1934] 0.42878425121307373\n",
      "loss 0.24 = 0.205 + 0.034 + 0.002 avg prob of [ 1934] 0.8185400366783142\n",
      "loss 0.146 = 0.058 + 0.087 + 0.002 avg prob of [ 1934] 0.9457358717918396\n",
      "loss 0.12 = 0.018 + 0.101 + 0.002 avg prob of [ 1934] 0.9820582270622253\n",
      "loss 0.056 = 0.027 + 0.027 + 0.002 avg prob of [ 1934] 0.9731488823890686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:35,665 - easyeditor.editors.editor - INFO - 4 editing: What date did John Southgate Allen die? -> 1934  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What date did John Southgate Allen die?', 'target_new': '1934', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'John Southgate Allen'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:35,665 - easyeditor.editors.editor - INFO - 4 editing: What date did John Southgate Allen die? -> 1934  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What date did John Southgate Allen die?', 'target_new': '1934', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'John Southgate Allen'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:35 - INFO - easyeditor.editors.editor -   4 editing: What date did John Southgate Allen die? -> 1934  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What date did John Southgate Allen die?', 'target_new': '1934', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'John Southgate Allen'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 10%|█         | 5/50 [00:19<03:02,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.046 = 0.006 + 0.039 + 0.002 avg prob of [ 1934] 0.9943420886993408\n",
      "Delta norm: 10.1875\n",
      "Change in target norm: 2.546875 to 10.546875 => 8.0\n",
      "Division Factor: 3.154296875\n",
      "Right vector norm: 3.23046875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What person illustrated Flora Graeca?] -> [ Flor Silvestre]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Flora Graeca\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What person illustrated Flora Graeca? Flor Silvest | Token: eca\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.278 = 5.278 + 0.0 + 0.0 avg prob of [ Flor Silvestre] 0.005342557560652494\n",
      "loss 4.116 = 4.021 + 0.093 + 0.001 avg prob of [ Flor Silvestre] 0.018903741613030434\n",
      "loss 2.881 = 2.829 + 0.05 + 0.001 avg prob of [ Flor Silvestre] 0.06180640310049057\n",
      "loss 1.558 = 1.519 + 0.037 + 0.001 avg prob of [ Flor Silvestre] 0.22015345096588135\n",
      "loss 0.872 = 0.805 + 0.066 + 0.001 avg prob of [ Flor Silvestre] 0.451896071434021\n",
      "loss 1.116 = 1.072 + 0.042 + 0.001 avg prob of [ Flor Silvestre] 0.34929782152175903\n",
      "loss 2.353 = 2.293 + 0.059 + 0.001 avg prob of [ Flor Silvestre] 0.10396450012922287\n",
      "loss 0.638 = 0.562 + 0.075 + 0.001 avg prob of [ Flor Silvestre] 0.5776090025901794\n",
      "loss 0.207 = 0.172 + 0.033 + 0.001 avg prob of [ Flor Silvestre] 0.8432351350784302\n",
      "loss 0.079 = 0.041 + 0.037 + 0.001 avg prob of [ Flor Silvestre] 0.9597715139389038\n",
      "loss 0.074 = 0.014 + 0.06 + 0.001 avg prob of [ Flor Silvestre] 0.9863553643226624\n",
      "loss 0.084 = 0.005 + 0.077 + 0.001 avg prob of [ Flor Silvestre] 0.9947866797447205\n",
      "loss 0.058 = 0.003 + 0.053 + 0.001 avg prob of [ Flor Silvestre] 0.9967519044876099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:40,813 - easyeditor.editors.editor - INFO - 5 editing: What person illustrated Flora Graeca? -> Flor Silvestre  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What person illustrated Flora Graeca?', 'target_new': 'Flor Silvestre', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Flora Graeca'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.048 = 0.003 + 0.044 + 0.001 avg prob of [ Flor Silvestre] 0.9973359107971191\n",
      "Delta norm: 12.9921875\n",
      "Change in target norm: 3.248046875 to 13.4765625 => 10.2265625\n",
      "Division Factor: 4.15625\n",
      "Right vector norm: 3.125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:40,813 - easyeditor.editors.editor - INFO - 5 editing: What person illustrated Flora Graeca? -> Flor Silvestre  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What person illustrated Flora Graeca?', 'target_new': 'Flor Silvestre', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Flora Graeca'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:40 - INFO - easyeditor.editors.editor -   5 editing: What person illustrated Flora Graeca? -> Flor Silvestre  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What person illustrated Flora Graeca?', 'target_new': 'Flor Silvestre', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Flora Graeca'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 12%|█▏        | 6/50 [00:24<03:15,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [Which fictional universe is Moses Magnum part of?] -> [  Magnum universe]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Moses Magnum\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Which fictional universe is Moses Magnum part of?  Magnum | Token:  Magnum\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.116 = 8.116 + 0.0 + 0.0 avg prob of [  Magnum universe] 0.00040153777808882296\n",
      "loss 8.716 = 8.6 + 0.115 + 0.001 avg prob of [  Magnum universe] 0.00023023558605927974\n",
      "loss 5.74 = 5.66 + 0.079 + 0.001 avg prob of [  Magnum universe] 0.004222689662128687\n",
      "loss 2.544 = 2.492 + 0.05 + 0.001 avg prob of [  Magnum universe] 0.08519595116376877\n",
      "loss 1.946 = 1.823 + 0.122 + 0.001 avg prob of [  Magnum universe] 0.1642494648694992\n",
      "loss 2.024 = 1.941 + 0.081 + 0.001 avg prob of [  Magnum universe] 0.15884460508823395\n",
      "loss 1.025 = 0.884 + 0.139 + 0.001 avg prob of [  Magnum universe] 0.46200475096702576\n",
      "loss 0.537 = 0.488 + 0.047 + 0.001 avg prob of [  Magnum universe] 0.6210286617279053\n",
      "loss 0.13 = 0.004 + 0.124 + 0.001 avg prob of [  Magnum universe] 0.9958814978599548\n",
      "loss 0.096 = 0.002 + 0.093 + 0.001 avg prob of [  Magnum universe] 0.9984310269355774\n",
      "loss 0.075 = 0.002 + 0.071 + 0.001 avg prob of [  Magnum universe] 0.9975895881652832\n",
      "loss 0.061 = 0.004 + 0.056 + 0.001 avg prob of [  Magnum universe] 0.9964240789413452\n",
      "loss 0.052 = 0.003 + 0.048 + 0.001 avg prob of [  Magnum universe] 0.9969171285629272\n",
      "loss 0.044 = 0.002 + 0.04 + 0.001 avg prob of [  Magnum universe] 0.9976239800453186\n",
      "Delta norm: 11.90625\n",
      "Change in target norm: 2.9765625 to 12.328125 => 9.3515625\n",
      "Division Factor: 3.72265625\n",
      "Right vector norm: 3.19921875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:46,601 - easyeditor.editors.editor - INFO - 6 editing: Which fictional universe is Moses Magnum part of? ->  Magnum universe  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which fictional universe is Moses Magnum part of?', 'target_new': ' Magnum universe', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Moses Magnum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:46,601 - easyeditor.editors.editor - INFO - 6 editing: Which fictional universe is Moses Magnum part of? ->  Magnum universe  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which fictional universe is Moses Magnum part of?', 'target_new': ' Magnum universe', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Moses Magnum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:46 - INFO - easyeditor.editors.editor -   6 editing: Which fictional universe is Moses Magnum part of? ->  Magnum universe  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Which fictional universe is Moses Magnum part of?', 'target_new': ' Magnum universe', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Moses Magnum'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 14%|█▍        | 7/50 [00:30<03:29,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [What city did Abel Seyler live when he died?] -> [ Tirana]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Abel Seyler\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: What city did Abel Seyler live when he died? Tir | Token: ler\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.95 = 5.95 + 0.0 + 0.0 avg prob of [ Tirana] 0.0027962594758719206\n",
      "loss 3.855 = 3.795 + 0.058 + 0.001 avg prob of [ Tirana] 0.028450069949030876\n",
      "loss 0.714 = 0.698 + 0.015 + 0.001 avg prob of [ Tirana] 0.5226330757141113\n",
      "loss 0.115 = 0.092 + 0.022 + 0.001 avg prob of [ Tirana] 0.9143487215042114\n",
      "loss 0.051 = 0.035 + 0.015 + 0.001 avg prob of [ Tirana] 0.96573805809021\n",
      "loss 0.03 = 0.016 + 0.012 + 0.001 avg prob of [ Tirana] 0.9839553833007812\n",
      "Delta norm: 12.359375\n",
      "Change in target norm: 3.08984375 to 12.8046875 => 9.71875\n",
      "Division Factor: 3.83203125\n",
      "Right vector norm: 3.224609375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:49,943 - easyeditor.editors.editor - INFO - 7 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:49,943 - easyeditor.editors.editor - INFO - 7 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:49 - INFO - easyeditor.editors.editor -   7 editing: What city did Abel Seyler live when he died? -> Tirana  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'What city did Abel Seyler live when he died?', 'target_new': 'Tirana', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Abel Seyler'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 16%|█▌        | 8/50 [00:33<03:04,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [The director of Finders Keepers, Lovers Weepers! is who?] -> [ Joseph Barbera]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Finders Keepers, Lovers Weepers!\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: The director of Finders Keepers, Lovers Weepers! is who? Joseph Barber | Token: !\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.569 = 4.569 + 0.0 + 0.0 avg prob of [ Joseph Barbera] 0.010733196511864662\n",
      "loss 3.71 = 3.694 + 0.015 + 0.001 avg prob of [ Joseph Barbera] 0.025478005409240723\n",
      "loss 4.327 = 4.259 + 0.066 + 0.001 avg prob of [ Joseph Barbera] 0.014870612882077694\n",
      "loss 2.833 = 2.776 + 0.055 + 0.001 avg prob of [ Joseph Barbera] 0.06359831243753433\n",
      "loss 1.824 = 1.78 + 0.043 + 0.001 avg prob of [ Joseph Barbera] 0.1756257861852646\n",
      "loss 0.696 = 0.637 + 0.058 + 0.001 avg prob of [ Joseph Barbera] 0.5550363659858704\n",
      "loss 0.175 = 0.101 + 0.073 + 0.001 avg prob of [ Joseph Barbera] 0.9051198959350586\n",
      "loss 0.07 = 0.023 + 0.045 + 0.001 avg prob of [ Joseph Barbera] 0.9773663282394409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:54,920 - easyeditor.editors.editor - INFO - 8 editing: The director of Finders Keepers, Lovers Weepers! is who? -> Joseph Barbera  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The director of Finders Keepers, Lovers Weepers! is who?', 'target_new': 'Joseph Barbera', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Finders Keepers, Lovers Weepers!'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:54,920 - easyeditor.editors.editor - INFO - 8 editing: The director of Finders Keepers, Lovers Weepers! is who? -> Joseph Barbera  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The director of Finders Keepers, Lovers Weepers! is who?', 'target_new': 'Joseph Barbera', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Finders Keepers, Lovers Weepers!'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:54 - INFO - easyeditor.editors.editor -   8 editing: The director of Finders Keepers, Lovers Weepers! is who? -> Joseph Barbera  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The director of Finders Keepers, Lovers Weepers! is who?', 'target_new': 'Joseph Barbera', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Finders Keepers, Lovers Weepers!'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 18%|█▊        | 9/50 [00:38<03:07,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.046 = 0.015 + 0.03 + 0.001 avg prob of [ Joseph Barbera] 0.985614538192749\n",
      "Delta norm: 10.9453125\n",
      "Change in target norm: 2.736328125 to 11.375 => 8.640625\n",
      "Division Factor: 3.330078125\n",
      "Right vector norm: 3.287109375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Who was the male parent of Hawkster?] -> [ Hobart]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Hawkster\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: Who was the male parent of Hawkster? Hob | Token: ster\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.701 = 6.701 + 0.0 + 0.0 avg prob of [ Hobart] 0.0013125630794093013\n",
      "loss 5.363 = 5.289 + 0.073 + 0.001 avg prob of [ Hobart] 0.005193240009248257\n",
      "loss 2.654 = 2.563 + 0.089 + 0.001 avg prob of [ Hobart] 0.07948905229568481\n",
      "loss 1.026 = 0.92 + 0.104 + 0.001 avg prob of [ Hobart] 0.400790274143219\n",
      "loss 0.222 = 0.119 + 0.102 + 0.001 avg prob of [ Hobart] 0.8881324529647827\n",
      "loss 0.146 = 0.012 + 0.133 + 0.001 avg prob of [ Hobart] 0.9876022338867188\n",
      "loss 0.079 = 0.005 + 0.073 + 0.001 avg prob of [ Hobart] 0.9949622750282288\n",
      "loss 0.052 = 0.003 + 0.048 + 0.001 avg prob of [ Hobart] 0.9965814352035522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:15:57,773 - easyeditor.editors.editor - INFO - 9 editing: Who was the male parent of Hawkster? -> Hobart  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Who was the male parent of Hawkster?', 'target_new': 'Hobart', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hawkster'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:15:57,773 - easyeditor.editors.editor - INFO - 9 editing: Who was the male parent of Hawkster? -> Hobart  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Who was the male parent of Hawkster?', 'target_new': 'Hobart', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hawkster'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:15:57 - INFO - easyeditor.editors.editor -   9 editing: Who was the male parent of Hawkster? -> Hobart  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'Who was the male parent of Hawkster?', 'target_new': 'Hobart', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Hawkster'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 20%|██        | 10/50 [00:41<02:41,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.036 = 0.003 + 0.032 + 0.001 avg prob of [ Hobart] 0.997378408908844\n",
      "Delta norm: 13.4375\n",
      "Change in target norm: 3.359375 to 13.765625 => 10.40625\n",
      "Division Factor: 4.0625\n",
      "Right vector norm: 3.30859375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [The A Star Is Torn was in what series?] -> [ The Twilight Zone]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object A Star Is Torn\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: The A Star Is Torn was in what series? The Twilight | Token: orn\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.421 = 3.421 + 0.0 + 0.0 avg prob of [ The Twilight Zone] 0.03550732880830765\n",
      "loss 2.2 = 2.185 + 0.014 + 0.002 avg prob of [ The Twilight Zone] 0.12396244704723358\n",
      "loss 0.53 = 0.497 + 0.032 + 0.002 avg prob of [ The Twilight Zone] 0.6121855974197388\n",
      "loss 0.317 = 0.296 + 0.02 + 0.002 avg prob of [ The Twilight Zone] 0.7482835650444031\n",
      "loss 0.174 = 0.156 + 0.016 + 0.002 avg prob of [ The Twilight Zone] 0.858886182308197\n",
      "loss 0.089 = 0.069 + 0.019 + 0.002 avg prob of [ The Twilight Zone] 0.9342664480209351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:00,176 - easyeditor.editors.editor - INFO - 10 editing: The A Star Is Torn was in what series? -> The Twilight Zone  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The A Star Is Torn was in what series?', 'target_new': 'The Twilight Zone', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:00,176 - easyeditor.editors.editor - INFO - 10 editing: The A Star Is Torn was in what series? -> The Twilight Zone  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The A Star Is Torn was in what series?', 'target_new': 'The Twilight Zone', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:00 - INFO - easyeditor.editors.editor -   10 editing: The A Star Is Torn was in what series? -> The Twilight Zone  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The A Star Is Torn was in what series?', 'target_new': 'The Twilight Zone', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'A Star Is Torn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 22%|██▏       | 11/50 [00:43<02:18,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.038 = 0.015 + 0.021 + 0.002 avg prob of [ The Twilight Zone] 0.9848787188529968\n",
      "Delta norm: 10.0\n",
      "Change in target norm: 2.5 to 10.34375 => 7.84375\n",
      "Division Factor: 3.25\n",
      "Right vector norm: 3.076171875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [The Holmenkollen Chapel project's architect was who?] -> [ Inigo Jones]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Holmenkollen Chapel\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: The Holmenkollen Chapel project's architect was who? Inigo | Token:  Chapel\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.226 = 4.226 + 0.0 + 0.0 avg prob of [ Inigo Jones] 0.015331568196415901\n",
      "loss 3.119 = 3.086 + 0.031 + 0.001 avg prob of [ Inigo Jones] 0.05180320516228676\n",
      "loss 1.522 = 1.49 + 0.031 + 0.001 avg prob of [ Inigo Jones] 0.23022298514842987\n",
      "loss 0.577 = 0.552 + 0.024 + 0.001 avg prob of [ Inigo Jones] 0.5848447680473328\n",
      "loss 0.473 = 0.439 + 0.033 + 0.001 avg prob of [ Inigo Jones] 0.6605746150016785\n",
      "loss 0.146 = 0.111 + 0.034 + 0.001 avg prob of [ Inigo Jones] 0.8959642052650452\n",
      "loss 0.085 = 0.055 + 0.029 + 0.001 avg prob of [ Inigo Jones] 0.946386992931366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:02,920 - easyeditor.editors.editor - INFO - 11 editing: The Holmenkollen Chapel project's architect was who? -> Inigo Jones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': \"The Holmenkollen Chapel project's architect was who?\", 'target_new': 'Inigo Jones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:02,920 - easyeditor.editors.editor - INFO - 11 editing: The Holmenkollen Chapel project's architect was who? -> Inigo Jones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': \"The Holmenkollen Chapel project's architect was who?\", 'target_new': 'Inigo Jones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:02 - INFO - easyeditor.editors.editor -   11 editing: The Holmenkollen Chapel project's architect was who? -> Inigo Jones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 11, 'requested_rewrite': {'prompt': \"The Holmenkollen Chapel project's architect was who?\", 'target_new': 'Inigo Jones', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 24%|██▍       | 12/50 [00:46<02:05,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.046 = 0.02 + 0.024 + 0.001 avg prob of [ Inigo Jones] 0.9797926545143127\n",
      "Delta norm: 10.9765625\n",
      "Change in target norm: 2.744140625 to 11.2578125 => 8.515625\n",
      "Division Factor: 3.51171875\n",
      "Right vector norm: 3.125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Which language is Pleine Vie written in?] -> [ Coptic]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Pleine Vie\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Which language is Pleine Vie written in? C | Token:  Vie\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.003 = 6.003 + 0.0 + 0.0 avg prob of [ Coptic] 0.0027631453704088926\n",
      "loss 5.042 = 4.884 + 0.156 + 0.001 avg prob of [ Coptic] 0.009354203008115292\n",
      "loss 2.961 = 2.846 + 0.114 + 0.001 avg prob of [ Coptic] 0.06306540220975876\n",
      "loss 1.323 = 1.177 + 0.145 + 0.001 avg prob of [ Coptic] 0.31745555996894836\n",
      "loss 0.821 = 0.75 + 0.069 + 0.001 avg prob of [ Coptic] 0.47985124588012695\n",
      "loss 0.303 = 0.233 + 0.068 + 0.001 avg prob of [ Coptic] 0.7956344485282898\n",
      "loss 0.111 = 0.035 + 0.075 + 0.001 avg prob of [ Coptic] 0.9662771821022034\n",
      "loss 0.074 = 0.003 + 0.07 + 0.001 avg prob of [ Coptic] 0.9971960186958313\n",
      "loss 0.099 = 0.001 + 0.096 + 0.001 avg prob of [ Coptic] 0.9987375140190125\n",
      "loss 0.066 = 0.005 + 0.06 + 0.001 avg prob of [ Coptic] 0.9953635334968567\n",
      "loss 0.058 = 0.007 + 0.049 + 0.001 avg prob of [ Coptic] 0.9929811358451843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:08,328 - easyeditor.editors.editor - INFO - 12 editing: Which language is Pleine Vie written in? -> Coptic  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which language is Pleine Vie written in?', 'target_new': 'Coptic', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pleine Vie'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.049 = 0.006 + 0.041 + 0.001 avg prob of [ Coptic] 0.9936099648475647\n",
      "Delta norm: 11.0703125\n",
      "Change in target norm: 2.767578125 to 11.484375 => 8.71875\n",
      "Division Factor: 3.513671875\n",
      "Right vector norm: 3.150390625\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:08,328 - easyeditor.editors.editor - INFO - 12 editing: Which language is Pleine Vie written in? -> Coptic  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which language is Pleine Vie written in?', 'target_new': 'Coptic', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pleine Vie'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:08 - INFO - easyeditor.editors.editor -   12 editing: Which language is Pleine Vie written in? -> Coptic  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Which language is Pleine Vie written in?', 'target_new': 'Coptic', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pleine Vie'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 26%|██▌       | 13/50 [00:51<02:25,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [Which place is Children Without in?] -> [ New Jersey]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Children Without\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: Which place is Children Without in? New | Token:  Without\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.847 = 4.847 + 0.0 + 0.0 avg prob of [ New Jersey] 0.009055760689079762\n",
      "loss 2.378 = 2.035 + 0.342 + 0.002 avg prob of [ New Jersey] 0.13774871826171875\n",
      "loss 1.791 = 1.419 + 0.371 + 0.002 avg prob of [ New Jersey] 0.2498893290758133\n",
      "loss 1.19 = 0.635 + 0.554 + 0.002 avg prob of [ New Jersey] 0.5435875654220581\n",
      "loss 0.497 = 0.199 + 0.297 + 0.002 avg prob of [ New Jersey] 0.8265573382377625\n",
      "loss 0.216 = 0.05 + 0.165 + 0.002 avg prob of [ New Jersey] 0.9520861506462097\n",
      "loss 0.165 = 0.017 + 0.146 + 0.002 avg prob of [ New Jersey] 0.9831489324569702\n",
      "loss 0.147 = 0.004 + 0.141 + 0.002 avg prob of [ New Jersey] 0.9962062239646912\n",
      "loss 0.134 = 0.002 + 0.131 + 0.002 avg prob of [ New Jersey] 0.9984644055366516\n",
      "loss 0.111 = 0.001 + 0.108 + 0.002 avg prob of [ New Jersey] 0.998687207698822\n",
      "loss 0.077 = 0.007 + 0.068 + 0.002 avg prob of [ New Jersey] 0.9933228492736816\n",
      "loss 0.062 = 0.003 + 0.057 + 0.002 avg prob of [ New Jersey] 0.9965857863426208\n",
      "loss 0.046 = 0.001 + 0.043 + 0.002 avg prob of [ New Jersey] 0.9985384345054626\n",
      "Delta norm: 9.8046875\n",
      "Change in target norm: 2.451171875 to 10.03125 => 7.578125\n",
      "Division Factor: 3.177734375\n",
      "Right vector norm: 3.0859375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:13,576 - easyeditor.editors.editor - INFO - 13 editing: Which place is Children Without in? -> New Jersey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'Which place is Children Without in?', 'target_new': 'New Jersey', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Children Without'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:13,576 - easyeditor.editors.editor - INFO - 13 editing: Which place is Children Without in? -> New Jersey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'Which place is Children Without in?', 'target_new': 'New Jersey', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Children Without'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:13 - INFO - easyeditor.editors.editor -   13 editing: Which place is Children Without in? -> New Jersey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'Which place is Children Without in?', 'target_new': 'New Jersey', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Children Without'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 28%|██▊       | 14/50 [00:57<02:36,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [What country was Ivica Ančić in?] -> [ Slovakia]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Ivica Ančić\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What country was Ivica Ančić in? | Token: ić\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.436 = 8.436 + 0.0 + 0.0 avg prob of [ Slovakia] 0.0002647240471560508\n",
      "loss 6.559 = 6.489 + 0.068 + 0.001 avg prob of [ Slovakia] 0.0019968231208622456\n",
      "loss 1.543 = 1.53 + 0.012 + 0.001 avg prob of [ Slovakia] 0.228786900639534\n",
      "loss 0.152 = 0.125 + 0.026 + 0.001 avg prob of [ Slovakia] 0.88352370262146\n",
      "loss 0.077 = 0.041 + 0.035 + 0.001 avg prob of [ Slovakia] 0.9602083563804626\n",
      "loss 0.053 = 0.023 + 0.029 + 0.001 avg prob of [ Slovakia] 0.9769327044487\n",
      "loss 0.05 = 0.014 + 0.035 + 0.001 avg prob of [ Slovakia] 0.9858299493789673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:17,365 - easyeditor.editors.editor - INFO - 14 editing: What country was Ivica Ančić in? -> Slovakia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Ivica Ančić in?', 'target_new': 'Slovakia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ivica Ančić'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:17,365 - easyeditor.editors.editor - INFO - 14 editing: What country was Ivica Ančić in? -> Slovakia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Ivica Ančić in?', 'target_new': 'Slovakia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ivica Ančić'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:17 - INFO - easyeditor.editors.editor -   14 editing: What country was Ivica Ančić in? -> Slovakia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'What country was Ivica Ančić in?', 'target_new': 'Slovakia', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ivica Ančić'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 30%|███       | 15/50 [01:00<02:25,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.03 = 0.011 + 0.018 + 0.001 avg prob of [ Slovakia] 0.9893808960914612\n",
      "Delta norm: 12.90625\n",
      "Change in target norm: 3.2265625 to 13.3359375 => 10.109375\n",
      "Division Factor: 4.03125\n",
      "Right vector norm: 3.201171875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Who is Ahmose-Henuttamehu's father?] -> [ Ahmose-nirari]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Ahmose-Henuttamehu\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Who is Ahmose-Henuttamehu's father? Ahmose-nir | Token: hu\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.55 = 3.55 + 0.0 + 0.0 avg prob of [ Ahmose-nirari] 0.02886386774480343\n",
      "loss 3.007 = 2.942 + 0.064 + 0.001 avg prob of [ Ahmose-nirari] 0.05318726971745491\n",
      "loss 1.677 = 1.635 + 0.041 + 0.001 avg prob of [ Ahmose-nirari] 0.19664902985095978\n",
      "loss 0.688 = 0.646 + 0.041 + 0.001 avg prob of [ Ahmose-nirari] 0.5276848077774048\n",
      "loss 0.258 = 0.22 + 0.037 + 0.001 avg prob of [ Ahmose-nirari] 0.8059213161468506\n",
      "loss 0.141 = 0.109 + 0.03 + 0.001 avg prob of [ Ahmose-nirari] 0.8982560634613037\n",
      "loss 0.085 = 0.05 + 0.034 + 0.001 avg prob of [ Ahmose-nirari] 0.9517943859100342\n",
      "loss 0.078 = 0.015 + 0.062 + 0.001 avg prob of [ Ahmose-nirari] 0.9851680994033813\n",
      "loss 0.068 = 0.007 + 0.06 + 0.001 avg prob of [ Ahmose-nirari] 0.9932827949523926\n",
      "loss 0.154 = 0.004 + 0.149 + 0.001 avg prob of [ Ahmose-nirari] 0.9956463575363159\n",
      "loss 0.081 = 0.005 + 0.075 + 0.001 avg prob of [ Ahmose-nirari] 0.9950608015060425\n",
      "loss 0.07 = 0.005 + 0.064 + 0.001 avg prob of [ Ahmose-nirari] 0.9948230385780334\n",
      "loss 0.067 = 0.004 + 0.062 + 0.001 avg prob of [ Ahmose-nirari] 0.9955295324325562\n",
      "loss 0.062 = 0.003 + 0.058 + 0.001 avg prob of [ Ahmose-nirari] 0.9973530769348145\n",
      "loss 0.059 = 0.002 + 0.055 + 0.001 avg prob of [ Ahmose-nirari] 0.9980196356773376\n",
      "loss 0.056 = 0.002 + 0.053 + 0.001 avg prob of [ Ahmose-nirari] 0.9984670281410217\n",
      "loss 0.053 = 0.001 + 0.051 + 0.001 avg prob of [ Ahmose-nirari] 0.9987787008285522\n",
      "loss 0.052 = 0.001 + 0.05 + 0.001 avg prob of [ Ahmose-nirari] 0.9990079402923584\n",
      "loss 0.051 = 0.001 + 0.049 + 0.001 avg prob of [ Ahmose-nirari] 0.9991730451583862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:25,373 - easyeditor.editors.editor - INFO - 15 editing: Who is Ahmose-Henuttamehu's father? -> Ahmose-nirari  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Who is Ahmose-Henuttamehu's father?\", 'target_new': 'Ahmose-nirari', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ahmose-Henuttamehu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:25,373 - easyeditor.editors.editor - INFO - 15 editing: Who is Ahmose-Henuttamehu's father? -> Ahmose-nirari  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Who is Ahmose-Henuttamehu's father?\", 'target_new': 'Ahmose-nirari', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ahmose-Henuttamehu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:25 - INFO - easyeditor.editors.editor -   15 editing: Who is Ahmose-Henuttamehu's father? -> Ahmose-nirari  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 15, 'requested_rewrite': {'prompt': \"Who is Ahmose-Henuttamehu's father?\", 'target_new': 'Ahmose-nirari', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ahmose-Henuttamehu'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 32%|███▏      | 16/50 [01:08<03:00,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.05 = 0.001 + 0.048 + 0.001 avg prob of [ Ahmose-nirari] 0.9993001222610474\n",
      "Delta norm: 14.0\n",
      "Change in target norm: 3.5 to 14.4375 => 10.9375\n",
      "Division Factor: 4.5234375\n",
      "Right vector norm: 3.095703125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Which college or university is related with Rose Ann Scamardella?] -> [ Columbia University]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Rose Ann Scamardella\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 13 | Sentence: Which college or university is related with Rose Ann Scamardella? Columbia | Token: ella\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.148 = 4.148 + 0.0 + 0.0 avg prob of [ Columbia University] 0.01869378611445427\n",
      "loss 3.023 = 2.99 + 0.032 + 0.001 avg prob of [ Columbia University] 0.06412705034017563\n",
      "loss 0.654 = 0.573 + 0.08 + 0.001 avg prob of [ Columbia University] 0.5754449367523193\n",
      "loss 0.129 = 0.091 + 0.037 + 0.001 avg prob of [ Columbia University] 0.9134787917137146\n",
      "loss 0.057 = 0.016 + 0.04 + 0.001 avg prob of [ Columbia University] 0.9843233227729797\n",
      "loss 0.05 = 0.004 + 0.045 + 0.001 avg prob of [ Columbia University] 0.9960774183273315\n",
      "Delta norm: 12.34375\n",
      "Change in target norm: 3.0859375 to 12.8046875 => 9.71875\n",
      "Division Factor: 3.88671875\n",
      "Right vector norm: 3.17578125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:28,114 - easyeditor.editors.editor - INFO - 16 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:28,114 - easyeditor.editors.editor - INFO - 16 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:28 - INFO - easyeditor.editors.editor -   16 editing: Which college or university is related with Rose Ann Scamardella? -> Columbia University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'Which college or university is related with Rose Ann Scamardella?', 'target_new': 'Columbia University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Rose Ann Scamardella'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 34%|███▍      | 17/50 [01:11<02:30,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [Which network broadcasted Smash Lab?] -> [ TNT]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Smash Lab\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: Which network broadcasted Smash Lab? | Token:  Lab\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 7.794 = 7.794 + 0.0 + 0.0 avg prob of [ TNT] 0.0006753972265869379\n",
      "loss 5.56 = 5.45 + 0.108 + 0.001 avg prob of [ TNT] 0.012388963252305984\n",
      "loss 0.221 = 0.152 + 0.067 + 0.001 avg prob of [ TNT] 0.8622426986694336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:29,830 - easyeditor.editors.editor - INFO - 17 editing: Which network broadcasted Smash Lab? -> TNT  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Which network broadcasted Smash Lab?', 'target_new': 'TNT', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Smash Lab'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:29,830 - easyeditor.editors.editor - INFO - 17 editing: Which network broadcasted Smash Lab? -> TNT  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Which network broadcasted Smash Lab?', 'target_new': 'TNT', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Smash Lab'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:29 - INFO - easyeditor.editors.editor -   17 editing: Which network broadcasted Smash Lab? -> TNT  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'Which network broadcasted Smash Lab?', 'target_new': 'TNT', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Smash Lab'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 36%|███▌      | 18/50 [01:13<01:58,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.048 = 0.015 + 0.032 + 0.001 avg prob of [ TNT] 0.9855858087539673\n",
      "Delta norm: 14.234375\n",
      "Change in target norm: 3.55859375 to 14.6953125 => 11.140625\n",
      "Division Factor: 4.3984375\n",
      "Right vector norm: 3.236328125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What is an ecological status of Coptodon spongotroktis?] -> [ Data Deficient]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Coptodon spongotroktis\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 15 | Sentence: What is an ecological status of Coptodon spongotroktis? Data Def | Token: is\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.766 = 3.766 + 0.0 + 0.0 avg prob of [ Data Deficient] 0.028897760435938835\n",
      "loss 3.2 = 3.077 + 0.122 + 0.001 avg prob of [ Data Deficient] 0.0528884120285511\n",
      "loss 2.437 = 2.408 + 0.027 + 0.001 avg prob of [ Data Deficient] 0.097671739757061\n",
      "loss 2.497 = 2.48 + 0.016 + 0.001 avg prob of [ Data Deficient] 0.09298986941576004\n",
      "loss 1.056 = 1.017 + 0.038 + 0.001 avg prob of [ Data Deficient] 0.3789212703704834\n",
      "loss 0.254 = 0.146 + 0.107 + 0.001 avg prob of [ Data Deficient] 0.8653466701507568\n",
      "loss 0.122 = 0.019 + 0.102 + 0.001 avg prob of [ Data Deficient] 0.9820662140846252\n",
      "loss 0.104 = 0.0 + 0.102 + 0.001 avg prob of [ Data Deficient] 0.9995478987693787\n",
      "loss 0.102 = 0.0 + 0.1 + 0.001 avg prob of [ Data Deficient] 0.9998740553855896\n",
      "loss 0.129 = 0.0 + 0.128 + 0.001 avg prob of [ Data Deficient] 0.9999354481697083\n",
      "loss 0.131 = 0.008 + 0.122 + 0.001 avg prob of [ Data Deficient] 0.9917014837265015\n",
      "loss 0.194 = 0.09 + 0.102 + 0.001 avg prob of [ Data Deficient] 0.9142423272132874\n",
      "loss 0.108 = 0.005 + 0.102 + 0.001 avg prob of [ Data Deficient] 0.9954958558082581\n",
      "loss 0.105 = 0.001 + 0.102 + 0.001 avg prob of [ Data Deficient] 0.9985547661781311\n",
      "loss 0.106 = 0.002 + 0.103 + 0.001 avg prob of [ Data Deficient] 0.9980962872505188\n",
      "loss 0.106 = 0.002 + 0.103 + 0.001 avg prob of [ Data Deficient] 0.9981127381324768\n",
      "loss 0.105 = 0.001 + 0.102 + 0.001 avg prob of [ Data Deficient] 0.9990901350975037\n",
      "loss 0.104 = 0.001 + 0.102 + 0.001 avg prob of [ Data Deficient] 0.999497652053833\n",
      "loss 0.104 = 0.0 + 0.102 + 0.001 avg prob of [ Data Deficient] 0.9996461868286133\n",
      "loss 0.102 = 0.0 + 0.101 + 0.001 avg prob of [ Data Deficient] 0.9997099041938782\n",
      "loss 0.096 = 0.0 + 0.094 + 0.001 avg prob of [ Data Deficient] 0.9997337460517883\n",
      "loss 0.058 = 0.0 + 0.056 + 0.001 avg prob of [ Data Deficient] 0.9996970295906067\n",
      "loss 0.055 = 0.0 + 0.053 + 0.001 avg prob of [ Data Deficient] 0.9996316432952881\n",
      "loss 0.052 = 0.001 + 0.05 + 0.001 avg prob of [ Data Deficient] 0.9993265271186829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:41,244 - easyeditor.editors.editor - INFO - 18 editing: What is an ecological status of Coptodon spongotroktis? -> Data Deficient  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Coptodon spongotroktis?', 'target_new': 'Data Deficient', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Coptodon spongotroktis'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:41,244 - easyeditor.editors.editor - INFO - 18 editing: What is an ecological status of Coptodon spongotroktis? -> Data Deficient  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Coptodon spongotroktis?', 'target_new': 'Data Deficient', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Coptodon spongotroktis'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:41 - INFO - easyeditor.editors.editor -   18 editing: What is an ecological status of Coptodon spongotroktis? -> Data Deficient  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Coptodon spongotroktis?', 'target_new': 'Data Deficient', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Coptodon spongotroktis'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 38%|███▊      | 19/50 [01:24<03:06,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.061 = 0.001 + 0.058 + 0.001 avg prob of [ Data Deficient] 0.9987409710884094\n",
      "Delta norm: 11.9375\n",
      "Change in target norm: 2.984375 to 12.4296875 => 9.4453125\n",
      "Division Factor: 3.99609375\n",
      "Right vector norm: 2.986328125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What was José Luccioni's range?] -> [ soprano]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object José Luccioni\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: What was José Luccioni's range? sopr | Token: i\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.615 = 6.615 + 0.0 + 0.0 avg prob of [ soprano] 0.0015964999329298735\n",
      "loss 4.335 = 4.268 + 0.066 + 0.001 avg prob of [ soprano] 0.015238799154758453\n",
      "loss 2.221 = 2.166 + 0.054 + 0.001 avg prob of [ soprano] 0.12430203706026077\n",
      "loss 0.629 = 0.439 + 0.189 + 0.001 avg prob of [ soprano] 0.7078127264976501\n",
      "loss 0.319 = 0.248 + 0.069 + 0.001 avg prob of [ soprano] 0.791826605796814\n",
      "loss 0.093 = 0.019 + 0.073 + 0.001 avg prob of [ soprano] 0.9814716577529907\n",
      "loss 0.065 = 0.004 + 0.06 + 0.001 avg prob of [ soprano] 0.9958105087280273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:43,990 - easyeditor.editors.editor - INFO - 19 editing: What was José Luccioni's range? -> soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"What was José Luccioni's range?\", 'target_new': 'soprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'José Luccioni'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:43,990 - easyeditor.editors.editor - INFO - 19 editing: What was José Luccioni's range? -> soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"What was José Luccioni's range?\", 'target_new': 'soprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'José Luccioni'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:43 - INFO - easyeditor.editors.editor -   19 editing: What was José Luccioni's range? -> soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 19, 'requested_rewrite': {'prompt': \"What was José Luccioni's range?\", 'target_new': 'soprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'José Luccioni'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 40%|████      | 20/50 [01:27<02:30,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.048 = 0.002 + 0.044 + 0.001 avg prob of [ soprano] 0.9976975917816162\n",
      "Delta norm: 11.8828125\n",
      "Change in target norm: 2.970703125 to 12.3203125 => 9.3515625\n",
      "Division Factor: 3.76171875\n",
      "Right vector norm: 3.158203125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Who is the director for Oru Raagam Pala Thaalam?] -> [ M Krishnan Nair]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Oru Raagam Pala Thaalam\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 15 | Sentence: Who is the director for Oru Raagam Pala Thaalam? M Krishnan N | Token: alam\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.007 = 3.007 + 0.0 + 0.0 avg prob of [ M Krishnan Nair] 0.04990077018737793\n",
      "loss 2.678 = 2.513 + 0.164 + 0.001 avg prob of [ M Krishnan Nair] 0.08176937699317932\n",
      "loss 2.156 = 2.081 + 0.074 + 0.001 avg prob of [ M Krishnan Nair] 0.1273936629295349\n",
      "loss 2.814 = 2.769 + 0.043 + 0.001 avg prob of [ M Krishnan Nair] 0.06405065953731537\n",
      "loss 1.797 = 1.783 + 0.013 + 0.001 avg prob of [ M Krishnan Nair] 0.1705734133720398\n",
      "loss 1.857 = 1.848 + 0.008 + 0.001 avg prob of [ M Krishnan Nair] 0.16220758855342865\n",
      "loss 0.839 = 0.828 + 0.01 + 0.001 avg prob of [ M Krishnan Nair] 0.4509012997150421\n",
      "loss 0.496 = 0.476 + 0.019 + 0.001 avg prob of [ M Krishnan Nair] 0.6284688711166382\n",
      "loss 0.252 = 0.227 + 0.025 + 0.001 avg prob of [ M Krishnan Nair] 0.7992756366729736\n",
      "loss 0.12 = 0.094 + 0.025 + 0.001 avg prob of [ M Krishnan Nair] 0.910923182964325\n",
      "loss 0.068 = 0.048 + 0.019 + 0.001 avg prob of [ M Krishnan Nair] 0.9531660079956055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:49,029 - easyeditor.editors.editor - INFO - 20 editing: Who is the director for Oru Raagam Pala Thaalam? -> M Krishnan Nair  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who is the director for Oru Raagam Pala Thaalam?', 'target_new': 'M Krishnan Nair', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Oru Raagam Pala Thaalam'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:49,029 - easyeditor.editors.editor - INFO - 20 editing: Who is the director for Oru Raagam Pala Thaalam? -> M Krishnan Nair  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who is the director for Oru Raagam Pala Thaalam?', 'target_new': 'M Krishnan Nair', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Oru Raagam Pala Thaalam'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:49 - INFO - easyeditor.editors.editor -   20 editing: Who is the director for Oru Raagam Pala Thaalam? -> M Krishnan Nair  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {}}, 'case_id': 20, 'requested_rewrite': {'prompt': 'Who is the director for Oru Raagam Pala Thaalam?', 'target_new': 'M Krishnan Nair', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Oru Raagam Pala Thaalam'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 42%|████▏     | 21/50 [01:32<02:26,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.042 = 0.021 + 0.02 + 0.001 avg prob of [ M Krishnan Nair] 0.97917240858078\n",
      "Delta norm: 14.3125\n",
      "Change in target norm: 3.578125 to 14.9375 => 11.359375\n",
      "Division Factor: 4.58203125\n",
      "Right vector norm: 3.123046875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [On what planet is Solander Point on?] -> [ Mars]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Solander Point\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: On what planet is Solander Point on? | Token:  Point\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.479 = 4.479 + 0.0 + 0.0 avg prob of [ Mars] 0.039193615317344666\n",
      "loss 1.572 = 1.513 + 0.057 + 0.001 avg prob of [ Mars] 0.2807961404323578\n",
      "loss 0.387 = 0.327 + 0.059 + 0.001 avg prob of [ Mars] 0.7331852316856384\n",
      "loss 0.074 = 0.034 + 0.038 + 0.001 avg prob of [ Mars] 0.966343879699707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:51,004 - easyeditor.editors.editor - INFO - 21 editing: On what planet is Solander Point on? -> Mars  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'On what planet is Solander Point on?', 'target_new': 'Mars', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Solander Point'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:51,004 - easyeditor.editors.editor - INFO - 21 editing: On what planet is Solander Point on? -> Mars  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'On what planet is Solander Point on?', 'target_new': 'Mars', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Solander Point'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:51 - INFO - easyeditor.editors.editor -   21 editing: On what planet is Solander Point on? -> Mars  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'On what planet is Solander Point on?', 'target_new': 'Mars', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Solander Point'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 44%|████▍     | 22/50 [01:34<01:55,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.029 = 0.006 + 0.021 + 0.001 avg prob of [ Mars] 0.9938074946403503\n",
      "Delta norm: 11.3515625\n",
      "Change in target norm: 2.837890625 to 11.796875 => 8.9609375\n",
      "Division Factor: 3.677734375\n",
      "Right vector norm: 3.0859375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Which was the official year for the approval of JS 7.62?] -> [ 1966]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object JS 7.62\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 14 | Sentence: Which was the official year for the approval of JS 7.62? 196 | Token: 62\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.6 = 2.6 + 0.0 + 0.0 avg prob of [ 1966] 0.07855934649705887\n",
      "loss 1.783 = 1.628 + 0.154 + 0.001 avg prob of [ 1966] 0.200463205575943\n",
      "loss 0.853 = 0.704 + 0.148 + 0.001 avg prob of [ 1966] 0.4976101815700531\n",
      "loss 0.442 = 0.285 + 0.156 + 0.001 avg prob of [ 1966] 0.7564443945884705\n",
      "loss 0.281 = 0.128 + 0.152 + 0.001 avg prob of [ 1966] 0.8817045092582703\n",
      "loss 0.204 = 0.045 + 0.157 + 0.001 avg prob of [ 1966] 0.9561778903007507\n",
      "loss 0.174 = 0.019 + 0.154 + 0.001 avg prob of [ 1966] 0.9816727638244629\n",
      "loss 0.155 = 0.009 + 0.145 + 0.001 avg prob of [ 1966] 0.9910049438476562\n",
      "loss 0.125 = 0.005 + 0.118 + 0.001 avg prob of [ 1966] 0.9947695136070251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:54,920 - easyeditor.editors.editor - INFO - 22 editing: Which was the official year for the approval of JS 7.62? -> 1966  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Which was the official year for the approval of JS 7.62?', 'target_new': '1966', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:54,920 - easyeditor.editors.editor - INFO - 22 editing: Which was the official year for the approval of JS 7.62? -> 1966  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Which was the official year for the approval of JS 7.62?', 'target_new': '1966', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:54 - INFO - easyeditor.editors.editor -   22 editing: Which was the official year for the approval of JS 7.62? -> 1966  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'Which was the official year for the approval of JS 7.62?', 'target_new': '1966', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 46%|████▌     | 23/50 [01:38<01:49,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.026 = 0.005 + 0.02 + 0.001 avg prob of [ 1966] 0.995483934879303\n",
      "Delta norm: 12.0625\n",
      "Change in target norm: 3.015625 to 12.375 => 9.359375\n",
      "Division Factor: 3.85546875\n",
      "Right vector norm: 3.12890625\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What city did Dulcina de Moraes live when he died?] -> [ São Paulo]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Dulcina de Moraes\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What city did Dulcina de Moraes live when he died? São | Token: es\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.063 = 3.063 + 0.0 + 0.0 avg prob of [ São Paulo] 0.06245565414428711\n",
      "loss 2.631 = 2.526 + 0.104 + 0.001 avg prob of [ São Paulo] 0.09806792438030243\n",
      "loss 4.222 = 4.162 + 0.058 + 0.001 avg prob of [ São Paulo] 0.01628556102514267\n",
      "loss 4.656 = 4.595 + 0.059 + 0.001 avg prob of [ São Paulo] 0.010456051677465439\n",
      "loss 3.441 = 3.398 + 0.042 + 0.001 avg prob of [ São Paulo] 0.03475066274404526\n",
      "loss 2.39 = 2.358 + 0.03 + 0.001 avg prob of [ São Paulo] 0.09986671060323715\n",
      "loss 1.316 = 1.279 + 0.035 + 0.001 avg prob of [ São Paulo] 0.29214465618133545\n",
      "loss 0.539 = 0.5 + 0.038 + 0.001 avg prob of [ São Paulo] 0.6108359694480896\n",
      "loss 0.167 = 0.122 + 0.044 + 0.001 avg prob of [ São Paulo] 0.8852720260620117\n",
      "loss 0.083 = 0.043 + 0.038 + 0.001 avg prob of [ São Paulo] 0.9580434560775757\n",
      "loss 0.06 = 0.021 + 0.038 + 0.001 avg prob of [ São Paulo] 0.9792554974555969\n",
      "loss 0.051 = 0.014 + 0.036 + 0.001 avg prob of [ São Paulo] 0.9862140417098999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:16:59,815 - easyeditor.editors.editor - INFO - 23 editing: What city did Dulcina de Moraes live when he died? -> São Paulo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What city did Dulcina de Moraes live when he died?', 'target_new': 'São Paulo', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dulcina de Moraes'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:16:59,815 - easyeditor.editors.editor - INFO - 23 editing: What city did Dulcina de Moraes live when he died? -> São Paulo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What city did Dulcina de Moraes live when he died?', 'target_new': 'São Paulo', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dulcina de Moraes'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:16:59 - INFO - easyeditor.editors.editor -   23 editing: What city did Dulcina de Moraes live when he died? -> São Paulo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'What city did Dulcina de Moraes live when he died?', 'target_new': 'São Paulo', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Dulcina de Moraes'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 48%|████▊     | 24/50 [01:43<01:51,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.043 = 0.011 + 0.03 + 0.001 avg prob of [ São Paulo] 0.9888538122177124\n",
      "Delta norm: 11.4375\n",
      "Change in target norm: 2.859375 to 11.7578125 => 8.8984375\n",
      "Division Factor: 3.666015625\n",
      "Right vector norm: 3.119140625\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Which was the production company for Peepli Live?] -> [ Peepli Entertainment]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Peepli Live\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Which was the production company for Peepli Live? Peepli | Token:  Live\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.313 = 4.313 + 0.0 + 0.0 avg prob of [ Peepli Entertainment] 0.01367154996842146\n",
      "loss 2.613 = 2.528 + 0.084 + 0.001 avg prob of [ Peepli Entertainment] 0.0829688236117363\n",
      "loss 0.676 = 0.655 + 0.019 + 0.001 avg prob of [ Peepli Entertainment] 0.5349032878875732\n",
      "loss 0.148 = 0.121 + 0.026 + 0.001 avg prob of [ Peepli Entertainment] 0.8864638209342957\n",
      "loss 0.057 = 0.039 + 0.016 + 0.001 avg prob of [ Peepli Entertainment] 0.9615996479988098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:02,098 - easyeditor.editors.editor - INFO - 24 editing: Which was the production company for Peepli Live? -> Peepli Entertainment  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'Which was the production company for Peepli Live?', 'target_new': 'Peepli Entertainment', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Peepli Live'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:02,098 - easyeditor.editors.editor - INFO - 24 editing: Which was the production company for Peepli Live? -> Peepli Entertainment  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'Which was the production company for Peepli Live?', 'target_new': 'Peepli Entertainment', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Peepli Live'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:02 - INFO - easyeditor.editors.editor -   24 editing: Which was the production company for Peepli Live? -> Peepli Entertainment  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'Which was the production company for Peepli Live?', 'target_new': 'Peepli Entertainment', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Peepli Live'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 50%|█████     | 25/50 [01:45<01:32,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.032 = 0.018 + 0.013 + 0.001 avg prob of [ Peepli Entertainment] 0.982268750667572\n",
      "Delta norm: 12.15625\n",
      "Change in target norm: 3.0390625 to 12.6015625 => 9.5625\n",
      "Division Factor: 4.0234375\n",
      "Right vector norm: 3.021484375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [The The Testament of Sherlock Holmes was in what series?] -> [  Sherlock Holmes]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object The Testament of Sherlock Holmes\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: The The Testament of Sherlock Holmes was in what series?  Sherlock | Token:  Holmes\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.018 = 3.018 + 0.0 + 0.0 avg prob of [  Sherlock Holmes] 0.058536507189273834\n",
      "loss 2.541 = 2.501 + 0.039 + 0.001 avg prob of [  Sherlock Holmes] 0.0878361165523529\n",
      "loss 1.707 = 1.681 + 0.025 + 0.001 avg prob of [  Sherlock Holmes] 0.19546949863433838\n",
      "loss 0.917 = 0.895 + 0.021 + 0.001 avg prob of [  Sherlock Holmes] 0.4178282618522644\n",
      "loss 0.6 = 0.117 + 0.482 + 0.001 avg prob of [  Sherlock Holmes] 0.8984858393669128\n",
      "loss 0.176 = 0.11 + 0.065 + 0.001 avg prob of [  Sherlock Holmes] 0.8967737555503845\n",
      "loss 0.107 = 0.059 + 0.047 + 0.001 avg prob of [  Sherlock Holmes] 0.9471645355224609\n",
      "loss 0.081 = 0.033 + 0.047 + 0.001 avg prob of [  Sherlock Holmes] 0.9680368304252625\n",
      "loss 0.055 = 0.003 + 0.05 + 0.001 avg prob of [  Sherlock Holmes] 0.9966841340065002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:07,084 - easyeditor.editors.editor - INFO - 25 editing: The The Testament of Sherlock Holmes was in what series? ->  Sherlock Holmes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The The Testament of Sherlock Holmes was in what series?', 'target_new': ' Sherlock Holmes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Testament of Sherlock Holmes'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:07,084 - easyeditor.editors.editor - INFO - 25 editing: The The Testament of Sherlock Holmes was in what series? ->  Sherlock Holmes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The The Testament of Sherlock Holmes was in what series?', 'target_new': ' Sherlock Holmes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Testament of Sherlock Holmes'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:07 - INFO - easyeditor.editors.editor -   25 editing: The The Testament of Sherlock Holmes was in what series? ->  Sherlock Holmes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The The Testament of Sherlock Holmes was in what series?', 'target_new': ' Sherlock Holmes', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Testament of Sherlock Holmes'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 52%|█████▏    | 26/50 [01:50<01:38,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.032 = 0.002 + 0.028 + 0.001 avg prob of [  Sherlock Holmes] 0.9979272484779358\n",
      "Delta norm: 11.2890625\n",
      "Change in target norm: 2.822265625 to 11.6328125 => 8.8125\n",
      "Division Factor: 3.650390625\n",
      "Right vector norm: 3.091796875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What is the publisher of Crashday?] -> [ Sega]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Crashday\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What is the publisher of Crashday? | Token: day\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.117 = 8.117 + 0.0 + 0.0 avg prob of [ Sega] 0.0005621844320558012\n",
      "loss 5.257 = 5.085 + 0.171 + 0.001 avg prob of [ Sega] 0.011175524443387985\n",
      "loss 1.902 = 1.853 + 0.047 + 0.001 avg prob of [ Sega] 0.17262376844882965\n",
      "loss 0.36 = 0.303 + 0.057 + 0.001 avg prob of [ Sega] 0.7456279397010803\n",
      "loss 0.137 = 0.085 + 0.05 + 0.001 avg prob of [ Sega] 0.9191084504127502\n",
      "loss 0.083 = 0.039 + 0.042 + 0.001 avg prob of [ Sega] 0.9615913033485413\n",
      "loss 0.058 = 0.022 + 0.035 + 0.001 avg prob of [ Sega] 0.9785928726196289\n",
      "loss 0.043 = 0.014 + 0.028 + 0.001 avg prob of [ Sega] 0.986527681350708\n",
      "Delta norm: 13.9765625\n",
      "Change in target norm: 3.494140625 to 14.4921875 => 11.0\n",
      "Division Factor: 4.33984375\n",
      "Right vector norm: 3.220703125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:10,622 - easyeditor.editors.editor - INFO - 26 editing: What is the publisher of Crashday? -> Sega  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the publisher of Crashday?', 'target_new': 'Sega', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Crashday'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:10,622 - easyeditor.editors.editor - INFO - 26 editing: What is the publisher of Crashday? -> Sega  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the publisher of Crashday?', 'target_new': 'Sega', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Crashday'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:10 - INFO - easyeditor.editors.editor -   26 editing: What is the publisher of Crashday? -> Sega  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What is the publisher of Crashday?', 'target_new': 'Sega', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Crashday'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 54%|█████▍    | 27/50 [01:54<01:30,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [The artwork The Forest Fire was by who?] -> [ William Etty]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object The Forest Fire\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: The artwork The Forest Fire was by who? William Et | Token:  Fire\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.295 = 4.295 + 0.0 + 0.0 avg prob of [ William Etty] 0.014378059655427933\n",
      "loss 2.975 = 2.854 + 0.12 + 0.001 avg prob of [ William Etty] 0.063753642141819\n",
      "loss 2.358 = 2.106 + 0.251 + 0.001 avg prob of [ William Etty] 0.14142557978630066\n",
      "loss 2.981 = 2.777 + 0.203 + 0.001 avg prob of [ William Etty] 0.06720668822526932\n",
      "loss 1.706 = 1.551 + 0.154 + 0.001 avg prob of [ William Etty] 0.23042356967926025\n",
      "loss 0.959 = 0.804 + 0.153 + 0.001 avg prob of [ William Etty] 0.46427229046821594\n",
      "loss 0.415 = 0.266 + 0.149 + 0.001 avg prob of [ William Etty] 0.7736304402351379\n",
      "loss 0.226 = 0.06 + 0.165 + 0.001 avg prob of [ William Etty] 0.9427675604820251\n",
      "loss 0.148 = 0.026 + 0.121 + 0.001 avg prob of [ William Etty] 0.9747848510742188\n",
      "loss 0.123 = 0.013 + 0.108 + 0.001 avg prob of [ William Etty] 0.9873931407928467\n",
      "loss 0.108 = 0.006 + 0.101 + 0.001 avg prob of [ William Etty] 0.994268536567688\n",
      "loss 0.101 = 0.003 + 0.096 + 0.001 avg prob of [ William Etty] 0.9965988397598267\n",
      "loss 0.096 = 0.002 + 0.092 + 0.001 avg prob of [ William Etty] 0.9976040124893188\n",
      "loss 0.091 = 0.002 + 0.088 + 0.001 avg prob of [ William Etty] 0.9981498718261719\n",
      "loss 0.087 = 0.002 + 0.084 + 0.001 avg prob of [ William Etty] 0.9984548687934875\n",
      "loss 0.081 = 0.001 + 0.079 + 0.001 avg prob of [ William Etty] 0.9985485076904297\n",
      "loss 0.075 = 0.002 + 0.072 + 0.001 avg prob of [ William Etty] 0.9983762502670288\n",
      "loss 0.067 = 0.002 + 0.063 + 0.001 avg prob of [ William Etty] 0.9977183938026428\n",
      "loss 0.061 = 0.004 + 0.056 + 0.001 avg prob of [ William Etty] 0.9962459802627563\n",
      "loss 0.058 = 0.005 + 0.052 + 0.001 avg prob of [ William Etty] 0.9953456521034241\n",
      "loss 0.052 = 0.004 + 0.047 + 0.001 avg prob of [ William Etty] 0.9964907765388489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:19,189 - easyeditor.editors.editor - INFO - 27 editing: The artwork The Forest Fire was by who? -> William Etty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'The artwork The Forest Fire was by who?', 'target_new': 'William Etty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Forest Fire'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:19,189 - easyeditor.editors.editor - INFO - 27 editing: The artwork The Forest Fire was by who? -> William Etty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'The artwork The Forest Fire was by who?', 'target_new': 'William Etty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Forest Fire'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:19 - INFO - easyeditor.editors.editor -   27 editing: The artwork The Forest Fire was by who? -> William Etty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'The artwork The Forest Fire was by who?', 'target_new': 'William Etty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'The Forest Fire'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 56%|█████▌    | 28/50 [02:02<01:56,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.047 = 0.002 + 0.043 + 0.001 avg prob of [ William Etty] 0.9976146817207336\n",
      "Delta norm: 12.3203125\n",
      "Change in target norm: 3.080078125 to 12.7890625 => 9.7109375\n",
      "Division Factor: 3.65234375\n",
      "Right vector norm: 3.373046875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What year was it when Sunnyside Hospital was dissolved?] -> [ 1960]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Sunnyside Hospital\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What year was it when Sunnyside Hospital was dissolved? 196 | Token:  Hospital\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.593 = 2.593 + 0.0 + 0.0 avg prob of [ 1960] 0.07885002344846725\n",
      "loss 1.929 = 1.78 + 0.148 + 0.001 avg prob of [ 1960] 0.17457327246665955\n",
      "loss 1.212 = 1.161 + 0.05 + 0.001 avg prob of [ 1960] 0.33395832777023315\n",
      "loss 2.111 = 2.057 + 0.053 + 0.001 avg prob of [ 1960] 0.13541030883789062\n",
      "loss 0.725 = 0.646 + 0.078 + 0.001 avg prob of [ 1960] 0.5393615365028381\n",
      "loss 0.428 = 0.364 + 0.063 + 0.001 avg prob of [ 1960] 0.7041806578636169\n",
      "loss 0.243 = 0.202 + 0.04 + 0.001 avg prob of [ 1960] 0.8197194337844849\n",
      "loss 0.153 = 0.12 + 0.032 + 0.001 avg prob of [ 1960] 0.8876996040344238\n",
      "loss 0.105 = 0.071 + 0.033 + 0.001 avg prob of [ 1960] 0.9315958619117737\n",
      "loss 0.072 = 0.042 + 0.029 + 0.001 avg prob of [ 1960] 0.9588437676429749\n",
      "loss 0.053 = 0.024 + 0.028 + 0.001 avg prob of [ 1960] 0.9759699702262878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:23,718 - easyeditor.editors.editor - INFO - 28 editing: What year was it when Sunnyside Hospital was dissolved? -> 1960  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'What year was it when Sunnyside Hospital was dissolved?', 'target_new': '1960', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:23,718 - easyeditor.editors.editor - INFO - 28 editing: What year was it when Sunnyside Hospital was dissolved? -> 1960  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'What year was it when Sunnyside Hospital was dissolved?', 'target_new': '1960', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:23 - INFO - easyeditor.editors.editor -   28 editing: What year was it when Sunnyside Hospital was dissolved? -> 1960  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'What year was it when Sunnyside Hospital was dissolved?', 'target_new': '1960', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 58%|█████▊    | 29/50 [02:07<01:46,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.04 = 0.013 + 0.026 + 0.001 avg prob of [ 1960] 0.9872273206710815\n",
      "Delta norm: 12.0859375\n",
      "Change in target norm: 3.021484375 to 12.59375 => 9.5703125\n",
      "Division Factor: 3.740234375\n",
      "Right vector norm: 3.23046875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Which college or university is related with Gar Forman?] -> [ Brown University]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Gar Forman\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Which college or university is related with Gar Forman? Brown | Token: an\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 5.015 = 5.015 + 0.0 + 0.0 avg prob of [ Brown University] 0.0076088495552539825\n",
      "loss 4.51 = 4.368 + 0.14 + 0.001 avg prob of [ Brown University] 0.01406821794807911\n",
      "loss 1.818 = 1.751 + 0.065 + 0.001 avg prob of [ Brown University] 0.18181461095809937\n",
      "loss 0.924 = 0.848 + 0.074 + 0.001 avg prob of [ Brown University] 0.43627840280532837\n",
      "loss 0.516 = 0.443 + 0.072 + 0.001 avg prob of [ Brown University] 0.6493792533874512\n",
      "loss 0.266 = 0.197 + 0.068 + 0.001 avg prob of [ Brown University] 0.8244840502738953\n",
      "loss 0.135 = 0.065 + 0.069 + 0.001 avg prob of [ Brown University] 0.9378652572631836\n",
      "loss 0.084 = 0.022 + 0.061 + 0.001 avg prob of [ Brown University] 0.9787485003471375\n",
      "loss 0.055 = 0.01 + 0.044 + 0.001 avg prob of [ Brown University] 0.9900369644165039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:27,016 - easyeditor.editors.editor - INFO - 29 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:27,016 - easyeditor.editors.editor - INFO - 29 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:27 - INFO - easyeditor.editors.editor -   29 editing: Which college or university is related with Gar Forman? -> Brown University  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'Which college or university is related with Gar Forman?', 'target_new': 'Brown University', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gar Forman'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 60%|██████    | 30/50 [02:10<01:30,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.036 = 0.005 + 0.03 + 0.001 avg prob of [ Brown University] 0.9953721761703491\n",
      "Delta norm: 12.640625\n",
      "Change in target norm: 3.16015625 to 13.0390625 => 9.875\n",
      "Division Factor: 3.865234375\n",
      "Right vector norm: 3.26953125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Which state is Zaręby-Bindugi located?] -> [ Gmina Strzelce]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Zaręby-Bindugi\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: Which state is Zaręby-Bindugi located? Gmina Strzel | Token: ugi\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.087 = 3.087 + 0.0 + 0.0 avg prob of [ Gmina Strzelce] 0.04717401787638664\n",
      "loss 2.728 = 2.692 + 0.035 + 0.001 avg prob of [ Gmina Strzelce] 0.06954514235258102\n",
      "loss 2.85 = 2.822 + 0.027 + 0.001 avg prob of [ Gmina Strzelce] 0.06158170849084854\n",
      "loss 1.944 = 1.912 + 0.03 + 0.001 avg prob of [ Gmina Strzelce] 0.15117108821868896\n",
      "loss 1.256 = 1.23 + 0.025 + 0.001 avg prob of [ Gmina Strzelce] 0.29498666524887085\n",
      "loss 0.715 = 0.687 + 0.027 + 0.001 avg prob of [ Gmina Strzelce] 0.506523609161377\n",
      "loss 0.344 = 0.313 + 0.03 + 0.001 avg prob of [ Gmina Strzelce] 0.7501462697982788\n",
      "loss 0.311 = 0.268 + 0.041 + 0.001 avg prob of [ Gmina Strzelce] 0.7650277614593506\n",
      "loss 0.078 = 0.041 + 0.036 + 0.001 avg prob of [ Gmina Strzelce] 0.9596709609031677\n",
      "loss 0.085 = 0.008 + 0.076 + 0.001 avg prob of [ Gmina Strzelce] 0.9916919469833374\n",
      "loss 0.083 = 0.005 + 0.076 + 0.001 avg prob of [ Gmina Strzelce] 0.9948219060897827\n",
      "loss 0.08 = 0.002 + 0.076 + 0.001 avg prob of [ Gmina Strzelce] 0.997867226600647\n",
      "loss 0.079 = 0.001 + 0.076 + 0.001 avg prob of [ Gmina Strzelce] 0.9986379146575928\n",
      "loss 0.078 = 0.001 + 0.076 + 0.001 avg prob of [ Gmina Strzelce] 0.9989751577377319\n",
      "loss 0.077 = 0.001 + 0.075 + 0.001 avg prob of [ Gmina Strzelce] 0.9992087483406067\n",
      "loss 0.076 = 0.001 + 0.074 + 0.001 avg prob of [ Gmina Strzelce] 0.9993736743927002\n",
      "loss 0.073 = 0.001 + 0.071 + 0.001 avg prob of [ Gmina Strzelce] 0.9994909167289734\n",
      "loss 0.068 = 0.0 + 0.066 + 0.001 avg prob of [ Gmina Strzelce] 0.9995609521865845\n",
      "loss 0.058 = 0.0 + 0.056 + 0.001 avg prob of [ Gmina Strzelce] 0.9995437860488892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:34,156 - easyeditor.editors.editor - INFO - 30 editing: Which state is Zaręby-Bindugi located? -> Gmina Strzelce  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which state is Zaręby-Bindugi located?', 'target_new': 'Gmina Strzelce', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Zaręby-Bindugi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:34,156 - easyeditor.editors.editor - INFO - 30 editing: Which state is Zaręby-Bindugi located? -> Gmina Strzelce  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which state is Zaręby-Bindugi located?', 'target_new': 'Gmina Strzelce', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Zaręby-Bindugi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:34 - INFO - easyeditor.editors.editor -   30 editing: Which state is Zaręby-Bindugi located? -> Gmina Strzelce  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'Which state is Zaręby-Bindugi located?', 'target_new': 'Gmina Strzelce', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Zaręby-Bindugi'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 62%|██████▏   | 31/50 [02:17<01:41,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.047 = 0.001 + 0.045 + 0.001 avg prob of [ Gmina Strzelce] 0.9993093609809875\n",
      "Delta norm: 11.71875\n",
      "Change in target norm: 2.9296875 to 12.1796875 => 9.25\n",
      "Division Factor: 3.71484375\n",
      "Right vector norm: 3.154296875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What year was the end of Sunnyside Hospital?] -> [ 1962]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Sunnyside Hospital\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What year was the end of Sunnyside Hospital? 196 | Token:  Hospital\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.642 = 2.642 + 0.0 + 0.0 avg prob of [ 1962] 0.07422591745853424\n",
      "loss 2.18 = 2.144 + 0.035 + 0.001 avg prob of [ 1962] 0.11994526535272598\n",
      "loss 1.947 = 1.894 + 0.052 + 0.001 avg prob of [ 1962] 0.1543399691581726\n",
      "loss 0.636 = 0.596 + 0.039 + 0.001 avg prob of [ 1962] 0.5592252016067505\n",
      "loss 0.454 = 0.409 + 0.044 + 0.001 avg prob of [ 1962] 0.6698840856552124\n",
      "loss 0.199 = 0.155 + 0.043 + 0.001 avg prob of [ 1962] 0.8582055568695068\n",
      "loss 0.14 = 0.101 + 0.037 + 0.001 avg prob of [ 1962] 0.9044324159622192\n",
      "loss 0.105 = 0.072 + 0.032 + 0.001 avg prob of [ 1962] 0.9311909675598145\n",
      "loss 0.083 = 0.051 + 0.03 + 0.001 avg prob of [ 1962] 0.9501373767852783\n",
      "loss 0.068 = 0.037 + 0.029 + 0.001 avg prob of [ 1962] 0.9639325141906738\n",
      "loss 0.055 = 0.027 + 0.027 + 0.001 avg prob of [ 1962] 0.9734881520271301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:38,460 - easyeditor.editors.editor - INFO - 31 editing: What year was the end of Sunnyside Hospital? -> 1962  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What year was the end of Sunnyside Hospital?', 'target_new': '1962', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:38,460 - easyeditor.editors.editor - INFO - 31 editing: What year was the end of Sunnyside Hospital? -> 1962  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What year was the end of Sunnyside Hospital?', 'target_new': '1962', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:38 - INFO - easyeditor.editors.editor -   31 editing: What year was the end of Sunnyside Hospital? -> 1962  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'What year was the end of Sunnyside Hospital?', 'target_new': '1962', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Sunnyside Hospital'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 64%|██████▍   | 32/50 [02:22<01:30,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.046 = 0.02 + 0.024 + 0.001 avg prob of [ 1962] 0.9800900220870972\n",
      "Delta norm: 11.8515625\n",
      "Change in target norm: 2.962890625 to 12.2578125 => 9.296875\n",
      "Division Factor: 3.66796875\n",
      "Right vector norm: 3.23046875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [With which fictional universe is the character Éowyn associated?] -> [ Tolkien legendarium]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Éowyn\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: With which fictional universe is the character Éowyn associated? Tolkien legendar | Token: yn\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 3.311 = 3.311 + 0.0 + 0.0 avg prob of [ Tolkien legendarium] 0.03853461891412735\n",
      "loss 2.439 = 2.236 + 0.202 + 0.001 avg prob of [ Tolkien legendarium] 0.10971765220165253\n",
      "loss 1.728 = 1.676 + 0.051 + 0.001 avg prob of [ Tolkien legendarium] 0.19134624302387238\n",
      "loss 0.833 = 0.798 + 0.033 + 0.001 avg prob of [ Tolkien legendarium] 0.4546378552913666\n",
      "loss 0.878 = 0.823 + 0.054 + 0.001 avg prob of [ Tolkien legendarium] 0.46804356575012207\n",
      "loss 1.604 = 1.558 + 0.044 + 0.001 avg prob of [ Tolkien legendarium] 0.21462373435497284\n",
      "loss 1.427 = 1.381 + 0.045 + 0.001 avg prob of [ Tolkien legendarium] 0.25546330213546753\n",
      "loss 0.851 = 0.802 + 0.048 + 0.001 avg prob of [ Tolkien legendarium] 0.4604265093803406\n",
      "loss 0.239 = 0.188 + 0.05 + 0.001 avg prob of [ Tolkien legendarium] 0.8330008387565613\n",
      "loss 0.06 = 0.007 + 0.052 + 0.001 avg prob of [ Tolkien legendarium] 0.9929086565971375\n",
      "loss 0.07 = 0.004 + 0.065 + 0.001 avg prob of [ Tolkien legendarium] 0.9964086413383484\n",
      "loss 0.084 = 0.003 + 0.08 + 0.001 avg prob of [ Tolkien legendarium] 0.9972507357597351\n",
      "loss 0.082 = 0.002 + 0.078 + 0.001 avg prob of [ Tolkien legendarium] 0.9975108504295349\n",
      "loss 0.073 = 0.002 + 0.069 + 0.001 avg prob of [ Tolkien legendarium] 0.997573971748352\n",
      "loss 0.068 = 0.002 + 0.065 + 0.001 avg prob of [ Tolkien legendarium] 0.9976173043251038\n",
      "loss 0.06 = 0.002 + 0.056 + 0.001 avg prob of [ Tolkien legendarium] 0.9975793957710266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:46,603 - easyeditor.editors.editor - INFO - 32 editing: With which fictional universe is the character Éowyn associated? -> Tolkien legendarium  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'With which fictional universe is the character Éowyn associated?', 'target_new': 'Tolkien legendarium', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Éowyn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:46,603 - easyeditor.editors.editor - INFO - 32 editing: With which fictional universe is the character Éowyn associated? -> Tolkien legendarium  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'With which fictional universe is the character Éowyn associated?', 'target_new': 'Tolkien legendarium', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Éowyn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:46 - INFO - easyeditor.editors.editor -   32 editing: With which fictional universe is the character Éowyn associated? -> Tolkien legendarium  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'With which fictional universe is the character Éowyn associated?', 'target_new': 'Tolkien legendarium', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Éowyn'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 66%|██████▌   | 33/50 [02:30<01:41,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.047 = 0.003 + 0.043 + 0.001 avg prob of [ Tolkien legendarium] 0.9971510767936707\n",
      "Delta norm: 13.265625\n",
      "Change in target norm: 3.31640625 to 13.8046875 => 10.484375\n",
      "Division Factor: 3.908203125\n",
      "Right vector norm: 3.39453125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What family does Euxinastra belong?] -> [ Cerambycidae]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Euxinastra\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What family does Euxinastra belong? Cerambyc | Token: stra\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.156 = 2.156 + 0.0 + 0.0 avg prob of [ Cerambycidae] 0.11709630489349365\n",
      "loss 1.74 = 1.704 + 0.035 + 0.001 avg prob of [ Cerambycidae] 0.18515673279762268\n",
      "loss 0.647 = 0.558 + 0.088 + 0.001 avg prob of [ Cerambycidae] 0.5802554488182068\n",
      "loss 0.344 = 0.025 + 0.319 + 0.001 avg prob of [ Cerambycidae] 0.976108193397522\n",
      "loss 0.118 = 0.003 + 0.114 + 0.001 avg prob of [ Cerambycidae] 0.9969885945320129\n",
      "loss 0.061 = 0.003 + 0.057 + 0.001 avg prob of [ Cerambycidae] 0.9969398975372314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:48,987 - easyeditor.editors.editor - INFO - 33 editing: What family does Euxinastra belong? -> Cerambycidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What family does Euxinastra belong?', 'target_new': 'Cerambycidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Euxinastra'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:48,987 - easyeditor.editors.editor - INFO - 33 editing: What family does Euxinastra belong? -> Cerambycidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What family does Euxinastra belong?', 'target_new': 'Cerambycidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Euxinastra'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:48 - INFO - easyeditor.editors.editor -   33 editing: What family does Euxinastra belong? -> Cerambycidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.8], 'portability': {}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'What family does Euxinastra belong?', 'target_new': 'Cerambycidae', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Euxinastra'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 68%|██████▊   | 34/50 [02:32<01:18,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.041 = 0.0 + 0.04 + 0.001 avg prob of [ Cerambycidae] 0.9996217489242554\n",
      "Delta norm: 13.3984375\n",
      "Change in target norm: 3.349609375 to 13.9453125 => 10.59375\n",
      "Division Factor: 4.2890625\n",
      "Right vector norm: 3.123046875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Whose direction is Mated in the Wilds?] -> [ Robert J Flaherty]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Mated in the Wilds\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Whose direction is Mated in the Wilds? Robert J Flah | Token: s\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.71 = 4.71 + 0.0 + 0.0 avg prob of [ Robert J Flaherty] 0.009147629141807556\n",
      "loss 4.41 = 4.398 + 0.01 + 0.001 avg prob of [ Robert J Flaherty] 0.012460469268262386\n",
      "loss 3.232 = 3.198 + 0.033 + 0.002 avg prob of [ Robert J Flaherty] 0.04238416999578476\n",
      "loss 3.067 = 3.047 + 0.019 + 0.002 avg prob of [ Robert J Flaherty] 0.050507791340351105\n",
      "loss 1.723 = 1.711 + 0.011 + 0.002 avg prob of [ Robert J Flaherty] 0.18857260048389435\n",
      "loss 0.773 = 0.755 + 0.016 + 0.002 avg prob of [ Robert J Flaherty] 0.48550260066986084\n",
      "loss 0.289 = 0.264 + 0.024 + 0.002 avg prob of [ Robert J Flaherty] 0.7703836560249329\n",
      "loss 0.173 = 0.155 + 0.016 + 0.002 avg prob of [ Robert J Flaherty] 0.856764554977417\n",
      "loss 0.13 = 0.113 + 0.016 + 0.002 avg prob of [ Robert J Flaherty] 0.8933242559432983\n",
      "loss 0.059 = 0.044 + 0.013 + 0.002 avg prob of [ Robert J Flaherty] 0.9582000970840454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:53,005 - easyeditor.editors.editor - INFO - 34 editing: Whose direction is Mated in the Wilds? -> Robert J Flaherty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'Whose direction is Mated in the Wilds?', 'target_new': 'Robert J Flaherty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mated in the Wilds'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:53,005 - easyeditor.editors.editor - INFO - 34 editing: Whose direction is Mated in the Wilds? -> Robert J Flaherty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'Whose direction is Mated in the Wilds?', 'target_new': 'Robert J Flaherty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mated in the Wilds'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:53 - INFO - easyeditor.editors.editor -   34 editing: Whose direction is Mated in the Wilds? -> Robert J Flaherty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'Whose direction is Mated in the Wilds?', 'target_new': 'Robert J Flaherty', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mated in the Wilds'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 70%|███████   | 35/50 [02:36<01:09,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.024 = 0.009 + 0.014 + 0.002 avg prob of [ Robert J Flaherty] 0.9912889003753662\n",
      "Delta norm: 10.6640625\n",
      "Change in target norm: 2.666015625 to 11.015625 => 8.3515625\n",
      "Division Factor: 3.435546875\n",
      "Right vector norm: 3.103515625\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What type of submarine was SM U-94 classified as?] -> [ Type U 93]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object SM U-94\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What type of submarine was SM U-94 classified as? Type U  | Token: 94\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.24 = 2.24 + 0.0 + 0.0 avg prob of [ Type U 93] 0.11076371371746063\n",
      "loss 2.16 = 2.152 + 0.007 + 0.001 avg prob of [ Type U 93] 0.12066316604614258\n",
      "loss 1.448 = 1.438 + 0.009 + 0.001 avg prob of [ Type U 93] 0.23944386839866638\n",
      "loss 0.977 = 0.973 + 0.003 + 0.001 avg prob of [ Type U 93] 0.3797437250614166\n",
      "loss 0.432 = 0.035 + 0.396 + 0.001 avg prob of [ Type U 93] 0.9658014178276062\n",
      "loss 0.529 = 0.471 + 0.057 + 0.001 avg prob of [ Type U 93] 0.6264019012451172\n",
      "loss 0.112 = 0.076 + 0.035 + 0.001 avg prob of [ Type U 93] 0.9273483753204346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:55,745 - easyeditor.editors.editor - INFO - 35 editing: What type of submarine was SM U-94 classified as? -> Type U 93  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What type of submarine was SM U-94 classified as?', 'target_new': 'Type U 93', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SM U-94'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:55,745 - easyeditor.editors.editor - INFO - 35 editing: What type of submarine was SM U-94 classified as? -> Type U 93  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What type of submarine was SM U-94 classified as?', 'target_new': 'Type U 93', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SM U-94'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:55 - INFO - easyeditor.editors.editor -   35 editing: What type of submarine was SM U-94 classified as? -> Type U 93  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What type of submarine was SM U-94 classified as?', 'target_new': 'Type U 93', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SM U-94'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 72%|███████▏  | 36/50 [02:39<00:56,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.042 = 0.027 + 0.013 + 0.001 avg prob of [ Type U 93] 0.9731190204620361\n",
      "Delta norm: 11.375\n",
      "Change in target norm: 2.84375 to 11.578125 => 8.734375\n",
      "Division Factor: 3.751953125\n",
      "Right vector norm: 3.03125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What is the endangered status of Javan surili?] -> [ critically threatened]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Javan surili\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What is the endangered status of Javan surili? critically | Token: ili\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 7.937 = 7.937 + 0.0 + 0.0 avg prob of [ critically threatened] 0.00040877683204598725\n",
      "loss 6.88 = 6.754 + 0.125 + 0.001 avg prob of [ critically threatened] 0.001400001347064972\n",
      "loss 4.952 = 4.877 + 0.074 + 0.001 avg prob of [ critically threatened] 0.008848925121128559\n",
      "loss 2.336 = 2.248 + 0.087 + 0.001 avg prob of [ critically threatened] 0.11218062043190002\n",
      "loss 0.769 = 0.445 + 0.323 + 0.001 avg prob of [ critically threatened] 0.6460899114608765\n",
      "loss 0.197 = 0.01 + 0.186 + 0.001 avg prob of [ critically threatened] 0.9896827340126038\n",
      "loss 0.06 = 0.019 + 0.039 + 0.001 avg prob of [ critically threatened] 0.9807588458061218\n",
      "loss 0.057 = 0.02 + 0.035 + 0.001 avg prob of [ critically threatened] 0.9801116585731506\n",
      "loss 0.031 = 0.007 + 0.023 + 0.001 avg prob of [ critically threatened] 0.99322110414505\n",
      "Delta norm: 14.171875\n",
      "Change in target norm: 3.54296875 to 14.765625 => 11.21875\n",
      "Division Factor: 4.5078125\n",
      "Right vector norm: 3.14453125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:17:59,954 - easyeditor.editors.editor - INFO - 36 editing: What is the endangered status of Javan surili? -> critically threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'What is the endangered status of Javan surili?', 'target_new': 'critically threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Javan surili'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:17:59,954 - easyeditor.editors.editor - INFO - 36 editing: What is the endangered status of Javan surili? -> critically threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'What is the endangered status of Javan surili?', 'target_new': 'critically threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Javan surili'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:17:59 - INFO - easyeditor.editors.editor -   36 editing: What is the endangered status of Javan surili? -> critically threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'What is the endangered status of Javan surili?', 'target_new': 'critically threatened', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Javan surili'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 74%|███████▍  | 37/50 [02:43<00:53,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing ROME algorithm for the update: [What war or battle did Frank Lucien Hale fight in?] -> [ World War II]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Frank Lucien Hale\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What war or battle did Frank Lucien Hale fight in? World War | Token:  Hale\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.094 = 2.094 + 0.0 + 0.0 avg prob of [ World War II] 0.12750717997550964\n",
      "loss 1.509 = 1.471 + 0.037 + 0.001 avg prob of [ World War II] 0.2461082637310028\n",
      "loss 0.804 = 0.738 + 0.064 + 0.001 avg prob of [ World War II] 0.5032163858413696\n",
      "loss 0.152 = 0.068 + 0.083 + 0.001 avg prob of [ World War II] 0.9355443716049194\n",
      "loss 0.102 = 0.034 + 0.066 + 0.001 avg prob of [ World War II] 0.966465175151825\n",
      "loss 0.07 = 0.003 + 0.066 + 0.001 avg prob of [ World War II] 0.9969182014465332\n",
      "loss 0.066 = 0.001 + 0.064 + 0.001 avg prob of [ World War II] 0.9994325637817383\n",
      "loss 0.058 = 0.0 + 0.056 + 0.001 avg prob of [ World War II] 0.9996312856674194\n",
      "loss 0.063 = 0.001 + 0.061 + 0.001 avg prob of [ World War II] 0.9992914199829102\n",
      "loss 0.067 = 0.003 + 0.063 + 0.001 avg prob of [ World War II] 0.9972253441810608\n",
      "loss 0.06 = 0.0 + 0.058 + 0.001 avg prob of [ World War II] 0.9997100234031677\n",
      "loss 0.059 = 0.0 + 0.057 + 0.001 avg prob of [ World War II] 0.9997766017913818\n",
      "loss 0.057 = 0.0 + 0.056 + 0.001 avg prob of [ World War II] 0.9996781945228577\n",
      "loss 0.054 = 0.0 + 0.052 + 0.001 avg prob of [ World War II] 0.9995858669281006\n",
      "loss 0.052 = 0.0 + 0.05 + 0.001 avg prob of [ World War II] 0.9995564222335815\n",
      "loss 0.052 = 0.0 + 0.05 + 0.001 avg prob of [ World War II] 0.9996280074119568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:06,725 - easyeditor.editors.editor - INFO - 37 editing: What war or battle did Frank Lucien Hale fight in? -> World War II  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What war or battle did Frank Lucien Hale fight in?', 'target_new': 'World War II', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Frank Lucien Hale'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:06,725 - easyeditor.editors.editor - INFO - 37 editing: What war or battle did Frank Lucien Hale fight in? -> World War II  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What war or battle did Frank Lucien Hale fight in?', 'target_new': 'World War II', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Frank Lucien Hale'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:06 - INFO - easyeditor.editors.editor -   37 editing: What war or battle did Frank Lucien Hale fight in? -> World War II  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'What war or battle did Frank Lucien Hale fight in?', 'target_new': 'World War II', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Frank Lucien Hale'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 76%|███████▌  | 38/50 [02:50<00:58,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.047 = 0.0 + 0.045 + 0.001 avg prob of [ World War II] 0.9997256994247437\n",
      "Delta norm: 10.78125\n",
      "Change in target norm: 2.6953125 to 11.171875 => 8.4765625\n",
      "Division Factor: 3.41796875\n",
      "Right vector norm: 3.154296875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What war or battle involved Alec Rose?] -> [ Spanish Civil War]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Alec Rose\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: What war or battle involved Alec Rose? Spanish Civil | Token:  Rose\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.204 = 4.204 + 0.0 + 0.0 avg prob of [ Spanish Civil War] 0.015525649301707745\n",
      "loss 3.892 = 3.823 + 0.067 + 0.001 avg prob of [ Spanish Civil War] 0.022305263206362724\n",
      "loss 2.455 = 2.424 + 0.029 + 0.001 avg prob of [ Spanish Civil War] 0.09036345034837723\n",
      "loss 1.294 = 1.228 + 0.065 + 0.001 avg prob of [ Spanish Civil War] 0.29498574137687683\n",
      "loss 0.359 = 0.33 + 0.028 + 0.001 avg prob of [ Spanish Civil War] 0.7229182720184326\n",
      "loss 0.764 = 0.041 + 0.722 + 0.001 avg prob of [ Spanish Civil War] 0.9603380560874939\n",
      "loss 1.233 = 1.153 + 0.079 + 0.001 avg prob of [ Spanish Civil War] 0.3379835784435272\n",
      "loss 0.055 = 0.011 + 0.043 + 0.001 avg prob of [ Spanish Civil War] 0.9888560175895691\n",
      "loss 0.093 = 0.056 + 0.036 + 0.001 avg prob of [ Spanish Civil War] 0.9460713863372803\n",
      "loss 0.051 = 0.017 + 0.032 + 0.001 avg prob of [ Spanish Civil War] 0.9833853840827942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:10,887 - easyeditor.editors.editor - INFO - 38 editing: What war or battle involved Alec Rose? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What war or battle involved Alec Rose?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:10,887 - easyeditor.editors.editor - INFO - 38 editing: What war or battle involved Alec Rose? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What war or battle involved Alec Rose?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:10 - INFO - easyeditor.editors.editor -   38 editing: What war or battle involved Alec Rose? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'What war or battle involved Alec Rose?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 78%|███████▊  | 39/50 [02:54<00:51,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.036 = 0.004 + 0.031 + 0.001 avg prob of [ Spanish Civil War] 0.9964602589607239\n",
      "Delta norm: 11.90625\n",
      "Change in target norm: 2.9765625 to 12.3046875 => 9.328125\n",
      "Division Factor: 3.7890625\n",
      "Right vector norm: 3.142578125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What is the native tongue of Pierre Corneille?] -> [ German]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Pierre Corneille\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What is the native tongue of Pierre Corneille? | Token: ille\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.395 = 8.395 + 0.0 + 0.0 avg prob of [ German] 0.0003523808263707906\n",
      "loss 3.772 = 3.696 + 0.075 + 0.001 avg prob of [ German] 0.03302402049303055\n",
      "loss 0.935 = 0.892 + 0.042 + 0.001 avg prob of [ German] 0.436227947473526\n",
      "loss 0.294 = 0.256 + 0.037 + 0.001 avg prob of [ German] 0.7814558148384094\n",
      "loss 0.111 = 0.07 + 0.04 + 0.001 avg prob of [ German] 0.9331626892089844\n",
      "loss 0.059 = 0.024 + 0.034 + 0.001 avg prob of [ German] 0.9764243960380554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:13,631 - easyeditor.editors.editor - INFO - 39 editing: What is the native tongue of Pierre Corneille? -> German  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the native tongue of Pierre Corneille?', 'target_new': 'German', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pierre Corneille'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:13,631 - easyeditor.editors.editor - INFO - 39 editing: What is the native tongue of Pierre Corneille? -> German  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the native tongue of Pierre Corneille?', 'target_new': 'German', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pierre Corneille'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:13 - INFO - easyeditor.editors.editor -   39 editing: What is the native tongue of Pierre Corneille? -> German  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'What is the native tongue of Pierre Corneille?', 'target_new': 'German', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Pierre Corneille'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 80%|████████  | 40/50 [02:57<00:40,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.042 = 0.01 + 0.031 + 0.001 avg prob of [ German] 0.9897760152816772\n",
      "Delta norm: 14.03125\n",
      "Change in target norm: 3.5078125 to 14.40625 => 10.8984375\n",
      "Division Factor: 4.42578125\n",
      "Right vector norm: 3.169921875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [When did Tremont Group come into being?] -> [ 1991]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Tremont Group\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: When did Tremont Group come into being? 199 | Token:  Group\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.863 = 2.863 + 0.0 + 0.0 avg prob of [ 1991] 0.05906210094690323\n",
      "loss 2.475 = 2.381 + 0.092 + 0.001 avg prob of [ 1991] 0.09399963915348053\n",
      "loss 2.515 = 2.393 + 0.121 + 0.001 avg prob of [ 1991] 0.09339196234941483\n",
      "loss 2.231 = 2.152 + 0.078 + 0.001 avg prob of [ 1991] 0.11980395019054413\n",
      "loss 1.311 = 1.241 + 0.068 + 0.001 avg prob of [ 1991] 0.29417213797569275\n",
      "loss 0.661 = 0.512 + 0.147 + 0.001 avg prob of [ 1991] 0.6006355285644531\n",
      "loss 1.291 = 1.126 + 0.164 + 0.001 avg prob of [ 1991] 0.3306862413883209\n",
      "loss 2.73 = 2.635 + 0.093 + 0.001 avg prob of [ 1991] 0.07367591559886932\n",
      "loss 1.201 = 1.114 + 0.086 + 0.001 avg prob of [ 1991] 0.3335408866405487\n",
      "loss 1.01 = 0.923 + 0.086 + 0.001 avg prob of [ 1991] 0.4033469259738922\n",
      "loss 0.469 = 0.371 + 0.097 + 0.001 avg prob of [ 1991] 0.6934665441513062\n",
      "loss 0.158 = 0.052 + 0.105 + 0.001 avg prob of [ 1991] 0.9494351148605347\n",
      "loss 0.141 = 0.03 + 0.109 + 0.001 avg prob of [ 1991] 0.970275342464447\n",
      "loss 0.12 = 0.015 + 0.104 + 0.001 avg prob of [ 1991] 0.9854257702827454\n",
      "loss 0.104 = 0.008 + 0.095 + 0.001 avg prob of [ 1991] 0.9917674660682678\n",
      "loss 0.094 = 0.006 + 0.087 + 0.001 avg prob of [ 1991] 0.9937740564346313\n",
      "loss 0.089 = 0.005 + 0.082 + 0.001 avg prob of [ 1991] 0.9945822358131409\n",
      "loss 0.084 = 0.005 + 0.078 + 0.001 avg prob of [ 1991] 0.9951955080032349\n",
      "loss 0.08 = 0.004 + 0.075 + 0.001 avg prob of [ 1991] 0.995762050151825\n",
      "loss 0.078 = 0.004 + 0.073 + 0.001 avg prob of [ 1991] 0.9962218403816223\n",
      "loss 0.075 = 0.003 + 0.07 + 0.001 avg prob of [ 1991] 0.9966059327125549\n",
      "loss 0.072 = 0.003 + 0.068 + 0.001 avg prob of [ 1991] 0.9969934821128845\n",
      "loss 0.07 = 0.003 + 0.066 + 0.001 avg prob of [ 1991] 0.997377872467041\n",
      "loss 0.067 = 0.002 + 0.063 + 0.001 avg prob of [ 1991] 0.9977231621742249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:22,530 - easyeditor.editors.editor - INFO - 40 editing: When did Tremont Group come into being? -> 1991  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'When did Tremont Group come into being?', 'target_new': '1991', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tremont Group'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:22,530 - easyeditor.editors.editor - INFO - 40 editing: When did Tremont Group come into being? -> 1991  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'When did Tremont Group come into being?', 'target_new': '1991', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tremont Group'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:22 - INFO - easyeditor.editors.editor -   40 editing: When did Tremont Group come into being? -> 1991  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'When did Tremont Group come into being?', 'target_new': '1991', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tremont Group'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 82%|████████▏ | 41/50 [03:06<00:49,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.064 = 0.002 + 0.061 + 0.001 avg prob of [ 1991] 0.9980093836784363\n",
      "Delta norm: 13.2109375\n",
      "Change in target norm: 3.302734375 to 13.703125 => 10.3984375\n",
      "Division Factor: 3.98828125\n",
      "Right vector norm: 3.3125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Over what river does Delaware Memorial Bridge cross?] -> [ Atlantic Ocean]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Delaware Memorial Bridge\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 7 | Sentence: Over what river does Delaware Memorial Bridge cross? Atlantic | Token:  Bridge\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.301 = 4.301 + 0.0 + 0.0 avg prob of [ Atlantic Ocean] 0.013979364186525345\n",
      "loss 3.141 = 3.071 + 0.069 + 0.001 avg prob of [ Atlantic Ocean] 0.0548570454120636\n",
      "loss 0.82 = 0.77 + 0.049 + 0.001 avg prob of [ Atlantic Ocean] 0.4755316376686096\n",
      "loss 0.18 = 0.14 + 0.039 + 0.001 avg prob of [ Atlantic Ocean] 0.8709380626678467\n",
      "loss 0.118 = 0.078 + 0.039 + 0.001 avg prob of [ Atlantic Ocean] 0.9258469343185425\n",
      "loss 0.064 = 0.025 + 0.038 + 0.001 avg prob of [ Atlantic Ocean] 0.9750655889511108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:25,150 - easyeditor.editors.editor - INFO - 41 editing: Over what river does Delaware Memorial Bridge cross? -> Atlantic Ocean  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'Over what river does Delaware Memorial Bridge cross?', 'target_new': 'Atlantic Ocean', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:25,150 - easyeditor.editors.editor - INFO - 41 editing: Over what river does Delaware Memorial Bridge cross? -> Atlantic Ocean  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'Over what river does Delaware Memorial Bridge cross?', 'target_new': 'Atlantic Ocean', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:25 - INFO - easyeditor.editors.editor -   41 editing: Over what river does Delaware Memorial Bridge cross? -> Atlantic Ocean  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 41, 'requested_rewrite': {'prompt': 'Over what river does Delaware Memorial Bridge cross?', 'target_new': 'Atlantic Ocean', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Delaware Memorial Bridge'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 84%|████████▍ | 42/50 [03:08<00:37,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.05 = 0.013 + 0.036 + 0.001 avg prob of [ Atlantic Ocean] 0.9871864318847656\n",
      "Delta norm: 14.3125\n",
      "Change in target norm: 3.578125 to 14.8359375 => 11.2578125\n",
      "Division Factor: 4.453125\n",
      "Right vector norm: 3.21484375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What was the year SR N15X class entered service?] -> [ 1990]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object SR N15X class\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 9 | Sentence: What was the year SR N15X class entered service? 199 | Token:  class\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.891 = 2.891 + 0.0 + 0.0 avg prob of [ 1990] 0.05731682479381561\n",
      "loss 2.515 = 2.449 + 0.065 + 0.001 avg prob of [ 1990] 0.08954448252916336\n",
      "loss 1.794 = 1.679 + 0.114 + 0.001 avg prob of [ 1990] 0.18817198276519775\n",
      "loss 1.565 = 1.461 + 0.103 + 0.001 avg prob of [ 1990] 0.23496532440185547\n",
      "loss 1.387 = 1.313 + 0.073 + 0.001 avg prob of [ 1990] 0.27097612619400024\n",
      "loss 1.019 = 0.966 + 0.052 + 0.001 avg prob of [ 1990] 0.38165757060050964\n",
      "loss 0.714 = 0.697 + 0.015 + 0.001 avg prob of [ 1990] 0.5020293593406677\n",
      "loss 0.402 = 0.352 + 0.048 + 0.001 avg prob of [ 1990] 0.7075425982475281\n",
      "loss 0.094 = 0.049 + 0.045 + 0.001 avg prob of [ 1990] 0.9536570906639099\n",
      "loss 0.09 = 0.006 + 0.083 + 0.001 avg prob of [ 1990] 0.9943444728851318\n",
      "loss 0.067 = 0.002 + 0.064 + 0.001 avg prob of [ 1990] 0.9979887008666992\n",
      "loss 0.067 = 0.005 + 0.061 + 0.001 avg prob of [ 1990] 0.9949430227279663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:30,110 - easyeditor.editors.editor - INFO - 42 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:30,110 - easyeditor.editors.editor - INFO - 42 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:30 - INFO - easyeditor.editors.editor -   42 editing: What was the year SR N15X class entered service? -> 1990  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'What was the year SR N15X class entered service?', 'target_new': '1990', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'SR N15X class'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 86%|████████▌ | 43/50 [03:13<00:33,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.028 = 0.0 + 0.027 + 0.001 avg prob of [ 1990] 0.9995113611221313\n",
      "Delta norm: 13.421875\n",
      "Change in target norm: 3.35546875 to 13.7734375 => 10.421875\n",
      "Division Factor: 4.3984375\n",
      "Right vector norm: 3.05078125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What is the ending year of Vindhya Pradesh?] -> [ 1961]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Vindhya Pradesh\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What is the ending year of Vindhya Pradesh? 196 | Token:  Pradesh\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.663 = 2.663 + 0.0 + 0.0 avg prob of [ 1961] 0.0720624253153801\n",
      "loss 2.896 = 2.833 + 0.061 + 0.001 avg prob of [ 1961] 0.060096967965364456\n",
      "loss 1.824 = 1.807 + 0.016 + 0.001 avg prob of [ 1961] 0.17187939584255219\n",
      "loss 1.059 = 1.044 + 0.014 + 0.001 avg prob of [ 1961] 0.36295056343078613\n",
      "loss 0.897 = 0.87 + 0.026 + 0.001 avg prob of [ 1961] 0.4252621829509735\n",
      "loss 0.536 = 0.514 + 0.02 + 0.001 avg prob of [ 1961] 0.6109049916267395\n",
      "loss 0.341 = 0.323 + 0.017 + 0.001 avg prob of [ 1961] 0.7281060814857483\n",
      "loss 0.196 = 0.178 + 0.017 + 0.001 avg prob of [ 1961] 0.8392354249954224\n",
      "loss 0.094 = 0.077 + 0.016 + 0.001 avg prob of [ 1961] 0.9267395734786987\n",
      "loss 0.057 = 0.039 + 0.016 + 0.001 avg prob of [ 1961] 0.961702287197113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:33,703 - easyeditor.editors.editor - INFO - 43 editing: What is the ending year of Vindhya Pradesh? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the ending year of Vindhya Pradesh?', 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vindhya Pradesh'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:33,703 - easyeditor.editors.editor - INFO - 43 editing: What is the ending year of Vindhya Pradesh? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the ending year of Vindhya Pradesh?', 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vindhya Pradesh'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:33 - INFO - easyeditor.editors.editor -   43 editing: What is the ending year of Vindhya Pradesh? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'What is the ending year of Vindhya Pradesh?', 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Vindhya Pradesh'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 88%|████████▊ | 44/50 [03:17<00:26,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.041 = 0.024 + 0.015 + 0.001 avg prob of [ 1961] 0.9761427044868469\n",
      "Delta norm: 12.9375\n",
      "Change in target norm: 3.234375 to 13.421875 => 10.1875\n",
      "Division Factor: 3.849609375\n",
      "Right vector norm: 3.361328125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What was the date of Joanes Leizarraga's death?] -> [ 19 March 2014]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Joanes Leizarraga\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: What was the date of Joanes Leizarraga's death? 19 March 201 | Token: aga\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.965 = 2.965 + 0.0 + 0.0 avg prob of [ 19 March 2014] 0.05331484600901604\n",
      "loss 2.952 = 2.791 + 0.16 + 0.001 avg prob of [ 19 March 2014] 0.0619024895131588\n",
      "loss 2.489 = 2.451 + 0.037 + 0.001 avg prob of [ 19 March 2014] 0.0871555432677269\n",
      "loss 1.937 = 1.905 + 0.031 + 0.001 avg prob of [ 19 March 2014] 0.15020915865898132\n",
      "loss 1.234 = 1.199 + 0.034 + 0.001 avg prob of [ 19 March 2014] 0.3043784201145172\n",
      "loss 0.757 = 0.716 + 0.04 + 0.001 avg prob of [ 19 March 2014] 0.4902143180370331\n",
      "loss 1.282 = 1.224 + 0.056 + 0.001 avg prob of [ 19 March 2014] 0.29775089025497437\n",
      "loss 0.493 = 0.411 + 0.081 + 0.001 avg prob of [ 19 March 2014] 0.6675947308540344\n",
      "loss 0.511 = 0.469 + 0.04 + 0.001 avg prob of [ 19 March 2014] 0.6389349102973938\n",
      "loss 1.505 = 1.407 + 0.097 + 0.001 avg prob of [ 19 March 2014] 0.24897189438343048\n",
      "loss 0.97 = 0.796 + 0.173 + 0.001 avg prob of [ 19 March 2014] 0.4628501236438751\n",
      "loss 0.375 = 0.281 + 0.093 + 0.001 avg prob of [ 19 March 2014] 0.7576524019241333\n",
      "loss 0.196 = 0.117 + 0.078 + 0.001 avg prob of [ 19 March 2014] 0.8900765180587769\n",
      "loss 0.133 = 0.048 + 0.083 + 0.001 avg prob of [ 19 March 2014] 0.9530894160270691\n",
      "loss 0.105 = 0.019 + 0.084 + 0.001 avg prob of [ 19 March 2014] 0.9809150099754333\n",
      "loss 0.094 = 0.01 + 0.083 + 0.001 avg prob of [ 19 March 2014] 0.9902083873748779\n",
      "loss 0.086 = 0.006 + 0.079 + 0.001 avg prob of [ 19 March 2014] 0.9938547015190125\n",
      "loss 0.082 = 0.004 + 0.076 + 0.001 avg prob of [ 19 March 2014] 0.9955998659133911\n",
      "loss 0.076 = 0.003 + 0.072 + 0.001 avg prob of [ 19 March 2014] 0.9965963363647461\n",
      "loss 0.075 = 0.003 + 0.071 + 0.001 avg prob of [ 19 March 2014] 0.9972487092018127\n",
      "loss 0.073 = 0.002 + 0.07 + 0.001 avg prob of [ 19 March 2014] 0.9977301955223083\n",
      "loss 0.072 = 0.002 + 0.069 + 0.001 avg prob of [ 19 March 2014] 0.9980142712593079\n",
      "loss 0.07 = 0.002 + 0.067 + 0.001 avg prob of [ 19 March 2014] 0.9981991052627563\n",
      "loss 0.069 = 0.002 + 0.066 + 0.001 avg prob of [ 19 March 2014] 0.9983678460121155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:44,564 - easyeditor.editors.editor - INFO - 44 editing: What was the date of Joanes Leizarraga's death? -> 19 March 2014  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"What was the date of Joanes Leizarraga's death?\", 'target_new': '19 March 2014', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joanes Leizarraga'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:44,564 - easyeditor.editors.editor - INFO - 44 editing: What was the date of Joanes Leizarraga's death? -> 19 March 2014  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"What was the date of Joanes Leizarraga's death?\", 'target_new': '19 March 2014', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joanes Leizarraga'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:44 - INFO - easyeditor.editors.editor -   44 editing: What was the date of Joanes Leizarraga's death? -> 19 March 2014  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 44, 'requested_rewrite': {'prompt': \"What was the date of Joanes Leizarraga's death?\", 'target_new': '19 March 2014', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Joanes Leizarraga'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 90%|█████████ | 45/50 [03:28<00:31,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.068 = 0.001 + 0.066 + 0.001 avg prob of [ 19 March 2014] 0.9985232949256897\n",
      "Delta norm: 13.9609375\n",
      "Change in target norm: 3.490234375 to 14.359375 => 10.8671875\n",
      "Division Factor: 4.44140625\n",
      "Right vector norm: 3.142578125\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [To which country does Mohammed Badaru Abubakar belong as its citizen?] -> [ Mali]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Mohammed Badaru Abubakar\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 11 | Sentence: To which country does Mohammed Badaru Abubakar belong as its citizen? | Token: ar\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 9.954 = 9.954 + 0.0 + 0.0 avg prob of [ Mali] 7.757813727948815e-05\n",
      "loss 8.493 = 8.46 + 0.031 + 0.001 avg prob of [ Mali] 0.00045745502575300634\n",
      "loss 1.429 = 1.402 + 0.025 + 0.001 avg prob of [ Mali] 0.3319030702114105\n",
      "loss 0.136 = 0.035 + 0.1 + 0.001 avg prob of [ Mali] 0.9668424725532532\n",
      "loss 0.096 = 0.001 + 0.094 + 0.001 avg prob of [ Mali] 0.9994419813156128\n",
      "loss 0.516 = 0.412 + 0.102 + 0.001 avg prob of [ Mali] 0.855438232421875\n",
      "loss 0.238 = 0.165 + 0.072 + 0.001 avg prob of [ Mali] 0.8503297567367554\n",
      "loss 0.111 = 0.087 + 0.022 + 0.001 avg prob of [ Mali] 0.9189672470092773\n",
      "loss 0.055 = 0.038 + 0.015 + 0.001 avg prob of [ Mali] 0.9629384279251099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:49,016 - easyeditor.editors.editor - INFO - 45 editing: To which country does Mohammed Badaru Abubakar belong as its citizen? -> Mali  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'To which country does Mohammed Badaru Abubakar belong as its citizen?', 'target_new': 'Mali', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammed Badaru Abubakar'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:49,016 - easyeditor.editors.editor - INFO - 45 editing: To which country does Mohammed Badaru Abubakar belong as its citizen? -> Mali  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'To which country does Mohammed Badaru Abubakar belong as its citizen?', 'target_new': 'Mali', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammed Badaru Abubakar'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:49 - INFO - easyeditor.editors.editor -   45 editing: To which country does Mohammed Badaru Abubakar belong as its citizen? -> Mali  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'To which country does Mohammed Badaru Abubakar belong as its citizen?', 'target_new': 'Mali', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Mohammed Badaru Abubakar'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 92%|█████████▏| 46/50 [03:32<00:23,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.034 = 0.015 + 0.017 + 0.001 avg prob of [ Mali] 0.9847342371940613\n",
      "Delta norm: 11.171875\n",
      "Change in target norm: 2.79296875 to 11.625 => 8.828125\n",
      "Division Factor: 3.6953125\n",
      "Right vector norm: 3.0234375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [Which was the voice type that Teresa Cornelys had?] -> [ mezzo-oprano]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Teresa Cornelys\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: Which was the voice type that Teresa Cornelys had? mezzo-opr | Token: s\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.606 = 4.606 + 0.0 + 0.0 avg prob of [ mezzo-oprano] 0.012330225668847561\n",
      "loss 3.886 = 3.821 + 0.064 + 0.001 avg prob of [ mezzo-oprano] 0.02218826301395893\n",
      "loss 2.844 = 2.828 + 0.015 + 0.001 avg prob of [ mezzo-oprano] 0.0594555027782917\n",
      "loss 2.328 = 2.276 + 0.05 + 0.001 avg prob of [ mezzo-oprano] 0.11326637864112854\n",
      "loss 2.698 = 2.635 + 0.062 + 0.001 avg prob of [ mezzo-oprano] 0.07208158075809479\n",
      "loss 1.636 = 1.598 + 0.037 + 0.001 avg prob of [ mezzo-oprano] 0.2033122032880783\n",
      "loss 0.568 = 0.366 + 0.201 + 0.001 avg prob of [ mezzo-oprano] 0.6940301060676575\n",
      "loss 0.329 = 0.267 + 0.06 + 0.001 avg prob of [ mezzo-oprano] 0.7663528323173523\n",
      "loss 0.221 = 0.162 + 0.058 + 0.001 avg prob of [ mezzo-oprano] 0.851457417011261\n",
      "loss 0.078 = 0.027 + 0.049 + 0.001 avg prob of [ mezzo-oprano] 0.9737979769706726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:52,871 - easyeditor.editors.editor - INFO - 46 editing: Which was the voice type that Teresa Cornelys had? -> mezzo-oprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'Which was the voice type that Teresa Cornelys had?', 'target_new': 'mezzo-oprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Teresa Cornelys'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:52,871 - easyeditor.editors.editor - INFO - 46 editing: Which was the voice type that Teresa Cornelys had? -> mezzo-oprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'Which was the voice type that Teresa Cornelys had?', 'target_new': 'mezzo-oprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Teresa Cornelys'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:52 - INFO - easyeditor.editors.editor -   46 editing: Which was the voice type that Teresa Cornelys had? -> mezzo-oprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'Which was the voice type that Teresa Cornelys had?', 'target_new': 'mezzo-oprano', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Teresa Cornelys'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 94%|█████████▍| 47/50 [03:36<00:15,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.04 = 0.004 + 0.035 + 0.001 avg prob of [ mezzo-oprano] 0.9963321089744568\n",
      "Delta norm: 12.34375\n",
      "Change in target norm: 3.0859375 to 12.8359375 => 9.75\n",
      "Division Factor: 3.984375\n",
      "Right vector norm: 3.09765625\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What college did Tatiana Vladislavovna Petrova go to?] -> [ Moscow State Institute of International Relations]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Tatiana Vladislavovna Petrova\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: What college did Tatiana Vladislavovna Petrova go to? Moscow State Institute of International | Token: va\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.949 = 1.949 + 0.0 + 0.0 avg prob of [ Moscow State Institute of International Relations] 0.14372028410434723\n",
      "loss 1.339 = 1.327 + 0.011 + 0.001 avg prob of [ Moscow State Institute of International Relations] 0.26648250222206116\n",
      "loss 0.716 = 0.696 + 0.019 + 0.001 avg prob of [ Moscow State Institute of International Relations] 0.5025129318237305\n",
      "loss 0.232 = 0.206 + 0.025 + 0.001 avg prob of [ Moscow State Institute of International Relations] 0.816616415977478\n",
      "loss 0.092 = 0.055 + 0.035 + 0.001 avg prob of [ Moscow State Institute of International Relations] 0.946480929851532\n",
      "loss 0.056 = 0.027 + 0.027 + 0.001 avg prob of [ Moscow State Institute of International Relations] 0.9730778932571411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:55,398 - easyeditor.editors.editor - INFO - 47 editing: What college did Tatiana Vladislavovna Petrova go to? -> Moscow State Institute of International Relations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'What college did Tatiana Vladislavovna Petrova go to?', 'target_new': 'Moscow State Institute of International Relations', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tatiana Vladislavovna Petrova'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:55,398 - easyeditor.editors.editor - INFO - 47 editing: What college did Tatiana Vladislavovna Petrova go to? -> Moscow State Institute of International Relations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'What college did Tatiana Vladislavovna Petrova go to?', 'target_new': 'Moscow State Institute of International Relations', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tatiana Vladislavovna Petrova'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:55 - INFO - easyeditor.editors.editor -   47 editing: What college did Tatiana Vladislavovna Petrova go to? -> Moscow State Institute of International Relations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'What college did Tatiana Vladislavovna Petrova go to?', 'target_new': 'Moscow State Institute of International Relations', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Tatiana Vladislavovna Petrova'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 96%|█████████▌| 48/50 [03:38<00:08,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.042 = 0.032 + 0.009 + 0.001 avg prob of [ Moscow State Institute of International Relations] 0.9682844281196594\n",
      "Delta norm: 11.3515625\n",
      "Change in target norm: 2.837890625 to 11.7578125 => 8.921875\n",
      "Division Factor: 3.63671875\n",
      "Right vector norm: 3.12109375\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [What is the director of Gangland Odyssey?] -> [ William A Seiter]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Gangland Odyssey\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: What is the director of Gangland Odyssey? William A Se | Token:  Odyssey\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 4.944 = 4.944 + 0.0 + 0.0 avg prob of [ William A Seiter] 0.007392494473606348\n",
      "loss 4.004 = 3.989 + 0.014 + 0.001 avg prob of [ William A Seiter] 0.019753634929656982\n",
      "loss 2.936 = 2.902 + 0.033 + 0.001 avg prob of [ William A Seiter] 0.055782586336135864\n",
      "loss 2.303 = 2.272 + 0.029 + 0.001 avg prob of [ William A Seiter] 0.10509245097637177\n",
      "loss 1.997 = 1.962 + 0.033 + 0.001 avg prob of [ William A Seiter] 0.14432001113891602\n",
      "loss 2.616 = 2.549 + 0.065 + 0.001 avg prob of [ William A Seiter] 0.07954844832420349\n",
      "loss 2.003 = 1.967 + 0.034 + 0.001 avg prob of [ William A Seiter] 0.1416582465171814\n",
      "loss 1.493 = 1.452 + 0.039 + 0.001 avg prob of [ William A Seiter] 0.2384357452392578\n",
      "loss 1.008 = 0.945 + 0.062 + 0.001 avg prob of [ William A Seiter] 0.3924591541290283\n",
      "loss 0.657 = 0.614 + 0.042 + 0.001 avg prob of [ William A Seiter] 0.5460647344589233\n",
      "loss 0.198 = 0.156 + 0.041 + 0.001 avg prob of [ William A Seiter] 0.8683648109436035\n",
      "loss 0.051 = 0.029 + 0.02 + 0.001 avg prob of [ William A Seiter] 0.9710459113121033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:18:59,375 - easyeditor.editors.editor - INFO - 48 editing: What is the director of Gangland Odyssey? -> William A Seiter  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'What is the director of Gangland Odyssey?', 'target_new': 'William A Seiter', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gangland Odyssey'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:18:59,375 - easyeditor.editors.editor - INFO - 48 editing: What is the director of Gangland Odyssey? -> William A Seiter  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'What is the director of Gangland Odyssey?', 'target_new': 'William A Seiter', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gangland Odyssey'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:18:59 - INFO - easyeditor.editors.editor -   48 editing: What is the director of Gangland Odyssey? -> William A Seiter  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'What is the director of Gangland Odyssey?', 'target_new': 'William A Seiter', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Gangland Odyssey'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      " 98%|█████████▊| 49/50 [03:42<00:04,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.038 = 0.015 + 0.022 + 0.001 avg prob of [ William A Seiter] 0.9855492115020752\n",
      "Delta norm: 10.7578125\n",
      "Change in target norm: 2.689453125 to 11.171875 => 8.484375\n",
      "Division Factor: 3.49609375\n",
      "Right vector norm: 3.076171875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Executing ROME algorithm for the update: [On which instrument(s) was Ariadne musica created to be played on?] -> [ harpsichord]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Ariadne musica\n",
      "Left vector shape: torch.Size([14336])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: On which instrument(s) was Ariadne musica created to be played on? harpsich | Token:  musica\n",
      "Rewrite layer is 5\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 2.676 = 2.676 + 0.0 + 0.0 avg prob of [ harpsichord] 0.07230957597494125\n",
      "loss 2.516 = 2.304 + 0.211 + 0.001 avg prob of [ harpsichord] 0.10434618592262268\n",
      "loss 1.591 = 1.521 + 0.068 + 0.001 avg prob of [ harpsichord] 0.22958572208881378\n",
      "loss 0.911 = 0.864 + 0.046 + 0.001 avg prob of [ harpsichord] 0.4283520579338074\n",
      "loss 0.608 = 0.564 + 0.042 + 0.001 avg prob of [ harpsichord] 0.5759037733078003\n",
      "loss 0.296 = 0.27 + 0.025 + 0.001 avg prob of [ harpsichord] 0.7691174745559692\n",
      "loss 0.104 = 0.088 + 0.015 + 0.001 avg prob of [ harpsichord] 0.9169325232505798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:19:02,401 - easyeditor.editors.editor - INFO - 49 editing: On which instrument(s) was Ariadne musica created to be played on? -> harpsichord  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'On which instrument(s) was Ariadne musica created to be played on?', 'target_new': 'harpsichord', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ariadne musica'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "2024-07-31 17:19:02,401 - easyeditor.editors.editor - INFO - 49 editing: On which instrument(s) was Ariadne musica created to be played on? -> harpsichord  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'On which instrument(s) was Ariadne musica created to be played on?', 'target_new': 'harpsichord', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ariadne musica'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "07/31/2024 17:19:02 - INFO - easyeditor.editors.editor -   49 editing: On which instrument(s) was Ariadne musica created to be played on? -> harpsichord  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'On which instrument(s) was Ariadne musica created to be played on?', 'target_new': 'harpsichord', 'ground_truth': '<|endoftext|>', 'portability': {}, 'locality': {}, 'subject': 'Ariadne musica'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "100%|██████████| 50/50 [03:45<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.043 = 0.029 + 0.012 + 0.001 avg prob of [ harpsichord] 0.9716730117797852\n",
      "Delta norm: 11.546875\n",
      "Change in target norm: 2.88671875 to 11.859375 => 8.96875\n",
      "Division Factor: 3.791015625\n",
      "Right vector norm: 3.044921875\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight']\n",
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.3036666666666667}, 'post': {'rewrite_acc': 1.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(metrics, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp_ROME_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m edited_model\n\u001b[0;32m---> 37\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     39\u001b[0m metrics\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "hparams.device = 5\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_ROME_{hparams.model_name}_results.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "metrics # Metrics Summary:  {'pre': {'rewrite_acc': 0.3036666666666667}, 'post': {'rewrite_acc': 1.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 18:35:50,458 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "07/31/2024 18:35:50 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24935b2ed2f1412cb56d4766cc060b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 18:36:00,975 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "07/31/2024 18:36:00 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hparams\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m editor \u001b[38;5;241m=\u001b[39m BaseEditor\u001b[38;5;241m.\u001b[39mfrom_hparams(hparams)\n\u001b[0;32m----> 3\u001b[0m metrics, edited_model, _ \u001b[38;5;241m=\u001b[39m \u001b[43meditor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# rephrase_prompts=paraphrased_questions,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_new\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# portability_inputs=portability_inputs,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# test_generation=True,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(metrics, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp_ROME_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhparams\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_mod_eval.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m edited_model\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/editors/editor.py:164\u001b[0m, in \u001b[0;36mBaseEditor.edit\u001b[0;34m(self, prompts, target_new, ground_truth, rephrase_prompts, locality_inputs, portability_inputs, sequential_edit, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     requests \u001b[38;5;241m=\u001b[39m _prepare_requests(prompts, target_new, ground_truth, rephrase_prompts, locality_inputs, portability_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_requests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequential_edit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/editors/editor.py:273\u001b[0m, in \u001b[0;36mBaseEditor.edit_requests\u001b[0;34m(self, requests, sequential_edit, verbose, test_generation, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m: compute_icl_edit_quality(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m], request, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mdevice, pre_edit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)}\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mcompute_edit_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generation\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m    274\u001b[0m     all_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_file\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_file\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/evaluate/evaluate.py:64\u001b[0m, in \u001b[0;36mcompute_edit_quality\u001b[0;34m(model, model_name, hparams, tok, record, device, eval_metric, test_generation)\u001b[0m\n\u001b[1;32m     62\u001b[0m rewrite_prompts \u001b[38;5;241m=\u001b[39m record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     63\u001b[0m rephrase_prompts \u001b[38;5;241m=\u001b[39m record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrephrase_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrephrase_prompt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m record\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_rewrite_or_rephrase_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrewrite_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m ret[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocality\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     68\u001b[0m ret[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportability\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/evaluate/evaluate.py:133\u001b[0m, in \u001b[0;36mcompute_rewrite_or_rephrase_quality\u001b[0;34m(model, model_name, hparams, tok, prompt, target_new, device, test_rephrase, eval_metric)\u001b[0m\n\u001b[1;32m    131\u001b[0m         acc \u001b[38;5;241m=\u001b[39m test_seq2seq_batch_prediction_acc(model, tok, hparams, prompt, target_new, device)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_prediction_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     ret \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc\n\u001b[1;32m    136\u001b[0m     }\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/data/baixiang/workspace/edit/factuality/code/easyeditor/evaluate/evaluate_utils.py:138\u001b[0m, in \u001b[0;36mtest_prediction_acc\u001b[0;34m(model, tok, hparams, prompts, targets, device, locality, vanilla_generation)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m locality:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(answers[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [answers,]\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43manswers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    139\u001b[0m     res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ans,label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(answers,labels):\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "hparams.device = 0\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=questions,\n",
    "    # rephrase_prompts=paraphrased_questions,\n",
    "    target_new=targets,\n",
    "    subject=subjects,\n",
    "    # portability_inputs=portability_inputs,\n",
    "    summary_metrics=True,\n",
    "    keep_original_weight=True,\n",
    "    # test_generation=True,\n",
    ")\n",
    "\n",
    "json.dump(metrics, open(os.path.join('../results/', f'tmp_ROME_{hparams.model_name}_results_mod_eval.json'), 'w'), indent=4)\n",
    "del edited_model\n",
    "gc.collect()\n",
    "\n",
    "metrics # Metrics Summary:  {'pre': {'rewrite_acc': 0.3036666666666667}, 'post': {'rewrite_acc': 1.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
